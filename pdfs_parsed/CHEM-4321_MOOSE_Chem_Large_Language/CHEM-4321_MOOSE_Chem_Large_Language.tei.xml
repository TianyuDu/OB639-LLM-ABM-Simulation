<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOOSE-CHEM: LARGE LANGUAGE MODELS FOR REDISCOVERING UNSEEN CHEMISTRY SCIENTIFIC HYPOTHESES</title>
				<funder ref="#_aUVRFBq">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_RNmKBFH">
					<orgName type="full">Ministry of Education, Singapore</orgName>
				</funder>
				<funder>
					<orgName type="full">Shanghai Artificial Intelligence Laboratory</orgName>
				</funder>
				<funder>
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
							<email>zonglin.yang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanhao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Xie</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of New</orgName>
								<address>
									<country>South Wales</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">GreenDynamics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
							<email>zhoudongzhan@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MOOSE-CHEM: LARGE LANGUAGE MODELS FOR REDISCOVERING UNSEEN CHEMISTRY SCIENTIFIC HYPOTHESES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FAC8C5299BB2F3F95D58415FDDAAF795</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-11-30T00:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses-which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Discovering new science has long been one of the deepest desires of humanity, which can not only satisfy our curiosity to understand the universe but also contribute largely to the prosperity of human society <ref type="bibr" target="#b7">(Coccia, 2019)</ref>. Recently, there are some breakthroughs indicating that LLMs have the potential to assist scientists in accelerating the discovery process <ref type="bibr">(Luo et al., 2025)</ref>. <ref type="bibr">Yang et al. (2024b)</ref> first find that LLMs can generate novel and valid enough hypotheses evaluated by experts. They focus on the social science domain and make discoveries by developing a multi-agent system, leveraging an assumption that a majority of social science hypotheses can be divided into a research background concept and an inspiration concept. This assumption is largely valid because a social science hypothesis is about how an independent variable can influence another dependent variable <ref type="bibr" target="#b8">(Hair et al., 2007)</ref>. <ref type="bibr" target="#b20">Si et al. (2024)</ref> further validate this finding by employing a large group of scientists to evaluate LLMs' generated hypotheses in the NLP domain and show that LLM can generate more novel but slightly less valid research hypotheses than human researchers. However, it is still unclear LLMs' scientific discovery ability in natural science such as the chemistry domain. <ref type="bibr" target="#b21">Sprueill et al. (2023;</ref><ref type="bibr">2024)</ref> adopt LLMs to conduct a search process for catalyst discovery. However, their method is limited in the catalyst discovery domain, and their evaluation relies on whether LLMs can rediscover existing commercially used catalysts, potentially influenced by a data contamination problem. As a result, it is still unclear how good LLMs are for chemistry scientific discovery.</p><p>In this paper, we investigate this central research question: Can LLMs automatically discover novel and valid chemistry research hypotheses (even at the Nature level) given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question? With extensive discussions with chemistry experts, we find that the assumption used in social science, that a hypothesis can be divided into background and inspiration, can also apply to a majority of chemistry hypotheses. It is not too surprising, since cognitive science research has shown that creative ideas often result from the cohesive association of two seemingly unrelated pieces of knowledge <ref type="bibr" target="#b9">(Koestler, 1964;</ref><ref type="bibr" target="#b1">Benedek et al., 2012;</ref><ref type="bibr" target="#b11">Lee &amp; Chung, 2024)</ref>. A main difference is that chemistry might need more than one inspiration (e.g., adding several components to compose a novel chemistry system). With this key insight, we break the seemingly impossible-to-solve central question into three smaller, more practical, and executable fundamental questions that, when summed up, should be very close to a set of sufficient conditions for the central question. Specifically, the smaller questions are (1) whether LLM can identify inspiration papers that have the potential to help with the given research question; (2) given only known knowledge (from background and inspirations), whether LLMs can infer unknown knowledge that is highly likely to be valid; and (3) whether LLM can identify good hypotheses and rank them higher.</p><p>To investigate these three questions, we build a benchmark consisting of 51 chemistry papers annotated by chemistry PhD students, breaking every paper into a background, several inspirations, and a hypothesis. The goal is to rediscover the hypothesis with only the background by using LLMs trained with data up to December 2023. The papers are all published in Nature, Science, or a similar level in 2024, and they are only made public on the internet in 2024. The benchmark is designed to be similar to the Mathematical Olympiad Competition <ref type="bibr" target="#b26">(Trinh et al., 2024)</ref>, to provide several dozens of very difficult and meaningful questions to solve. Along with the benchmark, we propose a ranking task for scientific discovery (along with evaluation criteria), which has been largely overlooked in previous works <ref type="bibr">(Yang et al., 2024a;</ref><ref type="bibr">Wang et al., 2024b)</ref>. Ranking is important because although AI systems can generate a large number of hypotheses in a relatively short time, verifying them one by one requires a lot of experimental costs.</p><p>Motivated by this breakup into three smaller questions, we design a multi-agent framework named MOOSE-CHEM for chemistry scientific discovery. It in general includes three stages: (1) searching through chemistry literature to find inspiration papers, (2) leveraging the inspirations to propose hypotheses for the background research question, and (3) identifying high-quality hypotheses to give them a higher rank. Compared with <ref type="bibr">Yang et al. (2024b)</ref>'s method in social science that assumes a similar separation between background and inspiration for hypothesis formulation, MOOSE-CHEM adopts an evolutionary algorithm to foster a broader diversity of approaches in using inspiration for background, thereby capitalizing on the benefits derived from varied mutations. In addition, MOOSE-CHEM also adopts a multi-step design to collect more than one inspirations for chemistry discovery. Finally, it uses an efficient ranking method for better reference for scientists.</p><p>We design experiments with the benchmark to test the three fundamental questions and find that LLMs are highly capable. We also test MOOSE-CHEM with the benchmark, mimicking the setting to run it in the wild by only giving a background and a corpus of up to 3000 chemistry papers to select inspiration. Even in this challenging setting, MOOSE-CHEM can still rediscover many hypotheses with very high similarity with the ground truth ones, covering the main innovations.</p><p>Overall, the contributions of this paper are:</p><p>• We provide the first mathematical derivation on how to decompose the seemingly impossible-to-solve question P (hypothesis|research background) into many executable and practical smaller steps. This decomposition make P (hypothesis|research background) possible to be practical.</p><p>• We develop a scientific discovery framework directly based on the mathematical derivation. Different from previous works, we propose an evolutionary algorithm-based method to better associate background and inspiration, multi-step inspiration retrieval and composition, and an efficient ranking method. In addition, the framework can be applied to chemistry and material science, which are not covered by previous methods.</p><p>• We construct a benchmark by three chemistry PhD students, consisting of 51 chemistry papers published on Nature, Science, or a similar level, decomposing each paper into the research background, inspirations, and hypothesis.</p><p>• We propose an assumption, grounded in preliminary experiments, that LLMs may already possess numerous knowledge pairs capable of being associated to create novel knowledge-even when scientists have not previously recognized any relationship between them.</p><p>• For the first time, we show that an LLM-based framework can largely rediscover the main innovations of many chemistry hypotheses that have been published in Nature and Science. The rediscovery is not because of data contamination, because we have controlled the date of the training corpus of the LLM and the online date of the chemistry papers.  <ref type="bibr">(2024)</ref> focus on subsequent steps for scientific discovery, mainly developing and conducting experiments. <ref type="bibr" target="#b21">Sprueill et al. (2023;</ref><ref type="bibr">2024)</ref> focus on catalyst discovery, but their evaluation relies on whether can rediscover existing commercially used catalysts, which might cause data contamination problem. <ref type="bibr" target="#b10">Kumar et al. (2024)</ref> compare different LLMs on scientific discovery in different disciplines. <ref type="bibr" target="#b27">Tshitoyan et al. (2019)</ref> show that word embedding obtained from large-scale chemistry literature can recommend materials years before their discovery. <ref type="bibr" target="#b31">Xie et al. (2024)</ref> predict emerging thermoelectric materials by summarizing the sentiment in the existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BENCHMARK CONSTRUCTION</head><p>The goal of the benchmark, named TOMATO-Chem, is two-fold. Firstly, it is used to analyze LLM's ability in terms of the three smaller questions. Secondly, it serves as a challenge to rediscover naturelevel chemistry hypotheses with only a research background. The setting of the challenge is very similar to a real copilot setting, where scientists tell the copilot about the specific research question they are interested in, and optionally a small survey consisting of several paragraphs summarizing the existing best-performing methods for the research question.</p><p>To achieve the goals, we split each collected paper into the following components: &lt;background question, background question (strict), background survey, background survey (strict), one to three inspiration paper titles and their reason to serve as an inspiration, research hypothesis, experiments, reasoning process, summarization of inspirations&gt;. Every component is described by text.</p><p>The reason we add a strict version for background question and background survey is that many hypotheses are making relatively minor modifications based on existing methods covered by the survey, and the question can be very insightful to provide a hint on the general direction of the hypothesis. In practice, these situations are entirely possible, especially when the scientist users can provide a more comprehensive survey on existing methods, or contain deep insights in their question. Here, we also keep the strict version to make the task more challenging and encourage developing methods to better assist scientists even when they are also new to their research topic.</p><p>The reasoning process indicates the relation between the components of background, inspirations, and hypothesis. For example, the reasoning process can be "background + inspiration 1 + inspiration 2 = hypothesis", or "background + inspiration 1/inspiration 2 + inspiration 3 = hypothesis".</p><p>The benchmark consists of 51 chemistry and material science papers and is constructed by multiple chemistry PhD students. We only select those papers published on top chemistry venues and be public on the internet after January 2024. After constructing, the experts check again on (1) whether the identification of the inspirations is correct and whether more inspirations are needed; (2) whether the background does not contain any information in inspirations or hypothesis; and (3) whether the background and the identified inspirations can roughly logically lead to the hypothesis. The complete instruction on the check process is shown in § A.3.   <ref type="table" target="#tab_1">1</ref>, such as polymer material and organic material. Around 13 collected benchmark papers are inside the material science domain. Beyond them, more papers have intersections with material science. In this paper, we target both chemistry and material science, but for simplicity, we only refer to them as chemistry in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FUNDAMENTAL ASSUMPTION AND FOLLOWING DECOMPOSITION</head><p>We propose an assumption that a majority of chemistry hypotheses can originate from a research background and several inspirations. This assumption is not only supported by many chemistry researchers whom we have extensive discussions with but also by the cognitive science finding that "creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge" <ref type="bibr" target="#b9">(Koestler, 1964;</ref><ref type="bibr" target="#b1">Benedek et al., 2012;</ref><ref type="bibr" target="#b11">Lee &amp; Chung, 2024)</ref>. We design our method based on this fundamental assumption.</p><p>Denoting background knowledge as b, inspiration knowledge as i, and hypothesis as h, we translate this assumption as:</p><formula xml:id="formula_0">h = f (b, i 1 , . . . , i k ) (1)</formula><p>Here, k ∈ Z represents the number of inspirations needed for a particular h. Typically in chemistry, k ∈ [1, 3]. In other words, given existing knowledge in the background, a majority of chemistry research is about searching knowledge that previously not known to be related to the background but in fact can assist the background, then associate the background knowledge and the searched knowledge in a reasonable way to compose a hypothesis.</p><p>Based on this assumption, we can transform the seemingly impossible-to-solve P (h | b) into an equivalent form, where each step in the equivalent form is practical and executable.</p><formula xml:id="formula_1">P (h | b) ≈ k j=1 P (i j | b, h j-1 , I) • P (h j | b, i j , h j-1 ), where h 0 = ∅ (2)</formula><p>Here, I denotes the full (chemistry) literature, representing the full inspiration space to search for every single i. The full proof along with detailed analyses is shown in § A.1, which is the core of this paper (and therefore highly recommend to take a read). Equation 2 is meaningful in that by decomposing P (h | b) into more practical and executable smaller questions, the seemingly impossible-to-solve P (h | b) itself becomes practical. We analyze how P (i j | b, h j-1 , I) and P (h j | b, i j , h j-1 ) are practical and executable by LLMs in § 5.1 and § 5.2 correspondingly. Now we have clarified the steps to obtain h from b. However, it still might not be enough helpful in practice, since I can be on a large scale, and the search process might find lots of i, and finally lead to lots of h. Moreover, it is very time-consuming for scientists to conduct experiments to verify every single h. Therefore, it would be very helpful if the generated h could be ranked based on quality. Here, we adopt a straightforward and efficient way for ranking. Specifically, we design a rating function R(h), such that R(h) → R. Denoting the full set of generated h as H, we can obtain</p><formula xml:id="formula_2">P (H ranked ) = P (H, R), where H ranked = {h 1 , h 2 , . . . , h n | R(h i ) ≥ R(h i+1 ) for all i} (3)</formula><p>Supported by Equation 2 and Equation 3, as a result, to model P (h | b), the only three components we need to model are P (i j | b, h j-1 , I), P (h j | b, i j , h j-1 ), and R(h). The implementation details of the three components are illustrated in the remaining subsections in § 4. Analyses of LLM's ability on the three components are provided in § 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">THE FRAMEWORK DEVELOPED BASED ON THE ASSUMPTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">THE GENERAL PICTURE</head><p>Our methodology is developed based on the fundamental assumption discussed in § 4.1. Specifically, we use LLMs to perform P (i j | b, h j-1 , I), P (h j | b, i j , h j-1 ), and R(h), and organize them into a multi-agent LLM-based framework. The input to the framework is only a background question and/or background survey, together with a (large) chemistry literature corpus to search for inspiration. The output of the framework is a list of ranked research hypothesis.</p><p>The framework's design is shown in Figure <ref type="figure" target="#fig_0">1</ref> (overview in Figure <ref type="figure" target="#fig_4">2</ref>). It is a direct implementation of Equation 2 and 3. We develop it as simply as possible, retaining only the necessary parts.</p><p>In the general picture, given a research background b (research question and/or research survey), the framework first performs P (i 1 | b, h 0 = ∅, I) by screening through the literature corpus I to select many papers i, where each of them has the potential to serve as an inspiration. Then the framework performs P (h 1 | b, i 1 , h 0 = ∅), associating b and each i together to compose h. Then, it ranks h by assigning an evaluation score r on each of h 1 by R(h 1 ). We call these three steps as one round.</p><p>Another round means going through the three steps again, based on the previous round's results.</p><p>Since normally in chemistry, no more than three inspirations are needed for one hypothesis (k ∈ [1, 3]), the default setting for MOOSE-Chem is to perform three rounds for each b. In every other round, the number of i and h can expand exponentially. Here, we adopt beam search to select a fixed size of the top-ranked h to enter the next round. The default beam size is 15.</p><p>4.2.2 DESIGN DETAILS OF P (i j | b, h j-1 , I) AND ITS MOTIVATION</p><p>We use LLMs to conduct a screening process for P (i j | b, h j-1 , I). Specifically, for each inference, we (1) sequentially select a fixed number of papers from I, where the fixed number is called the screening window size (default is 15); (2) set up a prompt consisting of b, the title and abstract of the selected papers from I, and the previous h (if it is not ∅); and (3) instruct the LLM to generate three titles from the input that can best serve as i for b (and optionally previous h), and give reasons.</p><p>In particular, we use LLMs to choose potential inspiration i, but not choose i from citation nor semantic neighbors because i is supposed to be previously not known to be related to b (we have discussed it in § 4.1). If the chosen i is already known to be related to b, then the composed h probably would not be novel. If the chosen i contains similar semantic information with b, then probably it is not necessary to add i at all, since it does not introduce much (any) extra information.</p><p>Our bold assumption here is that advanced LLMs, trained on vast scientific literature, may already recognize novel knowledge pairs unknown to any scientist that can be associated to create novel knowledge. However, this may not be too bold, as <ref type="bibr" target="#b27">Tshitoyan et al. (2019)</ref> showed that unsupervised word embeddings from 3.3 million materials science abstracts could predict functional materials years before their discovery. Here, the functional applications can be seen as b, and the recommended materials can be seen as i, or even directly as h if it is enough similar. It probably indicates that LLMs trained with significantly more literature tokens and parameters might already be able to identify the relation between many knowledge pairs that are unknown to be related by any scientist.</p><p>We analyze this assumption in § 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">DESIGN DETAILS OF</head><formula xml:id="formula_3">P (h j | b, i j , h j-1 ) AND ITS MOTIVATION</formula><p>The retrieved i is expected to be not known to be related to b; therefore, it might be difficult to figure out an effective way to associate b and i together to compose h. Think of the time when backpropagation is about to be invented. Even if we are very familiar with b (multi-layer logistic regression) and have successfully retrieved i (chain rule in mathematics), can we invent backpropagation?</p><p>Our answer is, at least we might need to try multiple times and various ways to leverage the chain rule for multi-layer logistic regression. With this motivation, we develop a simple evolutionary algorithm-based method, shown in the top-right of Figure <ref type="figure" target="#fig_0">1</ref>. We call it "evolutionary unit" (EU).</p><p>Specifically, given b and i, EU will first generate multiple hypothesis "mutations" m, where each m is a unique way to associate b and i together. Then EU further develops each m independently by providing feedback to each m in terms of validness, novelty, clarity, and significance, and then refining them based on the feedback. <ref type="bibr">Yang et al. (2024b)</ref> first propose to provide feedback in terms of validness, novelty, and clarity to refine hypotheses. Here, we add an additional aspect, significance, since significance is an important evaluation criterion in chemistry. We assume the refined hypothesis should be of better quality so that the refined hypothesis is "selected", while the previous hypothesis is "eliminated" by the "environment". Finally EU "recombines" the remaining selected m, leveraging the advantages from every m to propose h to better associate b and i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">DESIGN DETAILS OF R(h) AND ITS MOTIVATION</head><p>We adopt a simple and efficient way for R(h), which is to prompt an LLM to output evaluation scores for an input h in terms of validness, novelty, significance, and potential. Validness and novelty are two fundamental requirements for such an inductive reasoning process as scientific discovery <ref type="bibr">(Yang et al., 2024a;</ref><ref type="bibr">b)</ref>. Significance is added because it is important for chemistry. We additionally add potential, because the generated h are about to be further developed by scientists, so we might want to pick those h that not only are currently in high quality but also have good potential to be further developed. We did not design R(h) in a more complicated way, since there are lots of h to rank, and we might want to save more inference time. <ref type="bibr">Yang et al. (2024b)</ref> use the scores as automatic evaluation for generated social science hypotheses and have shown a high consistency score between automatic evaluation and expert evaluation. However, in the chemistry domain, LLMs might not be reliable enough to directly evaluate the generated h <ref type="bibr" target="#b23">(Sprueill et al., 2024)</ref>. But it might still be able to provide a preliminary quality identifier to h: the ranking of the average score between the four aspects of an h determines whether it will enter the  </p><formula xml:id="formula_4">P (i j | b, h j-1 , I), (2) P (h j | b, i j , h j-1</formula><p>), and (3) R(h)? All experiments are performed by GPT-4o (its training data is up to October 2023).</p><p>5.1 HOW WELL CAN LLMS PERFORM P (i j | b, h j-1 , I)?</p><p>Here, we investigate the question (denoted as Q1): "whether LLM can identify inspiration papers which are unknown to be able to associate with the background (or at least unknown to associate in a certain way) but in fact can associate with the background to create novel knowledge?".</p><p>We first find 3000 most cited chemistry papers published in Nature, and construct a series of I in size of 150, 300, 1000, and 3000. I is constructed by first adding the ground truth inspiration papers (around 120), then randomly selecting the remaining papers from the 3000 papers, and finally randomizing the order of all the collected papers. Only title and abstract are needed for each paper in I. The default setting is that each inference of LLMs will screen 15 papers from I, and generate three titles that LLMs think can best assist b (and/or previous h). Screening through I for one round, only 20% of I will be selected. Screening another round will only leave 4%, and so on.</p><p>We use Hit Ratio as the evaluation metric, which is calculated by the number of selected ground truth inspiration papers divided by the number of all ground truth inspiration papers. All the Hit Ratio numbers shown in the tables are averaged across the 51 papers in the benchmark.</p><p>Table <ref type="table" target="#tab_4">3</ref> shows the main experiment results. The Hit Ratio is surprisingly high: More than 75% of the ground truth inspirations are covered by even only the 4% chosen papers from the chemistry literature corpus. It seems that LLMs are quite capable of finding inspiration papers that are unknown to be able to associate with the background but in fact, can associate with the background to create novel knowledge. It means our bold assumption in § 4.2.2 that "the most advanced LLMs might already know lots of knowledge pairs that are able to associate to create novel knowledge, where the knowledge pairs are not known by any scientist to be related" is possible to be true.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows the ablation study in terms of screen window size. It seems that a smaller window size can lead to better performance: a screen window size of 60 to keep 3 for one round will select 5% of the corpus, and the Hit Ratio is 71.6%; while a screen window size of 15 to keep 3 for two rounds will select only 4% of the corpus, but the Hit Ratio is as high as 83.7%.  For each screen window of 15 papers, 3 papers are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">points</head><p>Generated hypothesis covers three key points (or covers all the key points) and leverage them similarly as in the groundtruth hypothesis; Extra key points do not have apparent flaws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">points</head><p>Generated hypothesis covers three key points (or covers all the key points) and leverage them similarly as in the groundtruth hypothesis; Extra key points have apparent flaws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">points</head><p>Generated hypothesis covers two key points and leverage them similarly as in the groundtruth hypothesis, but does not cover more or all key points 2 points Generated hypothesis covers one key point and leverage it similarly as in the groundtruth hypothesis, but does not cover more or all key points 1 point Generated hypothesis covers at least one key point, but is used differently as in the groundtruth hypothesis 0 point Generated hypothesis does not cover any key point Table <ref type="table">6</ref>: Description of the Matched Score.</p><p>Table <ref type="table" target="#tab_7">5</ref> compares LLMs in different scales on inspiration retrieval ability. The results indicate that LLMs obtain the emergent ability for inspiration retrieval since a rather small parameter size, but then quickly plateau. § A.9 discusses research background options' influence on inspiration retrieval.</p><p>5.2 HOW WELL CAN LLMS PERFORM P (h j | b, i j , h j-1 )?</p><p>Here, we investigate the question (denoted as Q2): "Given only known knowledge, whether LLM can reason to unknown knowledge that has high probability to be valid?".</p><p>The first challenge to answer Q2 is the evaluation method: The benchmark covers a large range of chemistry topics, and chemistry is a very complex discipline that a slight change of research topic would make a chemist unable to provide a reliable enough evaluation. In fact, a chemistry researcher might not be able to provide a reliable enough evaluation even if the hypothesis is in his domain.</p><p>Therefore, we adopt a reference-based evaluation method called "Matched Score" (MS). The descriptions are shown in Table <ref type="table">6</ref>. It's on a 6-point Likert scale, roughly containing four stages. Denoting generated hypothesis as gh, and original hypothesis as oh, the four stages are (1</p><formula xml:id="formula_5">) gh ∩ oh = ∅ (0 point); (2) gh ∩ oh ̸ = ∅ (1/2/3 points); (3) gh ⊇ oh (4 points); (4) gh ≈ oh (5 points).</formula><p>We use MOOSE-Chem to investigate Q2. Specifically, we initialize I as only the ground truth inspiration papers and search i for k round, where k is the number of ground truth i needed for each  Table <ref type="table">9</ref>: Relation between the GPT-4o labeled Matched Score and average ranking ratio (↓).</p><p>b. MOOSE-Chem will not retrieve the same i already retrieved in previous rounds, guaranteeing that before generating the final h, the framework has already seen all the ground truth inspirations.</p><p>Table <ref type="table" target="#tab_8">7</ref> shows the results. For each b, the top two h with the highest MS by GPT-4o are selected for expert evaluation (by two chemistry PhD students). It indicates that LLMs are quite capable of associating known knowledge into unknown knowledge that has a high probability to be valid (very close to oh). In addition, providing a survey can assist the new knowledge-discovery process. We discuss the agreement between GPT-4o-based evaluation and expert evaluation in § A.14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">HOW WELL CAN LLMS PERFORM R(h)?</head><p>Here, we investigate Q3: "whether LLMs can select high-quality h to rank them higher?".</p><p>To investigate Q3, we run MOOSE-Chem with every b from the benchmark; |I| = 300, containing all the ground truth i. Every h is given a rating r = R(h), and is ranked based on r. For every generated h, we get the number of ground truth i it leveraged (#Matched i), and evaluate it with a GPT-4o evaluated MS (here MS is -1 means this h has not used any ground truth i).</p><p>Table <ref type="table" target="#tab_9">8</ref> shows the relation between the #Matched i and average ranking ratio (the lower, the better).</p><p>It shows a clear trend that the more ground truth i is leveraged, the better ranking score h can have. It indicates that h with a higher ranking ratio is more likely to be matched with better i.</p><p>Table <ref type="table">9</ref> shows the relation between the GPT-4o evaluated MS and the average ranking ratio. There is a trend that the higher the MS, the better the average rank ratio (when MS ∈ [2,4]). However, the disadvantage of those h without a positive MS is not very significant. It seems that LLMs have a certain ability to rank good h higher. But it is not sure how significant it is, because a part of the reason for these results is that those h generated without ground truth i could be also in high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT AND ABLATION STUDY</head><p>We perform experiments in a setting similar to the copilot in the wild setting. Only background question (strict), background survey (strict), and a chemistry corpus |I| = 300 are provided to the framework. Only the top 4% of I is selected and used to develop h. The evaluation metrics are Top MS and Average MS (the highest/average Matched Score of all generated h from one b), averaging across the benchmark. Experiments are conducted by GPT-4o (training data up to October 2023).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">BASELINES</head><p>MOOSE is a hypothesis discovery framework for the general social science domain. It leverages LLMs to retrieve inspirations and uses self-refine <ref type="bibr">(Madaan et al., 2023)</ref> to improve the validness, novelty, and clarity aspects. The difference is that (1) it does not adopt the mutation and recombination step to better associate background and inspiration; (2) it only retrieves one step of inspiration.</p><p>SciMON is a hypothesis discovery framework for the NLP and biochemical domain. It relies on semantic and citation neighbors to retrieve information to assist the background. As a result, the retrieved information could be very related to the background that might not be able to serve as an inspiration. To make the generated hypothesis more novel, it adopts self-refine to focus on improving  ), Claude-3.5-Sonnet (Table <ref type="table" target="#tab_4">13</ref>), and Gemini-1.5-Pro (Table <ref type="table" target="#tab_13">14</ref>).  the novelty aspect of the generated hypothesis. Here, we implement SciMON with LLM-based inspiration retrieval, the same as MOOSE-Chem. Table <ref type="table" target="#tab_4">3</ref> shows that the recall rate of LLM-based retrieval is 83.7%.</p><p>Qi et al. ( <ref type="formula">2024</ref>) work on hypothesis discovery in the biomedical domain. It retrieves information pertinent to the keywords in the background to generate hypotheses. As a result, the retrieved information might compose of a background survey, but not as inspiration. Self-refine is also adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RESULTS</head><p>Table <ref type="table" target="#tab_10">10</ref> shows the baseline results and the ablation study of MOOSE-Chem. It indicates that both mutation &amp; recombination and the multi-step designs can significantly improve the best-performing h. Mutation &amp; recombination leads to a drop of Average MS compared to the MOOSE baseline; we attribute the reason to that the mutation step forces LLMs to generate h different from previous h mutations from the same b and i, and therefore might generate many h that do not make a lot of sense. The assigned MS to these mutation h is low, and therefore lower down the Average MS.</p><p>To better understand the performance of MOOSE-Chem in this real copilot setting, for each b the top 4 generated h with the highest MS by GPT-4o are evaluated again by two experts in terms of MS. Table <ref type="table" target="#tab_11">11</ref> shows the expert evaluation results. Here, the top MS is the highest MS for each b, out of the 4 expert evaluated h for this b. Note that MS rated as three is already very high. Illustrated in Table <ref type="table">6</ref>, it means the generated h by MOOSE-Chem (that has not seen h) in the real copilot setting covers two main innovations of the chemistry hypothesis, which is published in Nature, Science or a similar level. Some case studies can be seen in § A.16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We investigated this central question: "Can LLMs automatically discover novel and valid chemistry (including material science) research hypotheses (even those which deserve a publication in Nature, Science, or a similar level) given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question?". We proposed a fundamental assumption to break up this seemingly impossible-to-solve central question into three smaller, more practical, and executable fundamental questions. Then, we investigated LLM's ability on each of them.</p><p>To this end, we constructed a benchmark consisting of chemistry and material science papers published and only be public in 2024. We also developed an LLM-based multi-agent framework consisting of three stages reflecting the three smaller fundamental questions. Experiments showed that the framework (runs in a copilot in-the-wild setting, with LLMs with training data up to October 2023) can rediscover many hypotheses with very high similarity with the ground-truth ones, covering the main innovations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 PROOF OF THE RIGOROUS DECOMPOSITION OF P (h | b) BASED ON THE FUNDAMENTAL ASSUMPTION</head><p>We propose an assumption that a majority of chemistry hypotheses can originate from a research background and several inspirations. This assumption is not only supported by many chemistry researchers whom we have extensive discussions with but also by the cognitive science finding that "creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge" <ref type="bibr" target="#b9">(Koestler, 1964;</ref><ref type="bibr" target="#b1">Benedek et al., 2012;</ref><ref type="bibr" target="#b11">Lee &amp; Chung, 2024)</ref>. We design our method based on this fundamental assumption.</p><p>This assumption is reminiscent of Swanson Linking <ref type="bibr" target="#b25">(Swanson, 1986)</ref> in the domain of literaturebased discovery (LBD), also known as the "ABC model", where two concepts A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers. Our assumption differs in that: (1) for a chemistry hypothesis published in a good venue, usually more than one inspirations are needed;</p><p>(2) background and inspiration are not necessarily linked by a path of intermediate papers;</p><p>(3) our assumption is applied to a majority of existing published chemistry hypotheses, while LBD has been considered to only focus on a very specific, narrow type of hypothesis <ref type="bibr">(Wang et al., 2024b)</ref>. It might indicate that a similar proportion of future chemistry hypotheses can also be resulted from linkages of existing literature.</p><p>Denoting background knowledge as b, inspiration knowledge as i, and hypothesis as h, we translate this assumption as:</p><formula xml:id="formula_6">h = f (b, i 1 , . . . , i k )<label>(4)</label></formula><p>Here, k ∈ Z represents the number of inspirations needed for a particular h. Typically in chemistry,</p><formula xml:id="formula_7">k ∈ [1, 3].</formula><p>Equation 4 expresses the idea that, for the majority of chemistry hypotheses (if not all), each hypothesis can be formulated as a composition of background knowledge and additional knowledge elements, which we refer to as inspirations. This functional form reflects a universal pattern in hypothesis formulation: regardless of where the inspiration originates-be it prior literature, serendipitous observation, or discussions with peers-the essential step is to identify correct inspirations and integrate them with existing background knowledge in a meaningful way. In this formulation, the process of first collecting and selecting the appropriate background knowledge and then identifying and integrating suitable inspirations constitutes both a necessary and sufficient condition for generating a valid hypothesis h.</p><p>Here's an example in chemistry:</p><p>• Research Question: How to obtain D 2 gas more efficiently?</p><p>• Background Knowledge: The best performing methods are electrocatalytic methods.</p><p>• Inspiration Knowledge 1: Ruthenium as catalyst</p><p>• Inspiration Knowledge 2: Nitrogen-doped electrode</p><formula xml:id="formula_8">• Inspiration Knowledge 3: D 2 O as chemical solution</formula><p>• Hypothesis: A nitrogen-doped ruthenium (Ru) electrode can effectively catalyze the reductive deuteration of (hetero)arenes in the presence of D 2 O in an electrocatalytic method, leading to efficient D 2 gas production.</p><p>Here's an example in AI:</p><p>• Research Question: How can we automatically update the parameters of a multi-layer logistic regression model using data?</p><p>• Background Knowledge: Multi-layer logistic regression</p><p>• Inspiration Knowledge: The chain rule from calculus</p><p>• Hypothesis: Backpropagation Here's another example in AI:</p><p>• Research Question: How can we improve reasoning performance in language models? • Background Knowledge: Chain-of-Thought prompting • Inspiration Knowledge: Majority voting over multiple reasoning paths • Hypothesis: Self-consistency decoding Here, "background knowledge" and "inspiration knowledge" as illustrated in the example above, can be understood as specific, well-defined knowledge pieces. In practice, however, knowledge retrieval is rarely so clean. Instead of isolating a single knowledge unit, we often retrieve a noisy cluster of information that contains the desired piece along with extraneous content. For instance, when retrieving a paper that includes a relevant inspiration, the paper will inevitably also contain unrelated information that may not be useful for the current research question. Conversely, a single clean inspiration i may be embedded across multiple papers in the literature. This redundancy is beneficial-it increases the likelihood of retrieving i even when searching imperfectly.</p><p>In other words, given existing knowledge in the background, a majority of chemistry research is about searching knowledge that previously not known to be related to the background but in fact can assist the background, then associate the background knowledge and the searched knowledge in a reasonable way to compose a hypothesis. Crucially, the inspiration should not be previously known to be related to the background-at least not in a way that has already been used to formulate hypotheses. Otherwise, the resulting hypothesis would lack novelty. This requirement positions the inspiration retrieval task as an inherently out-of-distribution (OOD) problem, where the goal is to surface connections that lie outside established knowledge associations.</p><p>Our goal is to transform the seemingly impossible-to-solve P (h | b) into an equivalent form, where each step in the equivalent form is practical and executable. Denoting the full inspiration knowledge space as I, such that P (I) = 1. Then a straightforward way of decomposing P (h | b) is by the chain rule based on Equation <ref type="formula" target="#formula_6">4</ref>: P (b,i1,...,i k ) • P (b,i1,...,i k )•P (I) P (b,i1,...,i k-1 )•P (I) • . . . • P (b,i1)•P (I)</p><formula xml:id="formula_9">P (h | b) = P (h, i 1 , . . . , i k | b)<label>(5)</label></formula><formula xml:id="formula_10">P (b)•P (I) if k &gt; 1 (6) = P (h | b, i 1 ) • P (i 1 | b, I) if k = 1 P (h | b, i 1 , . . . , i k ) • k j=2 P (i j | b, i 1 , . . . , i j-1 , I) • P (i 1 | b, I) if k &gt; 1 (7)</formula><p>Here, P (h, i 1 , . . . , i k | b) can be interpreted as the joint distribution over h and its associated inspirations conditioned on background knowledge b. Equation 5 holds because Equation 4 asserts that {i 1 , . . . , i k } constitutes a necessary and sufficient set of additional knowledge required to compose h. I denotes the full inspiration space-that is, the set of all possible knowledge pieces that could serve as an inspiration for generating a new hypothesis. This space includes not only all existing chemistry knowledge but also potentially relevant knowledge from other disciplines that could be leveraged in formulating novel chemistry hypotheses. However, computing over the full space I is computationally infeasible. To make the problem tractable, we approximate I with a large but manageable subset Î, consisting of approximately 3,000 top cited chemistry papers from the existing chemistry literature.</p><p>Equation 7 describes the process of P (h | b) from a knowledge-searching perspective. However, the terms P (h | b, i 1 , . . . , i k ) and P (i j | b, i 1 , . . . , i j-1 , I) may not fully capture how chemistry researchers actually discover new inspirations in practice. One key reason is that researchers typically reason in an incremental fashion, composing hypotheses by integrating one or two knowledge components at a time. It is cognitively and practically difficult to evaluate or integrate all candidate inspirations simultaneously. Instead, researchers iteratively assess partial combinations-gradually building toward a complete hypothesis.</p><p>To mimic how chemistry researchers conduct research and make it more practicable, we break P (h | b, i 1 , . . . , i k ) into a series of recursive smaller steps as</p><formula xml:id="formula_11">P (h k | b, i 1 , . . . , i k ) ≈ P (h k | b, f (b, i 1 , . . . , i k-1 ), i k ) if k &gt; 1 (8) = P (h k | b, h k-1 , i k ) if k &gt; 1 (9)</formula><p>Similarly, we can break P (i j+1 | b, i 1 , . . . , i j , I) as</p><formula xml:id="formula_12">P (i k+1 | b, i 1 , . . . , i k , I) ≈ P (i k+1 | b, f (b, i 1 , . . . , i k ), I) if k &gt; 1 (10) = P (i k+1 | b, h k , I) if k &gt; 1 (11)</formula><p>As a result, to achieve the final h k , we need to obtain {h 1 , . . . , h k-1 } first (if k &gt; 1). In addition, seeing h as a "state", and i as an "action", obtaining h and i through P (h k | b, h k-1 , i k ) and P (i k+1 | b, h k , I) correspondingly indicates a Markov property: (1) a new h only depends on b, its previous h, and the current i; and (2) an i only depends on b, I, and the previous h.</p><p>For brevity, we visualize this as:</p><formula xml:id="formula_13">b i1 -→ h 1 i2 -→ h 2 ••• -→ h k-1 i k -→ h k = h,</formula><p>where each transition is still conditioned on the background knowledge b, though b is omitted from the notation to emphasize the Markov structure of the progression.</p><p>Building on this visualization, we formalize the formation of a hypothesis h (specifically, h = h k ) as a constructive process that sequentially integrates a set of inspirations {i 1 , . . . , i k } into intermediate hypothesis states {h 1 , . . . , h k }.</p><p>Formally, the conditional probability P (h | b) can be expressed as a marginal over all valid sequences of inspirations that can generate h:</p><formula xml:id="formula_14">P (h | b) = π∈Π k P i π(1) , . . . , i π(k) , h 1 , . . . , h k | b ,<label>(12)</label></formula><p>where Π k denotes the set of all permutations of {1, . . . , k} applied to the inspirations {i 1 , . . . , i k } such that the resulting composition yields the final hypothesis h k = h, under the assumption in Equation <ref type="formula" target="#formula_6">4</ref>, which specifies that h is fully determined by b and {i 1 , . . . , i k }. In this context, h j represents the intermediate hypothesis state obtained after integrating i π(j) at step j.</p><p>The degree to which the order of inspirations {i 1 , . . . , i k } affects hypothesis formulation can vary across disciplines. In empirical sciences such as chemistry, the contributions of individual inspirations are largely interchangeable, and their order of integration has limited impact on the final hypothesis, resulting in a large |Π k |. Conversely, in disciplines like mathematics, where hypotheses (e.g., theorems) often require constructing a specific sequence of lemmas and prior results, the ordering of inspirations is more constrained and may follow a near-deterministic path.</p><p>Considering this variance depending on the disciplines, to make this problem tractable and easier to understand, we adopt a fixed constructive path assumption, i.e., |Π k | = 1, selecting a representative order of inspirations {i 1 , . . . , i k }.</p><p>Under this inspiration-fixed-order assumption, where hypothesis h is constructed through a specific sequence of inspirations {i 1 , . . . , i k }, we have:</p><formula xml:id="formula_15">P (h | b) = P (i 1 , . . . , i k , h 1 , . . . , h k | b),<label>(13)</label></formula><p>with h j denoting the intermediate hypothesis state after incorporating i j , and h k = h.</p><p>Therefore, if with this inspiration-fixed-order assumption for simplicity, if k &gt; 1,</p><formula xml:id="formula_16">P (h | b) = P (i 1 , . . . , i k , h 1 , . . . , h k | b) (14) = P (i 1 , h 1 | b) • P (i 2 , h 2 | b, i 1 , h 1 ) • . . . • P (i k , h k | b, i 1 , . . . , i k-1 , h 1 , . . . , h k-1 ) (15) ≈ P (i 1 , h 1 | b) • P (i 2 , h 2 | b, h 1 ) • . . . • P (i k , h k | b, h k-1 ) (16) = P (b, i 1 , I) P (b, I) • P (b, i 1 , h 1 ) P (b, i 1 ) • . . . • P (b, i k , h k-1 , I) P (b, h k-1 , I) • P (b, i k , h k-1 , h k ) P (b, i k , h k-1 ) (17) = P (i 1 | b, I) • P (h 1 | b, i 1 ) • k-1 j=1 P (i j+1 | b, h j , I) • P (h j+1 | b, i j+1 , h j ) (18) = k j=1 P (i j | b, h j-1 , I) • P (h j | b, i j , h j-1 ), where h 0 = ∅ (19)</formula><p>Although starting from k &gt; 1, Derivation 19 covers the situation when k = 1 in Equation <ref type="formula">7</ref>.</p><p>Therefore, in sum, we break up the seemingly impossible question P (h | b) into many practical and executable smaller questions as:</p><formula xml:id="formula_17">P (h | b) ≈ k j=1 P (i j | b, h j-1 , I) • P (h j | b, i j , h j-1 ), where h 0 = ∅ and k &gt;= 1 (20)</formula><p>Of course, without the inspiration-fixed-order assumption, a more complete derivation of P (h | b) involves marginalizing over all valid permutations in Π k :</p><formula xml:id="formula_18">P (h | b) = π∈Π k P (i π(1) , . . . , i π(k) , h 1 , . . . , h k | b) (21) ≈ π∈Π k k j=1 P (i π(j) | b, h (π) j-1 , I) • P (h (π) j | b, i π(j) , h (π) j-1 ), (<label>22</label></formula><formula xml:id="formula_19">)</formula><p>where h</p><p>(π) 0</p><p>= ∅, and k &gt;= 1. Here, h (π) j denotes the intermediate hypothesis state at step j in the permutation π, which results from sequentially incorporating inspirations in the order {i π(1) , . . . , i π(j) }. Figure <ref type="figure" target="#fig_4">2</ref> shows the input and output overview of the MOOSE-Chem framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MOOSE-CHEM OVERVIEW I/O FIGURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 THE FULL INSTRUCTION FOR BENCHMARK CHECKING</head><p>Please help us check again before finalizing the decomposition of each paper in the benchmark:</p><p>1. Whether the background question is correct.</p><p>2. Background survey shouldn't contain any information/method in inspiration or hypothesis (except if this information/method has been used for this particular background question before). It is encouraged to include the most similar existing method to the proposed method. For example, the proposal is to change BaCl2 to BaSO4. It is encouraged to include BaCl2 in the survey, but SO4 must not be included in the survey (since SO4 belongs to the inspiration).</p><p>3. Background question cannot contain any information in inspiration or hypothesis as well: It should be a little bit general question, instead of a specific question asking about how the inspiration can be leveraged to help with the question. It also shouldn't be too general that we can't understand which specific research domain it works on.</p><p>3. Whether the identification of inspirations really the main inspirations for this paper, and whether we need more main inspiration(s).</p><p>4. Whether the main hypothesis is correct and covers the main key points. 5. Whether the background survey + background question + identified inspirations can logically lead to the hypothesis (if not, we might need to identify more inspirations).</p><p>Thank you for the efforts! Your contribution is indispensable for the success of this research. Please let me know if you have any questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 PROMPT TO OBTAIN R(h)</head><p>You are known as a diligent and harsh reviewer in Chemistry and Material Science that will spend much time to find flaws when reviewing and therefore usually gives a relatively much lower score than other reviewers. But when you meet with a hypothesis you truly appreciate, you don't mind to give it good scores. Given a not yet peer reviewed research hypothesis in Chemistry or Material Science domain, try to evaluate the research hypothesis from four research aspects and give score according to evaluation guidelines provided below. All four aspects should be evaluated in a 5 point scale.</p><p>Aspect 1: Validness. 5 points: The hypothesis is a logical next step from current research, strongly supported by theory, perhaps with some indirect experimental evidence or highly predictive computational results. The experimental verification seems straightforward with a high probability of confirming the hypothesis; 4 points: Here, the hypothesis is well-rooted in existing theory with some preliminary data or computational models supporting it. It extends known science into new but logically consistent areas, where experiments are feasible with current technology, and there's a reasonable expectation of positive results; 3 points: This hypothesis is within the realm of theoretical possibility but stretches the boundaries of what's known. It might combine existing knowledge in very novel ways or predict outcomes for which there's no direct evidence yet. There's a conceptual framework for testing, but success is uncertain; 2 points: While the hypothesis might be grounded in some theoretical aspects, it significantly deviates from current understanding or requires conditions or materials that are currently impossible or highly improbable to achieve or synthesize; 1 point: The hypothesis proposes concepts or outcomes that are not only unsupported by current theory but also contradict well-established principles or data. There's no clear path to experimental testing due to fundamental theoretical or practical barriers.</p><p>Aspect 2: Novelty. 5 points: This level of novelty could fundamentally alter our understanding of chemistry or create entirely new fields. It often involves predictions or discoveries that, if proven, would require a significant overhaul of existing chemical theories; 4 points: The hypothesis significantly departs from established norms, potentially redefining how certain chemical phenomena are understood or applied. It might involve entirely new materials or theoretical frameworks; 3 points: This level involves a hypothesis that could potentially lead to new insights or applications. It might challenge minor aspects of current theories or introduce new methodologies or materials; 2 points: The hypothesis introduces a new angle or method within an established framework. It might involve known compounds or reactions but in contexts or combinations not previously explored; 1 point: The hypothesis involves minor tweaks or applications of well-known principles or techniques. It might slightly extend existing knowledge but doesn't introduce fundamentally new concepts.</p><p>Aspect 3: Significance. 5 points: This hypothesis could fundamentally change one or more branches of chemistry. It might introduce entirely new principles, theories, or methodologies that redefine the boundaries of chemical science; 4 points: This hypothesis challenges current understanding or introduces a concept that could lead to substantial changes in how a particular area of chemistry is viewed or applied. It might lead to new technologies or significant theoretical advancements; 3 points: this hypothesis proposes something new or an innovative approach that could lead to noticeable advancements in a specific area of chemistry. It might open new avenues for research or application but doesn't revolutionize the field; 2 points: This hypothesis might offer a small variation or incremental improvement on existing knowledge. It could potentially refine a known concept but doesn't significantly alter the field; 1 point: The hypothesis addresses a very narrow or already well-established aspect of chemistry. It might confirm what is already known without adding much new insight.</p><p>Aspect 4: Potential. 5 points: The hypothesis, while potentially intriguing now, holds the promise of being revolutionary with the addition of a key methodological component. This could introduce entirely new concepts or fields, fundamentally changing our understanding or capabilities in chemistry; 4 points: The hypothesis, though promising, could be transformative with the right methodological enhancement. This enhancement might lead to groundbreaking discoveries or applications, significantly advancing the field; 3 points: The hypothesis, while interesting in its current form, could be significantly elevated with the right methodological addition. This might lead to new insights or applications that go beyond the initial scope; 2 points: The hypothesis currently offers some value but has the potential for more substantial contributions if enhanced with a new methodological approach. This could lead to incremental advancements in understanding or application; 1 point: The hypothesis, as it stands, might be straightforward or well-trodden. Even with methodological enhancements, it's unlikely to significantly expand current knowledge or applications beyond minor improvements.</p><p>The hypothesis is: Please give a response to the initial question on scoring the hypothesis from four aspects. Remember that you are a diligent and harsh reviewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 AUTOMATIC EVALUATION BY CLAUDE AND GEMINI</head><p>To investigate whether the results and corresponding conclusions in the main text are caused by the usage of GPT-4o for automatic evaluation, here we use Claude-3.5-Sonnet and Gemini-1.5-Pro to evaluate all of the results that have been evaluated by GPT-4o.</p><p>Table <ref type="table" target="#tab_12">12</ref> covers the contents in Table <ref type="table" target="#tab_8">7</ref>, but with more results on using Claude-3.5-Sonnet and Gemini-1.5-Pro for automatic evaluation. When using different LLMs for automatic evaluation, the instruction is the same (can be found in § A.12). The robust results indicate again that LLMs are quite capable of associating known knowledge into unknown knowledge that has a high probability to be valid (very close to oh).</p><p>Table <ref type="table" target="#tab_4">13</ref> and Table <ref type="table" target="#tab_13">14</ref> evaluate the same hypotheses with Table <ref type="table" target="#tab_10">10</ref>, but using Claude-3.5-Sonnet and Gemini-1.5-Pro for automatic evaluation correspondingly (instead of GPT-4o). The results indicate the robustness of MOOSE-Chem and its components.  Table <ref type="table" target="#tab_14">15</ref> shows the number of hypotheses receiving high Matched Score from only non-EU branch, only EU branches, and only EU-recombination branch. Here, only non-EU branch can be seen as the hypotheses obtained directly without mutations. The hypotheses are from the same experiment in Table <ref type="table" target="#tab_10">10</ref>.</p><p>The result indicates that about one-third of high-quality hypotheses can be obtained directly without mutations. In addition, the recombination branch contains more high-quality hypotheses than the only non-EU branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 EFFECT OF SIGNIFICANCE FEEDBACK</head><p>Table <ref type="table" target="#tab_15">16</ref> presents an ablation study on the significance feedback. The results with significance feedback are from Table <ref type="table" target="#tab_12">12</ref>.</p><p>The results indicate that not using significance feedback can even lead to a better performance in terms of the Matched Score metric. We attribute this phenomenon to LLM's ability on creativity: when asked to generate significant hypotheses, LLMs tend to be more deviate from the existing information for more possible significance, resulting in a lower matched score. However, we should note that the matched score only measures the matching degree of one given ground truth hypothesis, and it is possible that the more deviated one is more significant.</p><p>A.8 RANKING OF GROUND TRUTH HYPOTHESES Intuitively if we rank the original hypothesis with the generated hypothesis, the original hypothesis may be ranked at the top for most of the time. But is it?</p><p>Table <ref type="table" target="#tab_16">17</ref> shows the result, where we assign each ground truth hypothesis with a reward value R(h) (in terms of validness, novelty, significance, and potential), and calculate its average rank ratio regarding the framework-generated hypotheses.</p><p>Surprisingly, the ground truth hypotheses are not ranked to the top. There are three possible reasons:</p><p>1. LLM does poorly on ranking hypotheses;</p><p>2. The generated hypotheses tend to describe their novelty and significance (although they are prompted to not to), which might influence the judgment;</p><p>3. The generated hypotheses may surpass the original in quality.</p><p>4. The generated hypotheses may sometimes have more details than the ground truth one (since the iterative usage of clarity feedback and refinement).  Table <ref type="table" target="#tab_18">18</ref> shows the ablation study in terms of whether to use strict background (discussed in § 3) or survey or not. It indicates that a survey can largely help with the inspiration retrieval process. Surprisingly, without a strict background, the Hit Ratio goes down a bit. We attribute it to the reason that mentioning information related to the inspiration will discourage retrieving that inspiration, since in the prompt, we ask LLMs to search for inspirations, and the demonstration example indicates that inspirations should not be too similar to the background (to bring in additional information).</p><p>A.10 DISCUSSION ON HALLUCINATION AND SCIENTIFIC DISCOVERY</p><p>In contrast to the traditional understanding that hallucination is purely a bad thing, LLM's scientific discovery ability in fact counts on its hallucination ability to find novel hypotheses: a novel hypothesis would not have been observed by itself, therefore all novel hypotheses come from the class of hallucination.</p><p>In essence, the research development of LLMs for automated scientific hypothesis discovery is to develop how to better leverage LLMs to hallucinate an unseen hypothesis that has more possibility to be valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 OTHER RELATED WORKS</head><p>A.11.1 REASONING Scientific discovery is highly related to reasoning, since it requires a set of very complex reasoning processes to lead to new discovery.</p><p>Inductive reasoning <ref type="bibr">(Yang et al., 2024a)</ref> is the most relevant reasoning type. It is about finding rules or hypotheses from observations. Scientific discovery is naturally an ultimate goal of inductive reasoning.</p><p>Inductive reasoning is a sub-reasoning type of logical reasoning. The other two sub-reasoning types are deductive reasoning <ref type="bibr">(Clark et al., 2020)</ref> and abductive reasoning <ref type="bibr" target="#b2">(Bhagavatula et al., 2020)</ref>. <ref type="bibr">Yang et al. (2023b)</ref> discuss their definitions and differences in detail.</p><p>Another relevant reasoning type is commonsense reasoning <ref type="bibr" target="#b32">(Yang et al., 2020;</ref><ref type="bibr">2023a)</ref>. Scientific discovery can be seen as an opposite task, which is to reason far outside of commonsense, even to discover unknown knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11.2 RETRIEVAL</head><p>The retrieval of inspiration is a retrieval task, and RAG (Lewis et al., 2020) also works on retrieval.</p><p>The main difference is that the current RAG method would most likely retrieve the information that is semantically the most similar to the input information (research background), while here our goal is to retrieve those information that was not known to be related to the input information before, but in fact is related. We assume that LLMs might have the ability to do it.</p><p>A.11.3 SELF CONSISTENCY Self-consistency <ref type="bibr" target="#b30">(Wang et al., 2023;</ref><ref type="bibr">Chen et al., 2023)</ref> might have a similar looking to the "evolutionary unit" (EU), as they all have expansion to several branches, and finally collect these branches into one.</p><p>A key difference is that EU is to explore more diverse options to choose the optimal one, while self-consistency is to find consistent voting between options.</p><p>A.12 PROMPT TO GPT-4O FOR MATCHED SCORE You are helping to evaluate the quality of a proposed research hypothesis in Chemistry by a phd student. The ground truth hypothesis will also be provided to compare. Here, we mainly focus on whether the proposed hypothesis has covered the key points in terms of the methodology in the ground truth hypothesis. You will also be given a summary of the key points in the methodology of the ground truth hypothesis for reference. Please note that for the proposed hypothesis to cover one key point, it is not necessary to explicitly mention the name of the key point, but might also can integrate the key point implicitly in the proposed method. The evaluation criteria is called 'Matched score', which is in a 6-point Likert scale (from 5 to 0). Particularly, 5 points mean that the proposed hypothesis (1) covers all the key points and leverage them similarly as in the methodology of the ground truth hypothesis, and ( <ref type="formula">2</ref>) does not contain any extra key point that has apparent flaws; 4 points mean that the proposed hypothesis (1) covers all the key points (or at least three key points) and leverage them similarly as in the methodology of the ground truth hypothesis, ( <ref type="formula">2</ref>) but also with extra key points that have apparent flaws; 3 points mean that the proposed hypothesis (1) covers at least two key points and leverage them similarly as in the methodology of the ground truth hypothesis, (2) but does not cover all key points in the ground truth hypothesis, (3) might or might not contain extra key points; 2 points mean that the proposed hypothesis (1) covers at least one key point in the methodology of the ground truth hypothesis, and leverage it similarly as in the methodology of ground truth hypothesis, (2) but does not cover all key points in the ground truth hypothesis, and (3) might or might not contain extra key points; 1 point means that the proposed hypothesis (1) covers at least one key point in the methodology of the ground truth hypothesis, (2) but is used differently as in the methodology of ground truth hypothesis, and (3) might or might not contain extra key points; 0 point means that the proposed hypothesis does not cover any key point in the methodology of the ground truth hypothesis at all. Please note that the total number of key points in the ground truth hypothesis might be less than three, so that multiple points can be given. E.g., there's only one key point in the ground truth hypothesis, and the proposed hypothesis covers the one key point, it's possible to give 2 points, 4 points, and 5 points. In this case, we should choose score from 4 points and 5 points, depending on the existence and quality of extra key points. 'Leveraging a key point similarly as in the methodology of the ground truth hypothesis' means that in the proposed hypothesis, the same (or very related) concept (key point) is used in a similar way with a similar goal compared to the ground truth hypothesis (not necessarily for the proposed hypothesis to be exactly the same with the groudtruth hypothesis to be classified as 'similar'). When judging whether an extra key point has apparent flaws, you should use your own knowledge to judge, but rather than to rely on the count number of pieces of extra key point to judge.</p><p>Please evaluate the proposed hypothesis based on the ground truth hypothesis.</p><p>The proposed hypothesis is:</p><p>The ground truth hypothesis is:</p><p>The key points in the ground truth hypothesis are: Please evaluate the proposed hypothesis based on the ground truth hypothesis, and give a score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.13 GENERATED HYPOTHESES WITH LOW MATCHED SCORE ARE NOT NECESSARILY BAD</head><p>MS only measures the similarity between the generated h and the ground truth h. Receiving an MS as 0 or 1 does not mean the generated h is bad. Only real lab experiments can check each h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.14 EVALUATION AGREEMENT BETWEEN EXPERT EVALUATION AND GPT-4O EVALUATION</head><p>Table <ref type="table" target="#tab_19">19</ref> shows the agreement between expert evaluation and automatic evaluation (by GPT-4o) on MS. Hard consistency is assigned to 1 only if the two scores are exactly the same, else is assigned to 0. Soft consistency is assigned to 1 only if the absolute difference between the two scores is less than 2, else is assigned to 0. The results show a medium to high consistency between expert evaluation and automatic evaluation.</p><p>The main reason is that, in practice, the automatic evaluation is usually 1 to 2 points higher than expert evaluation, since GPT-4o can usually find a way to explain how the generated hypothesis is related to the ground truth hypothesis in terms of the main innovations. While this explanation usually is not wrong, the experts might find that compared to the MS given by GPT-4o, the generated hypotheses might not be clear enough to deliver the new innovation information to scientists when using it as a copilot advice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.15 AGREEMENT BETWEEN EXPERT EVALUATION</head><p>Table <ref type="table" target="#tab_20">20</ref> shows the consistency score between experts. It is conducted by having the third chemistry expert check the randomly sampled expert evaluation from the previous two chemistry experts. Hard consistency is assigned to 1 only if the two scores are exactly the same, else is assigned to 0. Soft consistency is assigned to 1 only if the absolute difference between the two scores is less than 2, else is assigned to 0.</p><p>The results show a high consistency between experts, indicating a high reliability of reference-based expert evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.16 CASE STUDIES</head><p>A.16.1 CASE 1 (MS BY EXPERT: 3) Ground truth h <ref type="bibr" target="#b4">(Bu et al., 2024)</ref>: The main hypothesis is that a nitrogen-doped ruthenium (Ru) electrode can effectively catalyze the reductive deuteration of (hetero)arenes in the presence of D 2 O, leading to high deuterium incorporation into the resulting saturated cyclic compounds. The findings validate this hypothesis by demonstrating that this electrocatalytic method is highly efficient, scalable, and versatile, suitable for a wide range of substrates.</p><p>Generated h: A pioneering integrated electrocatalytic system leveraging ruthenium nanoparticles embedded in nitrogen-doped graphene, combined with a dual palladium-coated ion-exchange membrane reactor, will catalyze efficient, scalable, and site-selective reductive deuteration of aromatic hydrocarbons and heteroarenes. Utilizing deuterium sources from both D 2 gas and D 2 O, this system will optimize parameters through real-time machine learning-driven dynamic adjustments. Specific configurations include ruthenium nanoparticle sizes (2-4 nm), nitrogen doping levels (12-14%), precisely engineered palladium membranes (5 micrometers, ensuring 98% deuterium-selective permeability), and advanced cyclic voltammetry protocols (1-5 Hz, -0.5V to -1.5V).</p><p>Expert's analysis: The proposed hypothesis effectively covers two key points from the ground truth hypothesis: the incorporation of ruthenium (Ru) and the use of D 2 O as a deuterium source within the electrocatalytic system. However, the current content does not detail the mechanism by which Ru-D is produced, which is essential for explaining the process of reductive deuteration. Nevertheless, the results are still insightful. The specific level of nitrogen doping, for example, is highly suggestive and warrants further investigation. Overall, the match remains strong in its alignment with the original hypothesis while also presenting opportunities for deeper exploration.</p><p>A.16.2 CASE 2 (MS BY EXPERT: 4) Ground truth h <ref type="bibr">(Wang et al., 2024a)</ref>: By integrating guanidine sulfate (Gdm) 2 SO 4 into a poly vinyl alcohol (PVA) hydrogel and employing directional freezing to create aligned channels, it is possible to achieve a flexible thermogalvanic armor (FTGA) with a Carnot-relative efficiency exceeding 8% while maintaining high mechanical strength. This integration allows for enhanced thermopower and mechanical robustness, exceeding the performance of traditional quasi-solid thermocells.</p><p>Generated h: Leveraging an integrated multi-layer hydrogel-ion assembly, constructed through advanced freeze-casting, salting-out processes, and bioinformatics-driven macromolecular design, will enhance the Carnot-relative efficiency and mechanical robustness of flexible thermogalvanic devices. This approach reimagines the use of poly(vinyl alcohol) hydrogels and incorporates novel ion-specific interactions informed by the Hofmeister series with uniquely potent ions like magnesium and lithium. Precision ion concentrations (0.15 to 0.45 M) are optimized to amplify thermoelectric responses and ionic conductance. Molecular dynamics simulations, employing detailed quantum chemistry models, will validate the enhanced ionic interactions and entropy modulation. Comprehensive characterization through nanoindentation, electron microscopy, and advanced computational platforms will verify crystalline polymer structures and stratified ionic gel configurations. By setting new benchmarks in efficiency and flexibility, this innovation is poised to revolutionize energy solutions in wearable technologies, outperforming current models in both scalability and application feasibility.</p><p>Expert's Analysis:</p><p>The proposed hypothesis effectively covers key points, mirroring the ground truth hypothesis. It incorporates the Hofmeister series by mentioning "ion-specific interactions informed by the Hofmeister series," which aligns with the ground truth's reference to "guanidine sulfate (Gdm) 2 SO 4 " in the hydrogel integration, acknowledging that sulfate ions are a significant component of the Hofmeister series. Additionally, the proposed hypothesis employs "freeze-casting," which reflects the ground truth's "directional freezing" to create structured channels within the hydrogel. This approach enhances both mechanical strength and thermopower-objectives that are consistent with those of the ground truth hypothesis.</p><p>Moreover, the inclusion of salting-out processes in the proposed hypothesis does indeed contribute to improving the mechanical properties of thermoelectric materials. However, the selection of magnesium and lithium as specific examples is problematic. Overall, the proposed hypothesis provides valuable insights and serves as a source of inspiration for further exploration within this domain.</p><p>A.16.3 CASE 3 (MS BY EXPERT: 3) Ground truth h <ref type="bibr" target="#b19">(Shibahara et al., 2024)</ref>: Rh(I)-catalyzed vinylidene formation and enantioselective control via chiral diphosphine ligands can work together to enable a selective [2+2+1] cycloaddition reaction using three different 2π-components. The formation of the vinylidene intermediate ensures reaction selectivity, while the chiral ligands, through their dihedral angles, control the enantiomeric outcome. Together, these factors will result in the highly enantioselective synthesis of chiral 3-methylenecyclopent-1-ene derivatives.</p><p>Generated h: An enantioselective [2+2+1] cycloaddition involving cycloalkenes, acetylenecarboxylates, and terminal alkynes can be achieved using a dynamically tunable Rh(I) pincer complex system, specifically incorporating PNP and PONOP ligands, to control the alkyne-to-vinylidene transformation and enhance selectivity towards chiral 3-methylenecyclopent-1-ene derivatives. By adjusting the P-Rh-P bite angle with novel ligand modifications and utilizing a blended solvent system containing toluene and dichloromethane at controlled temperatures (25°C to 80°C), reaction conditions can be fine-tuned to suppress side reactions. Computational methods (DFT, QTAIM, and NBO) will guide the optimization process, supported by real-time monitoring using FTIR and NMR.</p><p>Exploring dual-ligand strategies, including phosphine/phosphinite combinations, will offer insights into previously unreported mechanistic pathways, ensuring a reproducible and scalable synthesis method applicable to pharmaceuticals and advanced material synthesis.</p><p>Expert's Analysis: The hypothesis generated in Case 2 receives a score of 3, reflecting its accurate identification of the metal and ligand necessary for the catalytic process, as well as its articulation of the mechanism underlying chiral control via ligand geometry. However, the inclusion of mixed solvents, which were not utilized in the actual study, prevents a higher rating. This prediction successfully asserts the necessity of rhodium metal and a phosphine ligand, while also proposing the significance of the chelating angle between the phosphine ligand and rhodium, in alignment with the hypothesis presented in the referenced article. Furthermore, although the DFT calculations mentioned in the hypothesis are not addressed in the article, they represent a promising avenue for future research in organic chemistry.</p><p>A.16.4 CASE 4 (MS BY EXPERT: 3) Ground truth h <ref type="bibr" target="#b24">(Suzuki et al., 2024)</ref>: The hypothesis is that phosphorus ylides, when exposed to photoredox catalysis, can undergo single-electron oxidation to generate radical cations, which engage in radical-polar crossover reactions. These intermediates can then sequentially form C-C and C=C bonds through a formal cycloaddition process, offering a powerful method for constructing six-membered carbocycles from simple substrates.</p><p>Generated h: A sophisticated dual-modal photoredox strategy synergistically integrates phosphorus ylides with a synchronized dual-wavelength excitation (450 nm and 520 nm) system to incite a regioselective three-component cycloaddition. This method exploits both net-neutral radical/polar crossover pathways and metal-free carbyne generation to convert inert C-H and C=P bonds into C-C and C=C bonds efficiently. Implemented within an optimized mixed solvent system (acetone/acetonitrile) under an ambient nitrogen atmosphere, the approach utilizes precise kinetic modulation and catalytic control, with computational validation (employing DFT models) underpinning mechanistic insights and regioselectivity. Experimental verification through in situ spectroscopic techniques (e.g., UV-Vis and NMR) enhances synchronization precision, curtailing side reactions. This methodology pledges substantial advancements in sustainable pharmaceutical synthesis and innovative polymer architecture formation, specifically by improving production efficiency for complex molecules and novel materials.</p><p>Expert's Analysis: The generated hypothesis also merits a score of 3, as it correctly anticipates the use of photocatalysis and highlights the significant influence of solvent on the reaction. However, since dual wavelength catalysis and solvent mixing were not employed in the actual experiment, a higher score is not warranted. Notably, despite the proposed mixed solvents not being used in the study, their composition comprises the two best-performing single solvents from the actual research, thus providing valuable insights that remain relevant to the ongoing investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The MOOSE-Chem framework. It receives b and I as input, and outputs a list of ranked h. The bottom-right legend describes the symbols in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Average MS (GPT-4o) 1 7 17 19 7 0 51 Top MS (GPT-4o) 25 2 19 5 0 0 51</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>=P</head><label></label><figDesc>(h,b,i1)    P (b,i1) • P (b,i1)•P (I)P (b)•P (I) if k = 1 P (h,b,i1,...,i k )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the input and output of the MOOSE-Chem framework.</figDesc><graphic coords="18,248.43,576.02,105.18,104.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Distribution of categories.</figDesc><table><row><cell>Category</cell><cell>Count</cell><cell cols="2">Publication Venue Count</cell></row><row><cell>Polymer Chemistry Organic Chemistry Inorganic Chemistry Analytical Chemistry</cell><cell>21 22 3 5</cell><cell>Nature / Science Nature Subjournals Other Top Journals</cell><cell>27 20 4</cell></row><row><cell>Total</cell><cell>51</cell><cell>Total</cell><cell>51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Distribution of publication venues.Table1and Table2show the statistics of the benchmark in terms of chemistry category and publication venue. Material science is a sub-category of chemistry and can belong to the categories in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Main table for Q1. For each screen window of 15 papers, 3 papers are selected.</figDesc><table><row><cell cols="5">Screen window size Hit Ratio (1 round) Hit Ratio (2 round) Hit Ratio (3 round) Hit Ratio (4 round)</cell></row><row><cell>10</cell><cell>98.0%</cell><cell>88.9%</cell><cell>79.4%</cell><cell>56.5%</cell></row><row><cell>15</cell><cell>96.7%</cell><cell>83.7%</cell><cell>60.8%</cell><cell>NA</cell></row><row><cell>20</cell><cell>91.2%</cell><cell>76.8%</cell><cell>58.8%</cell><cell>NA</cell></row><row><cell>40</cell><cell>88.9%</cell><cell>54.9%</cell><cell>NA</cell><cell>NA</cell></row><row><cell>60</cell><cell>71.6%</cell><cell>53.9%</cell><cell>NA</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation table on screen window size for Q1. The corpus size is 300. For each screen window no matter its size, 3 papers are selected to remain for the next round of screening.</figDesc><table /><note><p><p><p>next round of MOOSE-Chem by beam search. To understand how well LLMs can perform R(h), we analyze "how well LLMs can rank chemistry hypotheses" in § 5.3.</p>5 INVESTIGATION ON FUNDAMENTAL QUESTIONS</p>P (h | b) can be understood as the task to discover high-quality chemistry research hypothesis, given only a background question and/or background survey. Our central question to investigate is how well LLMs can perform P (h | b). Supported by Equation 2 and 3, we break up this main question into three smaller questions: how well can LLMs perform (1)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of Llama series and GPT-4o on inspiration retrieval. The corpus size is 300.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>#Matched i</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell cols="5">Average Rank Ratio NA 0.411 0.474 0.521</cell></row><row><cell>Size</cell><cell>0</cell><cell>302</cell><cell cols="2">2458 4899</cell></row></table><note><p>Main table for Q2. Average/Top MS means the average/highest Matched Score of all generated h from one b. Table 12 is a more complete version of this table including automatic evaluation results by Claude-3.5-Sonnet and Gemini-1.5-Pro.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Relation between the number of matched ground truth i and the average ranking ratio (↓).</figDesc><table><row><cell>Matched Score</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>-1</cell></row><row><cell cols="8">Average Rank Ratio 0.489 0.439 0.488 0.501 0.436 0.501 0.503</cell></row><row><cell>Size</cell><cell>210</cell><cell>36</cell><cell>404</cell><cell>427</cell><cell>29</cell><cell>102</cell><cell>6451</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Experiments and ablation study. The Matched Score (MS) is evaluated by</figDesc><table><row><cell>Method</cell><cell cols="2">Top MS Average MS</cell></row><row><cell>SciMON (Wang et al., 2024b)</cell><cell>2.549</cell><cell>2.281</cell></row><row><cell>MOOSE (Yang et al., 2024a)</cell><cell>2.882</cell><cell>2.464</cell></row><row><cell>Qi et al. (2024)</cell><cell>2.686</cell><cell>2.356</cell></row><row><cell>MOOSE-Chem</cell><cell>4.020</cell><cell>2.564</cell></row><row><cell>w/o multi-step</cell><cell>3.765</cell><cell>2.730</cell></row><row><cell>w/o multi-step &amp; EU</cell><cell>2.863</cell><cell>2.578</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>MOOSE-Chem runs with |I|=300, mimicking the copilot setting. This table shows the statistics of the top Matched Score across the benchmark. The evaluation is done by experts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Main table for Q2. Average/Top MS means the average/highest Matched Score of all generated h from one b. The numbers represent the statistics of Average/Top MS over the benchmark.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Top MS Average MS</cell></row><row><cell></cell><cell>SciMON (Wang et al., 2024b)</cell><cell>3.824</cell><cell>3.529</cell></row><row><cell></cell><cell>MOOSE (Yang et al., 2024a)</cell><cell>3.902</cell><cell>3.559</cell></row><row><cell></cell><cell>Qi et al. (2024)</cell><cell>3.431</cell><cell>3.092</cell></row><row><cell></cell><cell>MOOSE-Chem</cell><cell>4.471</cell><cell>3.697</cell></row><row><cell></cell><cell>w/o multi-step</cell><cell>4.216</cell><cell>3.592</cell></row><row><cell></cell><cell>w/o multi-step &amp; EU</cell><cell>3.941</cell><cell>3.614</cell></row><row><cell>Table 13:</cell><cell>Experiments and ablation study.</cell><cell cols="2">The Matched Score is evaluated by</cell></row><row><cell cols="2">Claude-3.5-Sonnet.</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="2">Top MS Average MS</cell></row><row><cell></cell><cell>SciMON (Wang et al., 2024b)</cell><cell>2.980</cell><cell>2.618</cell></row><row><cell></cell><cell>MOOSE (Yang et al., 2024a)</cell><cell>3.039</cell><cell>2.690</cell></row><row><cell></cell><cell>Qi et al. (2024)</cell><cell>2.216</cell><cell>1.846</cell></row><row><cell></cell><cell>MOOSE-Chem</cell><cell>3.686</cell><cell>2.443</cell></row><row><cell></cell><cell>w/o multi-step</cell><cell>3.588</cell><cell>2.529</cell></row><row><cell></cell><cell>w/o multi-step &amp; EU</cell><cell>2.902</cell><cell>2.631</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Experiments and ablation study. The Matched Score is evaluated by Gemini-1.5-Pro.</figDesc><table><row><cell cols="4">MS threshold only non-EU branch only EU branches only EU-recombination branch</cell></row><row><cell>5</cell><cell>16</cell><cell>46</cell><cell>20</cell></row><row><cell>4</cell><cell>19</cell><cell>54</cell><cell>24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc></figDesc><table><row><cell></cell><cell>4</cell><cell>3</cell><cell>2 1 0</cell></row><row><cell></cell><cell cols="3">w/ significance feedback</cell></row><row><cell cols="4">Average MS 4 19 15 10 3 0</cell></row><row><cell>Top MS</cell><cell cols="3">33 7 10 1 0 0</cell></row><row><cell></cell><cell cols="3">w/o significance feedback</cell></row><row><cell cols="4">Average MS 8 28 11 3 1 0</cell></row><row><cell>Top MS</cell><cell cols="2">34 13 4</cell><cell>0 0 0</cell></row></table><note><p>Number of hypotheses receiving high Matched Score (MS) from only non-EU branch, only EU branches, and only EU-recombination branch. Only the hypotheses with a MS that is higher than the MS threshold are counted.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Effect of significance feedback (evaluated by Claude-3.5-Sonnet).</figDesc><table><row><cell></cell><cell cols="5">Overall Validness Novelty Significance Potential</cell></row><row><cell>Average Rank Ratio</cell><cell>0.65</cell><cell>0.75</cell><cell>0.76</cell><cell>0.73</cell><cell>0.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 :</head><label>17</label><figDesc>Average rank ratio (↓) of the ground truth hypotheses (mixed with generated hypotheses) A.6 MORE ANALYSIS ON EU</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 18 :</head><label>18</label><figDesc>Ablation table on background options for Q1. The corpus size is 300. For each screen window of 15 papers, 3 papers are selected.A.9 INFLUENCE OF RESEARCH BACKGROUND OPTIONS TOINSPIRATION RETRIEVAL    </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 19 :</head><label>19</label><figDesc>Consistency score between expert evaluation and GPT-4o evaluation.</figDesc><table><row><cell cols="3">#Comparison Pairs Hard Consistency Score Soft Consistency Score</cell></row><row><cell>392</cell><cell>0.345</cell><cell>0.542</cell></row><row><cell cols="3">#Comparison Pairs Hard Consistency Score Soft Consistency Score</cell></row><row><cell>48</cell><cell>0.438</cell><cell>0.854</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 20 :</head><label>20</label><figDesc>Consistency score between experts in expert evaluation.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by the <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs>. This work is supported by <rs type="funder">Shanghai Artificial Intelligence Laboratory</rs>. This research/project is supported by the <rs type="funder">Ministry of Education, Singapore</rs> under its <rs type="programName">MOE Academic Research Fund Tier 2</rs> (<rs type="grantNumber">STEM RIE2025</rs> Award <rs type="grantNumber">MOE-T2EP20123-0005</rs>).</p><p>We thank <rs type="person">Mengsong Wu</rs> for his insightful discussions with us, and we thank <rs type="person">Yuwei Wan</rs> for her efforts to support this research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RNmKBFH">
					<idno type="grant-number">STEM RIE2025</idno>
					<orgName type="program" subtype="full">MOE Academic Research Fund Tier 2</orgName>
				</org>
				<org type="funding" xml:id="_aUVRFBq">
					<idno type="grant-number">MOE-T2EP20123-0005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Researchagent: Iterative research idea generation over scientific literature with large language models</title>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07738</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Associative abilities underlying creativity</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Könen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aljoscha C</forename><surname>Neubauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Aesthetics, Creativity, and the Arts</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byg1v1HKDB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autonomous chemical research with large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Boiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Macknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabe</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><surname>Gomes</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06792-0</idno>
		<ptr target="https://doi.org/10.1038/s41586-023-06792-0" />
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">624</biblScope>
			<biblScope unit="issue">7992</biblScope>
			<biblScope unit="page" from="570" to="578" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Electrocatalytic reductive deuteration of arenes and heteroarenes</title>
		<author>
			<persName><forename type="first">Faxiang</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dali</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiwen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Universal self-consistency for large language model generation</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renat</forename><surname>Aksitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2311.17311</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2311.17311" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformers as soft reasoners over language</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/537</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/537" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
	<note>ijcai.org, 2020</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why do nations produce science advances and new technology?</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Coccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology in society</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101124</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Research methods for business</title>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">H</forename><surname>Joseph F Hair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Money</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Samouel</surname></persName>
		</author>
		<author>
			<persName><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Education+ Training</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="336" to="337" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The act of creation</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Koestler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
			<publisher>Hutchinson</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tirthankar</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinayak</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.06185</idno>
		<title level="m">Can large language models unlock novel scientific research ideas? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical investigation of the impact of chatgpt on creativity</title>
		<author>
			<persName><forename type="first">Byung</forename><surname>Cheol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeyeon</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>b493230205f780e1bc26945df7481e5-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teerth</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.14033</idno>
		<title level="m">Mlr-copilot: Autonomous machine learning research based on large language models agents</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The AI scientist: Towards fully automated open-ended scientific discovery</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">Tjarko</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.06292</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.06292" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">LLM4SR: A survey on large language models for scientific research</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2501.04306</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2501.04306" />
		<imprint>
			<biblScope unit="page">2025</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhisattwa</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/91" />
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023, 2023</date>
		</imprint>
	</monogr>
	<note>edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mathematical discoveries from program search with large language models</title>
		<author>
			<persName><forename type="first">Biqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang-Ren Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zhou ; Mohammadamin Barekatain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">S</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengming</forename><surname>Ellenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Fawzi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06924-6</idno>
		<idno>doi: 10.1038/ S41586-023-06924-6</idno>
		<ptr target="https://doi.org/10.1038/s41586-023-06924-6" />
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="issue">7995</biblScope>
			<biblScope unit="page" from="468" to="475" />
			<date type="published" when="2024">2024. 2024</date>
			<pubPlace>Bernardino Romera-Paredes</pubPlace>
		</imprint>
	</monogr>
	<note>Large language models are zero shot hypothesis proposers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rh-catalysed enantioselective [2+ 2+ 1] cycloaddition reactions using three different 2π-components</title>
		<author>
			<persName><forename type="first">Kaito</forename><surname>Shibahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihito</forename><surname>Kayaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kairi</forename><surname>Yamashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Nagashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohei</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Synthesis</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Chenglei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.04109</idno>
		<title level="m">Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Sprueill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mariefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udishnu</forename><surname>Olarte</surname></persName>
		</author>
		<author>
			<persName><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutanay</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-EMNLP</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">Houda</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<idno type="DOI">10.18653/v1/2023.findings-emnlp.560</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.findings-emnlp.560" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CHEMREASONER: heuristic search over a large language model&apos;s knowledge space using quantum-chemical feedback</title>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">W</forename><surname>Sprueill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushbu</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mariefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udishnu</forename><surname>Olarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conrad</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutanay</forename><surname>Choudhury</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=3tJDnEszco" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">July 21-27, 2024. 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Photocatalytic carbyne reactivity of phosphorus ylides for three-component formal cycloaddition reactions</title>
		<author>
			<persName><forename type="first">Ryuhei</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiga</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fritz</forename><surname>Deufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohsuke</forename><surname>Ohmatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Ooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Synthesis</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Undiscovered public knowledge</title>
		<author>
			<persName><surname>Don R Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Library Quarterly</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="118" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Solving olympiad geometry without human demonstrations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Luong</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06747-5</idno>
		<ptr target="https://doi.org/10.1038/s41586-023-06747-5" />
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="issue">7995</biblScope>
			<biblScope unit="page" from="476" to="482" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised word embeddings capture latent knowledge from materials science literature</title>
		<author>
			<persName><forename type="first">John</forename><surname>Vahe Tshitoyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Dagdelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqin</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">A</forename><surname>Kononova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerbrand</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anubhav</forename><surname>Ceder</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-019-1335-8</idno>
		<ptr target="https://doi.org/10.1038/s41586-019-1335-8" />
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">571</biblScope>
			<biblScope unit="issue">7763</biblScope>
			<biblScope unit="page" from="95" to="98" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ran Tao, et al. Ultrastrong, flexible thermogalvanic armor with a carnot-relative efficiency over 8%</title>
		<author>
			<persName><forename type="first">Jinpei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6704</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scimon: Scientific inspiration machines optimized for novelty</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hope</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.18</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.acl-long.18" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="279" to="299" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2024</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=1PL1NIMMrw" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2023</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Opinion mining by convolutional neural networks for maximizing discoverability of nanomaterials</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ina</forename><surname>Østrøm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Grazian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bram</forename><surname>Hoex</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.3c00746</idno>
		<ptr target="https://doi.org/10.1021/acs.jcim.3c00746" />
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2746" to="2759" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving event duration prediction via time-aware pre-training</title>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.302</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.302" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event</title>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20">16-20 November 2020. 2020</date>
			<biblScope unit="page" from="3370" to="3378" />
		</imprint>
	</monogr>
	<note>EMNLP 2020 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end case-based reasoning for commonsense knowledge base completion</title>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.eacl-main.255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter</title>
		<meeting>the 17th Conference of the European Chapter<address><addrLine>Dubrovnik</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-05">May 2023</date>
			<biblScope unit="page" from="3509" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Logical reasoning over natural language as knowledge representation: A survey</title>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjie</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language models as inductive reasoners</title>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.eacl-long.13" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</editor>
		<meeting>the 18th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>St. Julian&apos;s, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">March 17-22, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="209" to="225" />
		</imprint>
	</monogr>
	<note>EACL 2024</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large language models for automated open-domain scientific hypotheses discovery</title>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-acl.804</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.findings-acl.804" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="page" from="13545" to="13565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Goal driven discovery of distributional differences via language descriptions</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/7" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>