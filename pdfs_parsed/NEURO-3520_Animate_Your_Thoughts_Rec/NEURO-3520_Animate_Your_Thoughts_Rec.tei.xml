<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ANIMATE YOUR THOUGHTS: RECONSTRUCTION OF DYNAMIC NATURAL VISION FROM HUMAN BRAIN AC-TIVITY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yizhuo</forename><surname>Lu</surname></persName>
							<email>luyizhuo2023@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Future Technology</orgName>
								<orgName type="department" key="dep2">Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changde</forename><surname>Du</surname></persName>
							<email>changde.du@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer and Artificial Intelligence</orgName>
								<orgName type="institution">Zhengzhou University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanliu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liuyun</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Future Technology</orgName>
								<orgName type="department" key="dep2">Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xujin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Future Technology</orgName>
								<orgName type="department" key="dep2">Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huiguang</forename><surname>He</surname></persName>
							<email>huiguang.he@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Future Technology</orgName>
								<orgName type="department" key="dep2">Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ANIMATE YOUR THOUGHTS: RECONSTRUCTION OF DYNAMIC NATURAL VISION FROM HUMAN BRAIN AC-TIVITY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F2200E1497DD43102B69C54C5E8792DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-11-30T00:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance. Although prior video reconstruction methods have made substantial progress, they still suffer from several limitations, including:</p><p>(1) difficulty in simultaneously reconciling semantic (e.g. categorical descriptions), structure (e.g. size and color), and consistent motion information (e.g. order of frames); (2) low temporal resolution of fMRI, which poses a challenge in decoding multiple frames of video dynamics from a single fMRI frame; (3) reliance on video generation models, which introduces ambiguity regarding whether the dynamics observed in the reconstructed videos are genuinely derived from fMRI data or are hallucinations from generative model. To overcome these limitations, we propose a two-stage model named Mind-Animator. During the fMRI-to-feature stage, we decouple semantic, structure, and motion features from fMRI. Specifically, we employ fMRI-vision-language tri-modal contrastive learning to decode semantic feature from fMRI and design a sparse causal attention mechanism for decoding multi-frame video motion features through a next-frame-prediction task. In the feature-to-video stage, these features are integrated into videos using an inflated Stable Diffusion, effectively eliminating external video data interference. Extensive experiments on multiple video-fMRI datasets demonstrate that our model achieves state-of-the-art performance. Comprehensive visualization analyses further elucidate the interpretability of our model from a neurobiological perspective. Project page: https://mind-animator-design.github.io/. 1 INTRODUCTION Advances in sensory neuroscience offer new perspectives on brain function and could enhance artificial intelligence development (Palazzo et al. (2021); Yargholi &amp; Hossein-Zadeh (2016)). One of the critical aspects to the research is neural decoding, which links visual stimuli to corresponding functional magnetic resonance imaging (fMRI) brain recordings. Neural decoding methods include classification, identification, and reconstruction, with this study focusing on the most challenging aspect: reconstruction. Prior methods have made significant strides in the classification (Yargholi &amp; Hossein-Zadeh (2016); Horikawa &amp; Kamitani (2017); Fujiwara et al. (2013)) and identification (Kay et al. (2008); Wildgruber et al. (2005)) of static stimulus images. Remarkably, some researchers have advanced to the point where they can reconstruct (Naselaris et al. (2009); Van Gerven et al. (2010); Chen et al. (2023); Takagi &amp; Nishimoto (2023); Ozcelik et al. (2022); Beliy et al. (2019)) images from brain signals that closely resemble the original stimulus images. In reality, the majority of visual stimuli we encounter in daily life are continuous and dynamic, hence there is a growing interest in reconstructing video from brain signals. Building on previous work that decoupled semantic and structural information from fMRI to reconstruct images (Scotti et al. (2024); Lu et al. (2023); Fang et al. ( <ref type="formula">2020</ref>)), we argue that when the visual stimulus shifts from static images to dynamic videos, as shown in Figure <ref type="figure">1</ref>, it is crucial to account for three dimensions: semantic, structural, and motion, considering the brain's processing of dynamic visual information.</p><p>Due to the inherent nature of fMRI, which relies on the slow blood oxygenation level dependent (BOLD) signal (Logothetis (2002); Kim &amp; Ogawa (2012)), neural activity is integrated over a period exceeding 10 seconds ( 300 video frames). This integration delay poses a fundamental challenge in capturing rapid motion dynamics. Consequently, the task of reconstructing videos from fMRI signals becomes exceedingly challenging.</p><p>What is this scenario? (High-level Semantic Information) --A soldier is walking in a desert.</p><p>Where is the object in the scenario, what is its size, color, shape? (Low-level Structure Information) --A black figure on the edge of a yellow background. How the objects in the scene move? (Motion Information) --The figure moves from right to left in the movie.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: The human brain's comprehension of dynamic visual scenes. When receiving dynamic visual information, human brain gradually comprehends low-level structural details such as position, shape and color in the primary visual cortex, discerns motion information, and ultimately constructs high-level semantic information in the higher visual cortex, such as an overall description of the scene.</p><p>To address this challenge, <ref type="bibr" target="#b47">Nishimoto et al. (2011)</ref> transforms the video reconstruction task into an identification task, employing the Motion-Energy model <ref type="bibr" target="#b0">(Adelson &amp; Bergen (1985)</ref>) and Bayesian inference to retrieve videos from a predefined video library. Subsequently, <ref type="bibr" target="#b22">Han et al. (2019)</ref>, <ref type="bibr" target="#b69">Wen et al. (2018)</ref> and <ref type="bibr" target="#b68">Wang et al. (2022)</ref> map brain responses to the feature spaces of deep neural network (DNN) to reconstruct video stimuli. To mitigate the scarcity of video-fMRI data, <ref type="bibr" target="#b37">Kupershmidt et al. (2022)</ref> utilizes self-supervised learning <ref type="bibr" target="#b35">(Kingma &amp; Welling (2014)</ref>) to incorporate a large amount of unpaired video data. While these efforts have confirmed the feasibility of video reconstruction from fMRI, the results are notably deficient in explicit semantic information. <ref type="bibr" target="#b11">Chen et al. (2024)</ref> utilizes contrastive learning to map fMRI to the Contrastive Language-Image Pre-training (CLIP) <ref type="bibr" target="#b51">(Radford et al. (2021)</ref>) representation space and co-training with a video generation model, successfully reconstructing coherent videos with clear semantic information for the first time. However, this work does not consider structure information such as color and position, and it is uncertain whether the motion information in the reconstructed videos originate from the fMRI or the external video data used in training the video generation model. In summary, current video reconstruction models face two challenges:</p><p>(1) They fail to simultaneously capture semantic, structure, and motion information within the reconstructed videos.</p><p>(2) The reliance on external video datasets and video generation models introduces ambiguity regarding whether the dynamics observed in the reconstructed videos are genuinely derived from fMRI data or are hallucinations from video generative model. To address the issues, we introduce Mind-Animator, a video reconstruction model that decouples semantic, structure, and motion information from fMRI, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Specifically, we map fMRI to the CLIP representation space and the Vector Quantized-Variational Autoencoder (VQ-VAE) <ref type="bibr" target="#b64">(Van Den Oord et al. (2017)</ref>) latent space to capture semantic and structure information. We design a Transformer-based <ref type="bibr" target="#b66">(Vaswani et al. (2017)</ref>) motion decoder to extract motion information frame by frame from fMRI through a next-frame-prediction task. Finally, the decoded semantic, structure, and motion information is fed into an inflated Stable Diffusion <ref type="bibr" target="#b54">(Rombach et al. (2022)</ref>; <ref type="bibr" target="#b72">Wu et al. (2023)</ref>) without any fine-tuning with video data to generate each frame of the video.</p><p>Our contributions are summarized as follows:</p><p>(1) Method: We propose Mind-Animator, which enables video reconstruction by decoupling semantic, structural, and motion information from fMRI data for the first time.</p><p>To address the mismatch in timescales between fMRI and video data, we propose a Consistency Motion Generator with Sparse Causal Attention. This model decodes subtle yet significant motion patterns through a next-frame token prediction task despite the limitations imposed by the slow BOLD signal integration in fMRI.</p><p>(2) Interpretability: We use voxel-wise and ROI-wise visualization techniques to elucidate the interpretability of our proposed model from a neurobiological perspective.</p><p>(3) Comprehensive evaluation: We introduce eight evaluation metrics that comprehensively assess the reconstruction results of our model and all previous models across three dimensions-semantic, structure, and spatiotemporal consistency-on three publicly available video-fMRI datasets. This establishes our work as the first unified benchmark for subsequent researchers. We will release all data and code to facilitate future research. The video reconstruction task involves recreating the video frames a subject was viewing based on their brain responses (e.g., fMRI). The challenge in video reconstruction lies in the significant discrepancy between the temporal resolution of fMRI (0.5 Hz) and the frame rate of the stimulus video (30 Hz), making it difficult to model the mapping between fMRI signals and video content.</p><p>To tackle this challenge, <ref type="bibr" target="#b47">Nishimoto et al. (2011)</ref> reframes video reconstruction as an identification task, using the Motion-Energy model <ref type="bibr" target="#b0">(Adelson &amp; Bergen (1985)</ref>) and Bayesian inference to retrieve videos from a predefined library. With the advancement of deep learning, early works by <ref type="bibr" target="#b22">Han et al. (2019)</ref>, <ref type="bibr" target="#b69">Wen et al. (2018)</ref> and <ref type="bibr" target="#b68">Wang et al. (2022)</ref>, as shown in Figure <ref type="figure">2</ref> (a), mapped brain responses to the feature spaces of deep neural networks (DNNs) for end-to-end video reconstruction. To address the scarcity of paired video-fMRI data, <ref type="bibr" target="#b37">Kupershmidt et al. (2022)</ref> further advanced this approach by leveraging self-supervised learning to incorporate a large volume of unpaired video data. Although these studies demonstrated the feasibility of reconstructing videos from fMRI signals, the results notably lacked explicit semantic information. As shown in Figure <ref type="figure">2</ref> (b), with advancements in multimodal and generative models, <ref type="bibr" target="#b11">Chen et al. (2024)</ref>, <ref type="bibr" target="#b60">Sun et al. (2024)</ref> used contrastive learning to map fMRI signals to the CLIP latent space for semantic decoding, followed by input into a video generation model for reconstruction. This approach produces semantically coherent and smooth videos, but it remains unclear whether the motion information in the reconstructions originates from the fMRI or from the external video data used to train the video generation model.</p><p>To address the above issues, we propose Mind-Animator. By independently decoding semantic, structural, and motion information from fMRI signals and inputting them into an inflated image generation model, we ensure that the motion in the reconstructed videos originates solely from the fMRI data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DIFFUSION MODELS FOR VIDEO GENERATION</head><p>After significant progress in text-to-image (T2I) generation, diffusion models have drawn interest for text-to-video (T2V) tasks. <ref type="bibr">Ho et al. (2022b)</ref> made a breakthrough by introducing 3D diffusion U-Net for video generation, followed by advancements like cascaded sampling frameworks and super-resolution methods <ref type="bibr">(Ho et al. (2022a)</ref>), and the integration of temporal attention mechanisms <ref type="bibr" target="#b59">(Singer et al. (2022)</ref>; <ref type="bibr" target="#b77">Zhou et al. (2022)</ref>; <ref type="bibr">He et al. (2022b)</ref>). However, due to limited paired text-video datasets and high memory demands of 3D U-Nets, alternative approaches have emerged, refining pre-trained T2I models for T2V tasks. <ref type="bibr" target="#b33">Khachatryan et al. (2023)</ref> and <ref type="bibr" target="#b72">Wu et al. (2023)</ref> introduced techniques like cross-frame attention and inter-frame correlation consideration to adapt T2I models for video generation.</p><p>In our work on video reconstruction from fMRI, we avoided pre-trained T2V models to prevent external video data from interfering with motion information decoding from fMRI. As shown in Figure <ref type="figure">2</ref> (c), we adapted an inflated T2I model to generate each frame. This ensured that the motion information in the reconstructed videos was derived solely from fMRI decoding, as the generative model had never been exposed to video data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masking Random Masking</head><p>: </p><formula xml:id="formula_0">1 • • 1 • 2 • 3 ⋯ • ⋯ 2 • 3 • • 1 • 2 1 • 3 ⋯ 1 • 2 • 1 • 2 • 3 ⋯ 2 • 3 • 1 3 • 2 • ⋯ 3 • • 1 • 2 • 3 ⋯ • ⋯ ⋯ ⋯ ⋯ ⋯</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROBLEM STATEMENT</head><p>We aim to decode videos from brain activity recorded with fMRI when healthy participants watch a sequence of natural videos. Let X and Y denote the voxel space and pixel space, respectively. Let x i ∈ R 1×n be the fMRI signal when a video v i,j ∈ R 1×3×512×512 is presented to the participant, where n is the number of fMRI voxels, j is the frame ID of video i and i ∈ [1, N ], j ∈ [1, 8], with N the total number of videos. Let Z(k) denotes the feature space, k ∈ {semantic, structure, motion}.</p><p>The goal of fMRI-to-feature stage is to train decoders D(k) : X → Z(k), and the goal of feature-tovideo stage is to construct a generation model G : Z(semantic)×Z(structure)×Z(motion) → Y , without introducing motion information from external video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FMRI-TO-FEATURE STAGE</head><p>Semantic Decoder. Due to the low signal-to-noise ratio of the fMRI signal x i and the substantial dimension discrepancy with the text condition c i ∈ R 1×20×768 of Stable Diffusion (SD), learning a mapping between them directly is prone to overfitting. Considering both the lower dimensionality (t i or v i ∈ R 1×512 ) and the robust semantic information embedded in the latent space of CLIP <ref type="bibr" target="#b16">(Gao et al. (2020)</ref>), and given that CLIP has been shown to outperform various single-modal DNNs in explaining cortical activity <ref type="bibr" target="#b68">(Wang et al.;</ref><ref type="bibr" target="#b78">Zhou et al. (2024)</ref>), we employ bidirectional InfoNCE loss to align the fMRI embedding f i with the latent space of CLIP (Vit-B/32)⊆ R 512 , followed by a two-layer MLP to map it to text condition c i . In this context,</p><formula xml:id="formula_1">f i = D Semantic (x i )</formula><p>, where D Semantic is a three-layer MLP,</p><formula xml:id="formula_2">L BiInf oN CE = - 1 B B i=1 log exp(s(ẑ i , z i )/τ ) B j=1 exp(s(ẑ i , z j )/τ ) + log exp(s(ẑ i , z i )/τ ) B k=1 exp(s(ẑ k , z i )/τ ) .<label>(1)</label></formula><p>where s(•, •) is the cosine similarity, z and ẑ are the latent representation from two modalities, B is the batch size, and τ is a learned temperature parameter. Then, given</p><formula xml:id="formula_3">f ∈ R B×512 , v ∈ R B×512 ,</formula><p>and t ∈ R B×512 as the respective embeddings of fMRI, video, and text, the fMRI-vision-language tri-modal loss is:</p><formula xml:id="formula_4">L Semantic = α • L BiInf oN CE (f , t) + (1 -α) • L BiInf oN CE (f , v).<label>(2)</label></formula><p>For stable training, we have also designed the following loss function to bring the fMRI and text embeddings closer together:</p><formula xml:id="formula_5">L P rojection1 = 1 B B i=1 ∥f i -t i ∥ 2 2 .</formula><p>(3) Subsequently, to map the fMRI embedding f i to the text condition c i for the purpose of conditioning generative image models, a projection loss is utilized,</p><formula xml:id="formula_6">L P rojection2 = 1 B B i=1 ∥M LP (f i ) -c i ∥ 2 2 . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>Finally, we combine the semantic and projection losses using tuned hyperparameters λ 1 , λ 2 ,</p><formula xml:id="formula_8">L Combined = L P rojection1 + λ 1 • L Semantic + λ 2 • L P rojection2 .<label>(5)</label></formula><p>Structure Decoder. For a short video clip, it can be assumed that the low-level feature (e.g. size, shape, and color) contained in each frame remains largely consistent with that of the first frame. Consequently, we utilize the token extracted from the first frame by VQ-VAE as structural feature and train the structural decoder (a two-layer MLP) using the standard mean squared error (MSE) loss function. Let Φ denote the encoder of VQVAE, the structure loss is defined as:</p><formula xml:id="formula_9">L Structure = 1 B B i=1 ∥D Structure (f i ) -Φ(v i,1 )∥ 2 2 . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>Consistency Motion Generator. Drawing inspiration from natural language processing, we treat each video frame token as a word embedding and develop an L-layer Transformer-based Consistency Motion Generator (CMG) to implicitly decode dynamic information between consecutive frames. Handling raw pixels for a video clip v i ∈ R f ×3×H×W in the CMG can be computationally intensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQ-VAE Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video stimulus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visible frame tokens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patchify</head><p>To overcome this, we follow Stable Diffusion's approach <ref type="bibr" target="#b54">(Rombach et al. (2022)</ref>) by projecting the video into a latent space using a pre-trained VAE tokenizer. After compressing the video clip to 8 , it is divided into patches, which are then converted into frame tokens Φ tok (v i ) ∈ R P ×d token through an embedding layer.</p><formula xml:id="formula_11">Φ(v i ) ∈ R f ×3× H 8 × W</formula><p>In the Temporal Module, visible frame tokens Φ tok (v i ) ∈ R m×d token and positional encoding E pos ∈ R m×d token are jointly input into a Sparse Causal Self-Attention layer to learn inter-frame temporal information. This attention layer incorporates a specially designed Sparse Causal Mask to ensure sparsity between frames. As illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, the mask is divided into fixed and random components. The fixed mask ensures that each frame cannot access information from subsequent frames, while the random mask maintains sparsity among visible frames, preventing the model from taking shortcuts <ref type="bibr" target="#b63">(Tong et al. (2022)</ref>). During inference, we eliminate the random mask. For other variants of the spatial-temporal module, please refer to Appendix D.2.</p><p>In the Spatial Module, to extract spatial information for subsequent frames from fMRI, the embedding of the visible frames z l serves as the Query, while the fMRI signal f , after passing through an embedding layer, serves as the Key and Value in the cross-attention block, as shown in Eq. ( <ref type="formula" target="#formula_12">7</ref>). Following residual connections and layer normalization, z l is input into the Feed Forward Network (FFN) to predict the subsequent unseen frame tokens Φ(v i,j ), j ∈ [m + 1, n]:</p><formula xml:id="formula_12">z l =CrossAttention(Q, K, V), l = 1, 2, . . . , L<label>(7)</label></formula><formula xml:id="formula_13">Q =W Q l • z l , K = W K l • Emb(f ), V = W V l • Emb(f ), z l =F F N (LN (z l ) + z l-1 ). l = 1, 2, . . . , L<label>(8)</label></formula><p>Then, the final motion consistency loss is defined as:</p><formula xml:id="formula_14">L Consistency = 1 B B i=1 n j=m+1 ∥ Φtok (v i,j ) -Φ tok (v i,j )∥ 2 2 . (<label>9</label></formula><formula xml:id="formula_15">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FEATURE-TO-VIDEO STAGE</head><p>Inflated Stable Diffusion for Video Reconstruction. Despite the rapid development of video generation models capable of producing vivid videos from text conditions, it is crucial to emphasize that the objective of our work is to disentangle semantic, structural, and motion information from fMRI to reconstruct the stimulus video. Utilizing pre-trained video generation models could obscure whether the motion information in the reconstructed video originates from the fMRI or external video data.</p><p>To address this issue, we employ the network inflation <ref type="bibr" target="#b8">(Carreira &amp; Zisserman (2017)</ref>; <ref type="bibr" target="#b33">Khachatryan et al. (2023)</ref>; <ref type="bibr" target="#b72">Wu et al. (2023)</ref>) technique to implement an inflated Stable Diffusion, which is used to reconstruct each frame of the video without introducing additional motion information. Specifically, after the motion features</p><formula xml:id="formula_16">Φ(v i ) ∈ R B×f ×3× H 8 × W 8 are decoded, they are reshaped ((B, f, 3, H 8 , W 8 ) → (B • f, 3, H 8 , W<label>8</label></formula><p>)) and input into the U-Net of Stable Diffusion for reverse denoising. The result is then mapped back to pixel space through the VQ-VAE decoder and reshaped ((B • f, 3, H, W ) → (B, f, 3, H, W )) to yield the final video v i ∈ R B×f ×3×H×W . In this context, B denotes the batch dimension, with B = 1 during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>In this study, we utilize three publicly available video-fMRI datasets, which encompass paired stimulus videos and their corresponding fMRI responses. As depicted in Table <ref type="table" target="#tab_0">1</ref>, these datasets collectively comprise brain signals recorded from multiple healthy subjects while they are viewing the videos. The video stimuli are diverse, covering animals, humans, and natural scenery. For detailed information on the datasets and preprocessing steps, please refer to Appendix B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EVALUATION METRICS</head><p>To comprehensively evaluate the performance of our model, we use the following evaluation metrics.</p><p>Semantic-level metrics. Following prior studies <ref type="bibr" target="#b10">(Chen et al. (2023;</ref><ref type="bibr">2024</ref>)), we use the N-way top-K accuracy classification test and VIFI-score as the semantic-level metrics. For the classification test, we implement two modes: image-based (2-way-I) and video-based (2-way-V). We describe this evaluation method in Algorithm 2. For the VIFI-score, we utilize a CLIP model fine-tuned on the video dataset (VIFICLIP) <ref type="bibr" target="#b52">(Rasheed et al. (2023)</ref>) to extract features from both the ground truth and predicted videos, followed by the calculation of cosine similarity.</p><p>Pixel-level metrics. We employ the structural similarity index measure (SSIM), peak signal-tonoise ratio (PSNR), and hue-based Pearson correlation coefficient <ref type="bibr" target="#b61">(Swain &amp; Ballard (1991)</ref>) (Hue-pcc) as pixel-level metrics.</p><p>Spatiotemporal (ST) -level metrics. We adopt CLIP-pcc, a widely used metric in video editing research <ref type="bibr" target="#b72">(Wu et al. (2023)</ref>), to evaluate the smoothness and consistency between consecutive video frames. This metric computes the CLIP image embeddings for each frame in the predicted videos and reports the average cosine similarity between all pairs of adjacent frames. Considering the input to the video reconstruction task contains substantial noise, there are instances where every pixel of each reconstructed video frame is either zero or noise, which would artificially inflate the CLIP score if used directly. Therefore, we calculate the CLIP score only when the VIFI-CLIP value exceeds the average level (0.6); otherwise, we assign a score of 0.</p><p>To measure the similarity of motion trajectories, we introduce End-Point Error (EPE) <ref type="bibr" target="#b2">(Barron et al. (1994)</ref>), which calculates the Euclidean distance between the predicted and ground truth endpoints for each frame.   Published as a conference paper at ICLR 2025</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">COMPARATIVE EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>We compare our model with all previous video reconstruction models on the aforementioned datasets.</p><p>In the computation of quantitative metrics, the results of <ref type="bibr" target="#b69">Wen et al. (2018)</ref> pertain to the first segment of the test set, whereas the results of other researchers are derived from the whole test set. Visual comparisons on CC2017 dataset are presented in Figure <ref type="figure" target="#fig_5">5</ref>, while quantitative comparisons are detailed in Table <ref type="table" target="#tab_1">2</ref>, which indicates that our model achieves SOTA performance in six out of eight metrics. Specifically, our model outperforms the previous SOTA model by 83% and 13% in terms of SSIM and EPE respectively, which underscores the benefits of incorporating structural and motion information.</p><p>The results in Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref> demonstrate that our model maintains strong performance on other datasets as well. For instance, our model outperforms Mind-video by 196%/ 21%/ 4% on the HCP and 275%/ 27%/ 5% on the Algonauts 2021 dataset across the three pixel-level metrics. In this subsection, we conduct a detailed ablation study to assess the effectiveness of the three decoders we proposed and evaluate the impact of various hyperparameters on video reconstruction. First, we present the results obtained using the full model. Then, based on the full model, we eliminate the semantic decoder (w/o Semantic) and the structure decoder (w/o Structure) separately, replacing their outputs with random Gaussian noise. For the consistency motion generator, we replace it with 8 simple MLPs to model each frame individually (w/o Motion). Table <ref type="table" target="#tab_4">5</ref> demonstrates that the removal of any decoder results in a significant decline in the model's performance across nearly all metrics, which shows the efficacy of our proposed decoders. Notably, the Hue-pcc significantly increased after removing the structure decoder. We hypothesize that while fMRI contains structure and motion information, its low signal-to-noise ratio introduces noise that affects the generation quality of Sable Diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">INTERPRETABILITY ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">HAVE WE TRULY DECODED MOTION INFORMATION FROM FMRI?</head><p>Following the work of <ref type="bibr" target="#b68">Wang et al. (2022)</ref>, we conduct shuffle tests on three subjects from the CC2017 dataset to evaluate whether the CMG accurately decodes motion information from fMRI, focusing on reconstructed videos with clear semantic decoding. Specifically, for each 8-frame reconstructed video clip from each subject, we shuffle the frame order 100 times randomly and compute spatiotemporallevel metrics on the original and shuffled frames. Subsequently, we estimate the P-value by the following formula: P = 100 i=1 δ i /100, where δ i = 1 if the i-th shuffle outperforms the reconstruction result in the original order based on the metrics; otherwise, δ i = 0. A lower P-value signifies a closer alignment between the sequential order of the reconstructed video and the ground truth. We repeat the shuffle test 5 times under conditions with and without the CMG, as illustrated in Figure <ref type="figure" target="#fig_6">6</ref>. It can be observed that the P-value of EPE is significantly lower than 0.05 when CMG is applied. However, although the P-value of CLIP-pcc is significantly smaller with CMG compared to without CMG, the P-value remains significantly greater than 0.05. To explain this, we further repeated the shuffle test on the reconstruction results' noise ceiling (videos generated directly using the test set features). The results show that even for the noise ceiling, the P-value of CLIP-pcc remains significantly greater than 0.05. This indicates that: (1) we have indeed decoded motion information from fMRI, and (2) EPE is a more effective metric than CLIP-pcc for evaluating the model's ability to decode motion information.</p><p>Figure <ref type="figure">7</ref>: Ablation experiment results of fMRI guidance on the test sets of subject 1 and subject 2 from the CC2017 dataset.</p><p>To further validate whether the decoded motion information originates from fMRI guidance or the CMG's autoregressive training, we removed the fMRI guidance during the CMG module's training (w/o fMRI guidance) by replacing the cross-attention in the Spatial Module with selfattention, while keeping the rest of the architecture and hyperparameters unchanged. As shown in Figure <ref type="figure">7</ref>, removing the fMRI guidance led to a significant deterioration in EPE, confirming that the proposed CMG effectively decodes motion information from fMRI. Additionally, comparing the removal of the entire CMG module (w/o Motion) with the removal of fMRI guidance (w/o fMRI guidance), we find that the latter accounts for the majority of the impact on EPE (i.e., 90% of the decrease in EPE can be attributed to the absence of fMRI guidance). This further underscores the critical role of fMRI guidance in accurately decoding motion information from brain signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">WHICH BRAIN REGIONS ARE RESPONSIBLE FOR DECODING DIFFERENT FEATURES, RESPECTIVELY?</head><p>To investigate voxels in which brain regions are responsible for decoding different features (semantic, structure, motion) during the fMRI-to-feature stage, we compute the voxel-wise importance maps in the visual cortex. Specifically, for a trained decoder, we multiply the weight matrix of the linear layers, then average the results across the feature dimension, and normalize them to estimate the importance weights for each voxel. A higher weight indicates that the voxel plays a more significant role in feature decoding. We project the importance maps of subject 1's voxels from the CC2017 dataset onto the visual cortex, as depicted in Figure <ref type="figure" target="#fig_8">8</ref>. To obtain ROI-wise importance maps, we calculate the average of the importance weights of voxels contained within each Region of Interest (ROI), with the results presented in Figure <ref type="figure" target="#fig_10">9</ref>. The results from other subjects are presented in Appendix F.     We also identify the following findings in Figure <ref type="figure" target="#fig_10">9:</ref> (1) MT shows significant activation for semantic decoding. This observation aligns with the functional segregation and interaction between the dorsal and ventral pathways during dynamic visual input processing <ref type="bibr" target="#b30">(Ingle et al. (1982)</ref>). Specifically, the dorsal-dorsal pathway is associated with action control, whereas the ventral-dorsal pathway is involved in action understanding and recognition <ref type="bibr" target="#b53">(Rizzolatti &amp; Matelli (2003)</ref>). This finding aligns with the latter. (2) V1 is predominantly activated when decoding motion features, reflecting the visual system's parallel processing capability. Motion information in the dorsal pathway does not strictly follow hierarchical processing <ref type="bibr" target="#b76">(Zeki &amp; Shipp (1988)</ref>). As noted by Nassi et al. <ref type="bibr" target="#b46">(Nassi &amp; Callaway (2009)</ref>), V1 directly projects motion-related information, such as direction and speed, to MT for further processing. For detailed neurobiological explanations, please refer to the Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We introduce a video reconstruction model (Mind-Animator) that decouples semantic, structural, and motion information from fMRI, achieving state-of-the-art performance across 3 public datasets. We mitigate the interference of external video data on motion information decoding through a rational experimental design. The results of the shuffle test demonstrate that the motion information we decoded indeed originates from fMRI, rather than being a spontaneity from generative model. Additionally, the visualization of voxel-wise and ROI-wise importance maps substantiate the neurobiological interpretability of our model. Thanks to Prof. Wei Wang for the discussions and suggestions provided for our revisions in Section 6.2. We are grateful to Prof.Juan Helen Zhou and Dr.Zijiao Chen for their patient answers to our questions and for making all the results of the Mind-video test set public. We also extend our thanks to Prof.Michal Irani, Dr.Ganit Kupershmidt, and Dr.Roman Beliy for providing us with all the reconstruction results of their models on the test set. We would like to express our appreciation to Prof.Zhongming Liu and Dr.Haiguang Wen for their open-sourced high-quality video-fMRI dataset and the preprocessing procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ETHICS STATEMENT</head><p>The pursuit of unraveling and emulating the brain's intricate visual processing systems has been a cornerstone endeavor for researchers in computational neuroscience and artificial intelligence. Recent advancements in neural decoding and the reconstruction of visual stimuli from brain activity have opened up numerous possibilities, fueling concerns about the potential harmful use cases of mind reading.</p><p>We argue that these concerns can be alleviated for two main reasons   <ref type="formula">2020</ref>)), a class of probabilistic generative models, have increasingly rivaled or surpassed the performance of Generative Adversarial Networks (GAN) <ref type="bibr" target="#b20">(Goodfellow et al. (2020)</ref>) in specific tasks within the field of computer vision. Diffusion models encompass a forward diffusion process and a reverse denoising process, each exhibiting Markovian behavior. The forward process incrementally introduces Gaussian noise into the original image, culminating in a transition to standard Gaussian noise. The forward diffusion process can be represented as q(x t |x t-1 ) = N (x t ; √ α t x t-1 , (1 -α t )I), where t denotes the time step of each noise addition. The reverse denoising process employs the U-Net <ref type="bibr" target="#b55">(Ronneberger et al. (2015)</ref>) architecture to accurately model the noise distribution at each timestep t. The image synthesis is achieved through a sequential denoising and sampling procedure, initiated from standard Gaussian noise.</p><p>In the context of image generation tasks, the conventional diffusion model executes two Markov processes in a large pixel space, resulting in substantial computational resource utilization. To address this issue, Latent Diffusion Models (LDM) <ref type="bibr" target="#b54">(Rombach et al. (2022)</ref>) employs a VQ-VAE (Van Den Oord et al. ( <ref type="formula">2017</ref>)) encoder to transform the pixel space into a low-dimensional latent space. Subsequently, the diffusion model's training and generation are performed in the latent space, with the final generated image obtained by utilizing the VQ-VAE decoder. This approach significantly reduces computational resource requirements and inference time while preserving the quality of generated images.</p><p>A However, due to the scarcity of paired text-video datasets and the high memory requirements for training 3D U-Nets, alternative approaches are being explored. These involve refining pre-trained T2I models to directly undertake T2V tasks. <ref type="bibr" target="#b33">Khachatryan et al. (2023)</ref> introduced two enhancements to enable zero-shot adaptation of T2I models to T2V tasks: (1) the implementation of cross-frame attention, ensuring that the generation of each current frame in a video considers information from preceding frames; and (2) the consideration of inter-frame correlations during noise sampling, rather than random sampling for each frame independently. <ref type="bibr" target="#b72">Wu et al. (2023)</ref> also employed cross-frame attention and achieved one-shot video editing by fine-tuning partial model parameters on individual videos.</p><p>In this work, tasked with video reconstruction from fMRI, we eschewed the use of pre-trained T2V models to mitigate the interference of external video data with the decoding of motion information from fMRI. Inspired by the cross-frame attention mechanism, we adapted a T2I model through network inflation techniques, enabling it to generate multi-frame videos. Consequently, the generative model employed in our study has never been exposed to video data, ensuring that the motion information in the reconstructed videos is solely derived from the fMRI decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATA PREPROCESSING</head><p>For the stimulus videos of the three datasets described below, we segmented them into 2-second clips, down-sampled the frame rate to 4Hz (i.e. evenly extracting 8 frames), then centrally cropped each frame, and resized each to a shape of 512×512. Following the approach of Chen et al. ( <ref type="formula">2024</ref>  <ref type="formula">2021</ref>)). During data acquisition, 10 participants passively view 1100 silent videos of everyday events, each approximately 3 seconds in duration, presented three times. The participants' fMRI is recorded using a 3T Trio Siemens scanner with a spatial resolution of 2.5 millimeters and a repetition time (TR) of 1.75 seconds.</p><p>The fMRI preprocessing involves steps such as slice-timing correction, realignment, coregistration, and normalization to the MNI space. Additionally, the fMRI data are interpolated to a TR of 2 seconds.</p><p>The dataset has been officially preprocessed, allowing us to extract brain responses from nine regions of interest (ROIs) within the visual cortex, including four primary and intermediate visual cortical areas V1, V2, V3, and V4, as well as five higher visual cortical areas: the Extrastriate Body Area (EBA), Fusiform Face Area (FFA), Superior Temporal Sulcus (STS), Lateral Occipital Cortex (LOC), and Parahippocampal Place Area (PPA). These areas selectively respond to body, face, biological motion and facial information, objects, and scene information, respectively. In the experiment, the average neural response across three stimulus repetitions is taken for brain activity. As the test set data are not yet public, we utilize the first 900 sets of data for training and the 900-1000 sets for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 DATA ACQUISITION</head><p>The open-source datasets used in this paper can be accessed via the following links:</p><p>(1) CC2017: https://purr.purdue.edu/publications/2809/1</p><p>(2) HCP: https://www.humanconnectome.org/ (3) Algonauts2021: http://algonauts.csail.mit.edu/2021/index.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS</head><p>C.1 HYPERPARAMETER SETTINGS For all three datasets employed in the experiments, during the training of the Semantic Decoder, we set α to 0.5, λ 1 to 0.01, and λ 2 to 0.5. The batch size is set to 64, and the learning rate is set to 2e-4, with training conducted 100 epochs. Given the critical role of data volume and augmentation methods in the training of contrastive learning models, and the scarcity of video-fMRI paired data, we implement specific data augmentation techniques to prevent overfitting. For fMRI data, we randomly select 20% of the voxels during each iteration, zero out 50% of their values. For image data, we randomly crop one frame from eight video frames to a size of 400x400 pixels, and then resize it to 224x224. When extracting the CLIP image features v for each video, we input each frame into the CLIP visual encoder and then compute the average across all frames. For text data, due to the presence of similar video clips in the training set (derived from a complete video segment), BLIP2 often provide identical textual descriptions, which leads to overfitting. To mitigate this, we apply more aggressive augmentation techniques. For an input sentence, we perform Synonym Replacement with a 50% probability and Random Insertion, Random Swap, and Random Deletion with a 20% probability each.</p><p>During the training of the Structural Decoder, we set the batch size to 64 and the learning rate to 1e-6. To stabilize the learning process, we conduct the training process for 100 epochs with a 50-step warmup.</p><p>When training the Consistency Motion Generator, we set the patch size to 64 and the mask ratio of the Sparse Causal mask to 0.6 during the training phase, with a batch size of 64 and a learning rate of 4e-5. Similarly, for stability, we implement a 50-step warmup followed by 300 epochs of training.</p><p>Taking three subjects from the CC2017 dataset as an example, we utilize the first 4000 data points as the training set and the subsequent 320 data points as the validation set. Following this, we retrain the model on the entire training set, which comprises 4320 data points. The loss curve is depicted in Figure <ref type="figure" target="#fig_13">10</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPATIOTEMPORAL-LEVEL</head><p>Considering the input to the video reconstruction task contains substantial noise, there are instances where every pixel of each reconstructed video frame is either zero or noise, which would artificially inflate the CLIP score if used directly. Therefore, we calculate the CLIP score only when the VIFI-CLIP value exceeds the average level; otherwise, we assign a score of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHUFFLE TEST</head><p>Since not every test sample is accurately reconstructed, some fail to be reconstructed in terms of semantics, structure, or motion information, and some reconstruction results are entirely noise (failed samples are detailed in Appendix E.6.3). Therefore, it is meaningless to perform the shuffle test on the entire test set; we only conduct the shuffle test on the samples that are successfully reconstructed.  <ref type="table" target="#tab_9">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 CONSISTENCY MOTION GENERATOR</head><p>Figure <ref type="figure" target="#fig_3">4</ref> illustrates the Consistency Motion Generator, which is primarily composed of two modules: the Temporal Module and the Spatial Module. The Temporal Module is tasked with learning the temporal dynamics from the visible frames. Given the severe information redundancy between video frame tokens, we specifically design a Sparse Causal mask. As shown in Figure <ref type="figure" target="#fig_3">4</ref> on the top, during training, the mask is divided into fixed and random components. The fixed mask ensures that each frame cannot access information from subsequent frames, while the random mask maintains sparsity among visible frames, preventing the model from taking shortcuts <ref type="bibr" target="#b63">(Tong et al. (2022)</ref>) and accelerating training. During inference, we eliminate the random mask to allow full utilization of information from all preceding frames for predicting future frames. Since a single fMRI frame captures information from several video frames, we design a cross attention mechanism within the Spatial Module to extract the necessary temporal and spatial information for predicting the next frame token from the fMRI data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 TEXT-TO-IMAGE NETWORK INFLATION</head><p>To leverage pre-trained weights from large-scale image datasets, such as ImageNet, for the pre-training of massive video understanding models, <ref type="bibr" target="#b8">Carreira &amp; Zisserman (2017)</ref> pioneered the expansion of filters and pooling kernels in 2D ConvNets into the third dimension to create 3D filters and pooling kernels. This process transforms N×N filters used for images into N×N×N 3D filters, providing a beneficial starting point for 3D video understanding models by utilizing spatial features learned from large-scale image datasets. In the field of generative model, several attempts have been made to extend generative image models to video models. A key technique employed in this work involves augmenting the Query, Key, and Value of the attention module, as illustrated below: i-th fMRI activity pattern,</p><formula xml:id="formula_17">Q = W Q • z vi , K = W K • [z v0 , z vi-1 ], V = W V • [z v0 , z vi-1 ],<label>(10)</label></formula><formula xml:id="formula_18">x i ∈ R 1×n v i,j Frames of i-th video , v i,j ∈ R 1×3×512×512 , j ∈ [1, 8] c i i-th text condition of Stable Diffusion, c i ∈ R 1×20×768 f i i-th fMRI embedding, f i ∈ R 1×512 v i i-th video embedding in CLIP space, v i ∈ R 1×512 t i i-th text embedding in CLIP space, t i ∈ R 1×512 W Q , W K , W V Projection matrices of attention mechanisms τ Learned temperature parameter α, λ 1 , λ 2 Hyperparameters of semantic loss function L Number of CMG blocks d token Dimension of frame tokens B Batch size ∥ • ∥ 2 L 2 -norm operator s(•, •)</formula><p>Cosine similarity where z vi denotes the latent of the i-th frame during the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL EXPERIMENTAL RESULTS</head><p>E.1 THE IMPACT OF DIFFERENT SPATIAL-TEMPORAL ARCHITECTURE DESIGNS.</p><p>Due to computational resource constraints, we adopted a design in the CMG architecture that separates temporal and spatial attention modules. To capture motion information between video frames from fMRI, we designed distinct interactions between the temporal and spatial information of fMRI and video data.</p><p>For feature interaction, we primarily adopt two strategies:</p><p>(1) Cross-attention: As shown in Eq. ( <ref type="formula" target="#formula_19">11</ref>), we treat video representations as Queries and fMRI representations as Keys and Values, using cross-attention to extract useful spatial and temporal information from fMRI.</p><formula xml:id="formula_19">z l =CrossAttention(Q, K, V), l = 1, 2, . . . , L Q =W Q l • z l , K = W K l • Emb(f ), V = W V l • Emb(f ).<label>(11)</label></formula><p>(2) Adaptive layer normalization: Inspired by the success of space-time self-attention in video modeling, we modulate the spatial and temporal information of video representations using fMRI representations, as shown in Eq. ( <ref type="formula" target="#formula_20">12</ref>).</p><formula xml:id="formula_20">adaLN (z l , f ) = Emb(f ) scale LayerN orm(z l ) + Emb(f ) shif t .<label>(12)</label></formula><p>As shown in Figure <ref type="figure">11</ref>, we designed four different architectures for the Spatial-Temporal Fusion Layer (STFL). Under the same hyperparameters used in Section C, the training/validation loss curves for these architectures are presented in Figure <ref type="figure">12</ref>. As shown in Figure <ref type="figure">12</ref>, regardless of whether cross-attention or adaptive layer normalization is used, any interaction between fMRI representations and temporal information in the video leads to extreme instability in training and difficulty in convergence. We hypothesize that this is due to the significantly lower temporal resolution of fMRI compared to video stimuli, making it difficult to explicitly extract useful temporal information from fMRI. Therefore, in the design of the CMG, we only allow interaction between fMRI representations and spatial information in the video. For temporal information, we use a sparse causal random mask to implicitly learn motion information between frames.</p><p>E.2 A DETAILED PARAMETER SENSITIVITY ANALYSIS ON PATCH SIZE. We conducted detailed experiments to investigate the sensitivity of video reconstruction to patch size.</p><p>As shown in Table <ref type="table" target="#tab_10">7</ref>, setting a smaller patch size hinders the model's learning. This is because a smaller patch size results in a larger number of patches, forcing fine-grained patch-level interactions between fMRI and video representations. However, the low signal-to-noise ratio of fMRI does not support such fine-grained interactions, and the excessive noise may even degrade the quality of the video representations.</p><p>E.3 A DETAILED PARAMETER SENSITIVITY ANALYSIS ON λ 1 AND λ 2 .</p><p>() 1 = 0 () 2 = 0 We conducted a sensitivity analysis on the selection of λ 1 and λ 2 using sub1 from the CC2017 dataset. As shown in Figure <ref type="figure" target="#fig_16">13</ref>, when either λ 1 or λ 2 is set to 0, the semantic decider fails to converge, indicating that both the contrastive loss and projection loss play crucial roles in decoding semantic information. In Table <ref type="table">8</ref>, we set λ 1 = 0.01 and λ 2 = 0.5 to ensure that both loss terms are balanced during optimization. When either λ 1 or λ 2 is adjusted, breaking this balance does not affect the In addition to the reconstruction task, we evaluate the retrieval task on the CC2017 dataset. We use top-10 accuracy and top-100 accuracy as evaluation metrics. To assess the model's generalization capability, we perform retrieval not only on the CC2017 test set with 1,200 samples ('Small') but also on an extended stimulus set. Specifically, we augment the collection with 3,040 video clips from the HCP dataset, resulting in a total of 4,240 samples ('Large').</p><p>As shown in Table <ref type="table" target="#tab_13">13</ref>, our model achieves superior performance across all three subjects in the CC2017 dataset. Compared to Mind-Video, our model exhibits a smaller performance drop when the stimulus set is expanded to the 'Large' scale, demonstrating its generalization capability. Notably, Wen's results surpass those of Kupershmidt largely and are comparable to Mind-Video. This can be attributed to their approach of reconstructing a single frame from each video segment, simplifying the video reconstruction task into an image reconstruction task, thereby achieving superior performance on the retrieval metrics.</p><p>E.6.2 COMPREHENSIVE QUANTITATIVE COMPARISON RESULTS ON THE CC2017 DATASET. The individual quantitative comparison results on the CC2017 dataset for the three subjects are displayed in Table <ref type="table" target="#tab_14">14</ref>.  In Figure <ref type="figure" target="#fig_18">16</ref>, we present additional reconstruction results from the CC2017 dataset, demonstrating that our model does not overfit despite the limited data volume. It is capable of decoding a rich array of video clips, such as two people conversing, an airplane in flight, and a dog turning its head, among others. These video clips encompass a wide range of natural scenarios found in everyday life, including human activities, animal behaviors, and natural landscapes. Additionally, to provide a comprehensive and objective assessment of our model, we also include some reconstruction failure cases in Figure <ref type="figure">17</ref>. The primary reasons for these failures are twofold.  <ref type="table" target="#tab_16">16</ref>, respectively. Although Table <ref type="table" target="#tab_16">16</ref> indicates that our model outperforms the earlier baseline in nearly all metrics across the 10 subjects, the reconstructed results presented in Figure <ref type="figure" target="#fig_20">19</ref> are not entirely satisfactory, with some video frames semantically misaligned with the stimulus video. We attribute this to the scarcity of training data,  which renders the video reconstruction task on this dataset challenging, given that the data volume per subject is approximately one-fifth of that in the CC2017 dataset.</p><p>Based on the experimental results across multiple datasets, we can draw the following conclusions:</p><p>(1) The volume of training data from a single subject significantly influences the performance of current video reconstruction models, with greater sample size and data diversity leading to better reconstruction performance. (2) There is an urgent need to develop a new model using incremental  <ref type="formula">2024</ref>)). To explore whether decoders trained with contrastive learning loss demonstrate advantages over those trained with mean squared error (MSE) loss in accuracy and generalization, we conduct the retrieval task and employ t-distributed stochastic neighbor embedding (t-SNE) for visualization. The retrieval task involves identifying which specific visual stimulus has evoked a given fMRI response from a predefined set. This task differs from the classification task, which only recognizes the category of the visual stimulus that evoked the fMRI response. In contrast, the retrieval task demands a finer level of detail, requiring not just the correct classification but also the precise identification of the particular visual stimulus. During the training process of the semantic decoder, we align the fMRI representation to the pre-trained CLIP representational space via a tri-model contrastive learning loss. This design enables the semantic decoder to be not only applicable to video reconstruction task but also extends its utility to retrieval task.</p><p>To the best of our knowledge, no prior work has been reported on conducting retrieval task using the CC2017 dataset. Therefore, we train a simple linear regression model and a three-layer MLP as our baselines. We employ top-10 accuracy<ref type="foot" target="#foot_0">1</ref> and top-100 accuracy as evaluation metrics. To validate the model's generalization capability, we not only conduct the retrieval task on the CC2017 test set, comprising 1200 samples and termed the 'small test set', but also expand the stimulus set to enhance its scope. Specifically, we integrate 3040 video clips from the HCP dataset into our collection, creating an extended stimulus set totaling 4,240 samples, which we label as the 'large test set'. The experimental results of the retrieval task on the CC2017 dataset across three subjects are depicted in Figure <ref type="figure">20</ref> and Table <ref type="table" target="#tab_17">17</ref>. As demonstrated in Table <ref type="table" target="#tab_17">17</ref>, our model outperforms the baseline models under both 'small test set' and 'large test set' conditions. Notably, when the stimulus set is expanded to nearly four times its original size, the performance of our model does not experience a sharp decline. This suggests that aligning the model's latent space to the CLIP representational space through contrastive learning loss is beneficial for enhancing the model's generalization capability. As illustrated in Figure <ref type="figure" target="#fig_21">21</ref>, we utilize t-SNE to visualize the predicted CLIP representations of the models. The visualization results indicate that the linear regression model exhibits the weakest generalization when performing neural decoding tasks. Increasing the number of layers in the linear model can improve its generalization ability to some extent. However, model trained with a contrastive learning loss demonstrate superior generalization performance. Figure <ref type="figure">22</ref>: Supplementary ROI-wise importance map on subject 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Overview of the video reconstruction paradigms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall architecture of Mind-Animator, a two-stage video reconstruction model based on fMRI. Three decoders are trained during the fMRI-to-feature stage to disentangle semantic, structural, and motion feature from fMRI, respectively. In the feature-to-video stage, the decoded information is input into an inflated Text-to-Image (T2I) model for video reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of CMG with Temporal Module and fMRI guided Spatial Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reconstruction results on CC2017 dataset. Our reconstructed results are highlighted with a red box, while those of Wen and Nishimoto are delineated by blue and green boxes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The results of shuffle test on the CC2017 dataset. The experiment is repeated 5 times on 3 subjects, with the mean and std presented in subplots (a), (b), and (c), respectively. Paired t-tests with Bonferroni correction are performed, with significance denoted as p &lt; 0.001( * * * ), p &lt; 0.01( * * ), p &lt; 0.05( * ), and p &gt; 0.05(N S) for non-significant results.</figDesc><graphic coords="9,237.98,109.93,146.19,109.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Voxel-wise importance maps projected onto the visual cortex of subject 1. The lighter the color, the greater the weight of the voxel in the interpretation of feature.</figDesc><graphic coords="10,368.23,270.32,134.47,99.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: ROI-wise importance maps in the visual cortex of subject 1.</figDesc><graphic coords="10,240.55,273.24,130.16,97.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8</head><label>8</label><figDesc>Figure8(a) indicates that high-level visual cortical areas (HVC, such as MT) contribute more significantly to the decoding of semantic feature, accounting for 60.5% of the total, as shown in Figure9(a). Figure8(c) and 9 (c) indicates that both LVC and HVC contribute to the decoding of motion information, with significant weight attributed to MT and TPOJ. This observation is consistent with previous work<ref type="bibr" target="#b6">(Born &amp; Bradley (2005)</ref>), which validates the function of MT and TPOJ in visual motion perception and processing. We also identify the following findings in Figure9:(1) MT shows significant activation for semantic decoding. This observation aligns with the functional segregation and interaction between the dorsal and ventral pathways during dynamic visual input processing<ref type="bibr" target="#b30">(Ingle et al. (1982)</ref>). Specifically, the dorsal-dorsal pathway is associated with action control, whereas the ventral-dorsal pathway is involved in action understanding and recognition<ref type="bibr" target="#b53">(Rizzolatti &amp; Matelli (2003)</ref>). This finding aligns with the latter. (2) V1 is predominantly activated when decoding motion features, reflecting the visual system's parallel processing capability. Motion information in the dorsal pathway does not strictly follow hierarchical processing<ref type="bibr" target="#b76">(Zeki &amp; Shipp (1988)</ref>). As noted by Nassi et al.<ref type="bibr" target="#b46">(Nassi &amp; Callaway (2009)</ref>), V1 directly projects motion-related information, such as direction and speed, to MT for further processing. For detailed neurobiological explanations, please refer to the Appendix G.</figDesc><graphic coords="10,109.51,271.31,131.71,100.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>HUMAN VISION FROM BRAIN ACTIVITIES RECONSTRUCTING IMAGES FROM BRAIN ACTIVITIES Building on Haxby et al. (2001)'s seminal work, the field of neural decoding has seen a proliferation of tasks with significant implications for guiding research. These tasks can be broadly classified into three categories: stimulus classification, identification, and reconstruction, with the latter being the most challenging and the focus of our study. Traditional image reconstruction techniques rely on linear regression models to correlate fMRI with manually defined image features (Kay (2008);<ref type="bibr" target="#b45">Naselaris et al. (2009)</ref>;<ref type="bibr" target="#b15">Fujiwara et al. (2013)</ref>), yielding blurry results and a heavy reliance on manual feature selection. However, the advent of deep learning has revolutionized this domain. Deep neural networks (DNNs) have become increasingly prevalent for their ability to address the scarcity of stimulus-fMRI pairs through semi-supervised learning<ref type="bibr" target="#b9">(Chapelle et al. (2009)</ref>), as demonstrated by<ref type="bibr" target="#b3">Beliy et al. (2019)</ref> and<ref type="bibr" target="#b17">Gaziv et al. (2022)</ref>. Yet, these models often fail to capture discernible semantic information.<ref type="bibr" target="#b10">Chen et al. (2023)</ref> employed a pre-training and fine-tuning approach on fMRI data, leveraging methods akin to Masked Autoencoder(MAE)(He  et al. (2022a)) and Latent Diffusion Models(LDM)<ref type="bibr" target="#b54">(Rombach et al. (2022)</ref>) to improve reconstruction quality.<ref type="bibr" target="#b48">Ozcelik et al. (2022)</ref> and<ref type="bibr" target="#b21">Gu et al. (2022)</ref> utilized self-supervised models for feature extraction, followed by iterative optimization to refine the reconstruction process. The integration of semantic information from text, facilitated by Contrastive Language-Image Pre-Training (CLIP)<ref type="bibr" target="#b51">(Radford et al. (2021)</ref>), has been instrumental in reconstructing complex natural images.<ref type="bibr" target="#b39">Lin et al. (2022)</ref> and<ref type="bibr" target="#b62">Takagi &amp; Nishimoto (2023)</ref> demonstrated the potential of aligning fMRI with CLIP representations and mapping fMRI to text and image features for high-fidelity reconstruction. While rapid advancements have been made in stimulus reconstruction, with some researchers achieving reconstructions from brain signals that closely approximate the original stimuli, the majority of prior work has focused on static image reconstruction. This study, however, shifts the focus to the more challenging task of video reconstruction.RECONSTRUCTING VIDEOS FROM BRAIN ACTIVITIESCompared to image reconstruction, the challenge in video reconstruction lies in the significant discrepancy between the temporal resolution of fMRI (0.5Hz) and the frame rate of the stimulus video (30Hz), which presents a substantial challenge in modeling the mapping between fMRI signals and video content. To overcome the challenge,<ref type="bibr" target="#b47">Nishimoto et al. (2011)</ref> transformed the video reconstruction task into a identification problem, employing the Motion-Energy model<ref type="bibr" target="#b0">(Adelson &amp; Bergen (1985)</ref>) and Bayesian inference to reconstruct videos from a predefined video library. Subsequently,<ref type="bibr" target="#b22">Han et al. (2019)</ref> and<ref type="bibr" target="#b69">Wen et al. (2018)</ref> mapped brain responses to the feature spaces of DNN to reconstruct down-sampled (with the frame rate reduced to 1Hz) video stimuli. Specifically,Han et al. mapped  fMRI data to a VAEKingma &amp; Welling (2014) pretrained on the ImageNet ILSVRC2012 Russakovsky et al. (2015) dataset to reconstruct a single frame, while Wen et al. mapped fMRI data to the feature space of AlexNetKrizhevsky et al. (2012) and used a deconvolutional neural networkZeiler et al. (2010) to reconstruct a single frame. The aforementioned studies have preliminarily validated the feasibility of reconstructing video frames from fMRI. Wang et al. (2022) developed an f-CVGAN that learns temporal and spatial information in fMRI through separate discriminators<ref type="bibr" target="#b20">(Goodfellow et al. (2020)</ref>). To mitigate the scarcity of fMRI-video data,<ref type="bibr" target="#b37">Kupershmidt et al. (2022)</ref> utilized self-supervised learning<ref type="bibr" target="#b35">(Kingma &amp; Welling (2014)</ref>) to incorporate a large amount of unpaired video data. These efforts have validated the feasibility of video reconstruction from fMRI, albeit with a lack of explicit semantic information in the results.<ref type="bibr" target="#b11">Chen et al. (2024)</ref> utilized contrastive learning to map fMRI to the CLIP representation space and fine-tuned inflated Stable Diffusion<ref type="bibr" target="#b54">(Rombach et al. (2022)</ref>;<ref type="bibr" target="#b72">Wu et al. (2023)</ref>) on a video-text dataset as a video generation model, successfully reconstructing coherent videos with clear semantic information for the first time. However, Chen did not consider structure information such as color and position, and it was uncertain whether the motion information in the reconstructed videos originated from the fMRI or the video generation model.A.2 DIFFUSION MODELS Diffusion models<ref type="bibr" target="#b70">(Wijmans &amp; Baker (1995)</ref>;Ho et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The training and validation loss curves for subject 1 in the CC2017 dataset.</figDesc><graphic coords="23,242.37,264.73,128.10,108.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>For details, please refer to the open-source code. Published as a conference paper at ICLR 2025 Algorithm 2 N-trial n-way top-1 classification test 1: Input pre-trained classifiers C image (•), C video (•), video pair (Generated Video x, Corresponding GT Video x), mode(video-based or image-based) 2: Output success rate r ∈ [0, 1] 3: if mode='video-based' then 4: for N trials do 5: ŷ ← C video (x) get the ground-truth class 6: {p 0 , ..., p 399 } ← C video (x) get the output probabilities 7: {p ŷ , p y1 , ..., p yn-1 } ← pick n-1 random classes8: success if arg max y {p ŷ , p y1 , ..., p yn-1 } = ŷ image (x i ) get the ground-truth class 15: {p 0 , ..., p 999 } ← C image (x i ) get the output probabilities 16: {p ŷi , p yi,1 , ..., p yi,n-1 } ← pick n-1 random classes 17: success if arg max yi {p ŷi , p yi,1 , ..., p yi,n-1 } = ŷi 18: OF FREQUENTLY USED SYMBOLS The symbols frequently used in this work are defined in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Variants of the Spatial-Temporal Fusion Layer.</figDesc><graphic coords="26,299.77,584.34,115.71,86.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Loss curves for variants of λ 1 and λ 2 .</figDesc><graphic coords="27,317.70,382.01,158.23,118.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>EFigure 15 :</head><label>15</label><figDesc>Figure 15: The reconstruction results on three subjects from the CC2017 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: More reconstruction results on the CC2017 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Figure 17: Reconstruction failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: The reconstruction results on four subjects from the Algonauts2021 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: t-SNE visualization presents a comparative analysis of the representations predicted by three decoders on 1200 samples from the CC2017 test set: a simple linear regression model trained with MSE loss, our semantic decoder trained with MSE loss, and our semantic decoder trained with contrastive learning loss. The red dots represent the real CLIP representations, while the blue dots denote the representations predicted by the decoders. The absolute Pearson correlation coefficient (i.e. r) between the real and predicted representations is displayed above each subfigure.</figDesc><graphic coords="37,110.37,318.96,128.61,92.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Characteristics of the video-fMRI datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell>Adopted subjects</cell><cell>TR</cell><cell cols="2">Train samples Test samples</cell></row><row><cell>CC2017 (Wen et al. (2018))</cell><cell>3</cell><cell>2s</cell><cell>4320</cell><cell>1200</cell></row><row><cell>HCP (Marcus et al. (2011))</cell><cell>3</cell><cell>1s</cell><cell>2736</cell><cell>304</cell></row><row><cell>Algonauts2021 (Cichy et al. (2021))</cell><cell>10</cell><cell>1.75s</cell><cell>900</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison of reconstruction results on the CC2017 dataset. All metrics are averaged across all samples and three subjects, with the best results highlighted in bold and the second-best results underlined. EV refers to the External Video Dataset. The symbol † refers to using Stable Diffusion fine-tuned on video data. 100 sample sets were constructed using the bootstrap method for hypothesis testing. Colors reflect statistical significance (Wilcoxon test for paired samples) compared to our model. p &lt; 0.0001 (purple); p &lt; 0.01 (pink); p &lt; 0.05 (yellow); p &gt; 0.05 (green).</figDesc><table><row><cell>Models</cell><cell>Training data</cell><cell cols="5">Semantic-level ↑ 2-way-I 2-way-V VIFI SSIM PSNR Hue-pcc CLIP↑ EPE↓ Pixel-level ↑ ST-level</cell></row><row><cell>Nishimoto (Nishimoto et al. (2011))</cell><cell>CC2017</cell><cell>0.742</cell><cell>--</cell><cell>--0.119 8.383</cell><cell>0.737</cell><cell>----</cell></row><row><cell>Wen (Wen et al. (2018))</cell><cell>CC2017</cell><cell>0.771</cell><cell>--</cell><cell>--0.130 8.031</cell><cell>0.637</cell><cell>----</cell></row><row><cell cols="3">Kupershmidt (Kupershmidt et al. (2022)) CC2017+EV 0.769</cell><cell cols="4">0.768 0.591 0.140 10.637 0.616 0.382 --</cell></row><row><cell>f-CVGAN (Wang et al. (2022))</cell><cell>CC2017</cell><cell>0.721</cell><cell cols="3">0.777 0.592 0.108 11.043 0.583</cell><cell>0.399 6.344</cell></row><row><cell>Mind-video  † (Chen et al. (2024))</cell><cell cols="2">CC2017+HCP 0.797</cell><cell cols="2">0.848 0.593 0.177 8.868</cell><cell cols="2">0.768 0.409 6.125</cell></row><row><cell>Ours</cell><cell>CC2017</cell><cell>0.805</cell><cell cols="2">0.830 0.608 0.321 9.220</cell><cell cols="2">0.786 0.425 5.422</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of reconstruction results on the HCP dataset. The full table can be found in Appendix E.7.</figDesc><table><row><cell>Models</cell><cell>Semantic-level ↑ 2-way-I</cell><cell cols="2">Pixel-level ↑ SSIM PSNR Hue-pcc</cell></row><row><cell>Nishimoto</cell><cell>0.658</cell><cell cols="2">0.321 11.316 0.645</cell></row><row><cell>Wen</cell><cell>0.702</cell><cell cols="2">0.058 10.197 0.727</cell></row><row><cell>f-CVGAN</cell><cell>--</cell><cell cols="2">0.159 13.033 --</cell></row><row><cell>Mind-video</cell><cell>0.779</cell><cell>0.116 9.275</cell><cell>0.793</cell></row><row><cell>Ours</cell><cell>0.786</cell><cell cols="2">0.344 11.233 0.829</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison of reconstruction results on the Algonauts 2021 dataset. The full table can be found in Appendix E.8.</figDesc><table><row><cell>Models</cell><cell>Semantic-level ↑ 2-way-I</cell><cell cols="2">Pixel-level ↑ SSIM PSNR Hue-pcc</cell></row><row><cell>Nishimoto</cell><cell>0.687</cell><cell>0.443 9.578</cell><cell>0.666</cell></row><row><cell>Wen</cell><cell>0.625</cell><cell>0.172 8.822</cell><cell>0.627</cell></row><row><cell>Mind-video</cell><cell>0.681</cell><cell>0.124 8.673</cell><cell>0.796</cell></row><row><cell>Ours</cell><cell>0.701</cell><cell cols="2">0.465 10.989 0.833</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation</figDesc><table><row><cell>Models</cell><cell cols="6">Semantic-level ↑ 2-way-I 2-way-V VIFI-score SSIM PSNR Hue-pcc CLIP-pcc↑ EPE↓ Pixel-level ↑ ST-level</cell></row><row><cell cols="2">w/o Semantic 0.679</cell><cell>0.766</cell><cell>0.523</cell><cell>0.097 8.005 0.737</cell><cell>0.123</cell><cell>8.719</cell></row><row><cell cols="2">w/o Structure 0.789</cell><cell>0.814</cell><cell>0.555</cell><cell>0.184 8.712 0.791</cell><cell>0.260</cell><cell>7.683</cell></row><row><cell>w/o Motion</cell><cell>0.674</cell><cell>0.789</cell><cell>0.585</cell><cell>0.136 8.611 0.715</cell><cell>0.376</cell><cell>6.374</cell></row><row><cell>Full Model</cell><cell>0.812</cell><cell>0.839</cell><cell>0.604</cell><cell>0.319 9.116 0.778</cell><cell>0.413</cell><cell>5.572</cell></row></table><note><p>study about our proposed decoders on subject 1 of CC2017 dataset. More results on subject 2 and 3 can be found in Appendix 11 and 12. 100 sample sets were constructed using the bootstrap method for hypothesis testing. Colors reflect statistical significance (Wilcoxon test for paired samples) compared to the Full Model. p &lt; 0.0001 (purple); p &lt; 0.01 (pink); p &lt; 0.05 (yellow); p &gt; 0.05 (green).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This work was supported in part by the Strategic Priority Research Program of the Chinese Academy of Sciences (XDB0930000); in part by Beijing Natural Science Foundation under Grant L243016; and in part by the National Natural Science Foundation of China under Grant 62206284 and 62020106015.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Reconstructing Videos from Brain Activities . . . . . . . . . . . . . . . . . . . . . 2.2 Diffusion Models for Video Generation . . . . . . . . . . . . . . . . . . . . . . . .3 HCP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Algonauts2021 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Hyperparameter Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The impact of different spatial-temporal architecture designs. . . . . . . . . . . . . E.2 A Detailed Parameter Sensitivity Analysis on Patch size. . . . . . . . . . . . . . . E.3 A Detailed Parameter Sensitivity Analysis on λ 1 and λ 2 . . . . . . . . . . . . . . . E.4 A Detailed Parameter sensitivity analysis on α and mask ratio. . . . . . . . . . . . E.5 Supplementary Ablation Studies on Subjects 2 and 3 of the CC2017 Dataset . . . . E.6 Further Results on the CC2017 Dataset . . . . . . . . . . . . . . . . . . . . . . . . E.6.1 THE FINE-GRAINED RETRIEVAL EXPERIMENTAL RESULTS. . . . . E.6.2 Comprehensive quantitative comparison results on the CC2017 Dataset. . . E.6.3 More Reconstruction Results on Multiple Subjects . . . . . . . . . . . . . E.7 Further Results on the HCP Dataset . . . . . . . . . . . . . . . . . . . . . . . . . E.8 Further Results on the Algonauts2021 Dataset . . . . . . . . . . . . . . . . . . . . F Further Results on Interpretability Analysis F.1 Why Utilize Contrastive Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Which brain regions are responsible for decoding different features, respectively? . G Supplementary Knowledge on the Mechanisms of Dynamic Visual Information Processing in the Visual Cortex.</figDesc><table><row><cell>Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity ----Appendix----CONTENTS 1 Introduction 2 Related work 2.1 7 Conclusion 8 ACKNOWLEDGEMENT 9 Ethics Statement A Related Work C Implementation Details C.1 E Additional Experimental Results E.1</cell></row></table><note><p><p><p><p>:</p>(1) Mind reading requires brain activity recording devices with very high spatial resolution, and data acquisition systems like fMRI, which possess high spatial resolution, are not easily portable; (2) Although there are now several portable brain activity recording devices, achieving mind reading would require the subject to maintain intense focus and cooperate with the data collection process.</p>3 Methodology</p>3.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 FMRI-to-feature Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Feature-to-video Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiment 4.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Results 5.1 Comparative Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Interpretability Analysis 6.1 Have we truly decoded motion information from fMRI? . . . . . . . . . . . . . . . 6.2 Which brain regions are responsible for decoding different features, respectively? . A.1 Reconstructing Human Vision from Brain Activities . . . . . . . . . . . . . . . . . A.2 Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Diffusion Models for Video Generation . . . . . . . . . . . . . . . . . . . . . . . B Data Preprocessing B.1 Video Captioning with BLIP2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 CC2017 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . BC.2 Evaluation Metric Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . D Model Architecture D.1 Definition of Frequently Used Symbols . . . . . . . . . . . . . . . . . . . . . . . D.2 Consistency Motion Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Text-to-Image Network Inflation . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.3 DIFFUSION MODELS FOR VIDEO GENERATION After achieving significant progress in text-to-image (T2I) generation tasks, diffusion models have piqued the interest of researchers in exploring their potential for text-to-video (T2V) generation. The pioneering work byHo et al. (2022b)  , introducing the 3D diffusion U-Net, marked significant progress in applying Diffusion Models to video generation. This was followed by further advancements byHo  et al. (2022a)  , who utilized a cascaded sampling framework and super-resolution method to generate high-resolution videos. Subsequent contributions have expanded upon this work, notably with the incorporation of a temporal attention mechanism over frames bySinger et al.  </figDesc><table /><note><p><p><p><p><p><p>(2022)  </p>in Make-A-Video.</p><ref type="bibr" target="#b77">Zhou et al. (2022)</ref> </p>with MagicVideo, and</p>He et al. (2022b)  </p>with LVDM, have integrated this mechanism into latent Diffusion Models, significantly enhancing video generation capabilities.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>leading to the random selection of one text description from either 1st or 6th frame to represent the video clip. Otherwise, indicating a scene transition, we concatenate the two text descriptions with ', then' to provide a comprehensive textual description of the video clip. PyTorch code for the video captioning process is depicted in Algorithm 1. et al. (2019) , we utilize BOLD signals with a 4-second lag to represent the movie stimulus responses, thereby accounting for the hemodynamic response delay. Due to the difficulty in directly acquiring fMRI data from multiple trials, we directly utilize the parcellation of the human cerebral cortex proposed by<ref type="bibr" target="#b19">Glasser et al. (2016)</ref> to extract voxels within the activated visual cortex. The resulting ROIs we extract include: V1, V2, V3, hV4, PPA, FFA, LO, PHC, MT, MST, and TPOJ, totaling 5820 voxels. Following the work of<ref type="bibr" target="#b47">Nishimoto et al. (2011)</ref> and<ref type="bibr" target="#b22">Han et al. (2019)</ref> , we utilize BOLD signals with a 4-second lag to represent the movie stimulus responses, thereby accounting for the hemodynamic response delay. Given that no prior work has conducted video reconstruction experiments on these three subjects from the HCP dataset, except for<ref type="bibr" target="#b68">Wang et al. (2022)</ref>, we randomly shuffle all video segments and allocate 90% for the training set, with the remaining 10% reserved for the test set.</figDesc><table><row><cell>B.3 HCP</cell></row><row><cell>This dataset is part of the Human Connectome Project (HCP) (Marcus et al. (2011)), encompassing</cell></row><row><cell>BOLD (blood-oxygen-level dependent) responses from 158 subjects. For the subsequent experiments,</cell></row><row><cell>three subjects (100610, 102816, and 104416) are randomly selected from this dataset. Data acquisition</cell></row><row><cell>is performed using a 7T MRI scanner with a spatial resolution of 1.6 millimeters and a repetition</cell></row><row><cell>time (TR) of 1 second. The utilized BOLD signals undergo standard HCP preprocessing procedures,</cell></row><row><cell>which include correction for head motion and distortion, high-pass filtering, and removal of temporal</cell></row><row><cell>artifacts via independent component analysis (ICA). The preprocessed BOLD responses are then</cell></row><row><cell>registered to the MNI standard space.</cell></row><row><cell>), we</cell></row><row><cell>employed BLIP2 (Li et al. (2023)) to obtain textual descriptions for each video clip, with lengths not</cell></row><row><cell>exceeding 20 words.</cell></row><row><cell>B.1 VIDEO CAPTIONING WITH BLIP2</cell></row><row><cell>Due to the absence of open-source video captioning models at the time of experimentation, we utilize</cell></row><row><cell>the image captioning model BLIP2 to obtain text descriptions corresponding to each video clip. Two</cell></row><row><cell>considerations are paramount in the design of the video captioning process: (1) the length of the</cell></row></table><note><p>text descriptions should not be excessively long, and (2) the text descriptions must reflect the scene transitions within the video segments. To achieve the first objective, we employ the following prompt: 'Question: What does this image describe? Answer in 20 words or less. Answer:'. Regarding the second point, we extracte 1st and 6th frames from every set of 8 frames, inputting them into BLIP2 to obtain their text descriptions. Subsequently, we calculate the CLIP similarity between each of the two text descriptions and 3rd frame. If the difference is no more than 0.05, it indicates minimal scene change, B.4 ALGONAUTS2021 This dataset is publicly released for the 2021 Algonauts Challenge (Cichy et al. (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Definition of frequently used symbols</figDesc><table><row><cell>Symbol</cell><cell>Definition</cell></row><row><cell>X</cell><cell>Voxel space</cell></row><row><cell>Y</cell><cell>Pixel space</cell></row><row><cell>Z(k)</cell><cell>Feature space, k ∈ {semantic, structure, motion}</cell></row><row><cell>D(k)</cell><cell>Feature decoder, k ∈ {semantic, structure, motion}</cell></row><row><cell>Φ(•)</cell><cell>Encoder of pretrained VQ-VAE</cell></row><row><cell>M LP (•)</cell><cell>Trainable multilayer perceptron</cell></row><row><cell>Emb(•)</cell><cell>Linear embedding layer</cell></row><row><cell>n</cell><cell>Number of voxels</cell></row><row><cell>x i</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Parameter sensitivity analysis on Patch size.</figDesc><table><row><cell>Patch size</cell><cell cols="7">Semantic-level ↑ 2-way-I 2-way-V VIFI-score SSIM PSNR Hue-pcc CLIP-pcc↑ EPE↓ Pixel-level ↑ ST-level</cell></row><row><cell>4</cell><cell>0.704</cell><cell>0.792</cell><cell>0.495</cell><cell>0.214 10.257</cell><cell>0.756</cell><cell>0.072</cell><cell>8.473</cell></row><row><cell>8</cell><cell>0.698</cell><cell>0.794</cell><cell>0.477</cell><cell>0.148 9.744</cell><cell>0.720</cell><cell>0.043</cell><cell>9.935</cell></row><row><cell>16</cell><cell>0.675</cell><cell>0.778</cell><cell>0.481</cell><cell>0.139 9.773</cell><cell>0.761</cell><cell>0.049</cell><cell>7.625</cell></row><row><cell>32</cell><cell>0.718</cell><cell>0.798</cell><cell>0.503</cell><cell>0.176 9.330</cell><cell>0.751</cell><cell>0.094</cell><cell>7.671</cell></row><row><cell>64</cell><cell>0.812</cell><cell>0.841</cell><cell>0.602</cell><cell>0.321 9.124</cell><cell>0.774</cell><cell>0.425</cell><cell>5.580</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Ablation study on subject 2 of CC2017 dataset. 100 sample sets were constructed using the bootstrap method for hypothesis testing. Colors reflect statistical significance (Wilcoxon test for paired samples) compared to the Full Model.</figDesc><table><row><cell>p &lt; 0.0001 (purple); p &lt; 0.01 (pink); p &lt; 0.05 (yellow); p &gt; 0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Ablation study on subject 3 of CC2017 dataset. 100 sample sets were constructed using the bootstrap method for hypothesis testing. Colors reflect statistical significance (Wilcoxon test for paired samples) compared to the Full Model. p &lt; 0.0001 (purple); p &lt; 0.01 (pink); p &lt; 0.05 (yellow); p &gt; 0.05</figDesc><table><row><cell>(green).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="7">Semantic-level↑ 2-way-I 2-way-V VIFI-score SSIM PSNR Hue-pcc CLIP-pcc↑ EPE↓ Pixel-level↑ ST-level</cell></row><row><cell cols="2">w/o Semantic 0.673</cell><cell>0.778</cell><cell>0.523</cell><cell>0.084 8.109</cell><cell>0.738</cell><cell>0.136</cell><cell>7.891</cell></row><row><cell cols="2">w/o Structure 0.810</cell><cell>0.831</cell><cell>0.566</cell><cell>0.186 8.619</cell><cell>0.794</cell><cell>0.299</cell><cell>7.415</cell></row><row><cell>w/o Motion</cell><cell>0.808</cell><cell>0.826</cell><cell>0.583</cell><cell>0.272 8.953</cell><cell>0.779</cell><cell>0.366</cell><cell>6.588</cell></row><row><cell>Full Model</cell><cell>0.792</cell><cell>0.823</cell><cell>0.607</cell><cell>0.348 9.287</cell><cell>0.791</cell><cell>0.419</cell><cell>5.356</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Retrieval accuracy (%) on CC2017 dataset. For the 'small test set', the chance-level accuracies for top-10 and top-100 accuracy are 0.83% and 8.3%, respectively. For the 'large test set', the chance-level accuracies for top-10 and top-100 accuracy are 0.24% and 2.4%, respectively. Five random seeds were used during the training of the semantic decoder, and the results were tested for statistical significance. * denotes our performance is significantly better than the compared method (Wilcoxon test, p&lt;0.05).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Subjet1</cell><cell cols="2">Subjet2</cell><cell cols="2">CC2017</cell><cell cols="2">Subjet3</cell><cell>Average</cell></row><row><cell>Model</cell><cell cols="10">Test set top-10 top-100 top-10 top-100 top-10 top-100 top-10 top-100</cell></row><row><cell>Wen (Wen et al. (2018))</cell><cell>Small</cell><cell>2.17  *</cell><cell>19.50  *</cell><cell>3.33  *</cell><cell cols="2">19.17  *</cell><cell cols="2">--</cell><cell>--</cell><cell>2.75  *</cell><cell>19.33  *</cell></row><row><cell>Kupershmidt (Kupershmidt et al. (2022))</cell><cell>Small</cell><cell>1.09  *</cell><cell>8.57  *</cell><cell>0.92  *</cell><cell cols="2">8.24  *</cell><cell cols="2">0.84  *</cell><cell>8.24  *</cell><cell>0.95  *</cell><cell>8.35  *</cell></row><row><cell>Mind-video (Chen et al. (2024))</cell><cell>Small</cell><cell>3.22  *</cell><cell>19.08  *</cell><cell>2.75  *</cell><cell cols="2">16.83  *</cell><cell cols="2">3.58  *</cell><cell>22.08  *</cell><cell>3.18  *</cell><cell>19.33  *</cell></row><row><cell>Ours</cell><cell>Small</cell><cell>3.08</cell><cell>22.58</cell><cell>4.75</cell><cell cols="2">26.90</cell><cell cols="2">4.50</cell><cell>24.67</cell><cell>4.11</cell><cell>24.72</cell></row><row><cell>Wen (Wen et al. (2018))</cell><cell>Large</cell><cell>1.41  *</cell><cell>11.58  *</cell><cell>2.08  *</cell><cell cols="2">9.58  *</cell><cell cols="2">--</cell><cell>--</cell><cell>1.75  *</cell><cell>10.58  *</cell></row><row><cell>Kupershmidt (Kupershmidt et al. (2022))</cell><cell>Large</cell><cell>0.17  *</cell><cell>2.94  *</cell><cell>0.17  *</cell><cell cols="2">2.77  *</cell><cell cols="2">0.25  *</cell><cell>2.18  *</cell><cell>0.19  *</cell><cell>2.63  *</cell></row><row><cell>Mind-video (Chen et al. (2024))</cell><cell>Large</cell><cell>1.75  *</cell><cell>7.17  *</cell><cell>0.83  *</cell><cell cols="2">5.17  *</cell><cell cols="2">1.25  *</cell><cell>9.00  *</cell><cell>1.28  *</cell><cell>7.11  *</cell></row><row><cell>Ours</cell><cell>Large</cell><cell>2.17</cell><cell>12.50</cell><cell>2.25</cell><cell cols="2">17.00</cell><cell cols="2">2.75</cell><cell>16.42</cell><cell>2.39</cell><cell>15.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Quantitative comparison of reconstruction results across three subjects from the CC2017 dataset. For the 2-way-I and 2-way-V metrics, 100 repetitions were conducted, while other metrics were evaluated using 100 bootstrap trials. All metrics are averaged over the entire test set. The best results are highlighted in bold, and the second-best are underlined. Colors indicate statistical significance (Wilcoxon test for paired samples) compared to our model. p &lt; 0.0001 (purple); p &lt; 0.01</figDesc><table><row><cell cols="2">(pink); p &lt; 0.05 (yellow); p &gt; 0.05 (green).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub ID</cell><cell>Models</cell><cell cols="8">Semantic-level ↑ 2-way-I 2-way-V VIFI-score SSIM PSNR Hue-pcc CLIP-pcc↑ EPE↓ Pixel-level ↑ ST-level</cell></row><row><cell></cell><cell>Nishimoto (Nishimoto et al. (2011))</cell><cell>0.727</cell><cell>--</cell><cell>--</cell><cell cols="2">0.116 8.012</cell><cell>0.753</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Wen (Wen et al. (2018))</cell><cell>0.758</cell><cell>--</cell><cell>--</cell><cell cols="2">0.114 7.646</cell><cell>0.647</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 01</cell><cell cols="2">Kupershmidt (Kupershmidt et al. (2022)) 0.764 f-CVGAN (Wang et al. (2022)) 0.713</cell><cell>0.771 0.773</cell><cell>0.585 0.596</cell><cell cols="2">0.135 8.761 0.118 11.432</cell><cell>0.606 0.589</cell><cell>0.386 0.402</cell><cell>--6.348</cell></row><row><cell></cell><cell>Mind-video (Chen et al. (2024))</cell><cell>0.792</cell><cell>0.853</cell><cell>0.587</cell><cell cols="2">0.171 8.662</cell><cell>0.760</cell><cell>0.408</cell><cell>6.119</cell></row><row><cell></cell><cell>Ours</cell><cell>0.812</cell><cell>0.841</cell><cell>0.602</cell><cell cols="2">0.321 9.124</cell><cell>0.774</cell><cell>0.425</cell><cell>5.580</cell></row><row><cell></cell><cell>Nishimoto (Nishimoto et al. (2011))</cell><cell>0.787</cell><cell>--</cell><cell>--</cell><cell cols="2">0.112 8.592</cell><cell>0.713</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Wen (Wen et al. (2018))</cell><cell>0.783</cell><cell>--</cell><cell>--</cell><cell cols="2">0.145 8.415</cell><cell>0.626</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 02</cell><cell cols="2">Kupershmidt (Kupershmidt et al. (2022)) 0.776 f-CVGAN (Wang et al. (2022)) 0.727</cell><cell>0.766 0.779</cell><cell>0.591 0.596</cell><cell cols="2">0.157 11.914 0.107 10.940</cell><cell>0.601 0.589</cell><cell>0.382 0.404</cell><cell>--6.277</cell></row><row><cell></cell><cell>Mind-video (Chen et al. (2024))</cell><cell>0.789</cell><cell>0.842</cell><cell>0.595</cell><cell cols="2">0.172 8.929</cell><cell>0.773</cell><cell>0.409</cell><cell>6.062</cell></row><row><cell></cell><cell>Ours</cell><cell>0.811</cell><cell>0.827</cell><cell>0.615</cell><cell cols="2">0.292 9.250</cell><cell>0.791</cell><cell>0.429</cell><cell>5.329</cell></row><row><cell></cell><cell>Nishimoto (Nishimoto et al. (2011))</cell><cell>0.712</cell><cell>--</cell><cell>--</cell><cell cols="2">0.128 8.546</cell><cell>0.746</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Wen (Wen et al. (2018))</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 03</cell><cell cols="2">Kupershmidt (Kupershmidt et al. (2022)) 0.767 f-CVGAN (Wang et al. (2022)) 0.722</cell><cell>0.766 0.778</cell><cell>0.597 0.584</cell><cell cols="2">0.128 11.237 0.098 10.758</cell><cell>0.641 0.572</cell><cell>0.377 0.392</cell><cell>--6.408</cell></row><row><cell></cell><cell>Mind-video (Chen et al. (2024))</cell><cell>0.811</cell><cell>0.848</cell><cell>0.597</cell><cell cols="2">0.187 9.013</cell><cell>0.771</cell><cell>0.410</cell><cell>6.193</cell></row><row><cell></cell><cell>Ours</cell><cell>0.792</cell><cell>0.823</cell><cell>0.607</cell><cell cols="2">0.349 9.287</cell><cell>0.794</cell><cell>0.421</cell><cell>5.356</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Quantitative comparison of reconstruction results across three subjects from the HCP dataset. For the 2-way-I and 2-way-V metrics, 100 repetitions were conducted, while other metrics were evaluated using 100 bootstrap trials. All metrics are averaged over the entire test set. The best results are highlighted in bold, and the second-best are underlined. Colors indicate statistical significance (Wilcoxon test for paired samples) compared to our model. p &lt; 0.0001 (purple); p &lt; 0.01</figDesc><table><row><cell cols="2">(pink); p &lt; 0.05 (yellow); p &gt; 0.05 (green).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub ID</cell><cell>Models</cell><cell cols="7">Semantic-level ↑ 2-way-I 2-way-V VIFI-score SSIM PSNR Hue-pcc CLIP-pcc↑ EPE↓ Pixel-level ↑ ST-level</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.658</cell><cell>--</cell><cell>--</cell><cell cols="2">0.307 11.711 0.649</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Wen (Wen et al. (2018))</cell><cell>0.728</cell><cell>--</cell><cell>--</cell><cell cols="2">0.052 10.805 0.751</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 01</cell><cell>f-CVGAN (Wang et al. (2022))</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell cols="2">0.154 13.200 --</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Mind-video (Chen et al. (2024))</cell><cell>0.798</cell><cell>0.752</cell><cell>0.605</cell><cell>0.123 9.302</cell><cell>0.774</cell><cell>0.486</cell><cell>12.746</cell></row><row><cell></cell><cell>Ours</cell><cell>0.819</cell><cell>0.783</cell><cell>0.613</cell><cell cols="2">0.325 10.757 0.820</cell><cell>0.476</cell><cell>7.825</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.661</cell><cell>--</cell><cell>--</cell><cell cols="2">0.338 11.249 0.643</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Wen (Wen et al. (2018))</cell><cell>0.688</cell><cell>--</cell><cell>--</cell><cell cols="2">0.055 10.475 0.720</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 02</cell><cell>f-CVGAN (Wang et al. (2022))</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell cols="2">0.178 13.700 --</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Mind-video (Chen et al. (2024))</cell><cell>0.761</cell><cell>0.777</cell><cell>0.611</cell><cell>0.115 9.414</cell><cell>0.804</cell><cell>0.483</cell><cell>7.358</cell></row><row><cell></cell><cell>Ours</cell><cell>0.756</cell><cell>0.759</cell><cell>0.609</cell><cell cols="2">0.371 11.894 0.834</cell><cell>0.485</cell><cell>6.624</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.657</cell><cell>--</cell><cell>--</cell><cell cols="2">0.319 10.988 0.645</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Wen (Wen et al. (2018))</cell><cell>0.691</cell><cell>--</cell><cell>--</cell><cell>0.067 9.312</cell><cell>0.710</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 03</cell><cell>f-CVGAN (Wang et al. (2022))</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell cols="2">0.147 12.200 --</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Mind-video (Chen et al. (2024))</cell><cell>0.779</cell><cell>0.778</cell><cell>0.612</cell><cell>0.118 9.109</cell><cell>0.803</cell><cell>0.529</cell><cell>7.767</cell></row><row><cell></cell><cell>Ours</cell><cell>0.781</cell><cell>0.793</cell><cell>0.634</cell><cell cols="2">0.336 11.018 0.834</cell><cell>0.573</cell><cell>6.792</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Quantitative comparison of reconstruction results across ten subjects from the Algo-nauts2021 dataset. For the 2-way-I and 2-way-V metrics, 100 repetitions were conducted, while other metrics were evaluated using 100 bootstrap trials. All metrics are averaged over the entire test set. The best results are highlighted in bold, and the second-best are underlined. Colors indicate statistical significance (Wilcoxon test for paired samples) compared to our model. p &lt; 0.0001 (purple); p &lt; 0.01 learning or cross-subject learning methods that can be trained using data collected from different subjects, which we consider as a direction for future research.F FURTHER RESULTS ON INTERPRETABILITY ANALYSISF.1 WHY UTILIZE CONTRASTIVE LEARNING? It is noted that there has been an increasing body of work utilizing contrastive learning for neural decoding<ref type="bibr" target="#b11">(Chen et al. (2024)</ref>;<ref type="bibr" target="#b57">Scotti et al. (2024)</ref>;Benchetrit et al. (</figDesc><table><row><cell cols="2">(pink); p &lt; 0.05 (yellow); p &gt; 0.05 (green).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub ID</cell><cell>Models</cell><cell cols="7">Semantic-level ↑ 2-way-I 2-way-V VIFI-score SSIM PSNR Hue-pcc CLIP-pcc↑ EPE↓ Pixel-level ↑ ST-level</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.688</cell><cell>--</cell><cell>--</cell><cell>0.446 9.626</cell><cell>0.672</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 01</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.653 0.702</cell><cell>--0.761</cell><cell>--0.568</cell><cell>0.147 9.802 0.135 8.642</cell><cell>0.653 0.794</cell><cell>--0.277</cell><cell>--8.368</cell></row><row><cell></cell><cell>Ours</cell><cell>0.722</cell><cell>0.790</cell><cell>0.599</cell><cell cols="2">0.401 10.088 0.824</cell><cell>0.439</cell><cell>4.420</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.682</cell><cell>--</cell><cell>--</cell><cell>0.443 9.553</cell><cell>0.676</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 02</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.626 0.698</cell><cell>--0.769</cell><cell>--0.573</cell><cell>0.231 8.456 0.132 9.004</cell><cell>0.677 0.773</cell><cell>--0.265</cell><cell>--7.458</cell></row><row><cell></cell><cell>Ours</cell><cell>0.734</cell><cell>0.765</cell><cell>0.596</cell><cell cols="2">0.465 10.932 0.796</cell><cell>0.425</cell><cell>3.806</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.679</cell><cell>--</cell><cell>--</cell><cell>0.441 9.576</cell><cell>0.682</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 03</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.647 0.701</cell><cell>--0.729</cell><cell>--0.564</cell><cell>0.172 8.973 0.117 8.796</cell><cell>0.611 0.806</cell><cell>--0.271</cell><cell>--7.659</cell></row><row><cell></cell><cell>Ours</cell><cell>0.679</cell><cell>0.794</cell><cell>0.591</cell><cell cols="2">0.466 11.089 0.863</cell><cell>0.397</cell><cell>3.406</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.702</cell><cell>--</cell><cell>--</cell><cell>0.446 9.537</cell><cell>0.665</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 04</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.592 0.665</cell><cell>--0.785</cell><cell>--0.556</cell><cell>0.087 8.473 0.126 8.439</cell><cell>0.534 0.811</cell><cell>--0.254</cell><cell>--8.011</cell></row><row><cell></cell><cell>Ours</cell><cell>0.673</cell><cell>0.810</cell><cell>0.587</cell><cell cols="2">0.479 11.410 0.848</cell><cell>0.381</cell><cell>3.089</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.676</cell><cell>--</cell><cell>--</cell><cell>0.442 9.498</cell><cell>0.650</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 05</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.651 0.664</cell><cell>--0.757</cell><cell>--0.529</cell><cell>0.136 7.599 0.140 8.597</cell><cell>0.589 0.792</cell><cell>--0.263</cell><cell>--8.124</cell></row><row><cell></cell><cell>Ours</cell><cell>0.689</cell><cell>0.810</cell><cell>0.592</cell><cell cols="2">0.458 10.814 0.807</cell><cell>0.406</cell><cell>3.237</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.694</cell><cell>--</cell><cell>--</cell><cell>0.444 9.526</cell><cell>0.665</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 06</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.642 0.690</cell><cell>--0.751</cell><cell>--0.549</cell><cell>0.131 9.675 0.137 9.011</cell><cell>0.690 0.795</cell><cell>--0.266</cell><cell>--7.431</cell></row><row><cell></cell><cell>Ours</cell><cell>0.709</cell><cell>0.783</cell><cell>0.597</cell><cell cols="2">0.489 11.337 0.834</cell><cell>0.446</cell><cell>3.399</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.674</cell><cell>--</cell><cell>--</cell><cell>0.446 9.630</cell><cell>0.672</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 07</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.628 0.687</cell><cell>--0.721</cell><cell>--0.574</cell><cell>0.215 8.578 0.109 8.409</cell><cell>0.648 0.783</cell><cell>--0.209</cell><cell>--7.652</cell></row><row><cell></cell><cell>Ours</cell><cell>0.681</cell><cell>0.802</cell><cell>0.578</cell><cell cols="2">0.458 10.889 0.857</cell><cell>0.329</cell><cell>3.845</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.696</cell><cell>--</cell><cell>--</cell><cell>0.444 9.664</cell><cell>0.662</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 08</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.596 0.658</cell><cell>--0.764</cell><cell>--0.590</cell><cell>0.205 9.431 0.114 8.251</cell><cell>0.610 0.817</cell><cell>--0.204</cell><cell>--6.597</cell></row><row><cell></cell><cell>Ours</cell><cell>0.709</cell><cell>0.802</cell><cell>0.592</cell><cell cols="2">0.467 10.893 0.820</cell><cell>0.376</cell><cell>3.757</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.673</cell><cell>--</cell><cell>--</cell><cell>0.445 9.573</cell><cell>0.661</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 09</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.574 0.679</cell><cell>--0.780</cell><cell>--0.609</cell><cell>0.135 8.675 0.117 8.673</cell><cell>0.614 0.784</cell><cell>--0.267</cell><cell>--8.102</cell></row><row><cell></cell><cell>Ours</cell><cell>0.731</cell><cell>0.788</cell><cell>0.594</cell><cell cols="2">0.502 11.310 0.820</cell><cell>0.400</cell><cell>3.551</cell></row><row><cell></cell><cell cols="2">Nishimoto (Nishimoto et al. (2011)) 0.685</cell><cell>--</cell><cell>--</cell><cell>0.441 9.598</cell><cell>0.662</cell><cell>--</cell><cell>--</cell></row><row><cell>sub 10</cell><cell>Wen (Wen et al. (2018)) Mind-video (Chen et al. (2024))</cell><cell>0.638 0.663</cell><cell>--0.770</cell><cell>--0.563</cell><cell>0.265 8.565 0.108 8.912</cell><cell>0.644 0.809</cell><cell>--0.185</cell><cell>--7.524</cell></row><row><cell></cell><cell>Ours</cell><cell>0.684</cell><cell>0.777</cell><cell>0.590</cell><cell cols="2">0.465 11.128 0.858</cell><cell>0.408</cell><cell>3.533</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>Results of retrieval task on CC2017 dataset. For the 'small test set', the chance-level accuracies for top-10 and top-100 accuracy are 0.83% and 8.3%, respectively. For the 'large test set', the chance-level accuracies for top-10 and top-100 accuracy are 0.24% and 2.4%, respectively.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Subjet1</cell><cell cols="2">Subjet2</cell><cell cols="2">CC2017</cell><cell cols="2">Subjet3</cell><cell>Average</cell></row><row><cell>Model</cell><cell cols="10">Test set top-10 top-100 top-10 top-100 top-10 top-100 top-10 top-100</cell></row><row><cell>Linear + MSE</cell><cell>Small</cell><cell>2.25</cell><cell>16.92</cell><cell>3.17</cell><cell cols="2">22.25</cell><cell cols="2">2.75</cell><cell>19.75</cell><cell>2.72</cell><cell>19.64</cell></row><row><cell>3-layer MLP + MSE</cell><cell>Small</cell><cell>1.00</cell><cell>9.42</cell><cell>1.42</cell><cell cols="2">11.75</cell><cell cols="2">0.92</cell><cell>9.50</cell><cell>1.11</cell><cell>10.22</cell></row><row><cell>3-layer MLP + Contrast (Ours)</cell><cell>Small</cell><cell>3.08</cell><cell>22.58</cell><cell>4.75</cell><cell cols="2">26.90</cell><cell cols="2">4.50</cell><cell>24.67</cell><cell>4.11</cell><cell>24.72</cell></row><row><cell>Linear + MSE</cell><cell>Large</cell><cell>1.08</cell><cell>7.83</cell><cell>1.92</cell><cell cols="2">10.75</cell><cell cols="2">1.92</cell><cell>9.92</cell><cell>1.64</cell><cell>9.50</cell></row><row><cell>3-layer MLP + MSE</cell><cell>Large</cell><cell>0.42</cell><cell>3.58</cell><cell>0.75</cell><cell cols="2">6.83</cell><cell cols="2">0.25</cell><cell>1.08</cell><cell>0.47</cell><cell>3.83</cell></row><row><cell>3-layer MLP + Contrast (Ours)</cell><cell>Large</cell><cell>2.17</cell><cell>12.50</cell><cell>2.25</cell><cell cols="2">17.00</cell><cell cols="2">2.75</cell><cell>16.42</cell><cell>2.39</cell><cell>15.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Top-k accuracy is the percentage of queries where the correct item is among the top k results returned by a retrieval model.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>B.2 CC2017 CC2017 <ref type="bibr" target="#b69">(Wen et al. (2018)</ref>) dataset was first used in the work of <ref type="bibr" target="#b69">Wen et al. (2018)</ref> This dataset include fMRI data from 3 subjects who view a variety of movie clips (23°×23°) with a central fixation cross (0.8°×0.8°). Clips are divided into 18 training movies and 5 testing movies, each eight minutes long, presented 2 and 10 times to each subject, respectively. MRI (T1 and T2-weighted) and fMRI data (2-second temporal resolution) are acquired using a 3-T system. The fMRI volumes are processed for artifact removal, motion correction (6 DOF), registered to MNI space, and projected onto cortical surfaces coregistered to a template. To extract voxels in the activated visual areas, we calculate the correlation of the time series for each voxel's activation across 2 trials within the training set. Subsequently, we apply Fisher's z-transform to the computed correlations, average the results across 18 sessions, and identify the most significant 4500 voxels using a paired t-test to form a mask. This mask is computed separately for each of the 3 subjects on their respective training sets and then applied to both training and test set data, with the selected voxels averaged across trials. Following the work of <ref type="bibr" target="#b47">Nishimoto et al. (2011)</ref>   During the training of Semantic Decoder, we control the weighting of the contrastive learning loss between fMRI-text ( L BiInf oN CE (f, t) ) and fMRI-video ( L BiInf oN CE (f, v) ) through the hyperparameter α. We set different values for α (0, 0.25, 0.5, 0.75, 1.0), where α=0 signifies the exclusion of L BiInf oN CE (f, t), and α=1 signifies the exclusion of L BiInf oN CE (f, v). Table <ref type="table">9</ref> indicates that despite achieving optimal results for the three semantic-level metrics when α is set to 0.5, variations in α do not significantly affect the results, except when α is 1, suggesting that the contrastive learning loss of fMRI-video predominates. During the training of CMG, we set multiple values for the mask ratio (0, 0.2, 0.4, 0.6, 0.8) and calculate the results on pixel-level and ST-level metrics, as shown in Table <ref type="table">10</ref>. The results indicate that setting a moderate mask ratio (0.6) can prevent the model from taking shortcuts during training and effectively capture the temporal features between frames.</p><p>E.5 SUPPLEMENTARY ABLATION STUDIES ON SUBJECTS 2 AND 3 OF THE CC2017 DATASET Figure <ref type="figure">14</ref>: Loss curves for CMG with or without fMRI guidance.</p><p>We also analyze the impact of different decoders on video reconstruction performance on subjects 2 and 3 of the CC2017 dataset, as shown in Tables <ref type="table">11</ref> and<ref type="table">12</ref>. It is evident from the aforementioned tables that the removal of semantic decoder leads to a significant decline in all metrics, whereas the removal of the other two decoders does not significantly affect the semantic metrics, thereby highlighting the crucial role of the semantic decoder in video reconstruction. To supplement the bar chart in Figure <ref type="figure">9</ref>, we normalized the importance values of each ROI in decoding the three features (semantic, structure, motion) and visualized them in Figure <ref type="figure">22</ref>. In this figure, darker colors represent higher contributions of a given ROI during decoding. A horizontal comparison for each feature reveals the following:</p><p>(1) Motion Decoding: V1, V2, MT, and TPOJ regions contribute more significantly to motion decoding. Notably, MT and TPOJ are regions in the dorsal pathway responsible for processing motion information. Meanwhile, V1 and V2 transmit motion-related attributes such as speed and direction directly to MT when processing motion. This finding aligns well with prior results in neuroscience <ref type="bibr" target="#b76">(Zeki &amp; Shipp (1988)</ref>; <ref type="bibr" target="#b46">Nassi &amp; Callaway (2009)</ref>).</p><p>(2) Structure Decoding: Lower-level regions like V1, V2, and V3 show greater contributions to structure decoding, while higher-level regions such as MT contribute less.</p><p>(3) Semantic Decoding: Mid-to-high-level regions, including V4, MT, and MST, play a more significant role in semantic decoding, while lower-level regions such as V1 and V2 contribute less. Interestingly, MT, typically a dorsal pathway region for motion processing, shows the highest contribution to semantic decoding. This phenomenon may be explained by the interplay between the dorsal and ventral pathways in processing dynamic visual input <ref type="bibr" target="#b30">(Ingle et al. (1982)</ref>). Specifically, the dorsal-dorsal pathway is concerned with the control of action, whereas the ventral-dorsal pathway is involved in action understanding and recognition <ref type="bibr" target="#b53">(Rizzolatti &amp; Matelli (2003)</ref>).</p><p>The visualization of voxel-wise and ROI-wise importance maps on Subject 2 and 3 is depicted in the aforementioned Figures.        <ref type="bibr" target="#b18">&amp; Li (2013)</ref>). Although the dorsal and ventral streams clearly make up two relatively separate circuits, the anatomical segregation between the two streams is by no means absolute <ref type="bibr" target="#b46">(Nassi &amp; Callaway (2009);</ref><ref type="bibr">Andersen et al. (1990)</ref>; <ref type="bibr" target="#b5">Blatt et al. (1990)</ref>; <ref type="bibr" target="#b44">Maunsell &amp; van Essen (1983)</ref>). Recently, the dorsal stream was shown to be divided into two functional streams in primates to mediate different behavioural goals: the dorsal-dorsal and ventral-dorsal streams <ref type="bibr" target="#b53">(Rizzolatti &amp; Matelli (2003)</ref>). The dorsal-dorsal pathway concerned with the control of action 'online' (while the action is ongoing) and the ventral-dorsal pathway concerned with space perception and 'action understanding' (the recognition of actions made by others).</p><p>The ventral pathway exhibits a hierarchical information extraction process (Markov et al. ( <ref type="formula">2014</ref>)). From V1, V2, V4 to the inferior temporal (IT) cortex, neurons progressively encode information, transitioning from basic visual features (such as orientation and color) to intermediate shape characteristics, and finally to high-level visual semantics (such as faces, limbs, and scenes). In contrast, the dorsal pathway does not typically follow a hierarchical information extraction process but rather adopts a parallel processing approach (Callaway (2005)). As shown in Figure <ref type="figure">27b</ref>, there are multiple input streams from the LGN to the MT area. The major ascending input to MT traverses the magnocellular layers of the LGN (yellow) and proceeds through layers 4Cα and 4B of the V1 <ref type="bibr" target="#b58">(Shipp &amp; Zeki (1989)</ref>; <ref type="bibr" target="#b46">Nassi &amp; Callaway (2009)</ref>). Experimental evidence suggests that the direct pathway from V1 to MT primarily conveys information about motion speed and direction, while the indirect pathway is responsible for transmitting disparity information <ref type="bibr" target="#b50">(Ponce et al. (2008)</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><surname>James R Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josa a</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Corticocortical connections of anatomically and physiologically defined subdivisions within the inferior parietal lobule</title>
		<author>
			<persName><forename type="first">C</forename><surname>Richard A Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Asanuma</surname></persName>
		</author>
		<author>
			<persName><surname>Essick</surname></persName>
		</author>
		<author>
			<persName><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Neurology</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="113" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>John L Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">S</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Beliy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gaziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Strappini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brain decoding: toward real-time reconstruction of visual perception</title>
		<author>
			<persName><forename type="first">Yohann</forename><surname>Benchetrit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Banville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Rémi</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual receptive field organization and cortico-cortical connections of the lateral intraparietal area (area lip) in the macaque</title>
		<author>
			<persName><forename type="first">Gene</forename><forename type="middle">J</forename><surname>Blatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gene</forename><forename type="middle">R</forename><surname>Stoner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Neurology</title>
		<imprint>
			<biblScope unit="volume">299</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="421" to="445" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure and Function of Visual Area MT</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="157" to="189" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure and function of parallel pathways in the primate early visual system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="19" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>chapelle, o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006">2006. 2009</date>
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding</title>
		<author>
			<persName><forename type="first">Zijiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiange</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Lin Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Helen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22710" to="22720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cinematic mindscapes: High-quality video reconstruction from brain activity</title>
		<author>
			<persName><forename type="first">Zijiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Helen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Algonauts Project 2021 Challenge: How the human brain makes sense of a world in motion</title>
		<author>
			<persName><forename type="first">Radoslaw</forename><surname>Martin Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lahner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lascelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Iamshchinina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Graumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><surname>Roig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13714</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reconstructing perceptive images from brain activity by shapesemantic GAN</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13038" to="13048" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modular encoding and decoding models derived from Bayesian canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Miyawaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukiyasu</forename><surname>Kamitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="979" to="1005" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SketchyCOCO: Image generation from freehand scene sketches</title>
		<author>
			<persName><forename type="first">Chengying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5174" to="5183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised natural image reconstruction and large-scale semantic classification from brain activity</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gaziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Beliy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Granot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Strappini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="page">119121</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Top-down influences on visual processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="350" to="363" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multi-modal parcellation of human cerebral cortex</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">S</forename><surname>Matthew F Glasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">C</forename><surname>Coalson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Essa</forename><surname>Harwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Yacoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesper</forename><surname>Ugurbil</surname></persName>
		</author>
		<author>
			<persName><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Christian F Beckmann</surname></persName>
		</author>
		<author>
			<persName><surname>Jenkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">536</biblScope>
			<biblScope unit="issue">7615</biblScope>
			<biblScope unit="page" from="171" to="178" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Decoding natural image stimuli from fMRI data with a surface-based convolutional network</title>
		<author>
			<persName><forename type="first">Zijin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Jamison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Kuceyeski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.02409</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational Autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiguang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun-Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page" from="125" to="136" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed and overlapping representations of faces and objects in ventral temporal cortex</title>
		<author>
			<persName><forename type="first">Ida</forename><surname>James V Haxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maura</forename><forename type="middle">L</forename><surname>Gobbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alumit</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><surname>Pietrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="issue">5539</biblScope>
			<biblScope unit="page" from="2425" to="2430" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Masked Autoencoders are Scalable Vision Learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Latent video diffusion models for high-fidelity long video generation</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagen Video: High definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<title level="m">Video Diffusion Models. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generic decoding of seen and imagined objects using hierarchical visual features</title>
		<author>
			<persName><forename type="first">Tomoyasu</forename><surname>Horikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukiyasu</forename><surname>Kamitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15037</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Analysis of visual behavior</title>
		<author>
			<persName><forename type="first">Melvyn</forename><forename type="middle">A</forename><surname>David J Ingle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard Jw</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName><surname>Mansfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Mit Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identifying natural images from human brain activity</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kendrick N Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="issue">7185</biblScope>
			<biblScope unit="page" from="352" to="355" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying natural images from human brain activity</title>
		<author>
			<persName><surname>Kn Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page" from="352" to="355" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Text2video-zero: Text-to-image diffusion models are zero-shot video generators</title>
		<author>
			<persName><forename type="first">Levon</forename><surname>Khachatryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andranik</forename><surname>Movsisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahram</forename><surname>Tadevosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shant</forename><surname>Navasardyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15954" to="15964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Biophysical and physiological origins of blood oxygenation leveldependent fMRI signals</title>
		<author>
			<persName><forename type="first">Seong-Gi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seiji</forename><surname>Ogawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cerebral Blood Flow &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1188" to="1206" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A penny for your (visual) thoughts: Self-supervised reconstruction of natural movies from brain activity</title>
		<author>
			<persName><forename type="first">Ganit</forename><surname>Kupershmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Beliy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gaziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03544</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19730" to="19742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mind Reader: Reconstructing Complex Images from Brain Activities</title>
		<author>
			<persName><forename type="first">Sikun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="29624" to="29636" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The neural basis of the blood-oxygen-level-dependent functional magnetic resonance imaging signal</title>
		<author>
			<persName><forename type="first">Nikos</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="1003" to="1037" />
			<date type="published" when="1424">1424. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Minddiffuser: Controlled image reconstruction from human brain activity with semantic and structural diffusion</title>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changde</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiguang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5899" to="5908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Informatics and data mining tools and strategies for the human connectome project</title>
		<author>
			<persName><forename type="first">John</forename><surname>Daniel S Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Harwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">F</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Glasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><forename type="middle">W</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Curtiss</surname></persName>
		</author>
		<author>
			<persName><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anatomy of hierarchy: feedforward and feedback pathways in macaque visual cortex</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Nikola T Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vezoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Chameau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Falchier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Quilodran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Huissoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Misery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Giroud</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of comparative neurology</title>
		<imprint>
			<biblScope unit="volume">522</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="225" to="259" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The connections of the middle temporal visual area (mt) and their relationship to a cortical hierarchy in the macaque monkey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Maunsell</surname></persName>
		</author>
		<author>
			<persName><surname>David C Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2563" to="2586" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian reconstruction of natural images from human brain activity</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendrick</forename><forename type="middle">N</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="902" to="915" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parallel processing strategies of the primate visual system</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Nassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">M</forename><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="360" to="372" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reconstructing visual experiences from brain activity evoked by natural movies</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1641" to="1646" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reconstruction of perceived images from fMRI patterns and semantic brain exploration using instance-conditioned GANs</title>
		<author>
			<persName><forename type="first">Furkan</forename><surname>Ozcelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavin</forename><surname>Choksi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rufin</forename><surname>Vanrullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decoding brain representations by multimodal learning of neural activity and visual features</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaak</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.2995909</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3833" to="3849" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Integrating motion and depth via parallel pathways</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Carlos R Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">T</forename><surname>Lomber</surname></persName>
		</author>
		<author>
			<persName><surname>Born</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="223" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fine-tuned CLIP Models are Efficient Video Learners</title>
		<author>
			<persName><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Uzair</forename><surname>Khattak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6545" to="6554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Two different streams form the dorsal visual system: anatomy and functions</title>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Rizzolatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Matelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental brain research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10-05">2015. October 5-9, 2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III 18</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reconstructing the Mind&apos;s Eye: fMRI-to-image with contrastive learning and diffusion priors</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Scotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atmadeep</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmie</forename><surname>Goode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stepan</forename><surname>Shabalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Verlinde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Yundler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weisberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The organization of connections between areas v5 and v1 in macaque monkey visual cortex</title>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semir</forename><surname>Zeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="332" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01590</idno>
		<title level="m">Neurocine: Decoding vivid video sequences from human brain activties</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><forename type="middle">H</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">High-resolution image reconstruction with latent diffusion models from human brain activity</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Nishimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14453" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">VideoMAE: Masked Autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10078" to="10093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficient Bayesian multivariate fMRI analysis using a sparsifying spatio-temporal prior</title>
		<author>
			<persName><forename type="first">Marcel Aj</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floris P De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="161" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Better models of human high-level visual cortex emerge from natural language supervision with a large and diverse dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendrick</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName><surname>Wehbe</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-023-00753-y</idno>
		<ptr target="https://doi.org/10.1038/s42256-023-00753-y" />
	</analytic>
	<monogr>
		<title level="j">Nat Mach Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1415" to="1426" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Reconstructing rapid natural vision with fMRI-conditional video generative adversarial network</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Shuang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huafu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4502" to="4511" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neural encoding and decoding with deep learning for dynamic natural vision</title>
		<author>
			<persName><forename type="first">Haiguang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun-Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4136" to="4160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The solution-diffusion model: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">W</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of membrane science</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Identification of emotional intonation evaluated by fMRI</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Wildgruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Riecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Hertrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Erb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Grodd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ethofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ackermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1233" to="1241" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">Jay Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7623" to="7633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Motion perception in healthy humans and cognitive disorders</title>
		<author>
			<persName><forename type="first">Takao</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shozo</forename><surname>Tobimatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Early detection and rehabilitation technologies for dementia: Neuroscience and biomedical applications</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="156" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Brain decoding-classification of hand written digits from fMRI data employing Bayesian networks</title>
		<author>
			<persName><forename type="first">'</forename><surname>Elahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholam-Ali</forename><surname>Yargholi</surname></persName>
		</author>
		<author>
			<persName><surname>Hossein-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in human neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">351</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Matthew D Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The functional logic of cortical connections</title>
		<author>
			<persName><forename type="first">Semir</forename><surname>Zeki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Shipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="issue">6188</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Magicvideo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11018</idno>
		<title level="m">Efficient video generation with latent diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding</title>
		<author>
			<persName><forename type="first">Qiongyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changde</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengpei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiguang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>