<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIBD: BRIDGING VISION AND GENOMICS FOR BIODIVERSITY MONITORING AT SCALE</title>
				<funder ref="#_53QKcm3">
					<orgName type="full">Government of Canada&apos;s New Frontiers in Research Fund</orgName>
					<orgName type="abbreviated">NFRF</orgName>
				</funder>
				<funder>
					<orgName type="full">Digital Research Alliance of Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada CIFAR</orgName>
				</funder>
				<funder ref="#_zwFY4gJ">
					<orgName type="full">Pioneer Centre for AI (DNRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zeming</forename><surname>Gong</surname></persName>
							<email>zmgong@sfu.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Austin</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoliang</forename><surname>Huo</surname></persName>
							<email>xiaolianghuo@sfu.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joakim</forename><forename type="middle">Bruslund</forename><surname>Haurum</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><forename type="middle">C</forename><surname>Lowe</surname></persName>
							<email>scott.lowe@vectorinstitute.ai</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<email>gwtaylor@uoguelph.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Alberta Machine Intelligence Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
							<email>angelx@sfu.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLIBD: BRIDGING VISION AND GENOMICS FOR BIODIVERSITY MONITORING AT SCALE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3107D127220C3A15E449D3AB811E925</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-11-30T00:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.</p><p>Recently, BioCLIP [61] used CLIP-style contrastive learning [50] to align images with common names and taxonomic descriptions to classify plants, animals, and fungi. While they showed that aligning image representations to text can help improve classification, taxonomic labels, which are not always available to the species level, are needed to obtain text descriptions. In this work, we study whether, by aligning to DNA barcodes (instead of text) during pretraining, we can learn improved representations of images for use in tasks relevant to biodiversity.</p><p>We propose CLIBD, which uses contrastive learning to map taxonomic labels, biological images and barcode DNA to the same embedding space. By leveraging DNA barcodes, we eliminate the reliance on manual taxonomic labels (as used for BioCLIP) while still incorporating rich taxonomic information into the representation. This is advantageous since DNA barcodes can be obtained at scale more readily than taxonomic labels, which require manual inspection from a human expert <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b60">60]</ref>. We also investigate leveraging partial taxonomic annotations, when available,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As environmental change and habitat loss accelerate, monitoring biodiversity is crucial to understand and maintain the health of ecosystems. Taxonomic classification of organisms at scale is especially important for understanding regional biodiversity and studying species interactions.</p><p>To assist in this, researchers have used computer vision to identify organisms in images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b41">41]</ref> for a variety of applications such as ecological monitoring <ref type="bibr" target="#b12">[13]</ref>. However, relying solely on images for identifying and classifying organisms fails to consider the rich evolutionary relationship between species and may miss fine-grained differences. To better capture these distinctions, researchers have used DNA sequences for genome understanding and taxonomic classification <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b80">79,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b54">54]</ref>. In particular, DNA barcodes <ref type="bibr" target="#b26">[26]</ref>, short sections of DNA from specific genes such as the mitochondrial COI gene <ref type="bibr" target="#b40">[40]</ref> in animals and ITS sequences in fungi, have been shown to be particularly useful for species identification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">54]</ref>. However, collecting DNA requires specialized equipment making it more expensive and less accessible than images. In this work, we investigate whether we can leverage recent advances in multi-modal representation learning <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b32">32]</ref> to use information from DNA barcodes to guide the learning of image embeddings appropriate for taxonomic classification.</p><p>to build a trimodal latent space that aligns all three modalities for improved representations. We demonstrate the power of using DNA as a signal for aligning image embeddings by conducting experiments for fine-grained taxonomic classification down to the species level. Our experiments show our pretrained embeddings that align modalities can (1) improve on the representational power of image and DNA embeddings alone by obtaining higher taxonomic classification accuracy and (2) provide a bridge from image to DNA to enable image-to-DNA based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We review work using images, DNA, and multi-modal models for fine-grained taxonomic classification of species and their application in biology. Prior work has primarily explored building unimodal models for either images or DNA, and largely relied on fine-tuning classifiers on a set of known species or higher-level taxa. This limits those approaches to a closed set of species, whereas we aim to also identify unseen species, for which we have no examples in the training set.</p><p>Taxonomic classification of images in biology. Many studies have explored image-based taxonomic classification of organisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b69">69]</ref>. However, visual identification of species remains difficult due to the abundance of fine-grained classes and data imbalance among species. To improve finegrained taxonomic classification, methods such as coarse and weak supervision <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b63">63]</ref> and contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b74">74]</ref> have been developed. Despite these advances, image-based species classification is still limited, so we leverage DNA alongside images to enhance representation learning while maintaining the relative ease of acquiring visual data for new organisms.</p><p>Representation learning for DNA. Much work has focused on machine learning for DNA, such as for genome understanding <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">35]</ref>. Recently, self-supervised learning has been used to develop foundation models on DNA, from masked-token prediction with transformers <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b80">79,</ref><ref type="bibr" target="#b1">2]</ref>, to contrastive learning <ref type="bibr" target="#b81">[80]</ref> and next-character prediction with state-space models <ref type="bibr" target="#b47">[47]</ref>. While much of this work focuses on human DNA, models have also been trained on large multispecies DNA datasets for taxonomic classification. BERTax <ref type="bibr" target="#b45">[45]</ref> pretrained a BERT <ref type="bibr" target="#b17">[18]</ref> model for hierarchical taxonomic classification but focused on coarser taxa like superkingdom, phylum, and genus, which are easier than fine-grained species classification. BarcodeBERT <ref type="bibr" target="#b1">[2]</ref> showed that models pretrained on DNA barcodes rather than general DNA can be more effective for taxonomic classification. Though some of these works use contrastive learning, they do not align DNA with images. We extend these models by using cross-modal contrastive learning to align DNA and image embeddings, addressing the higher cost of obtaining DNA samples while improving image-based classification and enabling cross-modal queries.</p><p>Multimodal models for biology. While most work on taxonomic classification has been limited to single modalities, recent work started developing multimodal models for biological applications <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b78">77,</ref><ref type="bibr" target="#b36">36]</ref>. Nguyen et al. <ref type="bibr" target="#b48">[48]</ref> introduced Insect-1M, applying contrastive learning across text and image modalities. BioCLIP <ref type="bibr" target="#b61">[61]</ref> pretrained multimodal contrastive models on images and text encodings of taxonomic labels in TreeOfLife-10M. However, these models focus only on images and text, limiting their use with new species where taxonomic labels are unavailable. They also miss leveraging the rich taxonomic knowledge from sources like the Barcode of Life Datasystem (BOLD), which at the time of writing has nearly 19 M validated DNA barcodes. Although many records include expert-assigned taxonomic labels, only 24% are labeled to the genus level and 9% to the species level in BOLD-derived datasets like BIOSCAN-1M and BIOSCAN-5M <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. By aligning images to DNA barcodes, we can use precise information in the DNA to align the image representations with the task of taxonomic classification, without requiring taxonomic labels.</p><p>One of the few works that uses both images and DNA is the Bayesian zero-shot learning (BZSL) approach by Badirli et al. <ref type="bibr" target="#b3">[4]</ref>. This method models priors for image-based species classification by relating unseen species to nearby seen species in the DNA embedding space. Badirli et al. <ref type="bibr" target="#b4">[5]</ref> similarly apply Bayesian techniques, with ridge regression to map image embeddings to the DNA space to predict genera for unseen species. However, this approach assumes prior knowledge of all genera and does not use taxonomic labels to learn its mapping, limiting its representational power. In this work, we show that aligning image and DNA modalities using end-to-end contrastive learning produces a more accurate model and useful representation space. By incorporating text during pretraining, we can leverage available taxonomic annotations without relying on their abundance.  Some prior work has investigated contrastive learning of image and DNA specifically for genetics and histology <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b44">44]</ref>. These works extend contrastive learning for aligning images and textintroduced by ConVIRT <ref type="bibr" target="#b79">[78]</ref> for medical images and popularized by CLIP <ref type="bibr" target="#b50">[50]</ref>-to images and human DNA. Here, we focus on multimodal contrastive learning with DNA barcodes and images for taxonomic classification. DNA barcodes (see Appendix D for background) are much shorter (400-800 base pairs) compared to sequences used in biomedical tasks (tens of thousands of base pairs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>To align representations of images, DNA barcodes, and textual taxonomic labels, we start with a pretrained encoder for each modality and fine-tune them with a multimodal contrastive loss, illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. During inference, we use our fine-tuned encoders to extract features for a query image and match them against a database of image and DNA embeddings (keys) with known taxonomic labels. To classify a query image, we take the taxonomic labels associated with the most similar key. Whilst we can also query against the taxonomic text embeddings, this approach does not work well for taxa that were not seen during training. In contrast, our model can match queries against embeddings of labelled images and barcodes acquired after training. Thus, images and DNA barcodes comprise a more robust and comprehensive set of records against which to query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING</head><p>Contrastive learning. We base our approach on a contrastive learning scheme similar to CLIP <ref type="bibr" target="#b50">[50]</ref>, which uses large-scale pretraining to learn joint embeddings of images and text. In contrastive learning, embeddings for paired specimens are pulled together while non-paired specimens are pushed apart, thus aligning the semantic spaces for cross-modal retrieval. Following prior work <ref type="bibr" target="#b55">[55]</ref>, we extend CLIP <ref type="bibr" target="#b50">[50]</ref> to three modalities by considering each pair of modalities with the NT-Xent loss <ref type="bibr" target="#b58">[58]</ref> between two modalities to align their representations. Let matrices V, D, and T represent the batch of ℓ 2 -normalized embeddings of the image, DNA, and text modalities. The i-th row of each matrix corresponds to the same physical specimen instance, thus rows V i and D i are image and DNA features from the same specimen, forming a positive pair. Features in different rows V i and D j , i ̸ = j, come from different specimens and are negative pairs. The contrastive loss for pair i is</p><formula xml:id="formula_0">L (V - →D) i = -log exp V T i D i /τ n j=1 exp V T i D j /τ , L (D-→V ) i = -log exp D T i V i /τ n j=1 exp D T i V j /τ</formula><p>, where τ is a trainable temperature initialized to 0.07 following Radford et al. <ref type="bibr" target="#b50">[50]</ref>. The total contrastive loss for a pair of modalities is the sum over the loss terms for each pairs of specimens,</p><formula xml:id="formula_1">L V D = n i=1 L (V - →D) i + L (D-→V ) i ,</formula><p>wherein we apply the loss symmetrically to normalize over the possible paired embeddings for each modality <ref type="bibr" target="#b79">[78,</ref><ref type="bibr" target="#b55">55]</ref>. We repeat this for each pair of modalities and sum them to obtain the final loss,</p><formula xml:id="formula_2">L = L V D + L DT + L V T .</formula><p>Pretrained encoders. For each modality we use a pretrained model to initialize our encoders.</p><p>Images: ViT-B<ref type="foot" target="#foot_0">1</ref> pretrained on ImageNet-21k and fine-tuned on ImageNet-1k <ref type="bibr" target="#b20">[21]</ref>. DNA barcodes:  The training set (used for contrastive learning) has records without any species labels as well as a set of seen species that are well-represented (at least 9 records per species). The validation and test sets include seen and unseen (not seen during training) species. These images are further split into subpartitions of queries (darker color) and keys (lighter color) for evaluation. We ensure that the validation and test sets have different unseen species. Since the seen species are common, we have a shared set of records (Key) that we use as keys for seen species and combine all key sets to form a reference database. We show the number of records in each box.</p><p>BarcodeBERT <ref type="bibr" target="#b1">[2]</ref> with 5-mer tokenization, pretrained using masked language modelling on 893 k DNA barcodes <ref type="bibr" target="#b18">[19]</ref> a dataset that is similar to but not overlapping with the DNA barcodes in the BIOSCAN-1M dataset, making it ideal for our study. Text: we use the pretrained BERT-Small <ref type="bibr" target="#b68">[68]</ref> for taxonomic labels. Besides these pretrained encoders, we also conduct experiments using larger variants (see Appendix B.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INFERENCE</head><p>To use the model to predict taxonomic labels, we calculate the cosine similarity between the embedded input image (query) and reference image or DNA embeddings (keys) sampled from available species. We use the taxonomic label (order, family, genus, species) associated with the closest key as our prediction. This method allows us to evaluate the model in a zero-shot setting on species which were not seen by the model during training, provided we have appropriately labelled specimens to use as keys. The embedding space also provides the flexibility to be used for other downstream tasks, such as a supervised classifier or a Bayesian model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TASK AND DATA</head><p>To evaluate our method, we perform taxonomic classification using different combinations of input and reference modalities. The input may be a biological image or DNA sequence; this is matched against a reference set of labelled DNA barcodes, labelled biological images, or known taxonomic labels. We evaluate predictions at each taxonomic level by averaging accuracy over samples (micro) and taxon groups (macro). Unlike fine-tuning a species classification head, our approach can identify unseen species using labelled reference images or DNA, without needing to know all potential species at training time. We split the BIOSCAN-1M dataset so some species are "unseen" during training and report prediction accuracy for both seen and unseen species to study model generalization. This simulates the case where once a model is deployed, there are new specimens that are catalogued and labelled over time, which were not initially available for training the encoders.</p><p>Dataset. The BIOSCAN-1M dataset <ref type="bibr" target="#b23">[23]</ref> is a curated collection of over one million insect data records sourced from a biodiversity monitoring workflow. Each record in the dataset includes a high-quality insect image, expert-annotated taxonomic label, and a DNA barcode. However, the dataset has incomplete taxonomic labels, with fewer than 10% of records labelled at the species level. This poses a challenge for conventional supervised methods, which require comprehensive species-level annotations, but our method is able to flexibly leverage partial or missing taxonomic information during contrastive learning. The dataset also possesses a long-tailed class imbalance, typical of real-world biological data. Given the vast biodiversity of insects-for which an estimated 80% is as-yet undescribed <ref type="bibr" target="#b62">[62]</ref>-and the necessity to discern subtle visual differences, this dataset offers a significant challenge and opportunity for our model. Data partitioning. We split BIOSCAN-1M into train/val/test sets to evaluate zero-shot classification and model generalization to unseen species. Records for well-represented species (at least 9 records) are partitioned at 80/20 ratio into seen and unseen, with seen records allocated to each of the splits and unseen records allocated to val and test. All records without species labels are used in Table <ref type="table">1</ref>: Top-1 macro-accuracy (%) on BIOSCAN-1M test set for different combinations of modality alignment (image, DNA, text) during contrastive training. Results using DNA-to-DNA, image-to-image, and image-to-DNA query and key combinations. As a baseline, we also show results for unimodal pretrained models before cross-modal alignment and unimodal training using SimCLR (✓ * ). We report the accuracy for seen and unseen species, and their harmonic mean (H.M.) (bold: highest acc, italic: second highest acc.). contrastive pretraining, and species with 2 to 8 records are divided between the unseen splits in the val and test sets. Importantly, we ensure that unseen species are mutually exclusive between the val and test sets and do not overlap with seen species for labelled records. Finally, among each of the seen and unseen sub-splits within the val and test sets, we allocate equal proportions of records as queries, to be used as inputs during evaluation, and keys, to be used as our reference database. Note that keys from seen and unseen species are combined to form the reference database. This procedure ensures we have a distribution of different cases in evaluation: both seen and unseen, and common and uncommon species. See Figure <ref type="figure" target="#fig_2">2</ref> for split statistics and Appendix A for details.</p><p>Data preprocessing. During inference, we resize images to 256×256 and apply a 224×224 center crop. For the DNA input, following Arias et al. <ref type="bibr" target="#b1">[2]</ref>, we set a maximum length of 660 for each sequence and tokenize the input into non-overlapping 5-mers. Similar to Stevens et al. <ref type="bibr" target="#b61">[61]</ref>, we concatenate the taxonomic levels of the insects together as text input. As we did not have the common names of each record, we used the Linnean order, family, genus, and species, up to the most specific available label per specimen. With this approach, we can still provide the model with knowledge of the higher-level taxonomy, even if some records do not have species-level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate the model's ability to retrieve correct taxonomic labels using images and DNA barcodes from the BIOSCAN-1M dataset <ref type="bibr" target="#b23">[23]</ref>. This includes species that were either seen or unseen during contrastive learning. We also experiment on the INSECT dataset <ref type="bibr" target="#b3">[4]</ref> for Bayesian zero-shot learning (BZSL) species-level image classification. We report the top-1 accuracy for the seen and unseen splits, as well as their harmonic mean (H.M.). We focus on evaluating the model using various combinations of modalities on the test set. Specifically, we assess the model's performance when using images and DNA barcodes as inputs, matched against their respective image and DNA reference sets, as well as the combination of image inputs matched to DNA references. In addition, we visualize the attention roll-out of the vision transformer we used as our image encoder to explore how the representation changes before and after contrastive learning and how aligning with different modalities affects the focus of the image encoder.  Implementation details. Models were trained on four 80GB A100 GPUs for 50 epochs with batch size 2000, using the Adam optimizer <ref type="bibr" target="#b33">[33]</ref> and one-cycle learning rate schedule <ref type="bibr" target="#b57">[57]</ref> with learning rate from 1e-6 to 5e-5. For efficient training, we use automatic mixed precision (AMP). We study the impact of AMP and batch size in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RETRIEVAL BY IMAGE AND DNA</head><p>We conducted experiments on BIOSCAN-1M <ref type="bibr" target="#b23">[23]</ref> to study whether the accuracy of taxonomic classification improves with contrastive learning, particularly with the inclusion of DNA barcodes as an additional modality. We compare the accuracy of models trained to align different combinations of modalities: image (I), DNA (D), and text (T). We also assess performance by using different modalities as the query (input at inference time) and as the key (the embedding used for matching).</p><p>As image is more readily available, we primarily focus on querying by image.</p><p>Taxonomic classification. In Table <ref type="table">1</ref>, we report the top-1 macro-accuracy on our BIOSCAN-1M test set for seen and unseen species (see Table <ref type="table">8</ref> for the top-1 micro-accuracy). We report the performance of the different alignment models at different taxonomic levels (order, family, genus, species).</p><p>As expected, the performance drops for more specific taxa (e.g., accuracy for order is much higher than for species), due to both the increased number of possible labels and the more fine-grained differences between them. When we consider unseen species, there is a drop in accuracy compared to seen species, suggesting there is a benefit to retraining encoders as more data is collected.</p><p>Are multimodal aligned embeddings useful? Our experiments show that by using contrastive learning to align images and DNA barcodes, we can 1) enable cross-modal querying and 2) improve the accuracy of our retrieval-based classifier. Unsurprisingly, we find that DNA-to-DNA retrieval is the most accurate, especially for species-level classification. Note that while direct DNA matching using methods like BLAST are quite effective, they tend to be slow and cannot be easily incorporated into neural networks for other downstream tasks <ref type="bibr" target="#b1">[2]</ref>. By using contrastive learning to align different modalities, we enhance the image representation's ability to classify (image-to-image), especially at the genus and species level where the macro H.M. accuracy jumps from 12.5% to 69% (for genus) and 6.27% to 52% (for species) for our best model (I+D+T). Note that with alignment, the DNA-to-DNA retrieval performance also improves. To verify that the improvements in unimodal performance come from the flow of information between modalities, we also use unimodal contrastive loss following SimCLR <ref type="bibr" target="#b10">[11]</ref> to fine-tune the image encoder on BIOSCAN-1M. Results (Table <ref type="table">1</ref>, second row) show that while the performance is better than the unaligned model, it is far from the gain we see in the I+D model (see Appendix B.1 for details).</p><p>Do DNA barcodes provide a strong alignment target? Table <ref type="table">1</ref> also shows that on BIOSCAN-1M, using DNA provides a better alignment target than using taxonomic labels. Comparing the model that aligns image and text (I+T, row 3) vs. the one that aligns image and DNA (I+D, row 4), we see that the I+D model consistently gives higher accuracy than the I+T model. At the species level, the I+D can even outperform the I+D+T model. This is likely because the I+D+T model, despite using all known labels for each sample, only has access to species labels for 3.36% of the pretraining data. While comprehensive species labels could yield a comparable model for taxonomic classification, the money and time cost for hiring expert scientific annotators makes it impractical to scale.</p><p>Cross-modal retrieval. Next we consider cross-modal retrieval performance from image to DNA. Without any alignment, image-to-DNA performance is effectively at chance accuracy, scoring extremely low for levels more fine-grained than order. By using contrastive learning to align image to DNA, we improve performance at all taxonomic ranks. While the cross-modal performance is still low compared to within-modal retrieval, we see that it is feasible to perform image-to-DNA retrieval, which unlocks the ability to classify taxa for which no images exist in reference databases.</p><p>Retrieval examples. Figure <ref type="figure" target="#fig_4">3</ref> shows examples of intra-modality image and DNA retrieval as well as image-to-DNA retrieval from our full model (aligning I+D+T), for which the retrieval is successful if the taxonomy of the retrieved key matches the image's. These examples show significant similarity between query and retrieved images across taxa, suggesting effective DNA and image embedding alignment despite differences in insect orientation and placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">COMPARISON WITH BIOCLIP</head><p>Next we compare our aligned embedding space with that of BioCLIP <ref type="bibr" target="#b61">[61]</ref> and investigate how well using taxonomic labels as keys would perform. We run experiments on BIOSCAN-1M by adapting the BioCLIP zero-shot learning demo script to perform species-level image classification. We use the BioCLIP pretrained model on the BIOSCAN-1M test set, with image query and either image or text embeddings as keys. For the text input for BioCLIP, we combined the four concatenated taxonomic levels with their provided openai templates as text input, while for CLIBD, we used the concatenated labels only.</p><p>In Table <ref type="table" target="#tab_2">2</ref>, we report the species-level macro-accuracy. Results on other taxonomic levels (Table <ref type="table" target="#tab_3">13</ref>) follow a similar trend. We see CLIBD consistently outperforms BioCLIP, regardless of whether images or text is used as the key, and even for CLIBD trained only on images and text. Since BioCLIP was trained on a much broader dataset, including but not limited to BIOSCAN-1M, it may perform worse on insects as it was also trained on non-insect domains. CLIBD can also leverage DNA features during inference, while BioCLIP is limited to image and text modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does matching images to taxonomic labels work better than matching to image or DNA embeddings?</head><p>The performance when using text as keys is much lower than using image or DNA keys. This shows that it is more useful to labeled samples with images (most preferred) or DNA. Nevertheless, if no such samples are available, it is possible to directly use text labels as matching keys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ANALYSIS</head><p>How does class size influence performance? Since we use retrieval for taxonomic classification, it is expected that performance is linked to the number of records in the key set. Figure <ref type="figure">4</ref> confirms this. In general, accuracy is higher for seen species compared to unseen species in cross-modal retrieval, but this difference is less noticeable in within-modality retrieval. This suggests that contrastive training has better aligned the data it has been trained on, but it is less effective for unseen species. The DNA-DNA retrieval performance remains high, regardless of number of records in the key set.</p><p>Attention visualization. To investigate how the contrastive training changed the model, we visualize the attention roll-out of the vision transformer for the image encoder <ref type="bibr" target="#b0">[1]</ref> in Figure <ref type="figure">5</ref>. We reference the implementation method mentioned in Dosovitskiy et al. <ref type="bibr" target="#b20">[21]</ref> by registering forward hooks in the ViT's attention blocks to capture attention outputs and using the Rollout method to calculate attention accumulation. We then apply the processed mask to the original image to generate an attention map of the image area. Inspired by Darcet et al. <ref type="bibr" target="#b15">[16]</ref>, we inspect the mask of each attention block and remove attention maps containing artifacts. Ultimately, we select the forward outputs of the second to sixth attention blocks to generate the attention map.</p><p>We show examples for both seen and unseen species. For examples where the aligned models are able to predict correctly, we see that the attention is more clearly focused on the insect. Furthermore, we see that only the I+D+T model activates highly on the full insect, compared to I+T and I+D. Thus, while quantitatively the models may perform comparably, the I+D+T yields the most visually interpretable predictions. This is ideal for biodiversity, as interpretability is important for practical adoption. We also visualize the embedding space before and after alignment (see Appendix B.3) and show more examples of attention visualization (see Appendix B.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">IMPROVING CROSS-MODAL CLASSIFICATION</head><p>We now more closely investigate how we can improve cross-modal ZS classification, where during inference time we have an image of an insect as a query, and we have a database of seen species (with images and DNA), and unseen species (with just DNA). Badirli et al. <ref type="bibr" target="#b3">[4]</ref> proposed a hierarchical Bayesian model to classify images, using training images to learn the distribution priors and DNA embeddings to build surrogate priors for unseen classes. Here, we consider a similar zero-shot (ZS)  setting using our embeddings from our pretrained model for Bayesian zero-shot learning (BZSL), demonstrating its utility for unseen species classification.</p><formula xml:id="formula_3">a) Nearest neighbor f i1 •f q f i2 •f q f i3 •f q f q f i1 f i2 f i3 Image encoder* ⋅⋅⋅ ⋅⋅⋅ False Unseen Species f I1 •f q f I2 •f q f I3 •f q f q f I1 f I2 f I3 DNA encoder ⋅⋅⋅ ⋅⋅⋅</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised classifier</head><p>We also consider a simpler strategy using the image embeddings for seen species, and DNA embeddings for unseen species. A two-stage approach first determines if a new image query represents a seen or unseen species (see Figure <ref type="figure" target="#fig_6">6</ref>). For seen, image-to-image matching determines the species, while for unseen, image-to-DNA matching (assuming a reference set of labeled DNA samples for unseen species) determines the species. This is denoted by "IS-DU" (image seen -DNA unseen).</p><p>Determining seen vs. unseen. We frame the problem as an open-set recognition task <ref type="bibr" target="#b70">[70]</ref> by using a classifier to determine whether an image query corresponds to a seen or an unseen species. This is useful for novel species detection as in practice we may not have a reference set of labeled samples or even set of species labels for unseen species. We compare using a 1-nearest neighbor (NN) classifier, and a linear supervised classifier with a fine-tuned image encoder (see Figure <ref type="figure" target="#fig_6">6</ref> left). See Appendix B.2 for details. We show that without access to any unseen samples at inference time, our learned embeddings can be used to distinguish between seen and unseen in Table <ref type="table" target="#tab_6">15</ref>. Using the image-to-image NN classifier, we obtain 83% accuracy on seen, 77% on unseen and a harmonic mean of 80%. For our linear classifier, we obtain lower accuracy on seen (73%), but higher on unseen (85%), with harmonic mean of 79%.</p><p>Evaluation on BIOSCAN-1M. Next we conduct experiments on BIOSCAN-1M to compare our IS+DU strategy vs. incorporating our learned embeddings in BZSL. We also compare against querying the seen and unseen DNA keys using 1-NN directly. Table <ref type="table" target="#tab_3">3</ref> reports the top-1 micro and macro accuracy of at the genus and species level (see Table <ref type="table" target="#tab_15">16</ref> for order and family level classification). We find that at the genus and species level, BZSL obtains good performance for seen species but that our simple NN-based approach actually outperforms BZSL on unseen species. Using our IS-DU strategy with the supervised linear classifier, we obtain the best macro top-1 accuracy on unseen species, demonstrating that the complexity of BZSL may not be necessary. Evaluation on INSECT dataset with BZSL. In this experiment, we study whether our image and DNA embeddings can improve performance when incorporated into BZSL. We evaluate on the INSECT dataset <ref type="bibr" target="#b3">[4]</ref>, which contains 21,212 pairs of insect images and DNA barcodes from 1,213 species. We record the performance of different combinations of encoders. We start with the original CNN-based DNA encoder and ResNet-101 image encoder pretrained on ImageNet-1K from Badirli et al. <ref type="bibr" target="#b3">[4]</ref>. We also consider ViT-B <ref type="bibr" target="#b20">[21]</ref>, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.</p><p>For DNA encoders, we compare against BarcodeBERT <ref type="bibr" target="#b1">[2]</ref> (which was used to initialize our CLIBD model), and DNABERT-2 <ref type="bibr" target="#b80">[79]</ref>, a BERT-based model trained on multi-species DNA.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows that incorporating CLIBD improves BZSL accuracy. With the baseline image encoder (RN-101), CLIBD-D performs the best of the DNA encoders (both with and without DNA finetuning). With BarcodeBERT, our CLIBD-I outperforms both RN-101 and ViT-B. CLIBD-I together with CLIBD-D gives the best performance with accuracy after fine-tuning of 57.9% for seen and 25.1% for unseen. This shows the benefits of using CLIBD to learn a shared embedding space relating image and DNA data, both in performance and flexibility of applying to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We introduced CLIBD, an approach for integrating biological images with DNA barcodes and taxonomic labels to improve taxonomic classification. It uses contrastive learning to align embeddings in a shared latent space. Our experiments show that, using barcode DNA as an alignment target for image representations, CLIBD outperforms models that only align images and text. Pretraining with text-encoded taxonomic labels, even when scarce at fine-grained levels, further improves performance. We further demonstrate the effectiveness of our aligned embeddings in zero-shot image-to-DNA retrieval. Our BIOSCAN-1M and INSECT dataset experiments showcase the value of using barcode DNA as an alignment target. However, the availability of barcode DNA is limited (see Appendix D for discussion) compared to human-curated image datasets with taxonomic labels such as TreeOfLife-10M <ref type="bibr" target="#b61">[61]</ref> and BioTrove <ref type="bibr" target="#b77">[76]</ref>. It would be interesting to couple the rich information found in DNA with these datasets that include natural images, and go beyond insects. It is also possible to investigate other multi-modal learning schemes beyond CLIP. Overall, we believe that using DNA barcodes for improved representation of organism images is a promising avenue for future research. We hope that our work can stimulate the application of multimodal machine learning with barcode DNA to help build improved automated tools for biodiversity monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES</head><p>We provide additional details on how we obtain our data split (Appendix A), additional results (Appendix B) on the validation set, using different image and text encoders, additional details about the cross-modal experiments, and visualize the embedding space. We also include experiments with hyperparameter settings such as the use of automatic mixed precision and batch size (Appendix C).</p><p>In Appendix D, we provide additional information on the utility and cost of DNA barcodes.</p><p>A ADDITIONAL DATA DETAILS Figure <ref type="figure">7</ref>: Data partitioning strategy. We first partition species among the splits based on the presence of a species label and the number of records per species, and then each species is designated as seen or unseen.</p><p>Records from each species are then partitioned among train (blue), validation (orange), and test (green). For the validation and test sets, some records are used as queries, and the rest are used as keys for the reference database for retrieval.</p><p>Data partitioning process. We use a multi-stage process to establish our split of BIOSCAN-1M <ref type="bibr" target="#b23">[23]</ref> for our experiments (see Figure <ref type="figure">7</ref>). Firstly, we separate records with and without species labels. Any record without a species label is allocated for pretraining, as we cannot easily use them during evaluation. Of the remaining records with labelled species, we partition species based on their number of samples. Species with at least 9 records are allocated 80/20 to seen and unseen, with unseen records split evenly between validation and test. Species with 2 to 8 records are used only as unseen species, with a partition of 50/50 between validation and test. This allows us to simulate real-world scenarios, in which most of our unseen species are represented only by a few records, ensuring a realistic distribution of species sets. Species with only one record are excluded, as we need at least one record each to act the query and the key, respectively.</p><p>Finally, we allocate the records within each species into designated partitions. For the seen species, we subdivide the records at a 70/10/10/10 ratio into train/val/test/key, where the keys for the seen species are shared across all splits. The unseen species for each of validation and test are split evenly between queries and keys. The allocation of queries and keys ensures that we have clearly designated samples as inputs and target references for inference. We note that some samples in our data may have the exact same barcode even though the image may differ.</p><p>Data statistics. As a result of this process, we obtain a seen split with 916 species, and unseen val and test splits with 1771 and and 1772 species respectively (see Figure <ref type="figure" target="#fig_2">2</ref> in the main paper).</p><p>We present the number of species and the distribution of records across species (Figure <ref type="figure">8</ref>) and the average number of records per taxa level (Table <ref type="table" target="#tab_6">5</ref>). In Figure <ref type="figure">8</ref>, we show the number of records for Figure <ref type="figure">8</ref>: Distribution of number of records per species for our BIOSCAN-1M train, query and key splits. We show the seen and unseen species in blue and orange, with the intensity of the color indicating the number of records for that species. Note that the seen species are shared across train, val, test while the val and test unseen species are distinct from each other. From the distribution, we see that most of the species in the unseen splits have fewer than 10 records (light orange). each species by color intensity, with lighter hues for species with fewer records and darker hues for species with more records. Due to the long-tailed distribution of species, there are just a few species with a large number of records, and most species with just a few records. This is especially true for the unseen split, where 90% of species have 4 or less records. Similarly, we see from Table <ref type="table" target="#tab_6">5</ref> that each species has an average of only 1 to 2 samples in the query set or the key set.</p><p>In Table <ref type="table">6</ref>, we present the number of classes at different taxonomy levels. Specifically, we show the overlap of classes between the seen and unseen sets at various levels. We separate the statistics for records that do not have any species labels (which go into the pretrain split), and the those that are used to establish the train/val/test splits. Note that in the pretrain split, the statistics are lower bounds at the genus and species level as records are missing the species label and potential genus label as well. Due to our data division method, there is no overlap at the species level. However, for higher taxonomic levels there are overlaps. As the taxonomy level increases from genus to order, the proportion of overlapping classes gradually increases, representing a larger percentage of the total number of classes at each level.</p><p>Task difficulty. To understand the difficulty of classification at different levels of the taxonomy, we consider the performance of a simple majority baseline classifier. In Table <ref type="table" target="#tab_7">7</ref>, we present the micro Table <ref type="table">6</ref>: Taxa overlap between seen and unseen species. Overlap at different levels of the taxa (order, family, genus) between seen and unseen species. We separately report the statistics for records without species labels (which form the pretrain set), and those that we used to build our train/val/test set. Note that for the pretrain split, the number of classes at the genus and species level are lower bounds, as some labels are missing. The macro accuracy is the same as chance performance of selecting a class at random, while the micro accuracy depends on the fraction of records with the most frequent class. The significant difference between micro and macro accuracy indicates the large class imbalance in our dataset. This imbalance reflects the phenomenon that some categories are more common than others in nature as well as the sampling bias present in data collection efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EXPERIMENTS</head><p>In this section, we include additional experimental results and visualizations. We provide additional results on BIOSCAN-1M (Appendix B.1) and image to DNA retrieval results (Appendix B.2). We also visualize the aligned embedding space (Appendix B.3) to show the model's capability in integrating and representing diverse biological data, and more attention visualizations (Appendix B.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 ADDITIONAL CLASSIFICATION RESULTS ON BIOSCAN-1M</head><p>Table <ref type="table">8</ref>: Top-1 micro accuracy (%) on our test set for BIOSCAN-1M and different combinations of aligned embeddings (image, DNA, text) during contrastive training. We show results for using image-to-image, DNAto-DNA, and image-to-DNA query and key combinations. As a baseline, we show the results prior to contrastive learning (uni-modal pretrained models without cross-modal alignment). We report the accuracy for seen and unseen species, and the harmonic mean (H.M.) between these (bold: highest acc, italic: second highest acc.). We also compare against unimodal training using SimCLR (2 nd row in each taxa, indicated with ✓ * ). Results for top-1 micro-accuracy and validation set. For completeness, we provide the top 1 micro-accuracy on the test set (Table <ref type="table">8</ref>), and results on the validation set (see Table <ref type="table" target="#tab_8">9</ref> for macro accuracy, and Table <ref type="table" target="#tab_9">10</ref> for micro accuracy).</p><p>Overall, we see a similar trend in results as for macro accuracy on the test set (see Table <ref type="table">1</ref> in the main paper), with the trimodal model that aligns image (I), DNA (D), and text (T) performing the best, and the I+D model outperforming the I+T model. We also observe that the micro averages (over individual samples) are much higher than the macro averages (over classes). This is expected as the rare classes are more challenging and pulls down the macro-average.</p><p>We further examine the reproducibility of our results across training runs. In Figure <ref type="figure">9</ref>, we plot the mean and standard deviation of the micro and macro accuracy evaluated on our BIOSCAN-1M validation set after training with four different random seeds. The results indicate that although there is some fluctuation, the results are mostly stable across training runs. As expected, we observe higher variation for macro accuracy as there are many classes with only a few query records (see Figure <ref type="figure">8</ref>). There is also more variation for image-to-DNA, where the embeddings are less aligned.</p><p>Figure <ref type="figure">9</ref>: Performance across four training runs for our I+D+T model that aligns all three embeddings during contrastive training. We plot the mean with the shaded regions for standard error for top-1 macro and micro accuracy (%) on our val set for BIOSCAN-1M. These plots clearly show that accuracy drops as we go from order to species, and that DNA-to-DNA retrieval is the most accurate with image-to-image second. Overall, the standard error is within 0.5% for the micro accuracy, with higher variation in the image-to-DNA setting.</p><p>Unimodal contrastive loss. To check whether the improved unimodal performance is a result of cross-modal contrastive learning between DNA and image, or just an artifact of training on the domain-specific data, we conduct experiments comparing our results with fine-tuning the image encoder with unimodal contrastive learning. We use SimCLR-style <ref type="bibr" target="#b10">[11]</ref> training on the image encoder with normalized temperature scaled cross-entropy loss (NT-XEnt), and form positive pairs by using data augmentation. For data augmentation, we used the same set of augmentations as <ref type="bibr" target="#b10">[11]</ref> by following SimCLR's code, which focused on enhancing image variability. The augmentation process begins with randomly cropping the image to a size of 224, and includes random horizontal flips and random applications of color jitter with an 80% probability. Additionally, images are converted to grayscale 20% of the time, and Gaussian blur is applied with a kernel size equal to 10% of the image size, which is 22 in our case.</p><p>We see from the results (denoted by ✓ * in Tables <ref type="table">1,</ref><ref type="table">8</ref>, 9, and 10), that while SimCLR training improves performance for image-to-image accuracy, it greatly underperforms our CLIBD multimodal model that aligns image to DNA. This shows that while training on the BIOSCAN-1M dataset helps to improve the otherwise generic image encoder, the signal from other modalities has the greatest benefit to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 DIFFERENT TRAINING STRATEGIES</head><p>CLIBD with SimCLR pretraining. We investigate whether starting with an image encoder that is already pretrained on BIOSCAN-1M images can improve the performance of our CLIBD model.</p><p>In Table <ref type="table" target="#tab_10">11</ref>, we show that starting with a image encoder that is fine-tuned on BIOSCAN-1M using SimCLR, and then performing full CLIBD training (by aligning image, DNA, and taxonomic labels), we achieve improved image-to-image performance, and image-to-DNA performance at finer taxonomic levels.</p><p>ImageBind-style alignment. To determine whether it is necessary to align all the modalities to each other, or whether it would be better use a ImageBind-style <ref type="bibr" target="#b25">[25]</ref> training where we freeze the image and text encoders and align the DNA encoder. For these experiments, we use BioCLIP's image and text encoder (which are pre-aligned) and BarcodeBERT as DNA encoder.  The first approach follows the ImageBind paradigm, where the DNA encoder (BarcodeBERT) is aligned only to BioCLIP's image encoder. The second approach follows our CLIBD protocol and allows the unfrozen DNA encoder to align simultaneously with BioCLIP's image encoder and text encoder. The results show that allowing the DNA encoder to align to both the image and text encoders vs. just binding to the image encoder results in slightly higher performance. Compared to the results in Table <ref type="table" target="#tab_10">11</ref>, it is evident that these approaches underperform compared to our proposed method CLIBD across all query and key combinations. Note that since we start with BioCLIP's pretrained models, and since BioCLIP's training data includes BIOSCAN-1M, it is not guaranteed that unseen species labels are actually unseen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 COMPARISON WITH BIOCLIP</head><p>Full taxonomic comparison with BioCLIP. In Table <ref type="table" target="#tab_3">13</ref>, we compare the performance of BioCLIP vs. CLIBD on classification for all four different taxonomic levels. We see that our CLIBD with I+T is able to outperform BioCLIP at every taxonomic level. From Table <ref type="table">1</ref>, we see that the I+D model can already outperform our I+T model. By incorporating DNA barcodes (I+D+T), we are able to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 USING LARGER MODELS</head><p>Experiments with OpenCLIP. We conduct experiments using OpenCLIP as our text and image encoder, as well as larger ViT and BERT models. We train our full trimodal model (with image, DNA, text alignment), and report the species-level top-1 macro accuracy on our validation set for BIOSCAN-1M in Table <ref type="table" target="#tab_4">14</ref>.</p><p>Table <ref type="table" target="#tab_3">13</ref>: Comparison of top-1 macro-accuracy (%) of BioCLIP and our CLIBD model on the test set, matching image embeddings (queries) against embeddings of different modalities for retrieval (image, DNA, and text keys). Note: the BioCLIP model <ref type="bibr" target="#b61">[61]</ref> was trained on data that included BIOSCAN-1M but used different species splits, so it may have seen most of the unseen species during its training. Table <ref type="table" target="#tab_4">14</ref>: Species-level top-1 macro accuracy (%) on our val set for BIOSCAN-1M with CLIBD using different image and text encoders. Results use image embedding to match against different embeddings for retrieval (Image, DNA, and Text). We compare using the OpenCLIP (OC) pretrained model with other models. For these experiments, we used OpenCLIP ViT-L/14 <ref type="bibr" target="#b30">[30]</ref> which is pre-trained on OpenAI's dataset that combines multiple pre-existing image datasets such as YFCC100M <ref type="bibr" target="#b66">[66]</ref>. For timm ViT-B/16 (vit base patch16 224) and timm ViT-L/16 (vit large patch16 224) <ref type="bibr" target="#b73">[73]</ref> both are trained on ImageNet <ref type="bibr" target="#b16">[17]</ref>. We also used bert-base-uncased as our text encoder, which was pretrained on BookCorpus <ref type="bibr" target="#b82">[81]</ref>. For the DNA encoder, we use BarcodeBERT (except for the first row, where we do not align the DNA embeddings). We highlight in gray the setting that uses the same vision and text encoder that we used in our other experiments. We select OpenCLIP ViT-L/14 <ref type="bibr" target="#b30">[30]</ref> as a representative of a pretrained vision-language model that is trained with contrastive loss. As the OpenCLIP model requires a large amount of memory, we use a batch size of 200. From Table <ref type="table" target="#tab_4">14</ref>, we see that using OpenCLIP (first two rows), we do achieve better performance (especially for image to text) compared to our choice of Timm VIT B/16 and BERT-small for the image and text encoder at batch size of 200 (row 3). To disentangle whether the better performance is from the prealigned image and text embeddings or from the larger model size, we compare with training with a larger batch size (with similar CUDA memory usage, row 4) and larger unaligned image and text encoder, e.g. Timm ViT-L/16 and Bert-Base (row 5). Using a larger batch size brings the image-to-image performance close to that of the model with OpenCLIP (row 2), and can be improved even further with larger batch size (see Table <ref type="table" target="#tab_20">20</ref>). However, the image-to-text performance is still lower, indicating that the pretrained aligned image-to-text model is helpful despite the domain gap between the taxonomic labels and the text that makes up most of the pretraining data for OpenCLIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ADDITIONAL CROSS-MODAL RETRIEVAL RESULTS</head><p>Details about the seen/unseen classifier for the IS-DU strategy. For the NN classifier, we compute the cosine similarity of the image query features with the image features of the seen species. If the most similar image key has a similarity higher than threshold t 1 , it is considered seen. In the supervised fine-tuning approach, we add a linear classifier after the image encoder and fine-tune the encoder and classifier to predict the species out of the set of seen species. If the softmax probability exceeds t 2 , the image is classified as seen.</p><p>We tuned t 1 and t 2 on the validation set using a uniform search over 1000 values between 0 and 1, maximizing the harmonic mean of the accuracy for seen and unseen species. We report the binary Table <ref type="table" target="#tab_6">15</ref>: Accuracy (%) of our I+D+T model in predicting whether an image query corresponds to a seen or unseen species, as a binary classification problem (evaluated on our BIOSCAN-1M test set). For the "DNA" strategy with nearest neighbour (NN), we use the nearest DNA feature to classify into seen or unseen. It serves as a form of "oracle" as it has access to the samples from unseen species. For the "IS+DU" strategy and NN, we threshold the highest cosine similarity score against image keys. For the supervised linear classifier (Linear), we threshold the confidence score of the prediction over seen species. We report accuracy for seen and unseen species, and their harmonic mean (H.M).  classification results on our BIOSCAN-1M test set in Table <ref type="table" target="#tab_6">15</ref>. In these experiments, we use the I+D+T model with images as the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BSZL details</head><p>In BZSL <ref type="bibr" target="#b3">[4]</ref>, the authors used a two-layer Bayesian model with features extracted from pretrained image and DNA encoders. The model relies on hyperparameters, which shape the size and scaling of the posterior distribution used in Bayesian inference and thus significantly affect its performance. Tuning the hyperparameters on the validation split of the INSECT dataset allows for a practioner to tradeoff between accuracy for seen vs. unseen classes. In our experiments, we use the same search space as in the original paper for our hyperparameter tuning, which takes about 4 hours. We find that the resulting accuracy is volatile and that expanding the search space can lead to an impractical search time.</p><p>Order and family results for BIOSCAN-1M. In Table <ref type="table" target="#tab_15">16</ref>, we report the performance of the direct image-to-DNA matching (NN with DNA), as well as our IS+DU strategy (with the NN and linear classifiers), as well as BZSL with embeddings from our CLIBD. In the IS-DU strategy, for both the NN and linear classifier, if a image is classified as seen, we will use image-to-image matching to identify the most similar key, and classify the species using that key. Otherwise, we match the image query features with the DNA key features for unseen species.</p><p>Results show that at the order and family-level, direct image-to-DNA matching and NN with IS-DU gives the highest performance, with BZSL being the worst performing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 EMBEDDING SPACE VISUALIZATION</head><p>To better understand the alignment of features in the embedding space, we visualize a mapping of the image, DNA, and text embeddings in Figure <ref type="figure" target="#fig_9">10</ref>. We use UMAP <ref type="bibr" target="#b42">[42]</ref> with a cosine similarity metric applied to the seen validation set to map the embeddings down to 2D space, and we mark points in the space based on their order classification. We show the embedding space before alignment (a),  with image and text (b), image and DNA (c), and all three modalities. We see that after aligning the modalities, samples for the same order (indicated by hue), from different modalities (indicated by lightness) tend to overlap each other. We observe that, for some orders, there are numerous outlier clusters spread out in the space. However, overall the orders demonstrate some degree of clustering together, with image and DNA features close to one another within their respective clusters. Furthermore, we note the text embeddings tend to lie within the Image or (more often) DNA clusters, suggesting a good alignment between text and other modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ATTENTION MAP VISUALIZATION</head><p>We provide more attention map visualization samples in Figure <ref type="figure" target="#fig_1">11</ref>, Figure <ref type="figure" target="#fig_2">12</ref> and Figure <ref type="figure" target="#fig_10">13</ref>, including both success cases and failure cases. Beneath each attention map, we show the closest retrieval for that model. From the figure, we see that the retrieved images are all highly similar to the query image. This illustrates the challenges of using image alone to perform taxonomic classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS AND HYPERPARAMETER SELECTION</head><p>In this section, we provide experiments to validate the choice of hyperparameter settings and design choices we made for efficient training of our model.   We compare training with and without AMP in Table <ref type="table" target="#tab_18">18</ref>. By applying AMP, we achieve comparable performance while using less memory. With AMP, the CUDA memory usage is reduced by about 15GB (∼20%) and the training time by 3 hour per epoch (∼75%). Although using full-precision (no AMP) yields slightly better accuracies, the lower memory usage and faster training time of AMP allows for more efficient experiments. Additionally, the lower memory usage with AMP enables us to use larger batch sizes and is more effective for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 LORA VS FULL FINE-TUNING</head><p>For efficient training, we also investigate the performance of using LoRA <ref type="bibr" target="#b28">[28]</ref> vs full fine-tuning. As shown in Table <ref type="table" target="#tab_19">19</ref>, we find that while LoRA does reduce the memory usage and training time, the performance is also notably worse, and thus we use full fine-tuning for the rest of our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 BATCH SIZE EXPERIMENTS</head><p>We conducted additional experiments to investigate the impact of training batch size (from 500 to 2000) on model performance. The choice of batch size ordinarily does not have a major impact on performance when using supervised learning, but can have larger impact when training using contrastive learning since each positive pair is normalized against the pool of negative pairs appearing in the same training batch.</p><p>Our results, shown in Table <ref type="table" target="#tab_20">20</ref>, confirm that the classification accuracy improves as the batch size increases. The effect on is more pronounced for the harder, more fine-grained, taxonomic levels. Due to resource limitations, we were only able to train up to a batch size of 2000. We anticipate that using larger batch sizes would further enhance the classification accuracy of CLIBD, especially on more fine-grained taxonomic levels. Images are the easiest to acquire, but visual differences between species can be hard to identify. Despite this, they are useful for analyzing similarities and differences between species. We attempt to leverage the benefits of all three modalities to learn a representation space for taxonomic classification and other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Taxonomic labels DNA barcodes Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DISCUSSION OF UTILITY AND COST OF DNA BARCODES</head><p>In this work, we focus on experiments on BIOSCAN-1M that showcase the value of using DNA barcodes as an alignment target. Here we discuss some limitations of using DNA barcodes as well as their advantages. We also provide details on the acquisition cost, as well as the computational and storage cost of using DNA barcodes.</p><p>D.1 WHEN ARE DNA BARCODES USEFUL?</p><p>While DNA barcodes are a promising alignment target, there are also challenges with using them. For one, repositories <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b5">6]</ref> and datasets <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref> that include DNA barcodes are relatively limited.</p><p>In contrast, there are far more datasets, at larger scale, that consists only of images and taxonomic labels. For instance, Tree-Of-Life10M <ref type="bibr" target="#b61">[61]</ref> has more than 10M images of over 454k species and BioTrove <ref type="bibr" target="#b77">[76]</ref> has more than 162M images of over 367k species, which is much larger than the recently introduced BIOSCAN-1M and BIOSCAN-5M. In addition, the BIOSCAN datasets are limited to images collected in a lab setting and thus models trained on such data may not readily transfer to images captured in the wild. DNA barcodes are also not perfect indicators of species, especially in cases of complex inheritance (e.g. hybridization). There may also be potential errors during the data collection and barcoding process (e.g. errors during sequencing, potential contamination from other specimens, etc). Thus, relying solely on DNA barcodes for species identification, especially those based purely on fixed thresholds of genetic distances, may lead to errors <ref type="bibr" target="#b43">[43]</ref>.</p><p>Nevertheless, we believe that DNA barcodes can be a useful signal, as validated by our experiments. Since their introduction <ref type="bibr" target="#b26">[26]</ref>, they have become widely used in large-scale biodiversity monitoring programs as they allow for relatively fast and reliable identification. The use of DNA barcodes for fine-grained species identification is highly accurate, with most errors coming from human errors <ref type="bibr" target="#b11">[12]</ref>. With the use of metabarcoding <ref type="bibr" target="#b56">[56]</ref>, it is also possible to identify multiple species from samples from the environment, allowing for the discovery of new species and monitoring of changes in biodiversity. There are ongoing efforts to scale up the collection of DNA barcoding, as well as developments in technology, such as nanopore sequencing <ref type="bibr" target="#b71">[71]</ref>, that allows for real-time barcoding of organisms in the field. With these technological advances, we foresee that there will be increasing amount of DNA barcode data in the coming years. These will make DNA barcodes an important modality to leverage for building machine learning models for biodiversity monitoring and species discovery <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">38]</ref>. Outside of biodiversity monitoring, the use of DNA barcodes for identification can be used for dietary analyses (fecal sampling to determine gut content) <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b49">49]</ref>, food authentication (e.g. whether fish is marketed under a different name, use of horse meat, etc.) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">27]</ref>, herbal medicine ingredient identification <ref type="bibr" target="#b9">[10]</ref>, whether there is illegal transfer of animal remains across borders, and many others.</p><p>DNA barcodes can also be especially useful for distinguishing between cryptic species, which are morphologically similar but genetically distinct, and understanding undiscovered species. For specimens that are not yet well established in the standard Linnaean taxonomy, clustering by DNA barcodes can serve as an way to identify a potential related group of organisms, called an Operational Taxonomic Unit. This is standardized through the use of Barcode Index Numbers (BINs) <ref type="bibr" target="#b52">[52]</ref>.</p><p>In our experiments, we attempted to simulate evaluation on unseen species by splitting the data so that there are records with a species label that were not seen during pretraining. However, our simulation does not necessarily match the real-world distribution of truly undiscovered species.</p><p>To truly investigate the usefulness of CLIBD, it is necessary to work with experts in biomonitoring in live-deployment, which is currently outside the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 WORKFLOW FOR SPECIES IDENTIFICATION</head><p>The use of DNA barcodes for fine-grained identification is highly accurate. However, no automated method is perfect and thus it is coupled with human verification of morphological features and inspection of the specimen. In a typical workflow, a machine learning algorithm is used to assign family and order to a new specimen, and barcodes are used to propose fine-grained BINs for the specimen. If there are inconsistencies between initial order identification and the proposed BIN, then human experts inspect the specimen and attempt to provide the correct taxonomic label. However, not all specimens have a well-established species that can be clearly associated with it, in which case the BIN serves as a placeholder until biologists come to a consensus on whether a new species is warranted or whether the specimen (or group of specimen with the same BIN) does indeed belong to a existing species. In BOLD <ref type="bibr" target="#b51">[51]</ref>, one of the largest repositories of DNA barcodes, and the source of the BIOSCAN-1M and BIOSCAN-5M datasets, there are 1.2M BINs but only 259k named species.</p><p>We hope that CLIBD can provide a multimodal model that can eventually be integrated into such a workflow, to provide potentially accurate classification, or tools for biologists to retrieve similar samples via either image or DNA. However, the study of how to integrate our model into the workflow is beyond the scope of this this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 ACQUISITION COST</head><p>Compared to acquiring taxonomic labels, DNA barcoding is an automated process that provides an identifying DNA sequence (e.g. the DNA barcode) per specimen. The Centre for Biodiversity Genomics (CBG), which contributed the BIOSCAN data, has been developing protocols that allow for barcode analysis for under $1. They are continuing to develop cheaper, less invasive DNA barcoding technology, with the goal of driving costs down to below $0.01 per specimen. Metabarcoding (the barcoding of multiple specimens at once) is even more promising from a cost perspective. CBG is now targeting Oxford Nanopore Technology flow cell-based protocols that will lower the sequencing cost for barcode acquisition from $0.05 to $0.001.</p><p>On the other hand, obtaining taxonomic labels is currently a human driven process which can cost an average of at least $25 per sample. Depending on whether the species is known, the detail in the image, and potential dissection involved, a taxonomic specialist may require a few minutes to often a couple hours to identify a sample. There are many specimens that are visually similar, and most specimens are not labelled to fine-grained taxa like genus or species because of ambiguities, expert disagreement, or lack of established labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 COMPUTATIONAL AND STORAGE COSTS</head><p>In addition to annotation costs, it is important to consider computational and storage costs. These costs are directly related to the length of the DNA sequence. In this work, we use DNA barcodes from the mitochondrial COI gene, which have a length of 660 base pairs (bp).</p><p>Token length. For the tokenization methods we use (5-mer non-overlapping tokens for DNA barcodes and WordPiece for taxonomic labels), we obtain approximately 133 tokens for DNA and 20 for taxonomic labels.</p><p>Computational cost. Validating on a batch of 500 samples, we find that the number of FLOPS for BarcodeBERT (87.4M parameters) is 5.90T, compared to 0.13T FLOPS for LoRA-BERT (29.2M parameters) for taxonomic labels. Despite the difference in FLOPS, we find that the runtime per batch is nearly equivalent for both models at 15 ms per batch.</p><p>Storage cost. Without compression, we find that the BIOSCAN-1M samples with species labels (84,450) require 53 MB of storage for DNA barcodes vs. 12 MB for taxonomic labels. Given that DNA consists primarily of four nucleotides (A, T, C, and G), a simple compression of 2 bits per nucleotide yields a size of 13.3 MB for the DNA barcodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of CLIBD. (a) Our model consists of three encoders for processing images, DNA barcodes, and text. During training, we use a contrastive loss to align the image, DNA, and text embeddings. (b) At inference, we embed a query image and match it to a database of existing image and DNA embeddings (keys).We use cosine similarity to find the closest key embedding and use its taxonomic label to classify the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Data partitioning. We split the BIOSCAN-1M data into training, validation, and test partitions.The training set (used for contrastive learning) has records without any species labels as well as a set of seen species that are well-represented (at least 9 records per species). The validation and test sets include seen and unseen (not seen during training) species. These images are further split into subpartitions of queries (darker color) and keys (lighter color) for evaluation. We ensure that the validation and test sets have different unseen species. Since the seen species are common, we have a shared set of records (Key) that we use as keys for seen species and combine all key sets to form a reference database. We show the number of records in each box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example query-key pairs. Top-3 nearest specimens from the unseen validation-key dataset retrieved based on the cosine-similarity for DNA-to-DNA, image-to-image, and image-to-DNA retrieval. Box color indicates whether the retrieved samples had the same species (green), genus (light-green), family (yellow), or order (orange) as the query or, else not matched (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure4: Average top-1 per-species accuracy, binned by count of species records in the key set, for different query and key combinations. We show seen in blue and unseen in orange, with size and color intensity indicating number of binned species. In all cases, the accuracy for seen species increases with the number of records. While the trend is similar for unseen species with intra-modal retrieval (a) and d)), cross-modal retrieval (b) and c)) achieve much lower performance, underscoring the challenge of cross-modal alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: When we have DNA barcodes but not images of unseen species, we can use a combination of image and DNA as key sets. We adapt CLIBD for using images as keys for seen species and DNA for unseen species (i.e. the IS+DU strategy) to predict the species. We first classify the input image query q against seen species, using either: (a) an 1-NN approach thresholding the cosine similarity score s = max k f ik • fq; or (b) a supervised classifier predicting over all seen species and thresholding the maximum softmax probability s = max y k by threshold t. If s &lt; t, we subsequently query with the image feature fq using 1-NN with the DNA keys f lk of the unseen species and predict the unseen species of the closest DNA feature. * During supervised classifier training, the image encoder is finetuned only for use in the supervised pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) No alignment (b) Align image and text (c) Align image and DNA (d) Align all three modalities</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Embedding visualization for order level. We visualize the embedding space with no alignment (a), image and text aligned (b), image and DNA aligned (c), and all three modalities aligned (d) over the seen validation set generated using UMAP on the image, DNA, and text embeddings, using a cosine similarity distance metric. Marker hue: order taxon. Marker lightness: data modality.</figDesc><graphic coords="25,222.88,188.59,150.01,324.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Failure cases. We visualize the attention and the nearest retrievals for queries from seen and unseen species for incorrect predictions by all four models (no align, I+T, I+D, I+D+T). Predicted genera + species are indicated in green text for correct, red for incorrect. Despite strong signalling on the insect, the model identifies alternative samples as the nearest, though the retrieved examples still exhibit high-level visual similarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: Overview of modalities. (a) Taxonomic labels rely on expert biologists for annotation, providing valuable insights for human understanding. However, they may be difficult to determine at the species level, especially given bias or expert disagreement. (b) DNA barcodes are effective for species identification but require specialized equipment. (c) Images are the easiest to acquire, but visual differences between species can be hard to identify. Despite this, they are useful for analyzing similarities and differences between species. We attempt to leverage the benefits of all three modalities to learn a representation space for taxonomic classification and other downstream tasks.</figDesc><graphic coords="29,229.66,568.04,147.82,50.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Species-level top-1 macro-accuracy (%) of BioCLIP and our CLIBD model on the test set, matching image embeddings (queries) against embeddings of different modalities for retrieval (image, DNA, and text keys). Note: the BioCLIP model<ref type="bibr" target="#b61">[61]</ref> was trained on data that included BIOSCAN-1M but used different species splits, so it may have seen most of the unseen species during its training.</figDesc><table><row><cell></cell><cell cols="3">Aligned embeddings</cell><cell cols="3">Image-to-Image</cell><cell cols="3">Image-to-DNA</cell><cell cols="2">Image-to-Text</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Img DNA</cell><cell>Txt</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>BioCLIP</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>20.4</cell><cell>14.8</cell><cell>17.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.2</cell><cell>3.1</cell><cell>3.6</cell></row><row><cell>CLIBD</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>54.2</cell><cell>33.6</cell><cell>41.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.6</cell><cell>4.6</cell><cell>8.5</cell></row><row><cell>CLIBD</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>59.3</cell><cell>45.0</cell><cell>51.2</cell><cell>51.6</cell><cell>8.6</cell><cell>14.7</cell><cell>56.0</cell><cell>4.8</cell><cell>8.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy (%) on our BIOSCAN-1M test set using the Image+DNA+Text model with image query. We compare nearest neighbour using only DNA keys (NN DNA), vs. our two strategies to use Image key for seen and DNA key for Unseen, either NN or a supervised linear classifier. We also compare against BZSL<ref type="bibr" target="#b3">[4]</ref> with our embeddings.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Micro top-1 acc</cell><cell cols="3">Macro top-1 acc</cell></row><row><cell>Taxa</cell><cell cols="2">Method Strategy</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>Genus</cell><cell>NN</cell><cell>DNA</cell><cell>87.6</cell><cell>54.9</cell><cell>67.5</cell><cell>70.6</cell><cell>20.8</cell><cell>32.1</cell></row><row><cell></cell><cell>NN</cell><cell>IS+DU</cell><cell>85.7</cell><cell>55.0</cell><cell>67.0</cell><cell>66.8</cell><cell>20.8</cell><cell>31.7</cell></row><row><cell></cell><cell>Linear</cell><cell>IS+DU</cell><cell>83.6</cell><cell>55.6</cell><cell>66.8</cell><cell>61.4</cell><cell>21.1</cell><cell>31.5</cell></row><row><cell></cell><cell>BZSL</cell><cell>IS+DU</cell><cell>86.8</cell><cell>46.5</cell><cell>60.6</cell><cell>75.7</cell><cell>14.4</cell><cell>24.2</cell></row><row><cell cols="2">Species NN</cell><cell>DNA</cell><cell>74.2</cell><cell>27.8</cell><cell>40.4</cell><cell>51.6</cell><cell>8.6</cell><cell>14.7</cell></row><row><cell></cell><cell>NN</cell><cell>IS+DU</cell><cell>76.1</cell><cell>26.2</cell><cell>39.0</cell><cell>54.8</cell><cell>8.5</cell><cell>14.8</cell></row><row><cell></cell><cell>Linear</cell><cell>IS+DU</cell><cell>72.6</cell><cell>25.5</cell><cell>37.7</cell><cell>41.6</cell><cell>9.4</cell><cell>15.3</cell></row><row><cell></cell><cell>BZSL</cell><cell>IS+DU</cell><cell>76.1</cell><cell>17.6</cell><cell>28.5</cell><cell>62.6</cell><cell>7.2</cell><cell>12.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Macro-accuracy (%) for species classification in a Bayesian zero-shot learning task on the INSECT dataset. We compare our CLIBD-D with several DNA encoders: CNN encoder<ref type="bibr" target="#b3">[4]</ref>, DNABERT-2<ref type="bibr" target="#b80">[79]</ref>, Bar-codeBERT<ref type="bibr" target="#b1">[2]</ref>. The baseline image encoder ResNet-101 used in Badirli et al.<ref type="bibr" target="#b3">[4]</ref> is compared against our image encoder before (ViT-B) and after (CLIBD-I) pretraining on BIOSCAN-1M (BS-1M). We indicate the pretraining set for DNA (Pre-DNA) as the multi-species (M.S.) set from Zhou et al.<ref type="bibr" target="#b80">[79]</ref>, anthropods from Arias et al.<ref type="bibr" target="#b1">[2]</ref>, or BS-1M. We compare models both with and without supervised fine-tuning (FT) for each encoder, except for CLIBD-D, where the comparison is with or without contrastive learning for fine-tuning on the INSECT dataset. We highlight the baseline from Badirli et al.<ref type="bibr" target="#b3">[4]</ref> and the variant with our fine-tuned encoders in gray.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Data sources</cell><cell></cell><cell cols="3">Species-level acc (%)</cell></row><row><cell>DNA enc.</cell><cell cols="4">Image enc. Pre-DNA FT-DNA FT-Img</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>CNN encoder</cell><cell>RN-101</cell><cell>-</cell><cell cols="2">INSECT -</cell><cell>38.3</cell><cell>20.8</cell><cell>27.0</cell></row><row><cell>DNABERT-2</cell><cell>RN-101</cell><cell>M.S.</cell><cell>-</cell><cell>-</cell><cell>36.2</cell><cell>10.4</cell><cell>16.2</cell></row><row><cell>DNABERT-2</cell><cell>RN-101</cell><cell>M.S.</cell><cell cols="2">INSECT -</cell><cell>30.8</cell><cell>8.6</cell><cell>13.4</cell></row><row><cell cols="2">BarcodeBERT RN-101</cell><cell>Arthro</cell><cell>-</cell><cell>-</cell><cell>38.4</cell><cell>16.5</cell><cell>23.1</cell></row><row><cell cols="2">BarcodeBERT RN-101</cell><cell>Arthro</cell><cell cols="2">INSECT -</cell><cell>37.3</cell><cell>20.8</cell><cell>26.7</cell></row><row><cell cols="2">BarcodeBERT ViT-B</cell><cell>Arthro</cell><cell cols="2">INSECT -</cell><cell>42.4</cell><cell>23.5</cell><cell>30.2</cell></row><row><cell cols="2">BarcodeBERT ViT-B</cell><cell>Arthro</cell><cell cols="2">INSECT INSECT</cell><cell>54.1</cell><cell>20.1</cell><cell>29.3</cell></row><row><cell>CNN encoder</cell><cell>CLIBD-I</cell><cell>-</cell><cell cols="2">INSECT -</cell><cell>37.7</cell><cell>16.0</cell><cell>22.5</cell></row><row><cell cols="2">BarcodeBERT CLIBD-I</cell><cell>Arthro</cell><cell cols="2">INSECT -</cell><cell>52.0</cell><cell>21.6</cell><cell>30.6</cell></row><row><cell cols="2">BarcodeBERT CLIBD-I</cell><cell>Arthro</cell><cell cols="2">INSECT INSECT</cell><cell>48.5</cell><cell>23.0</cell><cell>31.2</cell></row><row><cell>CLIBD-D</cell><cell>RN-101</cell><cell>BS-1M</cell><cell>-</cell><cell>-</cell><cell>34.7</cell><cell>21.3</cell><cell>26.4</cell></row><row><cell>CLIBD-D</cell><cell>RN-101</cell><cell>BS-1M</cell><cell cols="2">INSECT -</cell><cell>32.8</cell><cell>25.0</cell><cell>28.4</cell></row><row><cell>CLIBD-D</cell><cell>CLIBD-I</cell><cell>BS-1M</cell><cell>-</cell><cell>-</cell><cell>34.2</cell><cell>22.1</cell><cell>26.9</cell></row><row><cell>CLIBD-D</cell><cell>CLIBD-I</cell><cell>BS-1M</cell><cell cols="2">INSECT INSECT</cell><cell>57.9</cell><cell>25.1</cell><cell>35.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Data statistics. Average number of records per taxa level. Note that the seen and unseen species are distinct, but there is overlap at other levels of the taxa (order, family, genus) between seen and unseen species.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Species seen</cell><cell></cell><cell></cell><cell cols="2">Species unseen</cell><cell></cell></row><row><cell></cell><cell>Train</cell><cell cols="7">Key Val Query Test Query Val Key Val Query Test Key Test Query</cell></row><row><cell>order</cell><cell cols="2">60007.72 634.80</cell><cell>487.60</cell><cell>487.60</cell><cell>545.71</cell><cell>496.29</cell><cell>593.58</cell><cell>537.33</cell></row><row><cell>family</cell><cell>2305.88</cell><cell>41.75</cell><cell>33.74</cell><cell>33.74</cell><cell>38.99</cell><cell>36.34</cell><cell>34.89</cell><cell>31.25</cell></row><row><cell>genus</cell><cell>85.63</cell><cell>6.10</cell><cell>5.17</cell><cell>5.17</cell><cell>4.98</cell><cell>4.73</cell><cell>4.56</cell><cell>4.36</cell></row><row><cell>species</cell><cell>21.79</cell><cell>2.06</cell><cell>1.99</cell><cell>1.99</cell><cell>1.92</cell><cell>1.92</cell><cell>1.70</cell><cell>1.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Chance performance. Performance for most frequent class baseline for micro-accuracy and chance performance (random class) for macro-accuracy (%).</figDesc><table><row><cell>Gray</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Top-1 macro accuracy (%) on our val set for BIOSCAN-1M and different combinations of aligned embeddings (image, DNA, text) during contrastive training. We show results for using image-to-image, DNAto-DNA, and image-to-DNA query and key combinations. As a baseline, we show the results prior to contrastive learning (uni-modal pretrained models without cross-modal alignment), and unimodal training using SimCLR (✓ * ). We report the accuracy for seen and unseen species, and the harmonic mean (H.M.) between these (bold: highest acc, italic: second highest acc.).</figDesc><table><row><cell></cell><cell cols="3">Aligned embeddings</cell><cell cols="2">DNA-to-DNA</cell><cell></cell><cell cols="3">Image-to-Image</cell><cell cols="3">Image-to-DNA</cell></row><row><cell>Taxa</cell><cell cols="2">Img DNA</cell><cell>Txt</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>Order</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>99.1 -</cell><cell>98.5 -</cell><cell>98.8 -</cell><cell>88.8 94.4</cell><cell>90.8 94.1</cell><cell>89.8 94.3</cell><cell>10.5 -</cell><cell>11.0 -</cell><cell>10.7 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell cols="3">100.0 100.0 100.0</cell><cell>99.6</cell><cell>99.7</cell><cell>99.6</cell><cell>99.7</cell><cell>98.9</cell><cell>99.3</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="3">100.0 100.0 100.0</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>99.7</cell><cell>99.3</cell><cell>99.5</cell></row><row><cell>Family</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>96.2 -</cell><cell>93.8 -</cell><cell>95.0 -</cell><cell>52.9 67.5</cell><cell>60.0 69.7</cell><cell>56.2 68.6</cell><cell>1.0 -</cell><cell>1.1 -</cell><cell>1.0 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.7</cell><cell>92.2</cell><cell>93.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>99.8</cell><cell>99.2</cell><cell>99.5</cell><cell>95.9</cell><cell>93.1</cell><cell>94.5</cell><cell>95.8</cell><cell>84.6</cell><cell>89.9</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>100.0</cell><cell>99.5</cell><cell>99.7</cell><cell>96.2</cell><cell>93.7</cell><cell>94.9</cell><cell>96.5</cell><cell>87.1</cell><cell>91.6</cell></row><row><cell>Genus</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>93.4 -</cell><cell>89.0 -</cell><cell>91.1 -</cell><cell>30.1 43.2</cell><cell>38.7 48.9</cell><cell>33.9 45.9</cell><cell>0.2 -</cell><cell>0.1 -</cell><cell>0.1 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.2</cell><cell>77.1</cell><cell>81.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>99.2</cell><cell>96.9</cell><cell>98.0</cell><cell>88.6</cell><cell>82.1</cell><cell>85.2</cell><cell>87.8</cell><cell>51.3</cell><cell>64.8</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.5</cell><cell>97.9</cell><cell>98.7</cell><cell>89.3</cell><cell>82.3</cell><cell>85.7</cell><cell>87.6</cell><cell>54.9</cell><cell>67.5</cell></row><row><cell>Species</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>90.4 -</cell><cell>84.6 -</cell><cell>87.4 -</cell><cell>18.1 28.5</cell><cell>26.8 36.2</cell><cell>21.6 31.9</cell><cell>0.1 -</cell><cell>0.1 -</cell><cell>0.1 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.2</cell><cell>61.9</cell><cell>68.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>97.9</cell><cell>94.8</cell><cell>96.3</cell><cell>79.2</cell><cell>70.0</cell><cell>74.3</cell><cell>75.1</cell><cell>25.2</cell><cell>37.7</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>98.4</cell><cell>96.3</cell><cell>97.3</cell><cell>79.6</cell><cell>69.7</cell><cell>74.3</cell><cell>74.2</cell><cell>27.8</cell><cell>40.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Aligned embeddings</cell><cell cols="2">DNA-to-DNA</cell><cell></cell><cell cols="3">Image-to-Image</cell><cell cols="3">Image-to-DNA</cell></row><row><cell>Taxa</cell><cell cols="2">Img DNA</cell><cell>Txt</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>Order</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>99.2 -</cell><cell>98.4 -</cell><cell>98.8 -</cell><cell>89.3 94.2</cell><cell>90.7 94.3</cell><cell>90.0 94.2</cell><cell>32.2 -</cell><cell>29.8 -</cell><cell>31.0 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell cols="3">100.0 100.0 100.0</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>99.6</cell><cell>98.9</cell><cell>99.2</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="3">100.0 100.0 100.0</cell><cell>99.7</cell><cell>99.5</cell><cell>99.6</cell><cell>99.7</cell><cell>99.0</cell><cell>99.3</cell></row><row><cell>Family</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>96.4 -</cell><cell>94.2 -</cell><cell>95.3 -</cell><cell>54.6 67.6</cell><cell>61.7 71.9</cell><cell>57.9 69.7</cell><cell>2.9 -</cell><cell>3.7 -</cell><cell>3.3 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.9</cell><cell>92.9</cell><cell>94.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>99.8</cell><cell>99.4</cell><cell>99.6</cell><cell>95.9</cell><cell>93.3</cell><cell>94.6</cell><cell>95.9</cell><cell>85.7</cell><cell>90.5</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>100.0</cell><cell>99.7</cell><cell>99.8</cell><cell>96.5</cell><cell>94.3</cell><cell>95.4</cell><cell>96.5</cell><cell>86.8</cell><cell>91.4</cell></row><row><cell>Genus</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>92.9 -</cell><cell>89.0 -</cell><cell>90.9 -</cell><cell>30.3 42.9</cell><cell>41.4 51.4</cell><cell>35.0 46.8</cell><cell>0.4 -</cell><cell>0.3 -</cell><cell>0.3 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.1</cell><cell>79.3</cell><cell>83.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>99.2</cell><cell>97.2</cell><cell>98.2</cell><cell>88.9</cell><cell>83.6</cell><cell>86.2</cell><cell>87.2</cell><cell>58.2</cell><cell>69.8</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.5</cell><cell>98.2</cell><cell>98.8</cell><cell>89.6</cell><cell>84.5</cell><cell>87.0</cell><cell>86.4</cell><cell>59.8</cell><cell>70.7</cell></row><row><cell>Species</cell><cell>✗ ✓  *</cell><cell>✗ ✗</cell><cell>✗ ✗</cell><cell>89.5 -</cell><cell>84.8 -</cell><cell>87.1 -</cell><cell>18.1 28.4</cell><cell>31.6 41.1</cell><cell>23.0 33.6</cell><cell>0.1 -</cell><cell>0.1 -</cell><cell>0.1 -</cell></row><row><cell></cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.1</cell><cell>68.0</cell><cell>71.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>98.1</cell><cell>95.2</cell><cell>96.6</cell><cell>79.7</cell><cell>74.0</cell><cell>76.7</cell><cell>75.4</cell><cell>38.8</cell><cell>51.2</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>98.8</cell><cell>96.5</cell><cell>97.6</cell><cell>80.0</cell><cell>74.3</cell><cell>77.0</cell><cell>73.3</cell><cell>39.6</cell><cell>51.4</cell></row></table><note><p>Top-1 micro accuracy (%) on our val set for BIOSCAN-1M and different combinations of aligned embeddings (image, DNA, text) during contrastive training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Top-1 macro accuracy (%) on the test set of BIOSCAN-1M comparing starting with pre-trained image encoder that was fine-tuned using SimCLR (✓)or not (✗). We do full CLIBD training that align all three modalities (the I+D+T model).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DNA-to-DNA</cell><cell></cell><cell cols="3">Image-to-Image</cell><cell cols="3">Image-to-DNA</cell></row><row><cell>Taxa</cell><cell>SimCLR</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>Order</cell><cell>✗</cell><cell cols="3">100.0 100.0 100.0</cell><cell>99.7</cell><cell>94.4</cell><cell>97.0</cell><cell>99.4</cell><cell>88.5</cell><cell>93.6</cell></row><row><cell></cell><cell>✓</cell><cell cols="3">100.0 100.0 100.0</cell><cell>99.7</cell><cell>98.9</cell><cell>99.3</cell><cell>99.6</cell><cell>75.3</cell><cell>84.8</cell></row><row><cell>Family</cell><cell>✗</cell><cell>100.0</cell><cell>98.3</cell><cell>99.1</cell><cell>90.9</cell><cell>81.8</cell><cell>86.1</cell><cell>90.8</cell><cell>50.1</cell><cell>64.6</cell></row><row><cell></cell><cell>✓</cell><cell>100.0</cell><cell>99.5</cell><cell>99.7</cell><cell>92.2</cell><cell>85.8</cell><cell>88.9</cell><cell>92.6</cell><cell>52.1</cell><cell>66.7</cell></row><row><cell>Genus</cell><cell>✗</cell><cell>98.2</cell><cell>94.7</cell><cell>96.4</cell><cell>74.6</cell><cell>60.4</cell><cell>66.8</cell><cell>70.6</cell><cell>20.8</cell><cell>32.1</cell></row><row><cell></cell><cell>✓</cell><cell>98.5</cell><cell>94.5</cell><cell>96.4</cell><cell>77.2</cell><cell>63.5</cell><cell>69.6</cell><cell>73.5</cell><cell>23.4</cell><cell>35.5</cell></row><row><cell>Species</cell><cell>✗</cell><cell>95.6</cell><cell>90.4</cell><cell>92.9</cell><cell>59.3</cell><cell>45.0</cell><cell>51.2</cell><cell>51.6</cell><cell>8.6</cell><cell>14.7</cell></row><row><cell></cell><cell>✓</cell><cell>95.7</cell><cell>89.8</cell><cell>92.7</cell><cell>62.2</cell><cell>47.9</cell><cell>54.1</cell><cell>56.5</cell><cell>9.7</cell><cell>16.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Top-1 macro accuracy (%) on the test set of BIOSCAN-1M, binding DNA to frozen ( ) image and text encoders. We compare binding only to the image (following ImageBind<ref type="bibr" target="#b25">[25]</ref>) or aligning to both image and text. Here, we utilized BioCLIP's image and text encoders, with BarcodeBERT for DNA encoding.</figDesc><table><row><cell></cell><cell></cell><cell>Frozen</cell><cell cols="2">DNA-to-DNA</cell><cell></cell><cell cols="3">Image-to-Image</cell><cell cols="3">Image-to-DNA</cell></row><row><cell>Taxa</cell><cell>Bind to</cell><cell>Img DNA Txt</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>Order</cell><cell>Image</cell><cell>✗</cell><cell cols="3">100.0 100.0 100.0</cell><cell>88.5</cell><cell>86.0</cell><cell>87.2</cell><cell>87.8</cell><cell>64.8</cell><cell>74.6</cell></row><row><cell></cell><cell>Image &amp; Text</cell><cell>✗</cell><cell cols="3">100.0 100.0 100.0</cell><cell>88.5</cell><cell>86.0</cell><cell>87.2</cell><cell>88.7</cell><cell>71.9</cell><cell>79.4</cell></row><row><cell cols="2">Family Image</cell><cell>✗</cell><cell>100.0</cell><cell>97.6</cell><cell>98.8</cell><cell>84.3</cell><cell>68.4</cell><cell>75.5</cell><cell>79.6</cell><cell>43.1</cell><cell>55.9</cell></row><row><cell></cell><cell>Image &amp; Text</cell><cell>✗</cell><cell>100.0</cell><cell>97.2</cell><cell>98.6</cell><cell>84.3</cell><cell>68.4</cell><cell>75.5</cell><cell>78.1</cell><cell>45.4</cell><cell>57.5</cell></row><row><cell>Genus</cell><cell>Image</cell><cell>✗</cell><cell>98.2</cell><cell>92.9</cell><cell>95.5</cell><cell>62.6</cell><cell>47.1</cell><cell>53.7</cell><cell>56.0</cell><cell>17.9</cell><cell>27.1</cell></row><row><cell></cell><cell>Image &amp; Text</cell><cell>✗</cell><cell>98.9</cell><cell>93.2</cell><cell>96.0</cell><cell>62.6</cell><cell>47.1</cell><cell>53.7</cell><cell>56.5</cell><cell>19.7</cell><cell>29.2</cell></row><row><cell cols="2">Species Image</cell><cell>✗</cell><cell>95.1</cell><cell>87.9</cell><cell>91.3</cell><cell>44.1</cell><cell>32.4</cell><cell>37.4</cell><cell>39.5</cell><cell>7.4</cell><cell>12.5</cell></row><row><cell></cell><cell>Image &amp; Text</cell><cell>✗</cell><cell>95.9</cell><cell>88.6</cell><cell>92.1</cell><cell>44.1</cell><cell>32.4</cell><cell>37.4</cell><cell>38.0</cell><cell>8.5</cell><cell>13.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Top-1 accuracy (%) on our BIOSCAN-1M test set using the Image+DNA+Text model with image query. We compare nearest neighbour (NN) using only DNA keys, vs. our two strategies to use Image key for seen and DNA key for Unseen, either NN or a supervised linear classifier. We also compare against BZSL<ref type="bibr" target="#b3">[4]</ref> with our embeddings.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Micro top-1 acc</cell><cell cols="3">Macro top-1 acc</cell></row><row><cell>Taxa</cell><cell cols="2">Method Strategy</cell><cell cols="3">Seen Unseen H.M.</cell><cell cols="3">Seen Unseen H.M.</cell></row><row><cell>Order</cell><cell>NN</cell><cell>DNA</cell><cell>99.7</cell><cell>99.3</cell><cell>99.5</cell><cell>99.4</cell><cell>88.5</cell><cell>93.6</cell></row><row><cell></cell><cell>NN</cell><cell>IS+DU</cell><cell>99.4</cell><cell>99.2</cell><cell>99.3</cell><cell>99.3</cell><cell>89.3</cell><cell>94.1</cell></row><row><cell></cell><cell>Linear</cell><cell>IS+DU</cell><cell>99.5</cell><cell>99.2</cell><cell>99.3</cell><cell>99.3</cell><cell>88.3</cell><cell>93.5</cell></row><row><cell></cell><cell>BZSL</cell><cell>IS+DU</cell><cell>99.4</cell><cell>98.2</cell><cell>98.8</cell><cell>98.9</cell><cell>59.6</cell><cell>74.4</cell></row><row><cell cols="2">Family NN</cell><cell>DNA</cell><cell>96.5</cell><cell>87.1</cell><cell>91.6</cell><cell>90.8</cell><cell>50.1</cell><cell>64.6</cell></row><row><cell></cell><cell>NN</cell><cell>IS+DU</cell><cell>94.6</cell><cell>87.1</cell><cell>90.7</cell><cell>83.0</cell><cell>52.6</cell><cell>64.4</cell></row><row><cell></cell><cell>Linear</cell><cell>IS+DU</cell><cell>94.7</cell><cell>87.0</cell><cell>90.7</cell><cell>82.8</cell><cell>51.2</cell><cell>63.3</cell></row><row><cell></cell><cell>BZSL</cell><cell>IS+DU</cell><cell>95.6</cell><cell>80.3</cell><cell>87.3</cell><cell>88.0</cell><cell>32.5</cell><cell>47.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>Trainable temperature. Top-1 accuracy on the validation set for models contrastively trained with either a fixed or trainable temperature. We consider the performance when training for different durations (1 and 15 epochs).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Micro Top-1 Accuracy (%)</cell><cell></cell><cell></cell><cell>Macro Top-1 Accuracy (%)</cell><cell></cell></row><row><cell cols="8">Temperature Epochs DNA-to-DNA Image-to-Image Image-to-DNA DNA-to-DNA Image-to-Image Image-to-DNA</cell></row><row><cell>Fixed</cell><cell>1</cell><cell>97.2</cell><cell>62.6</cell><cell>27.4</cell><cell>93.1</cell><cell>41.6</cell><cell>11.0</cell></row><row><cell>Trainable</cell><cell>1</cell><cell>96.8</cell><cell>63.5</cell><cell>24.2</cell><cell>92.0</cell><cell>42.8</cell><cell>10.4</cell></row><row><cell>Fixed</cell><cell>15</cell><cell>98.0</cell><cell>74.2</cell><cell>57.8</cell><cell>94.7</cell><cell>52.2</cell><cell>32.4</cell></row><row><cell>Trainable</cell><cell>15</cell><cell>98.4</cell><cell>76.9</cell><cell>56.5</cell><cell>94.1</cell><cell>57.8</cell><cell>35.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 18 :</head><label>18</label><figDesc>Automatic mixed precision. Top-1 micro and macro accuracy on the validation set with models contrastively trained either with or without automatic mixed precision (AMP). We compare accuracies across different embedding alignments (image-to-image, DNA-to-DNA, and image-to-DNA). Both experiments have otherwise identical training conditions, including a batch size of 300 and 15 training epochs.</figDesc><table><row><cell></cell><cell></cell><cell>Micro Top-1 Accuracy (%)</cell><cell></cell><cell></cell><cell>Macro Top-1 Accuracy (%)</cell><cell></cell><cell>Memory</cell><cell>Training Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>per epoch↓</cell></row><row><cell>-AMP</cell><cell>98.07</cell><cell>74.97</cell><cell>57.38</cell><cell>95.18</cell><cell>54.47</cell><cell>32.22</cell><cell>75.54</cell><cell>4.03 hour</cell></row><row><cell>+AMP</cell><cell>97.85</cell><cell>74.31</cell><cell>56.23</cell><cell>97.85</cell><cell>53.65</cell><cell>30.99</cell><cell>60.64</cell><cell>1.15 hour</cell></row></table><note><p>DNA-to-DNA Image-to-Image Image-to-DNA DNA-to-DNA Image-to-Image Image-to-DNA CUDA (GB) ↓</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 19 :</head><label>19</label><figDesc>Low-rank adaptation. Top-1 micro/macro accuracy on the validation set for models contrastively trained with either with full fine-tuning or Low-Rank Adaptation (LoRA). We compare micro and macro Top-1 accuracies across different embedding alignments (image-to-image, DNA-to-DNA, and image-to-DNA). Both strategies use a batch size of 300 and are trained for a total of 15 epochs, allowing us to evaluate the impact of fine-tuning techniques on model performance and CUDA memory usage.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Micro Top-1 Accuracy (%)</cell><cell cols="3">Macro Top-1 Accuracy (%)</cell><cell>Memory</cell><cell>Training Time</cell></row><row><cell cols="8">Fine-tuning Method DNA-DNA Image-Image Image-DNA DNA-DNA Image-Image Image-DNA CUDA (GB) ↓</cell><cell>per epoch↓</cell></row><row><cell>Full Fine-Tuning</cell><cell>98.1</cell><cell>74.3</cell><cell>58.0</cell><cell>95.5</cell><cell>54.0</cell><cell>32.4</cell><cell>78.5GB</cell><cell>4.03 hour</cell></row><row><cell>LoRA</cell><cell>96.2</cell><cell>64.9</cell><cell>37.6</cell><cell>91.3</cell><cell>45.4</cell><cell>17.1</cell><cell>53.4GB</cell><cell>2.98 hour</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 20 :</head><label>20</label><figDesc>Batch size. Top-1 accuracy on the validation set for models contrastively trained with different batch sizes. Training at larger batch sizes helps improve accuracy at more fine-grained taxonomic levels such as genus and species.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Micro top-1 accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Macro top-1 accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Batch</cell><cell></cell><cell>Alignment</cell><cell></cell><cell cols="2">DNA to DNA</cell><cell cols="3">Image to Image</cell><cell cols="3">Image to DNA</cell><cell cols="2">DNA to DNA</cell><cell cols="3">Image to Image</cell><cell cols="2">Image to DNA</cell></row><row><cell>Taxa</cell><cell>size</cell><cell cols="3">Img DNA Txt</cell><cell cols="8">Seen Unseen H.M. Seen Unseen H.M. Seen Unseen H.M.</cell><cell cols="7">Seen Unseen H.M. Seen Unseen H.M. Seen Unseen H.M.</cell></row><row><cell>Order</cell><cell>500</cell><cell>✓</cell><cell>✓</cell><cell cols="4">✓ 100.0 100.0 100.0 99.6</cell><cell>99.6</cell><cell cols="2">99.6 99.6</cell><cell>99.2</cell><cell cols="2">99.4 100.0</cell><cell>92.9</cell><cell>96.3 99.6</cell><cell>98.5</cell><cell cols="2">99.0 99.1</cell><cell>75.8</cell><cell>85.9</cell></row><row><cell></cell><cell>1000</cell><cell>✓</cell><cell>✓</cell><cell cols="4">✓ 100.0 100.0 100.0 99.7</cell><cell>99.6</cell><cell cols="2">99.6 99.7</cell><cell>99.2</cell><cell cols="4">99.4 100.0 100.0 100.0 99.0</cell><cell>93.7</cell><cell cols="2">96.3 99.1</cell><cell>75.3</cell><cell>85.6</cell></row><row><cell></cell><cell>1500</cell><cell>✓</cell><cell>✓</cell><cell cols="4">✓ 100.0 100.0 100.0 99.7</cell><cell>99.6</cell><cell cols="2">99.7 99.7</cell><cell>99.2</cell><cell cols="2">99.4 100.0</cell><cell>92.8</cell><cell>96.3 99.5</cell><cell>94.3</cell><cell cols="2">96.8 99.6</cell><cell>73.6</cell><cell>84.6</cell></row><row><cell></cell><cell>2000</cell><cell>✓</cell><cell>✓</cell><cell cols="4">✓ 100.0 100.0 100.0 99.7</cell><cell>99.7</cell><cell cols="2">99.7 99.7</cell><cell>99.2</cell><cell cols="4">99.4 100.0 100.0 100.0 99.1</cell><cell>95.9</cell><cell cols="2">97.5 99.2</cell><cell>73.9</cell><cell>84.7</cell></row><row><cell cols="2">Family 500</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.9</cell><cell>99.6</cell><cell>99.8 95.6</cell><cell>93.9</cell><cell cols="2">94.7 94.8</cell><cell>86.2</cell><cell cols="2">90.3 100.0</cell><cell>97.6</cell><cell>98.7 88.8</cell><cell>79.3</cell><cell cols="2">83.8 83.5</cell><cell>52.5</cell><cell>64.5</cell></row><row><cell></cell><cell>1000</cell><cell>✓</cell><cell>✓</cell><cell cols="2">✓ 100.0</cell><cell>99.7</cell><cell>99.8 96.3</cell><cell>94.2</cell><cell cols="2">95.2 96.0</cell><cell>86.9</cell><cell>91.2</cell><cell>99.9</cell><cell>98.0</cell><cell>99.0 90.2</cell><cell>80.5</cell><cell cols="2">85.1 87.9</cell><cell>56.1</cell><cell>68.5</cell></row><row><cell></cell><cell>1500</cell><cell>✓</cell><cell>✓</cell><cell cols="2">✓ 100.0</cell><cell>99.6</cell><cell>99.8 96.5</cell><cell>94.3</cell><cell cols="2">95.4 96.7</cell><cell>87.3</cell><cell cols="2">91.8 100.0</cell><cell>97.3</cell><cell>98.6 92.0</cell><cell>81.3</cell><cell cols="2">86.3 91.7</cell><cell>53.9</cell><cell>67.9</cell></row><row><cell></cell><cell>2000</cell><cell>✓</cell><cell>✓</cell><cell cols="2">✓ 100.0</cell><cell>99.7</cell><cell>99.9 96.6</cell><cell>94.5</cell><cell cols="2">95.5 96.6</cell><cell>87.4</cell><cell cols="2">91.8 100.0</cell><cell>98.6</cell><cell>99.3 92.0</cell><cell>81.2</cell><cell cols="2">86.3 90.0</cell><cell>56.2</cell><cell>69.2</cell></row><row><cell>Genus</cell><cell>500</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.2</cell><cell>98.3</cell><cell>98.8 87.5</cell><cell>83.6</cell><cell cols="2">85.5 77.3</cell><cell>55.9</cell><cell>64.9</cell><cell>98.4</cell><cell>95.5</cell><cell>97.0 71.2</cell><cell>61.6</cell><cell cols="2">66.1 54.0</cell><cell>21.4</cell><cell>30.7</cell></row><row><cell></cell><cell>1000</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.4</cell><cell>97.9</cell><cell>98.6 88.7</cell><cell>84.5</cell><cell cols="2">86.6 82.4</cell><cell>58.2</cell><cell>68.2</cell><cell>98.4</cell><cell>94.9</cell><cell>96.6 74.4</cell><cell>63.9</cell><cell cols="2">68.8 61.2</cell><cell>24.3</cell><cell>34.8</cell></row><row><cell></cell><cell>1500</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.3</cell><cell>98.2</cell><cell>98.8 89.6</cell><cell>84.8</cell><cell cols="2">87.1 83.8</cell><cell>59.7</cell><cell>69.7</cell><cell>98.0</cell><cell>95.2</cell><cell>96.6 75.8</cell><cell>63.7</cell><cell cols="2">69.2 64.9</cell><cell>23.6</cell><cell>34.6</cell></row><row><cell></cell><cell>2000</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>99.4</cell><cell>98.4</cell><cell>98.9 89.6</cell><cell>84.9</cell><cell cols="2">87.2 84.8</cell><cell>60.1</cell><cell>70.3</cell><cell>98.9</cell><cell>96.5</cell><cell>97.7 76.1</cell><cell>64.2</cell><cell cols="2">69.6 64.9</cell><cell>24.0</cell><cell>35.0</cell></row><row><cell cols="2">Species 500</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>97.9</cell><cell>96.5</cell><cell>97.2 76.8</cell><cell>73.7</cell><cell cols="2">75.2 58.8</cell><cell>35.8</cell><cell>44.5</cell><cell>95.5</cell><cell>90.7</cell><cell>93.0 56.4</cell><cell>45.9</cell><cell cols="2">50.6 36.5</cell><cell>8.7</cell><cell>14.0</cell></row><row><cell></cell><cell>1000</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>98.2</cell><cell>96.0</cell><cell>97.1 78.8</cell><cell>74.8</cell><cell cols="2">76.7 67.2</cell><cell>37.7</cell><cell>48.3</cell><cell>95.8</cell><cell>89.4</cell><cell>92.5 59.8</cell><cell>47.3</cell><cell cols="2">52.8 44.3</cell><cell>9.2</cell><cell>15.3</cell></row><row><cell></cell><cell>1500</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>98.0</cell><cell>96.2</cell><cell>97.1 80.5</cell><cell>74.7</cell><cell cols="2">77.5 69.8</cell><cell>39.9</cell><cell>50.8</cell><cell>95.1</cell><cell>89.4</cell><cell>92.2 61.8</cell><cell>47.9</cell><cell cols="2">54.0 47.7</cell><cell>10.1</cell><cell>16.6</cell></row><row><cell></cell><cell>2000</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>98.5</cell><cell>96.5</cell><cell>97.5 80.0</cell><cell>74.9</cell><cell cols="2">77.3 70.5</cell><cell>41.3</cell><cell>52.1</cell><cell>96.9</cell><cell>90.9</cell><cell>93.8 61.3</cell><cell>48.0</cell><cell cols="2">53.9 47.7</cell><cell>9.9</cell><cell>16.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Loaded as vit base patch16</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="224" xml:id="foot_1"><p>in the timm library.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We acknowledge the support of the <rs type="funder">Government of Canada's New Frontiers in Research Fund (NFRF)</rs> [<rs type="grantNumber">NFRFT-2020-00073</rs>]. AXC and GWT are also supported by <rs type="funder">Canada CIFAR</rs> AI Chair grants. JBH is supported by the <rs type="funder">Pioneer Centre for AI (DNRF</rs> grant number <rs type="grantNumber">P1</rs>). This research was enabled in part by support provided by the <rs type="funder">Digital Research Alliance of Canada</rs> (alliancecan.ca). We also thank <rs type="person">Mrinal Goshalia</rs> and <rs type="person">Kian Hosseinkhani</rs> for testing the code, <rs type="person">Han-Hung Lee</rs> and <rs type="person">Yiming Zhang</rs> for helpful discussions, and <rs type="person">Manolis Savva</rs> for proofreading and feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_53QKcm3">
					<idno type="grant-number">NFRFT-2020-00073</idno>
				</org>
				<org type="funding" xml:id="_zwFY4gJ">
					<idno type="grant-number">P1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">11</ref><p>: We visualize the attention and the nearest retrievals for queries from seen and unseen species. Predicted genera + species are indicated green text for correct, red for incorrect. Here, we present the cases where only the I+D+T (Image+DNA+Text) model gives the correct results. In these cases, the attention map of the I+D+T model's image encoder activates well on the model's prediction results, while the other models largely do not attend visually to the entire insect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 TRAINABLE VS FIXED TEMPERATURE</head><p>We compare using a fixed temperature for the contrastive loss vs using trainable temperature (Table 17). We find that using the trainable temperature helps improve the performance, provided the model is trained for enough epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 AUTOMATIC MIXED PRECISION</head><p>For efficient training with large batch sizes, we use automatic mixed precision (AMP) with the bfloat16 data type. The bfloat16 data type gives a similar dynamic range as float32 at reduced precision, and provides stable training with reduced memory usage.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I+T I+D I+D+T</head><note type="other">Input</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.385</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Pablo Millan</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monireh</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">C</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Bruslund Haurum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuliia</forename><surname>Zarubiieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lila</forename><surname>Kari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Barcodebert</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2311.02401</idno>
		<idno type="arXiv">arXiv:2311.02401</idno>
		<title level="m">Transformers for biodiversity analysis</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective gene expression prediction from sequence by integrating long-range interactions</title>
		<author>
			<persName><forename type="first">Žiga</forename><surname>Avsec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Visentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Joseph R Ledsam</surname></persName>
		</author>
		<author>
			<persName><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kyle R Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Kelley</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-021-01252-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<idno type="ISSN">1548-7105</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1196" to="1203" />
			<date type="published" when="2021-10">Oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fine-grained zero-shot learning with DNA as side information</title>
		<author>
			<persName><forename type="first">Sarkhan</forename><surname>Badirli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">M</forename><surname>Dundar</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2021/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19352" to="19362" />
		</imprint>
	</monogr>
	<note>a18630ab1c3b9f14454cf70dc7114834-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying the unknown: Insect identification with deep hierarchical Bayesian learning</title>
		<author>
			<persName><forename type="first">Sarkhan</forename><surname>Badirli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">Johanna</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frannie</forename><surname>Richert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Dundar</surname></persName>
		</author>
		<idno type="DOI">10.1111/2041-210X.14104</idno>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1515" to="1530" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dennis A Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Cavanaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilene</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Karsch-Mizrachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Ostell</surname></persName>
		</author>
		<author>
			<persName><surname>Sayers</surname></persName>
		</author>
		<author>
			<persName><surname>Genbank</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkab1135</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.259</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SNP2Vec: Scalable self-supervised pre-training for genome-wide association study</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tze</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiffany</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuk</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.bionlp-1.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<editor>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Bretonnel Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 21st Workshop on Biomedical Language Processing<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="140" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rapid dissemination of taxonomic discoveries based on DNA barcoding and morphology</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matjaž</forename><surname>Kuntner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingi</forename><surname>Agnarsson</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep37066</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">37066</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DNA barcoding in herbal medicine: Retrospective and prospective</title>
		<author>
			<persName><forename type="first">Shilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianmei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwen</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpha.2023.03.008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Pharmaceutical Analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/chen20j/chen20j.pdf" />
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The devil is in the details: Problems in DNA barcoding practices indicated by systematic evaluation of insect barcodes</title>
		<author>
			<persName><forename type="first">Zhentao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fevo.2023.1149839</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Applications for deep learning in ecology</title>
		<author>
			<persName><forename type="first">Éric</forename><surname>Sylvain Christin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hervet</surname></persName>
		</author>
		<author>
			<persName><surname>Lecomte</surname></persName>
		</author>
		<idno type="DOI">10.1111/2041-210X.13256</idno>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1632" to="1644" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">When does contrastive visual representation learning work?</title>
		<author>
			<persName><forename type="first">Elijah</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14755" to="14764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The nucleotide transformer: Building and evaluating robust foundation models for human genomics</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Dalla-Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Mendoza-Revilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Lopez</forename><surname>Carranza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Henryk Grzywaczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Oteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Trop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Bernardo P De Almeida</surname></persName>
		</author>
		<author>
			<persName><surname>Sirelkhatim</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.01.11.523679</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision transformers need registers</title>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>id=2dnO3LLiJ1</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A reference library for Canadian invertebrates with 1.5 million barcodes, voucher specimens, and DNA samples</title>
		<author>
			<persName><forename type="first">Sujeevan</forename><surname>Jeremy R Dewaard</surname></persName>
		</author>
		<author>
			<persName><surname>Ratnasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Evgeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">V</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Borisenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">C</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">Hj</forename><surname>Telfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayme</forename><forename type="middle">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">R</forename><surname>Sones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Levesque-Beaudin</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0320-2</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">308</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DNA barcoding for detecting market substitution in salted cod fillets and battered cod chunks</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Di Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><forename type="middle">Di</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Terio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Bozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Bonerba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmondo</forename><surname>Ceci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppina</forename><surname>Tantillo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.foodchem.2013.05.093</idno>
	</analytic>
	<monogr>
		<title level="j">Food chemistry</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1757" to="1762" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Camille</forename><surname>Garcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Chouet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilien</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Titouan</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Salmon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pl@ntNet-300K: a plant image dataset with high label ambiguity and a long-tailed distribution</title>
		<ptr target="https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/7" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</editor>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>e7757b1e12abcb736ab9a754ffb617a-Paper-round2.pdf</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Gharaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Pellegrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuliia</forename><surname>Zarubiieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Bruslund Haurum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaclyn</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joschka</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Yun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jireh</forename><surname>Agda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujeevan</forename><surname>Ratnasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2023/file/87" />
		<title level="m">A step towards worldwide biodiversity assessment: The BIOSCAN-1M insect dataset</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="43593" to="43619" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. dbbdc3a685a97ad28489a1d57c45c1-Paper-Datasets_and_Benchmarks.pdf</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BIOSCAN-5M: A multimodal dataset for insect biodiversity</title>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Gharaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">C</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">Millan</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Pellegrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Bruslund Haurum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuliia</forename><surname>Zarubiieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lila</forename><surname>Kari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2024/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Mackey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="36285" to="36313" />
		</imprint>
	</monogr>
	<note>fdbb472813041c9ecef04c20c2b1e5a-Paper-Datasets_and_Benchmarks_Track. pdf</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageBind: One embedding space to bind them all</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023-06">Jun 2023</date>
			<biblScope unit="page" from="15180" to="15190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biological identifications through DNA barcodes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelley</forename><forename type="middle">L</forename><surname>Cywinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><surname>Dewaard</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspb.2002.2218</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Society of London. Series B: Biological Sciences</title>
		<meeting>the Royal Society of London. Series B: Biological Sciences</meeting>
		<imprint>
			<date type="published" when="1512">1512. 2003</date>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="page" from="313" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identification of meat and poultry species in food products using DNA barcoding</title>
		<author>
			<persName><forename type="first">Rosalee</forename><forename type="middle">S</forename><surname>Hellberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brenda</forename><forename type="middle">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">L</forename><surname>Hernandez</surname></persName>
		</author>
		<idno type="DOI">.org/10.1016/j.foodcont.2017.04.025</idno>
		<idno>1016/j.foodcont.2017.04.025</idno>
	</analytic>
	<monogr>
		<title level="j">Food Control</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="23" to="28" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nZeVKeeFYf9" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quilt-1M: One million image-text pairs for histopathology</title>
		<author>
			<persName><forename type="first">Wisdom</forename><surname>Ikezogwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saygin</forename><surname>Seyfioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Ghezloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatwir</forename><surname>Sheikh Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kumar Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2023/file/775" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="37995" to="38017" />
		</imprint>
	</monogr>
	<note>ec578876fa6812c062644964b9870-Paper-Datasets_and_Benchmarks.pdf</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><surname>Openclip</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5143773</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5143773" />
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</title>
		<author>
			<persName><forename type="first">Yanrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><forename type="middle">V</forename><surname>Davuluri</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btab083</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2112" to="2120" />
			<date type="published" when="2021-02">Feb 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/jia21b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection</title>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang-Thai</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van-Nui</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Su</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiolchem.2022.107732</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Biology and Chemistry</title>
		<idno type="ISSN">1476-9271</idno>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">107732</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning the histone codes with large genomic windows and three-dimensional chromatin interactions using transformer</title>
		<author>
			<persName><forename type="first">Dohoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-022-34152-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<idno type="ISSN">2041-1723</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6678</biblScope>
			<date type="published" when="2022-11">Nov 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining environmental DNA and remote sensing for efficient, fine-scale mapping of arthropod biodiversity</title>
		<author>
			<persName><forename type="first">Yuanheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Devenish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><forename type="middle">I</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damon</forename><forename type="middle">B</forename><surname>Lesmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Pichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taal</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2023.0123</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">2024</biblScope>
			<date type="published" when="1904">1904</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Applications of deep learning in understanding gene regulation</title>
		<author>
			<persName><forename type="first">Zhongxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elva</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juexiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.crmeth.2022.100384</idno>
	</analytic>
	<monogr>
		<title level="j">Cell Reports Methods</title>
		<idno type="ISSN">2667-2375</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100384</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating DNA barcoding for species identification and discovery in european gracillariid moths</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Lopez-Vaamonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Cama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camiel</forename><surname>Doorenweerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Godfray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Guiguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Gomboc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Huemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Franc ¸ois</forename><surname>Landry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Laštůvka</surname></persName>
		</author>
		<idno type="DOI">10.3389/fevo.2021.626752</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards a visual-language foundation model for computational pathology</title>
		<author>
			<persName><forename type="first">Ming Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Drew Fk Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Odintsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Phi Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2307.12914</idno>
		<idno type="arXiv">arXiv:2307.12914</idno>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The insect cytochrome oxidase I gene: evolutionary patterns and conserved primers for phylogenetic studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Szymura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Hewltt</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1365-2583.1996.tb00049.x</idno>
	</analytic>
	<monogr>
		<title level="j">Insect Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="165" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on image-based insect classification</title>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donatello</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Raveaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Arnault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Munier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Venturini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.12.020</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">UMAP: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1802.03426</idno>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DNA barcodes perform best with well-characterized taxa</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Paulay</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.0030435</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS Biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">435</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multimodal contrastive learning for spatial gene expression prediction using histology images</title>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiceng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changmiao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.08216</idno>
		<idno type="arXiv">arXiv:2407.08216</idno>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Taxonomic classification of DNA sequences beyond sequence similarity using deep neural networks</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Mock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fleming</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Kriese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Böcker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manja</forename><surname>Marz</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2122636119</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">35</biblScope>
			<date type="published" when="2022">2122636119. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wide-ranging barcoding aids discovery of one-third increase of species richness in presumably well-investigated moths</title>
		<author>
			<persName><forename type="first">Marko</forename><surname>Mutanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauri</forename><surname>Kaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Tabell</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep02901</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2901</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">HyenaDNA: Long-range genomic sequence modeling at single nucleotide resolution</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Faizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wornow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callum</forename><surname>Birch-Sykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Rabideau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2023/file/86" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="43177" to="43201" />
		</imprint>
	</monogr>
	<note>ab6927ee4ae9bde4247793c46797c7-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Insect-Foundation: A foundation model and large-scale 1M dataset for visual insect understanding</title>
		<author>
			<persName><forename type="first">Thanh-Dat</forename><surname>Hoang-Quan Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><forename type="middle">Bac</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khoa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Luu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2311.15206</idno>
		<idno type="arXiv">arXiv:2311.15206</idno>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Who is eating what: diet assessment using next generation sequencing</title>
		<author>
			<persName><forename type="first">Francois</forename><surname>Pompanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">E</forename><surname>Deagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Oc Symondson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">N</forename><surname>Jarman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Taberlet</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1365-294X.2011.05403.x</idno>
	</analytic>
	<monogr>
		<title level="j">Molecular ecology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1931" to="1950" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/radford21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">BOLD: The barcode of life data system</title>
		<author>
			<persName><forename type="first">Sujeevan</forename><surname>Ratnasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul Dn</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1471-8286.2007.01678.x</idno>
	</analytic>
	<monogr>
		<title level="j">Molecular ecology notes</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="364" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A DNA-based registry for all animal species: the barcode index number (BIN) system</title>
		<author>
			<persName><forename type="first">Sujeevan</forename><surname>Ratnasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul Dn</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0066213</idno>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">66213</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From categories to subcategories: large-scale image classification with partial class label refinement</title>
		<author>
			<persName><forename type="first">Marko</forename><surname>Ristin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298619</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun 2015</date>
			<biblScope unit="page" from="231" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">MycoAI: Fast and accurate taxonomic classification for fungal ITS sequences</title>
		<author>
			<persName><forename type="first">Andrius</forename><surname>Luuk Romeijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duong</forename><surname>Bernatavicius</surname></persName>
		</author>
		<author>
			<persName><surname>Vu</surname></persName>
		</author>
		<idno type="DOI">10.1111/1755-0998.14006</idno>
	</analytic>
	<monogr>
		<title level="j">Molecular Ecology Resources</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
			<date>14006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">TriCoLo: Trimodal contrastive loss for text to shape retrieval</title>
		<author>
			<persName><forename type="first">Han-Hung</forename><surname>Yue Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV57701.2024.00571</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5815" to="5825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Past, present, and future perspectives of environmental DNA (eDNA) metabarcoding: A systematic review in methods, monitoring, and applications of global eDNA</title>
		<author>
			<persName><forename type="first">Krista</forename><forename type="middle">M</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Saydur Rahman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.gecco.2019.e00547</idno>
	</analytic>
	<monogr>
		<title level="j">Global Ecology and Conservation</title>
		<idno type="ISSN">2351-9894</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">547</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1803.09820</idno>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2016/file/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>b180037abbebea991d8b1232f8a8ca9-Paper. pdf</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Analysing diet of small herbivores: the efficiency of DNA barcoding coupled with high-throughput pyrosequencing for deciphering the composition of complex plant mixtures</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Eeva M Soininen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Coissac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Miquel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Gielly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">K</forename><surname>Brochmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jørn</forename><forename type="middle">H</forename><surname>Brysting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><forename type="middle">A</forename><surname>Sønstebø</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><forename type="middle">G</forename><surname>Ims</surname></persName>
		</author>
		<author>
			<persName><surname>Yoccoz</surname></persName>
		</author>
		<idno type="DOI">10.1186/1742-9994-6-16</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in zoology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Towards a taxonomy machine: A training set of 5.6 million arthropod images</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujeevan</forename><surname>Ratnasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jireh</forename><surname>Agda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ait</forename><surname>Hamzah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaiah</forename><forename type="middle">C H</forename><surname>Boutou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaclyn</forename><forename type="middle">T A</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joschka</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine Y.-Y</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.3390/data9110122</idno>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<idno type="ISSN">2306-5729</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">BioCLIP: A vision foundation model for the tree of life</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Campolongo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carlyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Dahdul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger-Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52733.2024.01836</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2024-06">Jun 2024</date>
			<biblScope unit="page" from="19412" to="19424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How many species of insects and other terrestrial arthropods are there on Earth?</title>
		<author>
			<persName><forename type="first">Nigel</forename><forename type="middle">E</forename><surname>Stork</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-ento-020117-043348</idno>
		<idno type="PMID">28938083</idno>
	</analytic>
	<monogr>
		<title level="j">Annual review of entomology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="45" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A weakly supervised fine label classifier enhanced by coarse supervision</title>
		<author>
			<persName><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasser</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00656</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10">Oct 2019</date>
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ContIG: Self-supervised multimodal contrastive learning for medical imaging with genetics</title>
		<author>
			<persName><forename type="first">Aiham</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lippert</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.02024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20908" to="20921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Transfer learning enables predictions in network biology</title>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">V</forename><surname>Theodoris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeina</forename><forename type="middle">R Al</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Mantineo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Brydon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Shirley</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">T</forename><surname>Ellinor</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06139-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476- 4687</idno>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="issue">7965</biblScope>
			<biblScope unit="page" from="616" to="624" />
			<date type="published" when="2023-06">Jun 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<author>
			<persName><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/2812802</idno>
	</analytic>
	<monogr>
		<title level="m">YFCC100M: The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Grafit: Learning fine-grained image representations with coarse labels</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021-10">Oct 2021</date>
			<biblScope unit="page" from="874" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1908.08962</idno>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00914</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Open-set recognition: a good closed-set classifier is all you need?</title>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=5hLP5JY9S2d" />
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Nanopore sequencing technology, bioinformatics and applications</title>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Bollas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kin</forename><forename type="middle">Fai</forename><surname>Au</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41587-021-01108-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1348" to="1365" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fine-grained image analysis with deep learning: A survey</title>
		<author>
			<persName><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3126648</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="8927" to="8948" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<title level="m">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">What should not be contrastive in contrastive learning</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=CZ8Y3NzuVzO" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Spatially resolved gene expression prediction from histology images via bi-modal contrastive learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catia</forename><surname>Perciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonya</forename><surname>Macparland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">In</forename><forename type="middle">A</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2023/file/df656" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="70626" to="70637" />
		</imprint>
	</monogr>
	<note>6ed77b565e8dcdfbf568aead0a-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">BioTrove: A large curated image dataset enabling AI for biodiversity</title>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Feuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Jubery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Nakkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Zahid Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Chiranjeevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirmal</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asheesh</forename><surname>Baishnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumik</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirav</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinmay</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baskar</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><surname>Ganapathysubramanian</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2024/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Mackey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="102101" to="102120" />
		</imprint>
	</monogr>
	<note>b92854f80ba4feefb973959b259dbc2c-Paper-Datasets_and_Benchmarks_Track. pdf</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspreet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Valluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tupini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Mazzola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swadheen</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2303.00915</idno>
		<idno type="arXiv">arXiv:2303.00915</idno>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v182/zhang22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Machine Learning for Healthcare Conference</title>
		<editor>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Sendak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Sjoding</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</editor>
		<meeting>the 7th Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2022-08">Aug 2022</date>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="5" to="06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">DNABERT-2: Efficient foundation model and benchmark for multi-species genomes</title>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><forename type="middle">V</forename><surname>Davuluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oMLQB4EZE1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">DNABERT-S: Learning species-aware DNA embedding with genome foundation models</title>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Davuluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2402.08777</idno>
		<idno type="arXiv">arXiv:2402.08777</idno>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.11</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>