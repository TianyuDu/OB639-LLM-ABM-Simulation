paper_forum,paper_id,title,decision,review_id,reviewer_id,summary,strengths,weaknesses,questions
UHg1xTRzZK,UHg1xTRzZK,Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation,Reject,oMGCQ3oLMJ,ICLR.cc/2025/Conference/Submission13856/Reviewer_pUWZ,"The paper proposed rationale distillation, with a purpose on improving LLM-based models translation performance while keeping the general instruction-following abilities. The method first generates the explanation of the translation and use the synthetic data for fine-tuning along with the translation pairs.","The paper successfully did what they proposed, improving the translation performance with the proposed method can significantly reduce the damage in general instruction-following tasks.","1. The paper's goal is good —it aims to generalize translation models based on LLMs to other domains such as conversational tasks. However, the attempt to balance both translation quality and generalization does not yield a model that is truly useful, placing the paper in an awkward position. For example, RaDis performs substantially worse than its backbone model on conversations and instruction following, and its translation performance is inferior to that of translation-specific models. While I acknowledge that comparing RaDis with ALMA in terms of translation is not entirely fair due to differences in models and training data, this raises another issue: the model's performance is highly dependent on the backbone model. Consequently, RaDis cannot be effectively applied to low-resource languages like Icelandic, which were excluded from the training data. Achieving effective machine translation in LLMs requires substantial effort in multilingual pretraining and alignment; without this, English-centric LLM translation performance is less interesting. Despite the paper's claims of keeping both translation high performance and retaining generality, the translation quality is not top due to the high dependency of the backbone model, and the performance on other tasks is also worse.

2. Inference speed: Regarding inference speed, if the model is trained to output both a reference and an explanation, does this mean the generated output always includes both, requiring the user to extract the translation from the combined output? If so, this could significantly reduce inference speed. It would be beneficial to see a comparison of inference speeds and an analysis of how many additional tokens the model generates compared to translation-specific LLMs.

3. Missing baselines: A fundamental baseline would involve training on a combined dataset of translation parallel data and instruction-following data. Using synthesized data may be unnecessary given the abundance of existing data that can be combined with parallel data. However, I did not find results corresponding to this baseline in the paper. Including such comparisons would strengthen the evaluation and provide a clearer understanding of the model's performance relative to more straightforward approaches.",
UHg1xTRzZK,UHg1xTRzZK,Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation,Reject,WLVuZGI9k6,ICLR.cc/2025/Conference/Submission13856/Reviewer_JdnZ,The paper introduces RaDis (Rationale Distillation) -- a self-distillation technique to help instruction-tuned language models learn new tasks without losing their general capabilities or compromising safety alignment. The method first generates rationales - explanations for the instruction-response pairs of the target task using the same language model and these rationales serve as replay data that help retain the original capabilities by fairly capturing the data distribution. The generated rationales and instruction-response pairs are then used for subsequent training of the same language model on the new task. The authors conduct experiments for the task of machine translation and demonstrate that it improves the translation proficiency of models while preserving its overall performance on other tasks.,"1. The authors propose a straightforward method for LLMs to learn the translation task (in this study) without losing prior abilities by combining standard fine-tuning on reference translations with self-distillation on generated rationales.

2. The experimental results demonstrate translation improvements across 4 language pairs on 2 different models.

3. The authors study various aspects of the proposed method to understand the influence of these aspects on downstream and overall performance.","1. The paper only focuses on improving translation proficiency in LLMs without losing existing capabilities. However, I think the proposed method is more general and can work for other tasks. It should also be experimented on other tasks (e.g. summarization, open-ended QA, MQM annotation, etc) for a more comprehensive study.

2. In section 5.2, the authors compare the Mistral 7B model (student) with rationales generated from different models. However, this comparison may not be fair and it might be more appropriate to compare the rationale generation from a bigger model within the same family. For instance, the Llama 3 7B model (student) with rationales generated from Llama 3 70B. Could you clarify the reasons for not using models from the same families (e.g. Llama 3)?","The proposed method generates a rationale for each sample in the training set. Did you experiment with ablating whether you need a rationale for each sample to be included in the training set? How well does the proposed method perform when rationales aren’t available for every sample in the training set? What is the tradeoff between having rationales for all samples versus just a subset of *k* samples (e.g., 10%, 20%, 50% and 100% samples with rationales)?"
UHg1xTRzZK,UHg1xTRzZK,Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation,Reject,pEJQTfPVVv,ICLR.cc/2025/Conference/Submission13856/Reviewer_M6wr,"This paper proposes a novel method, RaDis, which uses self-generated rationales for sequence-level distillation. It can preserve the model's capabilities in general domains and safety while fine-tuning downstream tasks, addressing the problem of catastrophic forgetting. The paper conducts machine translation experiments on two 7B-sized models to illustrate the effectiveness of the method, and the analysis section provides the detailed observation of the experimental results.","1. This paper proposes a novel method, RaDis, which has a clear and straightforward motivation, and the writing is easy to follow.
2. The experimental results demonstrate that this method is effective and significantly surpasses the related baselines.","1. This method effectively maintains the generalization ability and safety of the models during downstream task fine-tuning. However, most translation performance degrades compared to the original fine-tuning baseline, which will limit its applicability in real-world scenarios. Additionally, the comparison of translation performance between the backbone and RaDis in lines 81-83 is unfair and the comparison with the vanilla fine-tuned model is more reasonable and informative.
2. Considering the baseline distillation methods (SeqKD and SDFT) largely relies on the generation quality of the teacher model, it is necessary to conduct the experiments with teachers with better capacity, especially for SeqKD, which is rarely used in self-distillation manner.
3. In the analysis section, the explanations on the performance of RaDis with three different teachers needs a more detailed analysis, similar to that in Table 4. Particularly, it is necessary to clarify why the machine translation performance and the general performance yield contrasting conclusions (e.g., the self-generated setting achieve best general performance and the worse translation performance among the three settings).
4. Typo: line 507, “SDFT greatly enhances” -> “RaDis greatly enhances”","1. In the context of machine translation, the emphasis on faithfulness over diversity suggests that paraphrasing references may not efficently enhance translation performance, which aligns with the primary experimental results presented in Tables 1 and 2. Consequently, to facilitate a more equitable comparison between RaDis and SDFT, as well as to explore the potential of RaDis in other tasks, it is essential to evaluate their corresponding performance on logical reasoning tasks, including OpenFunctions, GSM8K, and HumanEval.
2. See the weeknesses."
UHg1xTRzZK,UHg1xTRzZK,Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation,Reject,9eU4c0k0Zv,ICLR.cc/2025/Conference/Submission13856/Reviewer_cw7y,"This paper introduces Rationale Distillation (RaDis), a new approach for fine-tuning Large Language Models (LLMs) on machine translation (MT) tasks. Unlike traditional fine-tuning methods that rely solely on parallel corpora, RaDis uses LLM-generated rationales to enhance MT performance while maintaining the LLM’s general capabilities, such as instruction-following and reasoning.","RaDis is both straightforward and effective, leveraging the language model itself to generate explanations (rationales) for fine-tuning. Experimental results demonstrate that this method preserves the model’s general abilities while enhancing MT performance, although its MT performance remains slightly below established baselines like ALMA.","Firstly, the proposed method lacks novelty from my perspective, as leveraging LLMs to generate data from their own distribution for continual fine-tuning has been extensively explored in prior research (e.g., various PPO/DPO variants along with works on chain-of-thoughts that prompt LLM to generate reasoning trace, etc.,). Moreover, based on my experiences with machine translation (MT) and reinforcement learning with human feedback (RLHF), the generative quality of a 7B model is often unreliable, typically requiring filtering processes (i.e., from a reward model). Incorporating all rationales generated by the LLM without any scoring, filtering, or grounding is likely to reinforce the model's own hallucinations. Even if this approach preserves the model’s instruction-following abilities, the practical utility of such fine-tuning remains questionable.

Secondly, the rationales derived from a translation dataset may lack diversity. A comparison or discussion with existing work, such as TOWER [1], is necessary. TOWER achieves high MT performance by utilizing parallel corpus and diverse instruction-following datasets during fine-tuning, which appears more effective on benchmarks (and the idea is more straightforward than rationales generation). Therefore, I believe another baseline that simply uses parallel data for fine-tuning + diverse instruction-following dataset for preserving the model's pretrained ability is needed.

[1] Alves et al., (2024). TOWER: An Open Multilingual Large Language Model for Translation-Related Tasks","1. some comparison and discussion on previous work like TOWER will be useful.
2. I am curious if there is any analysis of the generated rationales’ quality. I see in Table 4 there are some categorization but are all the generated responses relevant? I doubt that a 7B model can give very high-quality rationales for all translation pairs in the training data.
3. I like the analysis in section 5.2 and it actually resonates with my points in weakness. Fine-tuning model on its self-generated response is most helpful to preserve its instruction-following ability. However, I believe simply using all self-generated rationales is also limited. I am wondering if any on-policy-based mechanism for rationales generation is tried.
4. Overall, I find the experiments comprehensive but the improvement is not very surprising. The experiment also lacks an obvious baseline: using a combination of parallel corpus for MT and instruction-following dataset for preserving general ability (like the approach used in TOWER)"
599F4CZ0HB,599F4CZ0HB,Bench-O-Matic: Automating Benchmark Curation from Crowdsourced Data,Reject,KdGonTm6Hw,ICLR.cc/2025/Conference/Submission13585/Reviewer_EWC5,"The work proposes Bench-O-Matic, a system for automatically curating high-quality, open-ended LLM benchmarks by using large-scale, crowd-sourced data. This tool addresses the need for evolving benchmarks that adapt to the rapid development of LLMs without requiring human intervention.","- Bench-O-Matic efficiently creates high-quality benchmarks from crowd-sourced data without human input, the work addressed the scalability issue in benchmark curation.
- The work Introduces novel metrics like Separability with Confidence and Pair Rank Brier Score, enhancing the robustness and reliability of benchmark assessments.
- Eval-O-Matic achieves strong performance alignment with human preferences for only $20 per evaluation, and provides a cost-effective alternative to static benchmarks.","- Quality insurance. The seven quality criteria may not fully encompass the diversity of user tasks, potentially favoring specific types of prompts over others.
- The synthesis of data is reliant on on LLMs as Judges. The LLM-as-a-Judge framework may introduce stylistic or self-bias, even with adjustments, which could influence benchmark objectivity in certain cases.",Is there any estimation on the error/quality of the data generated? Or using some metrics to evaluate the similarity of the generated data with the real-world data?
599F4CZ0HB,599F4CZ0HB,Bench-O-Matic: Automating Benchmark Curation from Crowdsourced Data,Reject,IDxHwieXzT,ICLR.cc/2025/Conference/Submission13585/Reviewer_3LVj,"The paper introduces an automated pipeline, Bench-O-Matic, designed to curate prompts and create benchmarks for evaluating large language models (LLMs). The authors propose new metrics to assess benchmark quality, ensuring a clear separability of confidence scores and alignment with human preferences. The prompts are organized into topic clusters to ensure diversity, and an ""LLM-as-a-Judge"" approach is used to evaluate responses from various LLMs, fully automating the evaluation process. Additionally, the paper presents two novel benchmarks generated using this pipeline: Eval-O-Matic, based on Chatbot Arena, and Wild-O-Matic, derived from WildChat-1M.","- The problem statement is clearly defined.
- The paper addresses a significant challenge highly relevant to the current state of AI and places well in the current literature.
- The pipeline is flexible and open-ended, allowing for continuous improvements over time.
- The experiments are comprehensive, demonstrating that the pipeline effectively creates benchmarks based on the metrics defined in the paper, with multiple LLMs evaluated on Eval-O-Matic.
- The paper presents new ideas to evaluate benchmarks to overcome previous issues.","- Using an LLM to evaluate other LLMs’ responses may limit the complexity of the benchmark prompts. While employing an ensemble of judges partially mitigates this issue, there is still an inherent limitation. However, the advantages of an automated pipeline outweigh this concern, and the authors have implemented techniques to reduce evaluation biases.

I have a hard time finding weaknesses for the paper. It is a well-executed and solid paper, though not necessarily groundbreaking.

**Minor Comments** 
- On line 80, ""achieve 98.6% correlation"" should be ""achieve**s** 98.6% correlation"".
- On line 82, ""Our work**s** makes"" should be ""Our work makes"".
- On lines 206 and 352, ""Section C"" should probably be changed for ""Appendix C"" for clarity.
- On line 464, ""an regression based approach"" should be corrected to ""**a** regression-based approach.""","- Previous studies have shown that fine-tuning the LLM-as-a-Judge can significantly improve evaluation robustness. Has this been considered in the current work? This could help improve the quality of the judges, the main limitation of this benchmark.
- In Section 4.2, it states, ""We also ensure the final dataset is free from personally identifiable information or offensive content."" Could the authors elaborate on how this is achieved? Was this done manually or automatically with the help of an LLM?"
599F4CZ0HB,599F4CZ0HB,Bench-O-Matic: Automating Benchmark Curation from Crowdsourced Data,Reject,WMAEG9XbEZ,ICLR.cc/2025/Conference/Submission13585/Reviewer_5rNy,The paper proposes approaches to automate the benchmark generation process via the prompting of LLMs. The proposals for different characteristics to establish the baselines are fair and the contributions are around the different scoring mechanisms to 1) evaluate the quality of prompts 2) LLM-based judging of prompt outputs to generate 1-5 score instead of binary preferences and 3) combining them with statistical aggregators to differentiate end evaluate different LLM outputs.,"The promise of the paper is excellent if delivered -- Reconfigurable automated benchmarks without humans in the loop and via crowd sourced data. With a series of prompting techniques in the pipeline, the approach is fair and well studied. Key innovations are in the design of metrics to separate various models and crux of thesis on generating evaluation data that is of high quality and can be separable.","The key weaknesses around this paper are the claims that the proposed approach is human-free and easily configurable as shown in the Table comparing the multiple methods. Given that the approach leverages use of ChatBotArena supplied queries  and even though the quality filter will remove the specific poor quality queries, it is not free from the input i.e., humans prompting the different LLMs on the arena and easily being configured to a use case that end users may have in mind. Discussing results on adapting the evaluation framework to beyond what is available in Chat bot Arena would be needed to support the claims of the paper.  Also it would be good to discuss potential biases introduced by using ChatBotArena queries as a starting point. The paper could be strengthened by providing concrete examples or experiments showing how their approach could be adapted to different domains or use cases beyond ChatBot Arena data



An additional area of concern is that almost every step of the pipeline involves a prompt engineering exercise including scoring the final models on a scale of 1-5. This is standard but the question emerges on the fidelity of the LLMs themselves and when they hallucinate themselves. As evidenced by the score sorted by topic cluster, the data does show that for exact answer situations like Python game coding versus loose open ended questions the LLM-judges are not very good.  To strengthen the paper - discuss potential failure modes or biases introduced by relying heavily on LLMs, provide more detailed analysis of how performance varies across different types of questions or topics and suggest ways to mitigate or detect potential hallucinations or errors introduced by LLMs in the pipeline


The details of human annotation were very unclear. See questions below.","- Is the approach really adaptable/configurable ? Restate the claims if not. 
- Can the approach work irrespective of humans in the loop ? i.e., crowd-sourcer providing initial prompts. 
- human Annotation study; 
    How many human annotators were involved?
    What was the inter-annotator agreement?
    How were discrepancies between annotators resolved?
    Were the human annotators experts in any particular domains?"
UW0zetsx8X,UW0zetsx8X,Prompt Optimization with Human Feedback,Reject,pkZopzKgTP,ICLR.cc/2025/Conference/Submission13379/Reviewer_jWbb,"This paper introduces APOHF, for optimizing prompts for large language models (LLMs) using only human preference feedback rather than numeric scores. APOHF iteratively selects prompt pairs for user comparison, training a neural network to predict each prompt’s utility based on feedback. The algorithm selects prompts by combining utility prediction with an exploration-exploitation approach inspired by dueling bandits. Applied across tasks like instruction optimization and prompt tuning for text-to-image models, APOHF demonstrates more efficient prompt selection compared to baseline methods under limited feedback conditions.","* This paper presents a novel approach to prompt optimization by using human preference feedback alone, which is suitable for black-box LLMs.
* This paper is generally clear in its method description, though certain theoretical justifications could be expanded for a more rigorous understanding of APOHF’s design choices.
* The algorithm design balances prompt utility prediction with exploration, showing reliable performance across varied tasks.","* The method employs the Bradley-Terry-Luce (BTL) model to represent human preference feedback, which assumes consistency and transitivity in user preferences. This may oversimplify human feedback, especially in real-world applications where user preferences can be inconsistent or influenced by context. The reliance on binary feedback might also limit the granularity of information available to the model, potentially leading to suboptimal prompt choices. 
* The method relies on a user-provided initial task description to generate the prompt domain, assuming that these initial examples are representative of the task requirements. This dependency introduces the potential for bias if the initial examples do not capture the full scope of the task or if the user’s interpretation is inconsistent with the intended outcomes. This reliance can constrain the model's flexibility and lead to prompts that are effective only in limited or narrowly defined scenarios.
* APOHF presumes that user preferences are consistent and relevant across multiple iterations, assuming stability in what constitutes an optimal prompt. However, in complex, open-ended tasks or tasks that evolve over time, user preferences may shift, and certain prompts that were optimal initially may no longer be relevant. This assumption limits the model's adaptability to dynamic contexts, reducing its applicability in real-world tasks where user expectations or task goals may evolve.","* Have the authors examined how noise in user feedback impacts the accuracy of the prompt selection? Is the model robust to feedback inconsistencies or context-dependency in user preferences?
* Is there a way to iteratively refine or expand the prompt domain based on user feedback, to counteract biases introduced by an incomplete initial task description?
* Has the algorithm been tested in settings where user preferences or task requirements change over time? If so, how does APOHF handle shifts in user preferences?"
UW0zetsx8X,UW0zetsx8X,Prompt Optimization with Human Feedback,Reject,Lg6hXSy2no,ICLR.cc/2025/Conference/Submission13379/Reviewer_WviX,"This paper mentions a common issue of black-box LLM usages, where it is unavailable to automatically obtain a quality score for a given prompt, thus causing the difficulty of prompt optimization. Assuming that only human’s preference feedback is reliable, the authors propose APOHF, a framework to perform prompt optimization with human feedback. The proposed method aims to determine a good prompt via a prompt selection algorithm inspired by Dueling Bandits, based on a neural network performance predictor trained with the information from a small number of human feedback instances. The authors demonstrate quality improvement throughout iterations of the optimization. Overall, the experiment results show that the proposed framework is effective to obtain appropriate prompts with human feedback.","-  Inspired by the concept of RLHF and Dueling Bandits, the authors propose a prompt optimization framework leveraging human feedback. The experiment results show that the proposed method is indeed effective to a certain extent with actual demonstrations.

-  With the Bandit-fashioned prompt selection strategy, powered by a trained NN (MLP) model as the performance predictor, the proposed framework determines the prompt efficiently in terms of the number of required human feedback instances.

-  In the appendix, the authors also provide a brief coverage of theoretical explanations to justify the proposed prompt selection strategy.","-   Limited Comparison with Prompt Optimization Baselines:
The authors' decision to exclude comparisons with state-of-the-art prompt optimization methods (e.g. TextGrad), citing the lack of a scoring method, may be overly restrictive. While direct human scoring might not be feasible, surrogate evaluation metrics (e.g., embedding-similarity between LLM output and ground truth) could serve as viable alternatives for these comparisons. Given the superior performance of existing prompt optimization methods across various tasks, their inclusion in the performance comparisons would provide valuable context and a more comprehensive evaluation of APOHF's effectiveness relative to the current state of the art.

- Dependency on Neural Network Models for Prompt Selection:
The proposed framework's prompt-pair selection strategy heavily relies on a trained neural network (MLP) model for performance prediction based on input embeddings. This approach introduces two potential sources of variability:
a) The choice of embedding model
b) The architecture and training of the performance predictor (MLP)
Both components may significantly influence the accuracy of performance predictions, potentially leading to substantial variations in the final LLM performance. A more robust analysis of these components' impact on the overall framework would strengthen the paper's credibility.

- Ambiguity in Prompt Domain Generation:
The process of generating the ""discrete domain of prompts X"" lacks sufficient detail. While the authors mention using a powerful LLM (e.g., ChatGPT) for prompt generation via in-context learning, the specifics of this crucial step remain unclear. Moreover, the paper lacks experimental analysis demonstrating how different prompt domain generation methods affect APOHF's overall performance. This omission limits the reader's ability to fully assess the method's robustness and generalizability.",see weaknesses
UW0zetsx8X,UW0zetsx8X,Prompt Optimization with Human Feedback,Reject,n6xi8YuAFz,ICLR.cc/2025/Conference/Submission13379/Reviewer_Y27t,"This paper mainly studies the prompt optimization problem. In specific, the main motivation coms from that previous PO methods usually require the availability of a numeric score to assess the quality of every prompt. This score is difficult or sometime unable to get in the situation when human interact with a black-box LLM system. Therefore, the authors claimed that preference feedback is more feasible given the mentioned human-llm interaction scenario. The authors thus proposed a method named APOHF for prompt optimization with human feedback. Some results show that this method can find a good prompt with several preference feedbacks used.","The paper is well written and easy to follow
The motivation is clear compared to previous method. [which may not be practical as per the weakness part]","The main weakness is the prompt optimization from human feedback itself may not be practical. Let me explain the reasons. 
1) The advantage or the main efforts of LLM is to improve its instruction following abilities, I.e., to cover as many prompts as possible. Therefore, when human in the loop, the key efforts should be in the zero-shot/few-shot abilities. If you expect the user to give feedback to the prompt provided by him/her-self, it may negatively impact the feasible application of this LLM product. How does your approach complement efforts to improve zero-shot/few-shot abilities of LLMs? Are there specific scenarios where human feedback on prompts provides unique value, even as LLMs improve their general instruction-following capabilities?


2) Prompt optimization itself is important for sure, but the efforts on making it automatic are more useful. For instance, as many APO did, LLM can improve the prompt itself by self-reflecting (e.g., reasoning the errors and self-refining current prompts). How does your method compare to or complement automated prompt optimization techniques like self-reflection? Are there potential advantages to incorporating human feedback alongside automated methods?

3) If let’s say we need human preference, we can directly train a reward model. Given the generalization ability of the reward model, we can directly sample the best response with requiring the human-preference during inference. How does your approach compare to training a reward model on human preferences? Could you discuss the potential advantages or disadvantages of your method compared to using a pre-trained reward model?

In addition, the experiments can be done on more types of tasks, such as math, coding, role-play, etc.","Most of questions have been listed in the weakness part.

Another general question or suggestion would be: Could the author please raise up several real examples that can show the scenarios where the proposed method will be feasible to apply?"
UW0zetsx8X,UW0zetsx8X,Prompt Optimization with Human Feedback,Reject,6MO1LOWcEr,ICLR.cc/2025/Conference/Submission13379/Reviewer_bMVs,"The authors of this paper studied prompt optimization using dueling bandit. The basic idea is to ask users to provide preference feedback on responses generated by two prompts, which is directly attributed to the quality of the prompts for further selection. Experiments on prompt optimization in text-to-text and text-to-image generation tasks, and also in response optimization, demonstrate the effectiveness of the proposed solution.","1.	Prompt optimization is an important problem in LLM studies, and the paper provides a valid perspective.
2.	The clarity of the manuscript is satisfactory, which helps readers to best comprehend the technique details.","1.	The proposed solution is rather standard: running dueling bandits on top of ChatGPT generated rewrites of initial queries. The synergy between these two components is very weak. Ideally, the proposal of new prompts should be informed by the learnt scoring function. But the current way, the new prompts are sampled from ChatGPT independently from the scoring function.
2.	As the users’ feedback is about the response, rather than the prompt itself, the LLM that generates the responses matters. Specifically, the prompt quality is a function of generator LLM as well. This clearly limits the generality of the proposed solution: the scoring function learnt with one LLM might not work well for other LLMs. Unfortunately, there is no experiment investigating this factor. 
3.	A few statements made in the paper are somehow overclaiming, for example, using a text encoder can avoid the need of a whitebox LLM, which does not seem to be advantageous, since the availability of whitebox LLM is no worse than an opensource text encoder. And I do not see what the principle is behind choosing the highest scored prompt in history as the first prompt, except it works better than randomly choosing two.","1.	As the user feedback is provided to the response, instead of the prompt directly, how to we account for the variance caused by sampling from the LLM? For example, the user feels one result being better than another could be caused by LLM sampling, rather than the prompt. And I am not sure if this variance can be simply factored into the BT model.
2.	Does the learnt scoring function work across different generator LLMs? Similarly, does the learnt scoring function generalize across different tasks, e.g., question answering vs., text summarization?"
UW0zetsx8X,UW0zetsx8X,Prompt Optimization with Human Feedback,Reject,Im3e56yLwu,ICLR.cc/2025/Conference/Submission13379/Reviewer_X4tm,"In existing prompt optimization work, the vast majority of methods rely on numerical scores to select better-performing prompts. However, in certain real-world tasks (e.g., text-to-image generation), using numerical scores for evaluation may not be applicable. To address this issue, the authors propose a human feedback-based prompt optimization method—APOHF. This method first collects pairs of human preference data and trains a neural network to provide latent scores aligned with human preferences. Then, combining greedy search and the method of maximizing the upper confidence bound, the authors select two prompts from multiple directions, balancing both performance and diverse prompt exploration. Experimental results show that after several iterations, APOHF can generate high-quality prompts.","1. The APOHF method proposed by the authors aims to solve the problem of prompt optimization in tasks that are difficult to evaluate using numerical scores, marking a new attempt different from previous studies.
2. APOHF outperforms other baseline methods in terms of performance.
3. The wide range of experimental types further validates the effectiveness of APOHF.
4. The authors' writing is clear, and the content of the paper is concise and easy to understand.","### 1. Experiment

a) From Table 6 in the appendix, I understand that for the user instruction optimization task, the authors chose two tasks, ""rhymes"" and ""word sorting"" (presumably from the BigBench dataset), which can be evaluated using numerical scores (such as accuracy), making them compatible with methods like APE, OPRO, and APO. I believe the authors should discuss this in more detail. Considering the rebuttal time constraints, You may focus on an in-depth analysis of 1 or 2 methods (such as APO, OPRO). Additionally, regarding the ""sentiment"" task in Table 6, why did the authors choose an initial instruction with a score of 0? For a simple instruction like ""Determine the sentiment category (positive or negative) of a given sentence,"" a score of 0 should not occur.

b) I recommend that the authors revise the way the curves are represented in the figures. The current use of square, circular, and triangular symbols makes the figures cluttered, making it difficult to discern curve details.

### 2. Motivation
a) For prompt tasks that are difficult to evaluate with numerical scores (e.g., text-to-image generation), the optimization target of the APOHF method is individual samples (such as generating images of a garden or a street). I believe the practical significance of such single-sample optimization is limited. In everyday usage, this approach has high resource costs, making it inefficient. In industry, optimizing meta prompts holds greater practical value.

b) As I know, APOHF is not the first prompt optimization work considering human feedback. Harvard and MIT have published a similar paper named **PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling (https://arxiv.org/pdf/2402.08702)**. I think it maybe diminish the contribution of this work.","1. Regarding the resource consumption of APOHF, what is the specific cost? How many API calls were made in total, and how many tokens were consumed during the experiments?

2. In Figure 3, does it show the score of the newly generated prompt after each iteration? Why doesn't APOHF exhibit score fluctuations? Logically, the scores of results generated by different prompts should vary, making it unlikely for them to remain consistent.

3. In the text-to-image task, the authors measure the quality of generated images by calculating the similarity between the generated images and the ground truth. Could this similarity metric also be used as a scoring standard for methods like APO and OPRO?

4. How many data samples does the test set include?

5. Please answer my question refer to **Weakness  Section**"
LNL7zKvm7e,LNL7zKvm7e,Frame-Voyager: Learning to Query Frames for Video Large Language Models,Accept (Poster),eBwdjhho5d,ICLR.cc/2025/Conference/Submission13344/Reviewer_pMFu,"This paper proposed Frame-Voyager that learns to query informative frame combinations, based on the given textual queries in the task.
Authors introduced a new data collection and labeling pipeline, by ranking frame combinations using a pre-trained Video-LLM.
Extensive experiments are conducted to support the effectiveness of the proposed Frame-Voyager.","This is a reasonable extension from previous keyframe selection work [1] where it relies more on single keyframe selection, and does not consider temporal relations/modeling among frames. The proposed pseudo-label scheme from a VLM + list of combinations of frames makes sense to me. 
Authors also conduct extensive experiments across diverse popular benchmarks to show their effectiveness. 
Overall, I think the proposed Frame-Voyager makes a good contribution to the keyframe selection in video-language studies. 

[1] Self-chained image-language model for video localization and question answering. NeurIPS23","1. As the paper mentioned, this pseudo-label strategy is not scalable, and I think the frame combination part is a bit tricky. This is like creating artificial rewards according to video (like a sandbox in RL) to train the reward model. However,  the reward is not always reliable from a VLM even though we compute it according to GT answers. For example, as shown in some previous studies [2],  the model will generate correct answers when provided with wrong localized clips. It is true that we might give performance improvement when using such reward data created by a human-free pipeline, but it is more like an adapter / DPO style adaptation to the model rather than letting the model truly be grounded on query-related / informative frames. 

2. I think the weakness in 1 somehow affects the proposed method to require a relatively complex pre-training data construction/filtering/design, since the positive/negative signal is too sensitive according to video conditions in this framework from my view (e.g. frame blurring / redundancy).

3. Also, the keyframe selection for video-language understanding is not a brand new topic, many related works in track try to propose different ways, from continuous space learning to discrete pipeline, to address question-aware moment detection. I would suggest to include those works [1,2,3,4] for a more comprehensive study. 


[1] ViLA: Efficient Video-Language Alignment for Video Question Answering. ECCV24.  
[2] Can i trust your answer? visually grounded video question answering.  CVPR24.  
[3] TimeCraft: Navigate Weakly-Supervised Temporal Grounded Video Question Answering via Bi-directional Reasoning. ECCV24.  
[4] Self-Adaptive Sampling for Accurate Video Question Answering on Image Text Models. ACL24.","Please see the weaknesses. And extra questions:

(1) What is the trade-off between efficiency and effectiveness? Can you provide some metrics like running time/memory usage/flops to the proposed methods? As the model contains an extra keyframe localization stage, it is worthy showing those results to see the trade-off.

(2) I notice that the proposed method uses VILA as a reference model to label rewards, and uses the same VILA series models for downstream tasks. Is this also a kind of self-rewarding strategy shown in the sevilla work? Or the proposed method/modules can zero transfer to other VLM like llava-ov/qwen-vl-2?

(3) to answer my 1st weakness, I would suggest authors conduct extra experiments on Next-GQA to see the grounded QA results with grounded metrics.

(4) Can you show some off-shelf llm-based localization tools like sevila localizer in Table 2? It would be interesting to see the comparison between a llm-based reasoning method with the proposed reward imitation learning method.

(5)  Regarding the claim in Figure 3 and RQ3 (Lines 431-433), is this true? Related work (e.g., [1]) suggests that some observed issues may stem from limitations in the model side (VILA). Could the authors clarify this?

[1] LONGVIDEOBENCH: A Benchmark for Long-context Interleaved Video-Language Understanding"
LNL7zKvm7e,LNL7zKvm7e,Frame-Voyager: Learning to Query Frames for Video Large Language Models,Accept (Poster),0eReGS6JM9,ICLR.cc/2025/Conference/Submission13344/Reviewer_6u8S,"In this paper, the authors present Frame-Voyager which learns to query informative frame combinations, based on the given textual queries for video qa tasks. The authors create a data collection pipeline for the proposed model/training objection and evaluate it on four video QA benchmarks.","1. The motivation is clear and easy-to-follow. 

2. The presentation of the paper is of high quality. 

3. The analysis is comprehensive.","1. The inflexibility of the proposed frame selection method. Different videos usually have different “information density”, which means that some video could be represented by even a single frame and some need densely sampled frames. This also would be affected by the query type. The number of keyframes in the proposed framework seems to be a fixed hyper-parameter, which would be hard to generalize to all different videos in the wild (for different length / query types, need different hyper-parameter selection). Is it possible for the model to do adaptive/dynamic frame selection? 

2. The authors mention in line 170-172 that “the smaller combinations exhibit generalization capabilities when larger values of M and T are used during inference for longer video”. However, the process of this generalization is unclear (the introduction of the inference process, top-K selection is also unclear, in line 258-261). Does the inference directly apply the model to higher frame number or is the concatenation of several combinations with the highest score, what is the K setting for inference? Also, based on this, could the authors elaborate more on how Frame-Voyager makes good use of global video context? 

3. In Table 1 and Figure 3, the authors only compare Frame-Voyager with the uniform sampling baseline using the same frame number of frames for Question-Answering. However, Frame-Voyager actually sees more frames for keyframe selection (128 frames and select 8 frames), which creates unfair comparisons. Also there seems to be very little improvement in performance while using 8 keyframes from 128 frames compared to just uniform sample 16 frames. Could the authors show the comparison of Frame-Voyager against the best uniform sampling configurations for a more fair comparison?","All question in weakness and: 

1. Since the authors claim that the proposed method is a plug-and-play module, does the reference Video-LLM for data collection have to be the same model family with the MLLM for Frame-Voyager? Since different models have different features, whether the generated data only works for the ViLA-1.5 model family? It would be interesting to see the performance of these annotated data’s effectiveness on different model families like LLaVA-onevision and so on. 

2. The authors are encouraged to discuss the comparison with a related line of research which leverages LLM agents to select meaningful keyframes from the video input, including VideoAgent [1], VideoTree[2] … 

3. Also, could the authors validate the effectiveness of the frame selection method on temporal grounding as well? Which seems to be very similar in high-level (localizing key information given the text query). 

[1] VideoAgent: Long-form Video Understanding with Large Language Model as Agent

[2] VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos"
LNL7zKvm7e,LNL7zKvm7e,Frame-Voyager: Learning to Query Frames for Video Large Language Models,Accept (Poster),2h0Jgh5QM8,ICLR.cc/2025/Conference/Submission13344/Reviewer_qbz8,"This paper introduces Frame-Voyager, a novel frame selection method designed to improve the performance of Video-LLMs on video question-answering (QA) tasks. The key contributions are (1) an automated data collection strategy that ranks frame combinations according to the QA loss from a reference Video-LLM, and (2) a frame selection module integrated with a Video-LLM, trained using this ranked data. When applied to VILA, Frame-Voyager outperforms the uniform sampling baseline across four video QA benchmarks.","**1. Originality:** The novel formulation of frame selection as a ranking problem is compelling. This approach minimizes the need for extensive manual labeling to obtain ground truth, offering a fresh and efficient perspective on the task.

**2. Significance:** Frame selection is a critical challenge in video understanding due to the high dimensionality of video data and the computational demands involved. Traditional uniform sampling often misses relevant content, making this problem a key focus for advancing the field.

**3. Clarity:** The paper is well-written, with a clear explanation of the motivation and methodology. The authors effectively communicate the concepts, making the approach easy to understand.","**1. Data Collection Cost:**
The number of frame combinations increases exponentially with M and T (L166), leading the authors to limit M to 16 or 32 and T to 2 or 4. Despite these restrictions, evaluating $C(32, 4) \\approx 36K$ combinations for a single QA sample is still costly, raising concerns about data collection efficiency. Thus, this paper needs to provide more details on the computational resources and time required for data collection.

**2. Generalization to More Frames:** 
The method is trained to select up to T=4 frames from M=32 candidates. Although the authors claim the model generalizes well to larger M and T values (L170), Fig. 3 shows diminishing gains over uniform sampling while increasing the number of frames. Despite the authors attributing this to benchmark constraints (L431), prior research (e.g., LongVILA [1], LongVA [2]) has shown that additional input frames can enhance QA performance, suggesting potential limits in generalization. Given the impracticality of increasing M and T for data collection, robust generalization is crucial. Thus, the authors should extend their experiments to more input frames, possibly integrating Frame-Voyager with LongVILA, to help clarify the generalization capabilities and limitations of Frame-Voyager.

**3. Fairness of Comparisons:** 
The comparison between Frame-Voyager and CLIP is not fully equitable due to three factors: (i) Frame-Voyager uses superior backbones (SigLIP or InternViT-6B); (ii) it is trained on specialized ranking data, unlike CLIP; and (iii) it has additional parameters like self-attention modules. Since (ii) and (iii) are core contributions of Frame-Voyager, a fairer comparison would involve using SigLIP or InternViT-6B models with and without training on the collected data. Additionally, the authors should specify the versions of CLIP and SigLIP used.

**4. Latency Analysis:** 
The authors claim that Frame-Voyager outperforms LongVILA while processing fewer frames, but this only holds for the ""Long"" subset of the Video-MME benchmark. In other scenarios, LongVILA performs better. Given the extra parameters introduced by Frame-Voyager, a detailed efficiency comparison between VILA+Frame-Voyager and LongVILA is necessary, including latency, memory usage, and computational complexity.

---
[1] Xue F, Chen Y, Li D, et al. Longvila: Scaling long-context visual language models for long videos[J]. arXiv preprint arXiv:2408.10188, 2024.

[2] Zhang P, Zhang K, Li B, et al. Long context transfer from language to vision[J]. arXiv preprint arXiv:2406.16852, 2024.","My questions focus primarily on the weaknesses outlined above:

1. What are the data collection costs, particularly for the VideoChatGPT data?

2. How well does the method generalize when increasing the number of frame candidates and selected frames?

3. Can the proposed model outperform SigLIP and InternViT-6B, especially when they are fine-tuned on the same collected data?

4. What is the latency of the method?

5. A suggestion: Could CLIP-like models enhance data collection efficiency? Specifically, using them to estimate query-frame similarities might identify frames with extremely high or low similarities, indicating positive or negative frame combinations. This could reduce the search space and improve data collection efficiency."
LNL7zKvm7e,LNL7zKvm7e,Frame-Voyager: Learning to Query Frames for Video Large Language Models,Accept (Poster),vt40qHKW5S,ICLR.cc/2025/Conference/Submission13344/Reviewer_ULTS,"The paper presents FRAME-VOYAGER, a novel approach for improving Video Large Language Models by selecting informative frame combinations based on textual queries. Traditional methods like uniform frame sampling and text-frame retrieval do not account for variations in information density or complex instructions, leading to sub-optimal performance. FRAME-VOYAGER addresses this by learning to query frame combinations with lower prediction losses, using a pre-trained Video-LLM for supervision. The method is demonstrated to enhance performance in video question answering tasks across various benchmarks. The approach also includes a new data collection and labeling pipeline, which ranks frame combinations to train the model efficiently. FRAME-VOYAGER is proposed as a plug-and-play solution, improving Video-LLMs without significant computational overhead. Experiment results show that FRAME-VOYAGER achieves significant performance in several VQA datasets.","1.The authors designed an efficient keyframe data construction method, exploring keyframe selection using combinations of all frames. This approach achieved significant improvements across multiple datasets with 12K high-quality clips.

2.The authors state that they will open-source the data and related code, which is valuable for the open-source community.

3.The proposed method demonstrate strong performance across multiple datasets, proving the robustness of the approach, especially in Anet dataset. As a plug-and-play method, Frame-Voyager can serve as a valuable tool for video understanding.","1.Data scaling limitations. As the authors mentioned, selecting 4 frames from 32 results in 35960 candidates, leading to prohibitively high costs for data scaling. Furthermore, keyframe selection is even more critical for longer videos, and the costs become unmanageable when dealing with selections from 128 or 256 frames.

2.When testing on short video benchmarks, did VILA and VILA+FRAME-VOYAGER evaluate performance by selecting 2 frames out of 16? If so, please provide the results of VILA using 16 frames on the Next-QA and ANQA datasets. Additionally, I believe that comparing results based on only 2 frames is not meaningful, as most models typically evaluate performance using 8 or 16 frames.

3.Please provide additional results of the model on more benchmarks, such as MVbench and egoSchema.",See the above weakness. I am interested in how the authors consider data scaling. Constructing data with only 16 or 32 frames is not very helpful for understanding long videos.
jCDF7G3LpF,jCDF7G3LpF,EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING,Accept (Poster),QLKRXGEwtu,ICLR.cc/2025/Conference/Submission13287/Reviewer_GXrd,This paper proposes a novel Sequence of Context (SoC) jailbreak attack that leverages Multi-Armed Bandit (MAB) to automatically guide context selection. The authors provide an in-depth theoretical analysis of the upper bound on the expected sequence length. Experimental results demonstrate the effectiveness of the proposed method in jailbreaking language models.,"1. The use of MAB for automated context selection in jailbreaking is novel
2. The theoretical derivation of the upper bound for the SoC attack length is well-established
3. The experimental results effectively demonstrate the method's efficacy","1. Compared to other automatic jailbreak attacks (e.g., GCG, PAIR), this method requires dataset collection and policy model training, making it more resource-intensive and time-consuming
2. The proposed method is limited to pre-defined harmful query categories, and its extensibility to unseen categories is not thoroughly investigated
3. The paper lacks comparison with state-of-the-art jailbreak attacks in terms of attack success rate and computational cost","1. Could you explain the necessity of including the direct malicious query (DMQ) in the context? Given that language models with safety alignment typically reject DMQs, would it be possible to remove DMQ from the context to reduce context length?
2. How does the proposed judgment method compare with widely-used judgment systems (e.g., Llama Guard family) that are more common in jailbreak literature?"
jCDF7G3LpF,jCDF7G3LpF,EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING,Accept (Poster),PBzOGlvVM7,ICLR.cc/2025/Conference/Submission13287/Reviewer_uGpw,"This paper proposes a novel jailbreaking attack paradigm named the sequence of contexts (SoC) attacks. By leveraging techniques in multi-armed bandit (MAB), this paper maximizes the likelihood of a successful jailbreaking attack (decided by CSQ). A theoretical upper bound for the gap between the obtained and optimal rewards is presented. Experimental results show that the proposed strategy indeed enhances the effectiveness of jailbreaking attacks.","**About novelty**

+ This paper studies jailbreaking attacks from an MAB perspective, bringing new insight into this area of research.

**About contribution**

+ This paper proposes a DMQ dataset that includes 3000 queries collected from previous works. The following works can use this as a benchmark.
+ The experimental results (figures 2 and 3) demonstrate that the proposed method enhances the effectiveness of jailbreaking methods, compared to the naive strategy. 

**About presentation**

The presentation is clear. The algorithms and figures are well-made.","**About contribution**

+ This paper does not compare the proposed jailbreak attack with the existing methods.
+ The only theorem in this paper is almost trivial, and more importantly, the assumptions and limitations of this theorem are not discussed. 
It is encouraging to include theoretical analysis in LLM research. However, the results are not strong enough to serve as ""one of the main contributions"" (as stated in Line 253) of an ICLR paper.

In brief, the authors claim three main contributions: creating a dataset, proposing a novel jailbreaking attack strategy, and providing a theoretical analysis. However, I think contributions (ii) and (iii) are slightly overstated. That is why I gave a score of 5 for this paper. 

**About presentation**

+ This paper contains too many acronyms. I suggest adding a list of acronyms in the appendix. Besides, the authors do not provide a detailed explanation for some of the terms (e.g., sequence of context attack, and context switching queries) at their first appearance. It would make this paper more easy to follow if the authors add some explanations for the terms and reference them (e.g., see Section xxx for detailed discussions) when the term is mentioned for the first time.

+ The citation style (i.e., the green boxes) is dazzling.",See the weakness part.
jCDF7G3LpF,jCDF7G3LpF,EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING,Accept (Poster),F4qFFLalIO,ICLR.cc/2025/Conference/Submission13287/Reviewer_rSU9,"This paper discovers the LLMs intend to reject direct malicious queries (DMQs), but answer when followed by content switching queries (CSQs). For automatically generating DMQs and CSQs, the authors propose a framework based multi-armed bandit (MAB) to jailbreak LLM automatically. They introduce a dataset of CSQs based on MAB. And they give a mathematical derivation for the proposed method to prove key bounds.","### 1. The discovery about switching the context leads jailbroken
This paper presents a context switching attack, which is a novel method compared to existing works. And combined with multi-armed bandit, SoC can automatically jailbreak LLMs.


### 2. Theoretical Results
Section 4 establishes a upper bound on the length of SoC attack sequence. And I believe a method which has a mathematical proof is more solid than existing jailbreak works.","### 1. Poor readability
In the abstract and in the overview (Figure 1) , the authors mention ""multi-armed bandit (MAB)"", but they do not explain what is MAB in the introduction.I suggest the authors revise this, it confuses me until I have read related works.

In addition to MAB, the introduction has unreasonable content. With four paragraphs, more than half introduction is irrelevant to the contribution of this paper. And in the third paragraph, there are too many concepts are proposed but lack of details. I suggest that the authors adjust their introduction, current version has poor readability.

I think it is not appropriate to introduce how this paper uses MAB in related work. And why is T both rounds and sequence length (*total reward over T rounds*: line133; *attack sequence length T*:line137)? What does T represent in the subsequent content?

There are a lot of abrupt concepts introduced without much explanation in context. In line 215, I can not find any information or reference about *policy $\\pi$* and *action-value Q* (including cost C in line 240). I can only speculate that this has something to do with reinforcement learning. And in Algorithm 1, what does  $E_{explore}$ stand for and what does  $E_{exploit}$ stand for? I can not find any explanation.

### 2. Lack of comparison
This paper only demonstrate SoC can jailbreak LLMs, but has no comparison with prior works. I believe that there are many excellent baseline jailbreak methods. The SoC attack has similar format with the In-Context Attack[8] which also jailbreak LLMs based on context. Besides, the authors optimize their method with reinforcement learning, but the binary reward is very similar to PAIR, which use LLM optimize jailbreak prompt[9].


### 3. Few experiments
In the entire paper, only four figures in Figure 3 prove that SoC is effective, and there is a lack of comprehensive experiments from multiple angles. For example, with different hyperparameters such as J and K, I am not sure whether this method can be generalized or only performs well under certain specific parameters.

---

[8]  Jailbreak and guard aligned language models with only few in-context demonstrations

[9] Jailbreaking Black Box Large Language Models in Twenty Queries","### 1. Related Works
I believe that in the `White-Box Attack`, the authors should not cite a lot of attack but is irrelevant to jailbreak. Besides GCG, there are many white-box attack, such as fine-tuning attack[1, 2], improved GCG[3], interpretability-based[4, 5]. Nevertheless, I am only making a suggestion. Whether the author makes modifications or not will not change my rating.

### 2. Why does the authors think modern LLMs avoid responding to malicious questions by classifier?
In Section 3.1 line 155-line157, the authors mention ""In most instances, such queries fail to produce harmful responses and can be guarded using straightforward strategies, such as employing a classifier to flag harmful words and phrases"". Current LLMs do not use those filter, but are fine-tuned to align with human values[6, 7].

### 3. There is no update for $\\pi$ or $\\pi^{*}$
Algorithm 1 aims to optimize a policy $\\pi$, however, where is update for $\\pi$ or $\\pi^{\\star}$. Why can Algorithm 1 obtain a optimized policy $\\pi^{\\star}$? This confuses me as to how Soc Attack works.

### typos
1. Incorrect use of quotation marks: line 153-155, 
2. Do B and C refer to the appendix? line 201 & 202 & line 318 & line 321 & line 365
3. Do you mean harmful or harmless? line 213-214: *which assigns a binary reward indicating whether the response is harmful or unsafe.*
4. *see-D.0.1*, *see-D.0.2*, *see-D.0.3* and *see-D.0.4* mean Appendix D.0.1, Appendix D.0.2, Appendix D.0.3 and Appendix D.0.4?

---


[1] FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!

[2] SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS

[3] AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs

[4] Uncovering Safety Risks in Open-source LLMs through Concept Activation Vector

[5] How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States

[6] Training language models to follow instructions with human feedback

[7] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"
jCDF7G3LpF,jCDF7G3LpF,EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING,Accept (Poster),il94SJJiIH,ICLR.cc/2025/Conference/Submission13287/Reviewer_pzyz,"This paper proposes a novel method for jailbreaking large language models (LLMs) through ""Sequence of Contexts"" (SoC) attacks and utilizes a multi-armed bandit (MAB) framework to automate the optimization of the attack process. The paper also provides an in-depth theoretical analysis of the sequence length required for a successful jailbreak and the convergence of total rewards.","The study not only experimentally demonstrates the efficiency of the proposed method, achieving an attack success rate of over 95%, but also provides a solid theoretical foundation for LLM jailbreak attacks.","1. It is a natural question: why do you not compare your work with other methods，such as GCG[1] , AutoDAN[2], PAIR[3], RENELLM[4] and so on? 
2. Since your work requires selecting a sequence of context-switching queries, I am curious about it time complexity. 
3. In my opinion, testing only on Mistral and Llama is not sufficient to demonstrate the advantages of your work. Moreover, you have only chosen small LLMs (up to 8B), which is not convincing. As far as I know, Llama has a 13B version. What's more, CHATGPT is necessary to be choose.
In conclusion, without comparisons, it is difficult for me to fully assess the contributions of this paper, especially considering that there are many papers on LLM jailbreaks. If you address my concerns, I will consider giving a higher score.

Ref:
[1] Universal and transferable adversarial attacks on aligned language models.
[2] Autodan: Generating stealthy jailbreak prompts on aligned large language models.
[3] Jailbreaking black box large language models in twenty queries.
[4] A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",See weekness.
MF7ljU8xcf,MF7ljU8xcf,Compute-Optimal LLMs Provably Generalize Better with Scale,Accept (Poster),FjNwKoFByy,ICLR.cc/2025/Conference/Submission13266/Reviewer_RfBZ,"This paper investigates why LLMs generalise better, using new theoretical bounds that quantify generalization in LLMs. Authors introduce an empirical Freedman-type martingale concentration inequality refining generalization sounds by considering the variance in the loss function, and influenced by 1) The parameters per token 2) The loss variance and 3) The quantisation error. They show that generalization gap decreases predictably as LLMs scale up. The bounds are empirically tested using Pythia model checkpoints on the Pile dataset, illustrating how loss variance and quantisation error decrease with scale. Last, they argue from an information theory perspective that information content in a model grows sublinearly with scale, allowing models to generalize better with fewer bits per parameters, hence improving quantizability.","The paper stands out for its ambition in taking down a complex question with originality, and its potential to inform future model scaling and generalisation research in LLMs. The use of a Freedman-type martingale concentration inequality, which incorporates the empirical variance of the loss function is both creative and original. The focus on optimal scaling through the chinchilla law, along with a brand new perspective (to my knowledge) on quantizability and information transfer, showcase originality. The authors well motivate the use of Freedman inequality by the need for higher bounds. The choice to base the empirical evaluation on the Pythia model also provides a reasonable foundation to explore the theoretical claims.","While the paper makes an ambitious attempt to understand generalization in LLMs, there are a few core weaknesses. The theoretical assumptions and methods could benefit from further grounding and justification. The empirical Freedman inequality and the notion of loss variance lack through derivation and validation across different settings, which may leave doubts about the generality of these bounds for LLMs in diverse applications. The empirical results, are limited to the single Pythia model architecture, constraining the scope of the findings (Paper would benefit expanding the evaluation to other widely-used LLMs eg. GPT, T5, BERT…). Moreover, some sections are overly technical without sufficient motivation, making presentation difficult to follow, especially for the proposed quantisation bound and its integration with the chinchilla scaling laws. Some of the notation is dense, and (if) introduced with minimal intuition. Examples:

— $\\Sigma$: The term “loss variance” that is central is introduced without a clear mathematical definition or discussion. A clarification whether this variance refers to the empirical variance of the loss function over tokens or across model params would make it more understandable.
— $C$ The complexity term is introduced with very limited intuition and authors do not explain how this is derived, or what aspects of model size, dataset, or quantisation contribute to it.","— Have you empirically tested the argument on quantizability from the Hessian spectrum and QuIP framework beyond GPTQ? 
— The information-theoretic argument using prequential coding is interesting but could benefit from further empirical support. Have you tested the information content predictions under varied trading or scaling setups? For instance, would be nice to understand how your complexity term $L(h)/D$ changes for models trained on different datasets or architectures
— Are there any specific factors that you believe yolk affect the generalization bound accuracy? Eg. Dataset size, architecture choice"
MF7ljU8xcf,MF7ljU8xcf,Compute-Optimal LLMs Provably Generalize Better with Scale,Accept (Poster),xyncY34SgP,ICLR.cc/2025/Conference/Submission13266/Reviewer_JfkK,"This paper explains the high generalization performance of large language models (LLMs) by introducing a new Freedman-type concentration inequality and constructing a generalization bound based on the Chinchilla scaling law. The generalization bound is decomposed into three factors: the number of parameters in the model, the token-wise loss variance, and the performance gap associated with quantization. This bound quantitatively shows how the generalization gap reduces as the size of the LLM scales. In particular, it shows that the amount of information grows sublinearly with the LLM size, indicating that the resources required for the model to efficiently integrate information are reduced. This finding provides an information-theoretic explanation for why LLMs can improve generalization ability with computationally optimal scaling and point out a new relationship between the model size and information.","The authors' approach to explaining LLM performance improvements by breaking down the generalization bound into three key components is particularly valuable. Furthermore, introducing a new inequality based on martingale concentration enables analysis specific to LLMs, where data is non-independent and identically distributed. It opens up a new path to theoretical understanding of generalization performance. This is a significant contribution to LLM research.","This paper contributes to the theoretical understanding of LLMs and confirms their validity using real LLM data. However, further verification is needed for practical applications. In particular, the merits of decomposing the generalization bound are unclear from the demonstrations in Fig.2(right) and Fig.3(center), for example.","* As shown in Eq.(6), for example, it is explained that $K(h)$ is scaled by $\\tilde{O}(D^{1-\\beta})$, with this exponent $\\beta$ coming from the Chinchilla scaling law. If this exponent $\\beta$ can take a non-trivial value, is it possible to constrain its value using the authors' theory?

* The implications of the theoretical results are discussed in Sec.5.3, but I'm not entirely sure I understand the argument. I assume the authors focus on Pythia due to the availability of models with various parameter sizes; however, investigating other language models in the same way may be difficult. Given these limitations, how do the authors envision extending this type of analysis? Could they elaborate on potential insights or outcomes if similar analyses were feasible for other models?

* Minors
* The citation to the appendix on line 186 should be ""Appendix"" rather than ""Section."" There are several similar cases.
* Please check the ICLR format for citing equations."
MF7ljU8xcf,MF7ljU8xcf,Compute-Optimal LLMs Provably Generalize Better with Scale,Accept (Poster),FqtJejoflb,ICLR.cc/2025/Conference/Submission13266/Reviewer_y45d,"This work provides a novel generalization bound for LLMs consisting of three components depending on the ratio of the number of parameters to tokens, loss variance, the performance gap from model quantization. The paper first provides a new martingale concentration inequality that can be evaluated empirically. The rest of the components of the bound come from loss variation, a smoothing constant, and quantization gap, all of which can be computed empirically. Experiments show that the obtained bounds are very close to evaluated loss functions in LLMs. The paper also provides an alternate bound based on information accumulation in LLMs using prequential coding, which is a less empirical bound. Nevertheless, gives insight on model complexity with changing model scale while keeping the ratio of dataset to parameters constant. Overall, the paper provides novel generalization bounds for LLMs with interesting insights to the working of LLMs and how they vary with scale.","- The paper presents two interesting generalization bounds for LLMs, one of them can be empirically calculated and shows results close to actual loss obtained in experiments. The other bounds gives insight on generalization bounds in LLMs with increase in model size with constant data to parameters ratio.
- For computing the first bound, the model derives a new concentration inequality based on the Freedman’s inequality that can be empirically computed and is likely to be of interest in future works.
- The empirical evidence provided in the paper showing that it is close to the obtained bounds in interesting.","- It would be good to improve the readability of the paper a bit by distilling down the mathematics and giving an intuition for both the bounds in summary.
- It would be better to provide the empirical evidence after both the bounds instead of providing it in between. E.g., some plots are provided in Sec. 5, whereas the empirical evidence is given in Sec. 4.","- A general question: Are the empirical bounds extendable to LLM performance with inference-compute scaling?, e.g. in [1]

[1] Sardana, Nikhil, et al. ""Beyond chinchilla-optimal: Accounting for inference in language model scaling laws."" arXiv preprint arXiv:2401.00448 (2023)."
MF7ljU8xcf,MF7ljU8xcf,Compute-Optimal LLMs Provably Generalize Better with Scale,Accept (Poster),HN8HsEYw8R,ICLR.cc/2025/Conference/Submission13266/Reviewer_wrVe,"The authors present a new upper bound on the single-token generalisation error of language models. Evaluations of this bound are compared with selected empirical data from the training history of a language model.


EDIT: I have raised the score, on the understanding that the authors will amend the title as discussed and significantly clarify the exposition of the theorems, including to clearly state that the 'with probability 1- \\delta' refers to the sampling of both the X_k and Y_k.",The authors' main result is interesting: an apparently novel generalisation bound which can be evaluated from data. The comparisons they make between their bound and empirical data is also of interest. Hopefully this bound will be useful in future for more efficiently guiding the development of AI models,"1. I am left wondering whether this is really an appropriate venue for this work. Proving the results stated in the actual paper requires 10 pages of dense technical appendices introducing a wealth of additional lemmas, theorems and corollaries which are not reviewable in the timeframe. The results themselves do not appear to be standard, and I feel would benefit from a proper presentation in a format, such as a journal, which gives sufficient space for a detailed investigation of the technical details.
2. The statement of several of the main results do not appear to be complete, and require extra clarification. In particular, the authors should take care to ensure that all notation is defined.
    - Does the 'with probability $1 - \\delta$' in Theorems 3.1, 3.2, and 3.4 refer to the sampling of $X_k$, $Y_k$, or both? If both, the authors should clarify how their results on the *joint* probability of $X_k$ and the fictitious random variable $Y_k$ can be connected to standard generalisation bounds which look at the probability with respect to $X_k$. The random variables over which probabilities are being measured should be explicitly stated in all results.
    - The random variables $Y_k$ implicitly appear in the result of Theorem 3.4 (through $\\Sigma$), but are not defined in the statement of the theorem.
    - In Theorem 3.1, what is $\\mathcal{F}_k$? I expect this is some form of filtration, but it is undefined.
    - In Theorem 3.1, ${X_{k}}$ are ${\\mathcal{F}{k}}$-measurable, while $Y_k$ are $\\mathcal{F}_{k-1}$-measurable. Given the ambiguities of the notation, it is unclear what this distinction actually refers to.
    - The term 'NLL' is frequently used throughout, but is never defined.
3. In Theorems 3.1, 3.2 and 3.4, the size of the finite set $K$ is a key component of the 'complexity' measure $\\mathcal{C}$. Yet, $K$ itself seems to be just a technical construction in the proof, rather than something fundamental to the model. The authors should clarify what role this set plays, and why it should be considered as part of the complexity of the model, rather than simply an artefact of the proof. They should also indicate how this set can be chosen in practice (since the bounds are supposed to be computable). I see from Section 4 that they use 1000 points, but offer no indication of why this choice was made.
4. Additional random variables $Y_k$ are introduced and used in most of the main results. The authors state after Theorem 3.1 that they take this to be the mean of the model NLL, but I do not see what this can be guaranteed to satisfy the requirement that $Y_k - X_k > -\\Delta$ (and how is the value of $\\Delta$ chosen for this setup?). In fact, why not just take $Y_k = X_k - \\Delta + \\epsilon$ for some fixed value of $\\Delta$ and a small value of $\\epsilon > 0$? I do not see anything in the statement of the theorem preventing this, and it appears to optimise the bound.
4. The authors only compare their computable bound to empirical data for selected so-called 'compute-optimal' checkpoints. Why is this choice made? How does the bound compare for other checkpoints? This does not appear to be a requirement of the theorems, and so the rationale should be made explicitly clear.
5. I am not convinced that the results presented by the authors justify the claim in the title. It seems to me that, in fact, their results show that increasing the size of the model will in general *increase* their generalisation bound, by increasing the complexity term. In fact, the authors themselves appear to advocate for compressing and quantising models. The authors should explicitly justify such a strong claim in the paper, or change the title to better reflect the results they present.",Please see the questions in the Weaknesses box.
MF7ljU8xcf,MF7ljU8xcf,Compute-Optimal LLMs Provably Generalize Better with Scale,Accept (Poster),SGBwfKRTIz,ICLR.cc/2025/Conference/Submission13266/Reviewer_dkJs,"Building on an empirical version of Freedman's inequality - a concentration bound for martingales - the authors derive generic generalization bounds for large language models. These bounds are obtained in a PAC setting and do not take into account the training of the neural network considered or the model's precise architecture. They are empirical, meaning that they can be computed in practice with a limited computational budget, and are shown to hold for a single architecture and dataset. The authors then argue that these bounds get tighter as the number of parameters increases by analyzing information accumulation within the model.","**On the overall presentation of the paper.**

Overall, the paper is well written. The contributions are explained in great detail and are not overstated. Most of the mathematical and theoretical background needed to understand the article is provided in the main text. Figures are clear and well designed. 

**On the empirical version of Freedman's inequality.**

This theorem and its consequences for empirical loss minimization are the central contributions of this article. I believe this concentration bound to be valuable and interesting, mostly because of its empirical variance part which can be approximated as the solution of an optimization problem on a discrete real-valued grid. 

**On the derived generalisation bounds.**

The authors combine this bound with the smoothing technique of the model of Lotfi et al. (2024a), which in a nutshell convolutes the model with a uniform distribution to smoothen the prediction function, hence incurring an error but providing an interesting proxy of the empirical unsmoothed loss. Theorem 3.4 then combines this technique with the empirical Freedman's inequality to obtain a quite generic (i.e. model and training independant) generalization bound.","**On the comparison with related work.**

While the authors cite some related work on generalization bounds for deep models, their literature review lacks in my opinion a precise comparison with previous approaches. In particular, I would enjoy both a theoretical, mathematical and empirical comparison of the derived bounds with other bounds present in the literature - see their section 6. 

**On the tightness of the generalization bounds.** 

From what I understand, the generalisation bounds display a strong linear dependency in the number of parameters - at least in the form presented in theorem 3.4. The authors later on argue that this bound can be improved, by improving on the inequality $L(h) \\leq bN \\log 2$. While interpretable and empirical, these bounds display an extreme dependence in the number of parameters, which is undesirable in the case of LLMs. 

**On empirical evaluation.** 

The validity of the author's claims is assessed on a single LLM on a given dataset. I am unsure whether this evaluation is sufficient. I have little experience in the evaluation and training of LLMs, hence my analysis of this part is limited. I will revise my judgement based on other reviews and discussions with the authors. 

**On Section 5.**

My principal concern lies in the arguments made in section 5. The authors argue that while their bound is linear in the number of parameters in the form presented in Theorem 3.4, one can refine this bound and obtain a bound which is independent of $N$ the number of parameters. This section is not clear and hard to understand --- I stress that I am not familiar with prequential coding and might hence not be the targeted audience here. In the end, I do not understand how the authors conclude from Equation $(6)$ that their generalization bound improves with the number of parameters. From what I understand, the authors claim that their bound improves as the number of parameters increases only if the size of the dataset increases as well. If this is the case, I believe that the significance of their results is overclaimed.  

**Supplementary comment.**

I also raise to the other reviewers and chairs attention that this article relies on the smoothing technique designed by Lotfi et al. (2024a). The validity of the results of this paper has been questioned during review (see https://openreview.net/forum?id=GY1fKFXG5i&noteId=s9ciWh5QAI) leading to a rejection of this paper at another conference. While I am unable to assess whether this interrogations are justified, and if the possible errors in this paper might carry over to the results exposed here, I believe that this information should be shared among reviewers and chairs.","* Could you please provide supplementary thorough explanations of section 5 ? I am puzzled by the arguments made here. 

* In regard of this last question, could you consider including a supplementary part which gives more background on these ideas ? 

* Could you give mathematical details on how your proof techniques rely on the results of Lotfi et al. (2024a) ? In the worst case, if these results were to be false, would your analysis still hold ?"
V5lBNcD65H,V5lBNcD65H,MTEEG: A Multi-Task Learning Framework for Enhanced Electroencephalography Analysis Using Low-Rank Adaptation,Reject,gIAqiGNnRv,ICLR.cc/2025/Conference/Submission12753/Reviewer_oDYw,"MTEEG is proposed to solve the problem of fine-tuning a pretrained feature extractor separately for multiple downstream tasks, with the goal of reducing the computational cost of fine-tuning multiple times. MTEEG uses task-agnostic EEG encoders (temporal) and task-specific LoRA adaptors on top of a versatile pre-trained EEG backbone (LaBraM). Results on unseen tasks/datasets indicate MTEEG either maintains or surpasses single-task fine-tuning performance while using fewer overall parameters.","- MTEEG highlights the possibility of shared task structure/knowledge in heterogeneous EEG task/application domains that can be exploited during modeling, especially in the emerging EEG + AI space.
- Use of LoRA leads to parameter efficiency compared to base EEG ""foundation"" models, which is a desirable property for deployment.","- Preprocessing of the EEG signal needs improvement: 1) physiologic signal content of scalp EEG is between 0.1-25Hz (lower frequencies). Anything beyond 25Hz (45Hz at most) is considered high-frequency noise. 2) notch filter depends on which country the data was collected in. Example -- TUAB/TUEV are collected in US where power line freq. is 60Hz. 3) Sampling frequency can be safely dropped to 128Hz or even 80Hz, which will save computation time. 3) EEGs ideally would be normalized channel-wise (using statistics calculated from the whole recording), otherwise amplitude variability across channels is lost. Consider approach in [1] for preprocessing.
- Reliability of results: 1) Variability that is important to report is of the test set (perhaps bootstrap samples?) rather than random seeds (Tables 2-4). 2) Significance testing of mean/group differences is needed to trust the ablation results (Figures 4, 5).
- As far as I understand, the proposed approach is only applicable when all downstream tasks of interest are known beforehand. As such, the problem/motivation/hypothesis and real-world need/utility are unclear to me (line 15-20, line 48-53). In future clinical environments, we may want to have multiple EEG models/tasks ""active"" at the same time rather than switch from one model/task to another, while the EEG spatial configuration remains the same (10-20 system), i.e., there is no spatial heterogeneity. Secondly, fine-tuning is done using few task labels given that EEG labels are very expensive to obtain. Therefore, convergence is typically achieved relatively quickly, i.e., the time/computation overhead is practically negligible to fine-tune a backbone separately for each task of interest.
- Overall presentation can be substantially improved: 1) discussion and limitations should be substantial, insightful, and be in the main text, 2) figures are taking up too much space for the insight they provide (can start the y-axis at 0.5 instead of 0.0), 3) the core argument/narrative of need of multi-task learning in EEG and central hypothesis/question of the study needs revision and clarity.


[1] Banville, Hubert, et al. ""Uncovering the structure of clinical EEG signals with self-supervised learning."" Journal of Neural Engineering 18.4 (2021): 046020.","- Its unclear to me what the central hypothesis and question of the study is. Is it simply about saving time during fine-tuning or inference when all downstream tasks are known apriori?
- Typo in equation at line 170: Do you mean C_p instead of C_k?
- Q: What exactly happens in line 174? Do you mean temporal and spatial positional encodings?
- p refers to tasks (line 208, figure 2) and datasets (line 154) interchangeably, which is quite confusing. You can explain/justify the equality at the beginning and use either meaning consistently throughout the paper.
- Q: is the variability shown in Figures 4 and 5 statistically significant?
- Q: Is the parameter efficiency of MTEEG only due to the low-rank adaptor module, or are there other differences b/w LaBraM-base and MTEEG-base?
- Consider adding experiments that can show the heterogeneous conflict issue in multi-task fine-tuning (line 59-60) without LoRA adaptors and how the knowledge isolation is acheived using LoRA in MTEEG? I think this clarity is needed for readers to fully understand and ""see"" the issue the paper claims to address.
- These studies may be of interest for future work: 1) for self-supervised EEG pre-training and generalizability [1][2] and 2) effect of EEG heterogeneity/variability on EEG-ML/AI models [3].


[1] Banville, Hubert, et al. ""Uncovering the structure of clinical EEG signals with self-supervised learning."" Journal of Neural Engineering 18.4 (2021): 046020.

[2] Wagh, Neeraj, et al. ""Domain-guided self-supervision of eeg data improves downstream classification performance and generalizability."" Machine Learning for Health. PMLR, 2021.

[3] Wagh, Neeraj, et al. ""Evaluating latent space robustness and uncertainty of EEG-ML models under realistic distribution shifts."" Advances in Neural Information Processing Systems 35 (2022): 21142-21156."
V5lBNcD65H,V5lBNcD65H,MTEEG: A Multi-Task Learning Framework for Enhanced Electroencephalography Analysis Using Low-Rank Adaptation,Reject,j4WUAWj1EP,ICLR.cc/2025/Conference/Submission12753/Reviewer_zWj6,"The article introduces MTEEG, a multi-task EEG recognition system that improves the adaptation of a pre-trained model to different tasks. MTEEG uses a pre-trained model in combination with a task-independent temporal encoder to capture global knowledge from EEG signals, while task-specific low-rank adaptation modules are employed to manage the different parameter spaces for each task. The training process of MTEEG takes place in two stages: In the first stage, a LaBraM model is pre-trained on unlabeled data to extract information from the raw EEG signals. The model is then fine-tuned for specific downstream datasets, while specific low-rank adapters for each task are integrated into the transformer encoder to ensure parameter isolation. The proposed approach was verified on 6 different datasets.","Although the authors used different data sets and conducted extensive experiments and ablation studies, the approach used is not particularly novel. A more innovative method or perspective would enhance the contribution of the research to the existing literature.","The paper lacks clarity and self-containment, making it difficult for readers to fully grasp the research objectives and methodologies.

The paper appears to be a combination of existing methodologies, such as employing large pre-trained models and fine-tuning layers for specific tasks. However, it lacks a significant novel contribution to the field, which limits its impact and originality in advancing the research.

The authors should clarify how their work differs from transfer learning, which typically involves freezing certain layers of a pre-trained model while training the remaining layers for a specific task. A comparison highlighting the unique aspects of their approach compared to standard transfer learning methods would improve understanding of their contribution to the field.","The authors should clarify how their work differs from transfer learning, which typically involves freezing certain layers of a pre-trained model while training the remaining layers for a specific task.

If the different data sets show variations in the number of channels and sampling frequencies of the EEG signals, the authors should provide a clear explanation of how they resampled the EEG signals to 200 Hz. This should include details of the methods used for upsampling and downsampling to ensure consistency between data sets."
V5lBNcD65H,V5lBNcD65H,MTEEG: A Multi-Task Learning Framework for Enhanced Electroencephalography Analysis Using Low-Rank Adaptation,Reject,NA0t0ShJSp,ICLR.cc/2025/Conference/Submission12753/Reviewer_2WaD,"This paper introduces MTEEG, a framework for multi-task EEG recognition that leverages low-rank adaptation (LoRA) to efficiently fine-tune a pre-trained EEG foundation model across multiple tasks. The study presents a pioneering attempt to apply parameter-efficient fine-tuning techniques, specifically LoRA, to the domain of EEG-based multi-task learning. MTEEG demonstrates its capability by outperforming established multi-task and single-task models across six different EEG datasets, covering tasks like abnormal detection, event classification, emotion recognition, and seizure detection.","1. This paper makes a commendable and pioneering effort in exploring parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA), for EEG foundation models in multi-task learning (MTL) scenarios. 
2. The experiments are robust, with adequate coverage of tasks and inclusion of critical ablation studies that help clarify the contribution of LoRA and other model components.","1. While the integration of LoRA into EEG models is an interesting step, the contribution appears limited. The paper does not introduce significant adaptations or improvements over the original LoRA method beyond its direct application in EEG-based MTL. This straightforward application of LoRA does not meet the high technical bar expected at ICLR.
2. The paper benchmarks the proposed MTEEG framework against older multi-task learning methods like HPS, MMoE, and CGC, which, while relevant, are not cutting-edge.
3. While LoRA is effectively evaluated, the paper does not explore how other advanced parameter-efficient fine-tuning methods could perform in the same context. For a more thorough assessment, it would be valuable to demonstrate if techniques like Adapters, Prefix-tuning, or Prompt Tuning could seamlessly replace LoRA or be integrated into the MTEEG framework. A more comprehensive evaluation, incorporating these baselines, would provide stronger evidence of the proposed framework's performance.","This paper uses LoRA for multi-task fine-tuning across multiple datasets. However, have you considered a simpler approac, that is using a task-specific classification head for each dataset while fine-tuning the entire pre-trained LaBraM model? This method would be a straightforward way to enable multi-task learning, and I'm curious how its results compare to the LoRA-based approach."
V5lBNcD65H,V5lBNcD65H,MTEEG: A Multi-Task Learning Framework for Enhanced Electroencephalography Analysis Using Low-Rank Adaptation,Reject,4sSNCScPK5,ICLR.cc/2025/Conference/Submission12753/Reviewer_muVQ,"The paper introduces a multi-task EEG classification model by fine-tuning a pre-trained large-scale EEG model, LaBraM. The authors employ a popular fine-tuning strategy, Low-Rank Adaptation (LoRA), to adapt the pre-trained model. The model architecture consists of a temporal encoder and a transformer encoder. The temporal encoder is responsible for mapping EEG patches into embeddings, while the transformer encoder captures global features. During training, the pre-trained weights in the transformer encoder are kept frozen, and only the LoRA adapters within the transformer are trainable. The model’s effectiveness is demonstrated across six EEG datasets, each representing a different learning task.","The paper is clear and readable, with well-designed figures highlighting key points. The authors aim to explore multi-task EEG recognition by training a model capable of handling multiple tasks simultaneously. Unlike prior studies on single-task learning, this work updates and trains the model on all tasks concurrently. The experimental section is both informative and thorough, providing valuable insights. Moreover, using a large EEG dataset demonstrates the potential of leveraging large-scale EEG models in this domain.","The novelty of the paper is somewhat limited. Based on the experimental results, I question the necessity of multi-task learning in this context. Theoretically, due to the inherent heterogeneity of EEG data—such as differences in channel numbers, collection protocols, and medical equipment—fine-tuning all downstream tasks together might not be the most effective strategy. Additionally, your design of task-specialized LoRA for training appears nearly identical to fine-tuning each task individually. This concern is supported by the experimental results, which show that the multi-task learning approach (MTEEG) sometimes leads to only marginal improvements or even worse performance compared to single-task learning methods.

To demonstrate the necessity of multi-task fine-tuning, an ablation study should be conducted to compare fine-tuning each task individually against multi-task fine-tuning. Furthermore, it would strengthen the paper to include comparisons with TCN [1] and Medformer [2] as baseline methods. From my experience, despite their complexity, many EEG domain models struggle to outperform TCN on classification tasks. Medformer, a recent method designed specifically for EEG and ECG classification, should also be considered. Ensure that both TCN and Medformer have at least six layers and use default parameters during evaluation to maintain consistency. Due to the limited time, you can only evaluate on datasets TUEV and SEED-V.

[1] An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
[2] Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification.","Could you provide more details about your data split? I have checked your appendix, but I'd like to know whether you are using the subject-dependent or subject-independent setup. The subject-dependent setup, while often yielding higher performance metrics, is generally inapplicable in real-world scenarios and risks potential data leakage, as the model might learn subject-specific characteristics rather than generalizable patterns. This can lead to deceptively high performance compared to the subject-independent setup."
hZ3QE0rUt1,hZ3QE0rUt1,How to distill task-agnostic representations from many teachers?,Reject,nTodN9GtPB,ICLR.cc/2025/Conference/Submission12387/Reviewer_LqTb,The work aims perform Knowledge Distillation (KD) of task-agnostic representations from multiple teacher networks.,"1. The work is motivated by a challenging and realistic setting in representation KD. The method is built upon a rigorous, bounded formulation, concerning the multi-teacher and task-agnostic challenges.

2. In details, the method train embedders to project the teacher's and student's representations to the same space, then minimize the divergence between the student's representations with those of the teacher's. While existing methods usually minimize a sample-wise divergence loss (KL Divergence, cross-entropy), novelly, the proposed method minimize the divergence between the student's representations with a conditional, parametric Gaussian distribution.

3. The evalutation spans three different modalities: vision, language, and molecular modelling, proving the flexibility of the method.","1. There are some questioning concerns regarding the empirical results, which hinders the significance of the work
- The CelebA dataset is mentioned but, surprisingly, never evaluated.
- Even though KD performance of the proposed methods is the highest with the proposed module, it is strangely sub-par to:
    - Training a student model on the training set, especially considering the student is a ResNet18 - a medium-sized models. For instance: 99%+ on MNIST, 97%+ on SVHN and 90%+ on CIFAR10
    - Standard KD methods: Early KD methods such as (Hinton et al., 2015) and FitNets (Romero et al., 2015) have also surpassed the reported KD performance.
    - Considering these deficits, is it possible that there exists any flaws in the training setup or it could be improved?

2. While the problem is formulated clearly and in details (Sec. 3.1-3.2), it is not majorly different from existing work, just in a different approach.",See the weaknesses.
hZ3QE0rUt1,hZ3QE0rUt1,How to distill task-agnostic representations from many teachers?,Reject,FFCSdYxkOT,ICLR.cc/2025/Conference/Submission12387/Reviewer_CDoW,"The paper proposes a method for task-agnostic distillation from multiple pretrained teachers.
The problem is formulated as minimizing the disagreement between the student's predictions and the teacher's for any given task, which can be bounded by the conditional entropy of the teacher's embedding given the student's. This upper-bound does not depend on the specific task, and can therefore be chosen as a loss for task-agnostic distillation. The conditional distribution of teacher's embedding given the student's is parametrized by a Gaussian model whose mean and covariance are learned.

The distillation method is evaluated on three different applications:
- **Image classification** on CIFAR10, STL10, SVHN and variants of MNIST, with a ResNet18 as student and 9 teachers ranging in size from SqueezeNet to WideResNet50-2. Task-agnostic distillation is performed on the combination of datasets used for downstream evaluation.
- **Molecular property prediction (regression and classification)** on the ADMET and HTS tasks, with 9 teachers trained on different modalities (textual representations of molecular graphs, 2D-GNN and 3D-GNN) and a 10-layer GINE as the student. Task-agnostic distillation is performed on two datasets (ZINC-250k and ZINC-2M).
- **Natural language processing** on the MTEB benchmark, with four large teachers and the snowflake Merrick models as students. Task-agnostic distillation is performed on 6 million entries from a collection of datasets from the Huggingface Hub.

In these different settings, the teachers are chosen to be diverse in size, and with a performance varying across tasks. The trained student is shown to have consistently good results across all tasks, and to outperform all teachers on average.","- The paper tackles the important question of task-agnostic knowledge distillation from multiple teachers.  While task-agnostic knowledge distillation and task-specific multi-teacher distillation have been broadly studied, task-agnostic multi-teacher distillation is still understudied. 
- The proposed distillation loss is original and well-motivated.
- The proposed approach is evaluated on diverse applications: classification in 2D vision, molecular property prediction with GNNs, and natural language processing.","**Related work.**
The related work overlooks a large portion of the literature on task-agnostic distillation. The question has been extensively studied, see e.g, Abbasi Koohpayegani
et al., 2020; Fang et al., 2021; Xu et al., 2022; Gao et al., 2022; Navaneet et al., 2021; Wu et al., 2022; Duval et al., 2023. None of these works require, *finetuning the teacher*, or *jointly training the teacher and the student* as claimed in the related work, and most of them do not require *the teacher and the student to have the same architecture*.

In light of the abundance of works on task-agnostic knowledge distillation, the following claim of the study could be revised:
*To the best of our knowledge, there are few methods that address task-agnostic representation distillation, especially in the context of multi-teacher approaches.*


**Experimental validation.**

Being unfamiliar with the literature on molecular property prediction and natural language processing, my remarks are focusing on the experiments on 2D vision.

1. The evaluation protocol for vision tasks is based on very small datasets (MNIST, STL10, SVHN, CIFAR10) with very small image sizes (MNIST is 28 $\\times$ 28, CIFAR10 is 32 $\\times$ 32). Using larger, more varied datasets would better showcase the method’s applicability across real-world scenarios. Even if computational resources were a constraint, there are numerous small-scale datasets more relevant than CIFAR or MNIST, such as those from the fine-grained classification community (e.g., CUB bird classification, FGVC-Aircraft, DTD for texture classification).

2. The teachers are small CNN models, which makes it challenging to fully appreciate the contributions of the authors. This study would benefit greatly from evaluations using larger ViT teachers such as DINOv2. 

3. Lastly, as there is no data augmentation applied (Appendix D.1 only mentions a resizing to $(225, 225)$), the experimental results are very low. The achieved results are far lower than what could be obtained from training from scratch with proper data augmentation. It is not clear whether in a more challenging scenario with properly trained teachers, the conclusion would still hold. I suggest using standard augmentation strategies for training the teacher and student models, such as color jittering, cutout and random cropping (for CIFAR or MNIST), random resizing and cropping (for larger RGB images).

**Minor remarks.**

- *To do so, we first measure the agreement between the student’s Bayes classifier and the teachers’ for any given task. We show that it can be bounded by the conditional entropy of the teacher’s embedding given the student’s,*
$\\rightarrow$ the conditional entropy bounds the **disagreement**, not the **agreement**.","**Summary of suggestions** - please refer to Weaknesses section
- In the related work, mention the abundant literature on task-agnostic knowledge distillation
- For vision tasks, using **larger pretrained teachers** such as DINOv2, **evaluating on more impactful downstream tasks**, and **training models with appropriate data augmentation**. These experiments would demonstrate the applicability of the proposed approach to real-world scenarios."
hZ3QE0rUt1,hZ3QE0rUt1,How to distill task-agnostic representations from many teachers?,Reject,xFQwqM8dJR,ICLR.cc/2025/Conference/Submission12387/Reviewer_qU1H,"The paper presents a multi-teacher knowledge distillation approach for creating task-agnostic representations, leveraging conditional entropy and Gaussian kernel-based learning. The authors aim to transfer diverse information from multiple pretrained teacher models to a student model, with applications in NLP, vision, and molecular modeling. The method introduces a task-agnostic loss function optimized by minimizing negative log-likelihood, showing improvements over baseline distillation methods (e.g., MSE and cosine similarity) across tasks.","1. The authors introduce a task-agnostic knowledge distillation framework, moving beyond traditional task-specific approaches. This is particularly valuable for generalizing representations across diverse tasks.

2. The paper offers a strong theoretical grounding for the proposed task-agnostic distillation loss, including insights into its formulation using conditional entropy and mutual information maximization.

3. The method is evaluated across three domains: vision, molecular modeling, and NLP, demonstrating consistent improvements in each, suggesting general applicability.

4. Results consistently show that the student models trained with this method outperform those trained with traditional distillation methods in accuracy, robustness, and generalizability.","1. The paper lacks a detailed discussion of potential challenges, such as computational overhead in training with multiple teacher models or the scalability of the method to even larger models and datasets.

2. While the approach is tested against traditional distillation baselines, comparing with recent task-agnostic distillation methods, if any, would strengthen the empirical claims.

3. Although theoretically task-agnostic, the method’s robustness across a more extensive range of tasks within each domain is less explored. Additional benchmarks, particularly in NLP, could solidify the approach’s adaptability.","1. It is recommended to clarify how the task-agnostic nature of the proposed approach is ensured. For example, does this approach impose any constraints to PREVENT the method from IMPLICITLY learning task-specific nuances from the data?

2. How does the task-agnostic method generalize across tasks of a highly varying nature (e.g., between NLP, vision, and molecular modeling)? Are there tasks where performance might degrade without task-specific fine-tuning? Discussing them more will enhance the expressiveness of the paper.

3. The paper discusses the computational efficiency of the approach. It is suggested to provide additional details or empirical results on the computational cost, particularly when applying this method to large datasets or models.

4. How does the computational cost of this method compare to other baseline or existing SOTA task-agnostic and task-specific distillation techniques?

5. Is there a specific reason for choosing Gaussian kernel-based learning over alternative distance metrics for teacher-student alignment?

6. Although the method is compared to standard baselines, it is highly recommended to provide a comparison against recent task-agnostic distillation methods, if any exist. A comparative analysis would provide a clearer view of the approach’s relative advantages.

6. Please clarify if this approach uses different hyperparameters for the three domains (NLP, vision, molecular modeling) or maintains a consistent configuration across tasks.

7. The empirical evaluation seems broad, but how does the model handle fine-grained tasks or those with limited training data? For instance, is it still robust in few-shot learning scenarios?

8. Does the proposed method require specific architectures for the teacher and student models? Would the method be effective if applied to architectures like transformers versus CNNs?"
hZ3QE0rUt1,hZ3QE0rUt1,How to distill task-agnostic representations from many teachers?,Reject,xzl6Y0x1pB,ICLR.cc/2025/Conference/Submission12387/Reviewer_yAYt,This paper proposes a simple and effective method for distilling task-agnostic representations from an ensemble of teacher models. The authors frame the multi-teacher distillation problem as a task-enabling problem and introduce a novel approach using Gaussian kernels to estimate the conditional distribution of teacher embeddings given the student embedding.,"1) Simplicity and Effectiveness: The proposed method is remarkably simple and yet achieves surprisingly strong results on various tasks. This is a significant strength of the paper.

2) Strong Empirical Gains: The experimental results demonstrate clear empirical gains over existing methods, showcasing the effectiveness of the proposed approach.

3) Comprehensive Evaluation: The authors evaluate their method on a diverse set of tasks, including image classification, text classification, demonstrating the versatility of the distillation technique.","1) Metric Space Preservation: As acknowledged by the authors, the proposed method might not always preserve the metric space of the teacher embeddings, depending on the task at hand. This could be a potential limitation but not something I would hold against the paper -- sometimes things might work better for a specific aspect, it would be nice to investigate why or how to fix things given normal embedding models are generalists for this one reason. 

2) Limited Model and Data Scale: The experiments are primarily conducted with relatively small models and datasets. It would be more compelling to see results with larger models for both vision and language and larger datasets to assess the scalability of the method.","This paper presents an interesting and effective method for multi-teacher distillation with strong empirical results. However, the potential issue with metric space preservation and the limited exploration of larger models and datasets slightly weaken the current submission.

Recommendation:

I am leaning towards acceptance, but I believe the paper could be strengthened by addressing the following points:

1) Provide further analysis and discussion on the potential impact of metric space distortion on different tasks.
2) Consider experimenting with larger models and datasets to demonstrate the scalability of the method.

I am interested in seeing other reviewers' thoughts on these points during the discussion phase."
mXZ98iNFw2,mXZ98iNFw2,Visual Prompting with Iterative Refinement for Design Critique Generation,Reject,9rGK2IFIu1,ICLR.cc/2025/Conference/Submission12336/Reviewer_JBj1,"This paper proposes a pipeline for generating design critique through interactions among large language models (LLMs), which provide design suggestions for UI interfaces and generate corresponding bounding boxes. By incorporating feedback techniques, the system refines bounding boxes or text based on the output of “Validation” step, resulting in more accurate text or bounding boxes. The authors also explore the model’s performance in tasks such as open vocabulary attribute detection and object detection.","1. The paper proposes Validation and Iterative Refinement modules that, depending on the specific context, selectively optimize either the box or the text, thereby further enhancing the model's accuracy.

2. The model outperforms previous works on the UICrit dataset.","1. **Minor Contributions**: Compared to previous work, the authors primarily added the Validation and Iterative Refinement modules; however, these modules lack novel and crucial design. Additionally, the IOU performance of the box generation module only reaches around 35%, limiting its practical applicability.

2. **Poor Result Presentation**: For both the design critique task and OVAD task, the paper provides minimal visual examples of the model's actual outputs. Instead, it predominantly uses ground truth images as illustrations, lacking clear demonstrations of the model's generated results.","1. **Visualization of Results** 

- **a)** Showcase as many successful and failed cases as possible. Provide a clear analysis of improvements compared to previous models and discuss reasons for unsuccessful cases.

- **b)** Visualize the refinement process with intuitive displays of box and text feedback, as well as refinement results. This will help validate the roles of the respective modules.

2. **Trainable VLM Choice** : Would directly fine-tuning advanced open-source VLM models, such as LLava or InternVL, on the UICrit dataset yield more accurate results?

3. **Simultaneous Generation of Box and Comment** : Since comments correspond to specific boxes, humans typically focus on the area (box) first and then generate corresponding text. Generating the text before locating the box seems counterintuitive. How would generating the comment and box simultaneously impact the effectiveness of the model?

4. **Ambiguous Metrics** : In both attribute detection and object detection tasks, mAP is a metric based on the percentage of successful samples, so performance differences are typically reported using absolute differences. However, in the paper, performance improvements are expressed as percentage increases in several sections, including the abstract, line 075, and line 483. This unconventional representation may introduce ambiguity."
mXZ98iNFw2,mXZ98iNFw2,Visual Prompting with Iterative Refinement for Design Critique Generation,Reject,CsmtTPtIvo,ICLR.cc/2025/Conference/Submission12336/Reviewer_PCXV,"The paper presents a method that coordinates multiple LMMs for design critique generation. The method takes as input a UI design image along with a set of design principles, and produces text comments on the design issues as well as a set of bounding boxes for visual grounding of the comments (i.e., localizing the problematic regions).

The main contribution is to improve (Duan et al., 2024b) through iterative refinement of outputs and a set of specially designed prompting techniques.","1. Building computational methods for automatic generation of design critiques is an important problem to solve.

 2. The system is well designed and illustrated, and evaluated extensively.","1. The scale of technical novelty is limited. What the paper actually does is to bring an existing idea, i.e., iterative output refinement for LMMs (Madaan et al., 2023; Xu et al., 2024a) into an existing problem domain, i.e., multimodal design critique generation (Duan et al., 2024b). The paper introduces several prompting techniques, such as including zoom-in image regions for the predicted bounding boxes into the visual prompts for bounding box refinement. However, the amount of novelty involved in these simple prompting methods is not significant enough. To increase the level of technical novelty, one possibility is to come up with more sophisticated prompting techniques, e.g., to generate better few-shot examples than those created via simple random perturbation for BoxRefine, Validation and TextRefine, or to iteratively modify the UI image beyond adding coordinate markers for more efficient bounding box refinement.

2. The quantitative scores of (Duan et al., 2024b) in terms of Comment Similarity and Estimated IoU (used in Table 2) are missing, and should be added. 

3. The improvement upon (Duan et al., 2024b) in terms of comment quality (from 0.45 to 0.47 in Table 3) is small. For an incremental work, a more noticeable performance boost is expected, e.g., from 0.45 to 0.5 that almost lies midway between (Duan et al., 2024b) and Human.","In the human evaluation, why were the participants not asked to rate bounding box accuracy as in (Duan et al., 2024b)? Is “BBox IoU” in Table 3 computed by comparing with the ground truth bounding boxes? If so, is it possible for “BBox IoU” to penalize a predicted bounding box that is valid but different from any ground truth ones?"
mXZ98iNFw2,mXZ98iNFw2,Visual Prompting with Iterative Refinement for Design Critique Generation,Reject,K3chLpZXW2,ICLR.cc/2025/Conference/Submission12336/Reviewer_WLXH,"The paper presents an LLM based method to provide a text based critique (design feedback) on a UI design, given an image of that design.  In addition, the method can ground its feedback using bounding boxes overlaid on that image to indicate the basis for the feedback.

The technical contribution is the engineering of an LLM based pipeline, which consists of LLMs coupled together in multiple stages: 1) TextGen – generates design comments as text given a task/system prompt without any grounding; 2) TextFilter – a further LLM is used to prune spurious comments; 3) BoxGen – creates the grounding bounding boxes for each comment; 4) BoxRefine – Improves accuracy of the bounding boxes.

The paper explores the efficacy of various LLMs (GPT4 vs. Gemini1.5) at the task and ablates these stages and parameter choices within them to justify the pipeline design.  The experiments are done on Duan et al’s UICrit dataset and baselined against the recent Duan et al. CHI 2024 paper which addresses the same task.","Automated UI design critique seems a well motivated task; there is a potential for UX design efficiencies in using an LLM for this purpose although without actually sequencing / simulating the interaction with an interface the feedback is limited only to superficial judgements around appearances of UI elements.  There may be value therefore in this idea as a kind of visual ‘linter’ for UI design that could be an assistant or validation check for less experienced designers.  That said, the paper is not the first to propose the task – see the recent (2024) dataset and baseline of Duan et al. cited.

The paper is clearly written – the stages of the LLM pipeline are straightforward and reproducabile, as relevant system prompt examples are given in the appendix.

Given that the paper essentially proposes a 4 stage pipeline, the necessary ablation is in place to show the value of each of the 4 stages.  A brief baseline comparison is made to the prior work of Duan et al (CHI 2024) on the same dataset (UICrit).","There is limited technical innovation or scientific insight in the paper.  Essentially, the authors report a way to couple together several LLM based processes to create a grounded description of the image in the context of UX design guidelines (derived from the classic Jakub HCI paper on the same).  Whilst the design justifications for each stage are given, there is no real insight into the LLM’s capabilities or limitations.  Why is an LLM able to do this?  Which guidelines can it advise best on, and why?  Rather the innovation is presented at face value, as an engineering result.

The paper introduces a formalised notation for the UI design critique tasks which seems unused later in the paper.  Whilst it is welcome to have a clear task definition, I do not see the value of the math notation defined in Section 3 when it is not actually used later i.e. in the Method section of the paper.  The use of this formalism appears to distract from what is otherwise largely an engineering based contribution sequencing multiple LLMs together.

An attempt is made to assess the alignment of the UI critiques with human critiquers.  However the experiment is performed on only 33 UI examples, using 18 humans.  Whilst it is laudable to try to quantify human alignment, picking just 33 UIs out of a the huge design space of UIs can give no meaningful information on the performance of the LLMs at the general UI design critique task.  Also the task differs from the method proposed in the  baseline Duan et al. baseline, in that users are asked only to validate the comments for each UI region (i.e. check correctness) they are not asked to comment on the entire UI (i.e. check completeness).

I’m confused as to the purpose of Section 6 which seems to apply the UI critiquing 4 stage pipeline to the general open-world object detection/description task similar to Grounding DINO.  Why would this task be relevant to the proposed pipeline?  This entire section should be removed in my view.

Minor: The paper describes the pipeline as having 3 stages (comprising 6 LLMs) but then describes 4 stages (TextGen, TextFilter, BoxGen, BoxRefine).  pp.9 ‘Section’ typo in cross-reference:","Overall this paper addresses the relatively novel task of UI design critique using LLMs.  However the contribution is largely engineering based, creating a particular sequence of LLM promptings to create UI design critiques.  There is limited depth of insight as to the capabilities and limitations of this pipeline, beyond a baseline comparison to one prior work  and some ablation of the design itself.  The user study is so sparse as to not really provide any insight, and deviates without good justification from the approach of the baseline method.  An extra study (Section 6) that seems out of context, as well as some math formulation (Section 3) serve to bulk up the paper but don’t provide useful additionality to the exposition or problem defined."
mXZ98iNFw2,mXZ98iNFw2,Visual Prompting with Iterative Refinement for Design Critique Generation,Reject,jswIbceOWX,ICLR.cc/2025/Conference/Submission12336/Reviewer_S1K2,"This work focuses on generating high-quality design critiques, where the inputs are an UI screen and design guidelines and outputs are design comments along with corresponding boxes that map each comment to specific region in the screenshot. It proposes an iterative visual prompting approach for UI critique, where LLMs are leveraged to iteratively refine both the text output and bounding boxes using few-shot samples tailored for each step.","1.	It focuses on a novel, practical and difficult task - generating design critiques, which will have great impact on UI design by saving much design effort and accelerating design process.

2.	The insight into designing each component of the prompting pipeline is clearly explained.

3.	The experiments demonstrate the proposed method effectively.","1.	There is no qualitative results. Without good examples, I cannot concretely know what design critiques can be generated. For example, are they diverse enough? Can the difficult design critiques be generated? Without bad examples, I cannot clearly know the shortcomings of the proposed method.

2.	There is no discussion about the cost in terms of time or number of calls. The prompting pipelines involves many rounds of refinement and relies on the visual capabilities of LLMs. All of these will make it a slow one, which will influence the user experience.

3.	There is no comparison with methods based on fine-tuning, e.g., fine-tuning open-source multimodality LLMs like LLAVA or continually fine-tuning the one from Bravo et al. (2023). If fine-tuning based methods perform comparable or even better than the proposed prompting pipeline, we should use fine-tuning since the prompting pipeline with iterative refinement is slow and expensive.

======================

After discussion with authors, most of my concerns have been addressed. While the techniques may not be highly novel and could have a limited impact on the broader AI/ML community, I appreciate its contribution to the specific domain of AI-assisted design. I would like to raise my score from 5 to 6.",How many rounds of iterative refinement are used in the experiment (averagely)? What will happen if we use less rounds of refinement or allow for more rounds of refinement?
IGuLzOXTB9,IGuLzOXTB9,Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,Reject,rgrdjzAJqd,ICLR.cc/2025/Conference/Submission12155/Reviewer_2UhV,"This paper proposes Daily Oracle, a continuous evaluation benchmark for assessing LLMs' ability to predict future events using daily news. The authors automatically generate True/False and Multiple Choice question-answer pairs from daily news articles and evaluate various LLMs' temporal generalization capabilities, both with and without RAG. The experiments reveal performance degradation over time, particularly pronounced around models' knowledge cutoff dates.","1. Continuous evaluation: The daily updated benchmark effectively addresses data contamination concerns in LLM evaluation, providing a timely assessment mechanism.
2. Comprehensive experiments: The work presents thorough analyses of both open-source and closed-source models, clearly demonstrating the effect of knowledge cutoff dates on forecasting performance through moving average visualizations. The study reveals some valuable findings about model performance dropping after the cutoff date and how RAG of more recent data doesn't always help forecasting.","1. Limited technical novelty: The benchmark question format, construction prompt, and main steps are similar to TempLongBench.


- I suggest the authors provide a comparison table that clearly outlines the key differences between their pipeline and previous work like TempLongBench. 

- Additionally, consider including a flowchart or diagram of the dataset construction process with annotations explaining the rationale for each step.

- Also, the authors could add their prompts in the appendix for a better understanding.

---

2. Insufficient dataset quality evaluation: Though the construction pipeline has an LLM-based scoring and filtering step, there lacks an assessment of the final generated data quality.

I suggest the authors:
- Provide a breakdown of how many questions passed each principle in their quality control process
- Show or plot the distribution of the final data's score in each of the designed principle dimensions
- Conduct a human evaluation on a randomly sampled subset (e.g., 100 questions) of data, assessing both the news and QA data quality on specific metrics (e.g., Evidence, Reasonable, Plausible in TempLongBench)
- Also, conduct human forecasting performance on the sampled subset as a reference. Include inter-annotator agreement scores to demonstrate the reliability of their assessments.

---

3. Incomplete analysis of RAG results: The authors observe that RAG does not uniformly enhance performance for Llama3, with some RAG cutoffs performing worse than the closed-book setting, and conclude that outdated information may negatively impact performance.

I suggest the authors conduct a more detailed inspection of the RAG process:
- What articles does the model retrieve? Are they relevant to the question? 
- As the retriever is a simple BM25 model, does this process consider the temporal distance between the retrieved article and the target date? Will this influence the forecasting results? Will there be most of the cases that even if different RAG cutoffs are set, the model still retrieves the same and very old articles due to the BM25 limitation, such that the experiment condition of different RAG cutoffs becomes meaningless?

To make this analysis more concrete, the authors may consider:
- Compute and report the average relevance score of retrieved articles to the questions.
- Plot a histogram of the temporal distribution of retrieved articles for different RAG cutoffs.
- Analyze the correlation between article recency and model performance.
- Provide a specific case study of a few example questions, showing the full chain of retrieved articles and how they influenced the model's prediction.",Will the code base become public? Will the database be maintained and updated in a daily manner and be available publicly?
IGuLzOXTB9,IGuLzOXTB9,Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,Reject,IbQdfxolRi,ICLR.cc/2025/Conference/Submission12155/Reviewer_YiaM,"This paper focuses on evaluating large language models (LLMs) in a way that remains relevant over time, as traditional benchmarks fail to capture the dynamic, ever-evolving nature of real-world information. To tackle this issue, the paper proposes using daily news articles to continuously assess LLMs' forecasting abilities. By generating QA pairs from news in various categories (such as business, politics, and arts), the authors create an evaluation benchmark named Daily Oracle. This benchmark is designed to evaluate LLMs’ ability to predict near-future events and test their temporal generalization.",The problem of degradation of performance over time is relevant and this resource along with the framework can be of large interests for the community.,"The work presents a known problem and dive deep into potential impacts. While the paper has its merits, I don't think it is at a maturity level to be published yet (see my questions below).","The work relies on several automatic procedure to build the dataset that require clarification: 
- Did you evaluate the clustering approach? how is your clustering approach different from bert-topic? 
- What's the overlap rate between question and answer? 
- Degradation after cutoff is expected and RAG is commonly used to mitigate the problem. However, in this work the retrieval used to test RAG is quite weak. First, BM25 should be at least replaced with some hybrid or dense approach. Second, 5 top articles may not be enough (how did you choose 5?) and truncating the article at 512 can potentially cut off answers (
it is unclear if the provided information contain the answer. In other words, how do we know if the problem is the retrieval or the model ability to handle such information?"
IGuLzOXTB9,IGuLzOXTB9,Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,Reject,AIiBRv1RtK,ICLR.cc/2025/Conference/Submission12155/Reviewer_mAPh,"The paper presents a continuous evaluation benchmark for LLMs testing the ability to make predictions about real-word events and assess whether they show temporal generalisation and tests different LLMs using multiple eval configurations (closed-book setting, constrained open-book setting, etc).",The paper presents a very interesting idea as a benchmark for LLMs and describes in details the dataset construction and evaluation. I think this work would be very relevant as a benchmark for LLMs at ICLR.,"While the paper is extremely interesting from a dataset construction point of view, I have found it a bit hard to follow through the experiment section, especially in terms of the task performed and each stage and the knowledge that the model had. This for me is true in particular for the ""Constrained open book setting"" sub-section, but in general all through the evaluation overview. I would suggest the authors to adopt a running example, referring to a specific model with a clear cutoff date and a question regarding a piece of news, in order to highlight how the model would perform differently in different situations.","It seems that the dataset is completely constructed automatically through the usage of LLMs - I was wondering if the authors have performed any manual check to assess the quality of the construction and if they could add details and evaluation metrics about that.

Are the authors planning to set-up a platform where users could test LLMs against the created dataset?"
IGuLzOXTB9,IGuLzOXTB9,Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,Reject,7XrPezGBGi,ICLR.cc/2025/Conference/Submission12155/Reviewer_VuTG,"⁤This paper constructs a new benchmark to evaluate LLMs in real time by testing their ability to predict future events. ⁤⁤Traditional benchmarks quickly become outdated as LLMs and the world itself continuously evolve, limiting their ability to reflect current model performance. ⁤⁤To address this, the authors introduce Daily Oracle, a continuously updated dataset created from daily news articles. ⁤⁤Each day, they generate question-answer pairs about real-world events across domains like politics, science, and business, allowing for an assessment of whether LLMs can forecast future events based on prior knowledge. ⁤

⁤The findings show that LLMs experience a steady decline in performance over time as their training data becomes outdated, weakening their predictive abilities without regular updates. ⁤⁤Although techniques like RAG offer some enhancement by incorporating newer information, the models still struggle to maintain accuracy as the distance grows between training data and real-world events. ⁤⁤Overall, this paper presents Daily Oracle as a tool for ongoing evaluation of LLMs, focusing on their ability to generalize temporally through daily news-based question-answer pairs. ⁤","1. The writing in this paper is clear and easy to follow, with a well-organized structure. 
2. The authors' Daily Oracle benchmark covers more topics and more recent dates compared to previous benchmarks, and it provides continuous daily evaluation. 
3. In the experiments, the authors observe a notable performance drop across all LLMs in the closed-book setting after the knowledge cutoff date. They further analyze this degradation by testing with RAG and gold article settings, observing similar declines.","**Major Issues:**
1. **Limited Innovations in Data Construction**: Although the benchmark is one of the main contributions, the data construction approach is highly similar to existing work. Specifically, the authors also rely on the Common Crawl News Dataset as the data source, and their QA construction process and question formats closely resemble TCELongBench. Aside from slight differences in the prompting steps, the main distinction appears to be the inclusion of True/False question types. This suggests limited technical contributions in data construction. This paper also lacks comparisons with TCELongBench in terms of both the approach and the quality of the generated QA pairs.
2. **No Human Verification**: The authors rely entirely on GPT-3.5 and GPT-4 to generate the benchmark’s QA pairs, yet the reliability of this approach is unverified. For instance, in the QA filtering step, GPT-3.5 scores based on seven principles, criteria that could be challenging even for humans to judge objectively. The authors should include a rationale for choosing these specific principles, explain why a score of 13 or above indicates a quality question, and provide inter-rater reliability among human evaluators. It would also be necessary to assess the correlation between GPT-3.5’s scores and human scores to gauge data quality accurately, even if only on a subset.
3. **No Cost Description or Comparison**: Since continuous daily evaluation is highlighted as a major benefit of this benchmark, it would be helpful to provide specific cost estimates for using Daily Oracle to periodically (daily, weekly, monthly) evaluate LLMs. Additionally, a cost comparison with other data construction methods, such as TCELongBench, is needed to assess the feasibility of this approach.

**Minor Issues:**
1. **Relatively Trivial Conclusion**: The conclusion that LLM performance declines significantly after the knowledge cutoff date is fairly predictable. Additional analyses and insights would be beneficial, such as examining a time span beyond the last four years to study how LLMs’ performance in memorization changes over decades. The rise and drop in Figure 5’s gold article setting post-knowledge cutoff could also be further analyzed (e.g., does it relate to inconsistencies in LLMs’ parametric knowledge). Such experiments would deepen community understanding of LLMs’ temporal generalization.
2. **No Estimate of Human Performance**: Adding an estimated score for average human or domain expert performance would help contextualize the accuracy scores achieved by the LLMs.

**Typos:**
- Lines 312, 321, 425, 453: Figure 7 → Figure 3.",See the above Weaknesses.
GDjwSBZy6l,GDjwSBZy6l,ROLoRA: Rank Optimization  for Low-Rank Adaptation under Memory Constraints,Reject,aCoFzx0UG1,ICLR.cc/2025/Conference/Submission12044/Reviewer_vHgk,"This paper proposes ROLoRA, an iterative algorithm for optimizing rank configurations in LoRA-based fine-tuning of large language models. The key insight is adaptively adjusting ranks across different weight matrices while strictly adhering to memory constraints. The method iteratively sparsifies saturated adapters and grows underfitted ones within a Frank-Wolfe optimization framework. The authors evaluate ROLoRA on GLUE and SQUAD benchmarks using RoBERTa-base and DeBERTa-v3-base models, demonstrating improved performance over standard LoRA and AdaLoRA baselines while maintaining lower memory usage. A notable finding is that value matrices in transformer architectures require higher adaptation capacity compared to key/query matrices.","- The problem formulation effectively addresses a practical limitation of LoRA - the need to determine optimal rank configurations under strict memory constraints. However, although the problem is clearly defined, I'm not sure if it is important. Given that LoRA already works well and is simple and effective, do we need to further constrain the budget to achieve trivial improvements?
- The iterative optimization approach is theoretically grounded in the Frank-Wolfe framework, making the method more principled than heuristic alternatives.
- The empirical analysis is comprehensive, with clear ablation studies that reveal insights about the varying importance of different weight matrices.","- If I remember correctly, I have run AdaLoRA and even AdaLoRA is very time-consuming. Thus my major concern is the computational efficiency of the method. Does RoLoRA also face issues with computational overheads? Will there be detailed analysis? Given that RoLoRA's improvements over LoRA are not particularly significant, it's difficult to assess the merits of this method without detailed computational analysis. 
- I know that many LoRA works use the same experimental settings as this one, and it is convenient to make comparisons. However, I think using these models and benchmarks in 2024 might be somewhat outdated, as their behaviors may change as model capabilities continue to improve.","- Figure 1 can be further improved. For example, add legends to clarify which kinds of grids represent which kinds of matrices."
GDjwSBZy6l,GDjwSBZy6l,ROLoRA: Rank Optimization  for Low-Rank Adaptation under Memory Constraints,Reject,P3GRpBxkWW,ICLR.cc/2025/Conference/Submission12044/Reviewer_gY5x,"This paper proposes ROLoRA - a new PEFT (Parameter-Efficient Fine-Tuning) method that adjusts adapter ranks for different modules and layers under a constrained memory budget. Unlike LoRA, which applies the same rank for all adapters, but similar to AdaLoRA, ROLoRA suggests that different ranks for different modules are a better approach (confirmed by experiments). Compared to AdaLoRA, ROLoRA stays within the constrained budget during training but may increase training time (though by how much is unclear from the paper). ROLoRA is an iterative method involving pruning and then expanding ranks. The approach is tested on RoBERTa-base and DeBERTa-v3-base. Experiments show improvements over LoRA and competitive performance to AdaLoRA on the GLUE and SQuAD benchmarks.","Strengths

-Clear motivation and introduction

-Relevant problem

-New iterative framework that operates within a constrained budget throughout LLM finetuning

-Interesting analysis and insights on the importance of value matrices in finetuning","In general, I think this is a relevant problem and interesting approach. However, the biggest issue is that I would like to understand how this approach is better than AdaLoRA. Specifically, what is the increase in finetuning time introduced by ROLoRA, and how much does AdaLoRA exceed the computational budget during fine-tuning? What if we set the computational budget to be max N in AdaLoRA and exactly N in ROLoRA? How do the two algorithms compare? I would like to understand when someone would prefer to use ROLoRA, as it is a more complex algorithm (with longer fine-tuning time). I would also like to see the method’s behavior on larger models (decoder-only) and more recent tasks. Also, there is no mention of releasing code for this framework, which raises concerns about usability.

Weaknesses in points:

-Tested only on two encoder-only models and only GLUE and SQUAD benchmarks. I think that more models, possibly a larger decoder-only model, and some more recent benchmarks would be beneficial.

-The insights about value matrices are interesting, but they seem more observational than a motivating factor for ROLoRA design. In the conclusions, the value insight is highlighted as a main takeaway, but it’s only mentioned briefly in the ablation study at the end.

-The paper mentions balancing iterations and training time, but it would be helpful to see a clear analysis showing how much training time increases.

-Figure 2 could be improved to present a better side-by-side comparison (currently, it’s difficult to read).

-Table 4 shows average ranks, but a summary of parameter counts for LoRA, AdaLoRA, and ROLoRA would clarify the overall memory savings.

-The final sections of the paper are not as comprehensive as the earlier sections. The value matrix insight is introduced very quickly, and further analysis would be useful.","Questions (a few questions also in the Weaknesses section)

-How does training time increase per iteration in ROLoRA? Is it 3x normal training for 3 iterations, or is it faster?

-Given ROLoRA complexity, do you plan to release the code?

-L062-L064: Could you add citations here?

-I think this paper could benefit from a visualization of rank changes. This might offer interesting insights for future work. Is it possible to generate such a plot?

-About Frank-Wolfe framework: Could you please add more explanation of why the Frank-Wolfe framework was chosen? Can you clarify the term ""delicately designed"" (L023) in describing the Frank-Wolfe framework? Why “potential” in “potential theoretical guarantees”? Could you explain? (L024)

-How were the hyperparameters for the experiments chosen? This is not clear from the paper and may have an impact on the results."
GDjwSBZy6l,GDjwSBZy6l,ROLoRA: Rank Optimization  for Low-Rank Adaptation under Memory Constraints,Reject,zBifHCK5MA,ICLR.cc/2025/Conference/Submission12044/Reviewer_Rw6n,"The paper introduces ROLoRA, a method for optimizing adapter ranks in Low-Rank Adaptation (LoRA) to enable efficient fine-tuning of large language models within memory constraints. Unlike standard LoRA, which applies a fixed rank across all layers, ROLoRA iteratively adjusts ranks by pruning overfitted adapters and expanding under fitted ones, all within a specified memory budget. Experimental results demonstrate that ROLoRA outperforms existing methods like AdaLoRA on benchmarks such as GLUE and SQuAD, particularly by optimizing ranks for crucial weight matrices in transformer layers, such as the value matrices in attention mechanisms.","Overall, this is a good work. The algorithm is well-motivated and reasonably well-explained. The method is backed by adequate empirical evidence, with results that support the effectiveness of ROLoRA over existing approaches. In particular, the ablation study is a valuable addition, demonstrating that the rank assignments are not only adaptive but also focus on key components, such as the value matrices in attention layers, which are shown to benefit from higher ranks.","Weaknesses

1. Insufficient References in Key Claims (Lines 139-141): Certain claims in the paper are presented without adequate referencing, reducing their credibility, for instance “can often lead to a more favorable optimization landscape”.
   
2. Lack of Justification for Assumption 1: The authors assume that the SPARSIFY operator maintains memory constraints and can remove redundancy without sacrificing model performance. This assumption is pivotal to the algorithm, yet it lacks theoretical backing. Providing additional rationale here would reinforce the assumption’s validity.

3. Ambiguity in Proof of Proposition 1: The proof for Proposition 1, which posits that ROLoRA iteratively improves model performance, is not entirely convincing. The algorithm currently appears heuristic, without formal assurance that each iteration yields a performance improvement similar to the EM algorithm. A clearer proof structure or additional evidence supporting iterative improvement would strengthen this point.

4. Limited Explanation of Frank-Wolfe Connection and Convergence: While the authors mention a connection to the Frank-Wolfe algorithm, it needs further explanation. It is unclear how the discrete-to-continuous transition (which seems more heuristic) impacts convergence guarantees. Further elaboration on how the theoretical aspects would still hold after this discrete to continuous transition would enhance clarity.

5. Unclear Baseline Iteration Details: The paper lacks a detailed comparison of iteration counts between ROLoRA and baseline methods like LoRA* and AdaLoRA*. It is therefore uncertain whether these baselines received an equivalent level of optimization. Including these details would facilitate a more accurate assessment of relative performance.

6. Average Rank in Table 4: Table 4 indicates that ROLoRA achieves a lower average rank than sparsification-only methods like AdaLoRA, despite ROLoRA’s additional expansion steps. The reasoning behind this outcome is unclear. A more detailed explanation of how the rank pruning and expansion operations jointly lead to this effect would clarify the results.

7. Absence of Average Rank on SQuAD Datasets: The paper presents average rank results for GLUE but omits similar data for the SQuAD datasets. Providing this information would complete the evaluation, illustrating ROLoRA’s impact on question-answering tasks.

8. Scalability Testing on Larger Models: The paper’s evaluation on smaller models leaves open questions regarding scalability. Testing on larger models, such as those with 1B or 7B parameters, would confirm if ROLoRA’s efficiency extends to more substantial architectures, making the findings more broadly applicable.","Please see above.

I am curious to see how ROLoRA performs using the sparsification method from AutoLoRA (https://aclanthology.org/2024.naacl-long.282.pdf), which is built upon a similar motivation as AdaLoRA. It would be interesting to explore how this approach compares or complements the existing sparsification techniques used in this work."
GDjwSBZy6l,GDjwSBZy6l,ROLoRA: Rank Optimization  for Low-Rank Adaptation under Memory Constraints,Reject,5NELKxe66Q,ICLR.cc/2025/Conference/Submission12044/Reviewer_z1pc,The paper introduces a method called ROLoRA. ROLoRA is a novel method that efficiently discovers an effective rank configuration for low-rank adaptation while strictly adhering to a constrained computational budget during training. It outperforms standard LoRA on natural language processing tasks and is practical for resource-constrained scenarios.,">The writing is good and the paper is easy to follow.

>The method can efficiently discover an effective rank configuration for low-rank adaptation. 


>ROLoRA outperforms standard LoRA on some benchmarks under some model configurations.","> The experiments are conducted on models like RoBERTa and DeBERTa. Have any experiments been conducted on modern large language models like Llama or OPT? Including these experiments will make the method much more valuable in modern settings.

> The experimental results sometimes seem to fall behind baselines and the improvements are not that significant. For e.g., LoRA⋆ in Table 1 and AdaLoRA⋆ in Table 2.


> Any comparisons in running time and convergence speed with baselines?",See weanesses.
rcdR97P2Mp,rcdR97P2Mp,Towards continuous machine learning on periodic crystals by ultra-fast invariants,Reject,xu66KSHClG,ICLR.cc/2025/Conference/Submission11960/Reviewer_x6Eg,"This is a new method for searching crystals. Multiple databases were used for evaluation.

The authors claim large improvements from years to hours.

The paper does make some large claims. For example, in the abstract there is a free standing claim saying
""The proposed Lipschitz continuity under noise is a new essential requirement for machine learning on any
data objects that have ambiguous representations and live in continuous spaces."" This is for some types of
neural networks not for all algorithms. Correct? Please clarify by providing evidence or citations supporting the claim that this is an ""essential requirement"" for all ML on ambiguous data in continuous spaces. Alternatively, please clarify if this applies only to certain types of ML algorithms. 

The paper is written for an extremely specialized and narrowly focused domain and not for the general 
ML crowds found in ICLR.

The authors will need to add more explanations for general purpose ICLR audience. Otherwise this paper should be submitted to a Crystallography journal.

The main contribution is that the authors can  convert any PDD{h} into a fixed-size matrix,
which can be flattened into a vector for easy comparisons, while keeping the continuity and almost
all invariant data.

The authors also directly talk to the reviewers which is unexpected. For example, ""We thank all reviewers for supporting scientific integrity, now guaranteed by the proposed invariants."" or ""Thank you for reading all the proofs!"". Such claims are informal and not relevant for a publication in ICLR.","This works showcases large speedups in comparing crystalline structures.
The works extends Widdowson & Kurlin, 2022.
The paper shows and extensive amount of proofs and theorems.","The paper is not written for ML audience.
The paper is more relevant for Acta Crystallographica or similar journal.
The work is unnecessarily theoretical.  Giving the impression that there is not much novelty.

The title says ""Towards continuous machine learning .. "" but no ML example has been provided. Only similarity metrics.","It is not clear what is the theoretical vs algorithmic contribution of the work. Please explain.

It is not clear what is the difference with the paper Widdowson & Kurlin, 2022. Please explain.

It is not clear what is the underlying representation that allows for the acceleration. Please provide a detailed explanation of the data structure you use, and how it enables faster computations compared to previous methods.

It is not clear what is the relationship with other existing machine learning methods. Please explain.
For example, can you compare your method to particular ML techniques used in materials science or crystal structure analysis. Can this method be integrated with graph neural networks or other ML models used for predicting crystal properties? Such as resistance to temperature changes?"
rcdR97P2Mp,rcdR97P2Mp,Towards continuous machine learning on periodic crystals by ultra-fast invariants,Reject,xIYtnOfIdX,ICLR.cc/2025/Conference/Submission11960/Reviewer_DaqF,"The work presents a novel representation for periodic crystals that allows for efficient and effective identification of de-facto duplicates in current crystal databases. In particular the representation is Lipschitz-continuous under noise, i.e., tiny variations of crystal structures, e.g. caused by measurement noise, only yield tiny distances in representation space. The representation allows the efficient identification of duplicates or (noisy) near-duplicates in databases by pairwise comparisons in databases with in total almost two million entries, thus revealing a large number of previously unknown duplicates as well as labelling errors (same structure with different chemical composition, which is infeasible) in the databases. Last but not least, the proposed approach is able to distinguish all known counter-examples, i.e. examples indistinguishable by some previous representations.","The work presents novel representations for periodic crystals that allows for efficient and effective identification of (near-)duplicates in existing databases. Thus the work does not only yield highly relevant practical findings, but also prevents ""adversarial attacks"" via claimed-new ""discoveries"" of materials which are however de facto already known and were only possible by exploiting deficiencies of previously proposed representations.","The authors proposes a procedurally constructed representation, and ""machine learning"" in their work is limited to pairwise distance computation. In this regard, having ""machine lerning"" in the title is somewhat misleading. The contribution may be well suited also to be placed in a more application-oriented publication venue (Science?). However, it is clear that a representation (albeit engineered) with the stated properties that, furthermore, allows very fast distance computation is also highly relevant for any kind of machine learning on crystal structures, as manifested by the fact that previous works the authors build upon have appeared at NeurIPS and CVPR.

Note, I revised my score from 8 to 5 after contemplating the other Reviews and discussing with CLkj; see comment further down for details","-- In Problem 1.2, I assume (c) (2) should be symmetry, i.e., I guess d(b,c) is a typo and should be d(b,a)?

-- In 1.2. (d), lambda is not defined

-- p.2 l. 93f ""gigantic loophole"": It would be helpful to the reader if you could again cite the relevant reference (given earlier in the introduction)"
rcdR97P2Mp,rcdR97P2Mp,Towards continuous machine learning on periodic crystals by ultra-fast invariants,Reject,FxQXjJoIcY,ICLR.cc/2025/Conference/Submission11960/Reviewer_QApk,"This paper addresses identifying near duplicate crystal, using invariants computed from point distance distributions PDD. Apparently the authors found a fast method for detecting and describing near duplicate crystals, tested on retrieval experiments with several crystal databases.",The development of invariants to rapidly identify near duplicate structures is interesting.,"This paper seems to use a lot of language, assumptions references only clear to those studying crystal structures, and thus it is unclear if or how it links to the broader machine learning community. For example, ""points"" are discussed throughout the paper, it’s only in discussion where it is mentioned these points are in 3D space. There is no mention of other work using point distance distributions PDDs, e.g. distance transforms in classic computer vision, invariants such as the scale invariant feature transform SIFT used to identify near duplicate photos on a repeating pixel lattice. 

No discussion of other invariants, e.g. 3D SIFT used to characterize and detecting near duplicates based on non-repeating structures in human brain images, which would seem to work quite well in the case of redundant crystal structures, using precisely the same KD search tree structure mentioned in this work, however appear unknown to the authors, e.g.
[a] Chauvin, Laurent, et al. ""Neuroimage signature from salient keypoints is highly specific to individuals and shared by close relatives."" NeuroImage 204 (2020): 116208.","Why is there no mention of invariants from well-known work outside of work repeating crystal structures? Particularly given identical technologies used for retrieval, e.g. KD-tree search trees, using invariants derived from non-repeating structures (e.g. human body anatomy) which would appear to be a more difficult problem? If these are irrelevant, it would be good to know why."
rcdR97P2Mp,rcdR97P2Mp,Towards continuous machine learning on periodic crystals by ultra-fast invariants,Reject,Pq2D99I1Or,ICLR.cc/2025/Conference/Submission11960/Reviewer_jEka,"This paper presents a novel approach to periodic crystal representation by addressing limitations in existing methods for distinguishing unique crystalline structures. The authors propose a new invariant for periodic point sets, which is designed to improve the identification of unique crystals. This invariant provides a systematic approach to eliminating near-duplicate entries in material databases.","The paper seems to offer a substantial contribution to evaluating novelty in crystalline materials, presenting a practical and (as far as i can tell) theoretically sound method for ensuring data quality in large-scale material databases. This seems to address an important challenge in materials science research.","The relevance to ICLR is not particularly clear, as the work does not introduce a new ML method or focus on representations of ML models in the typical sense. The authors should address the relevance for iclr or machine learning in general to improve understanding by the ml community. A less technical introduction would also benefit understanding and readability. Overall, the paper may be better suited for a more specialized audience or a theory-focused computer science conference.","- Consider rephrasing or omitting the last sentence of the discussion, as it seems somewhat out of place.
- The paper is dense with technical details; consider moving some to the appendix (e.g. some examples) to allow more space for intuitive explanations and discussion.
- Invest more effort in improving the illustrations, particularly graphs. Figure 6’s text is small; consider adjusting the color scheme in Figure 1 to make it clearer and remove some of the elements to make it less crowded.
- The work may be suitable for a theory-focused ml conference, as it introduces a well-justified quality metric for material datasets rather than a new ML method.
- The language at times seems overly opinionated, with phrases like “notoriously hard problem” and “avoid such embarrassment.” These could be rephrased to maintain a more objective tone."
d8hYXbxX71,d8hYXbxX71,Policy Design in Long-run Welfare Dynamics,Accept (Poster),arOkMjMnLp,ICLR.cc/2025/Conference/Submission11925/Reviewer_DG4s,"This paper studies the behavior of Rawlsian and utilitarian policies in long-run welfare dynamics. They show that under a survival condition, Rawlsian policies are better than the idealized utilitarian approach almost surely in the long run. Under a ruin condition, a utilitarian policy will achieve better long-term social welfare. Simulation results are provided to validate the theories.",This paper is in general well-written and smooth to follow. The theoretical results are strong and the technical proofs seem to be rigorous and well-organized. The simulation results validates the theories clearly.,"The authors basically leave the survival and ruin conditions unjustified, and the simulation settings are ad-hoc. I would appreciate it if the authors could provide discussion and common example settings where the conditions hold.","- In line 195, the author motivates max-U policy with the setting where $f_i + g_i$ are increasing functions. However, this is not enough, right? Say $f_1 + g_1$ and $f_2 + g_2$ are such that $u_1 < u_2$ but  $f_1(u_1) + g_1(u_1) \\geq f_2(u_2) + g_2(u_2)  $ (although $f_1 + g_1$ and $f_2 + g_2$ themselves are increasing functions).
- In lines 218 and 219, why the max-g policy is considered a variant of Rawlsian policy?  I mean, $i=\\underset{j}{\\arg \\max }\\{g_j(U_j(t))\\} $ is still maximizing, rather than minimizing the welfare."
d8hYXbxX71,d8hYXbxX71,Policy Design in Long-run Welfare Dynamics,Accept (Poster),DRb0WHHx8T,ICLR.cc/2025/Conference/Submission11925/Reviewer_C3iz,"The paper compares the long-term social welfare resulting from two popular policy frameworks, a Rawlsian policy that focuses on the minimizing the smallest welfare loss and a utalitarian policy that focuses on maximizing immediate welfare gain. The comparison is conducted under a model on welfare decay and intervention return, assuming a survival condition and the Matthew effect. The conclusions are that 1) when the survival condition is satisfied, the Rawlsian policy achieves better long-term social welfare than the utalitarian policy; and 2) when the ruin condition is satisfied, the utalitarian policy becomes better.",The paper puts the welfare policy evaluation in a sequential decision making framework. All models and assumptions are written clearly. The theoretical results are backed with numerical experiments and mathematical proof.,"The theoretical results rely on several strong assumptions, and need more clarification.","## Major comments:

1. I am curious about whether the two essential assumptions for the theoretical results in the paper are realistic. These assumption include 1) the return and decay functions are monotone, 2) the bounds on the return and decay functions are uniform across all individuals. For assumption 1), the paper mentions that the social planner does not need knowledge of $f_i$ or $g_i$. In this case, how does the social planner know whether $f_i$ and $g_i$ are monotone in practice? For assumption 2), the ""rich-get-richer"" modeling condition also needs to be satisfied. However in reality, the rich gets richer at a much faster rate than other individuals. Is assuming the same upper bound for all indivuals realistic?

2. What is the motivation and advantage for considering max-f and max-g policies, as opposed to the max-fg policy? The paper mentions that the max-d policy only requires partial information about the interventions, which is less costly to measure compared to measuring both return and decay. However, the max-g policy still requires information about the decays, which then offsets the previously mentioned advantage.

3. As for the survival condition, how should a social planner assess whether this condition is satisfied in practice?


## Minor comments:

1. Equation (1): The definition of $\\mathcal{F}_t$ appears much later."
d8hYXbxX71,d8hYXbxX71,Policy Design in Long-run Welfare Dynamics,Accept (Poster),43GLU5pWtD,ICLR.cc/2025/Conference/Submission11925/Reviewer_Zhdw,"The paper studies sequential policy design with the goal of maximising long-term welfare. In particular the authors compare two policy classes—Rawlsian and utilitarian—and identify conditions under which each is optimal. They model the population as N agents, where each agent’s welfare naturally decays without intervention but improves with intervention. Sections 3 and 4 compare the policy classes in terms of long-term population welfare and individual welfare, respectively. Under regularity and survival conditions, the Rawlsian policy outperforms the utilitarian; however, under the ruin condition, this implication is reversed. Section 5 contains experiments with initial welfares drawn from SIPP data and the social welfare evolution supports the theory.","- The paper is well-structured and theoretically rigorous. Theorems 1 and 2 in Section 3 are particularly insightful, establishing necessary and sufficient conditions under which the Rawlsian and utilitarian policies are optimal.
- The individual welfare evolution and policy optimality for Rawlsian, utilitarian, and random policies in Section 4 is also novel, requiring new sub-martingale techniques in the proofs.
- Section 5 includes experimental results that support the theoretical finding and we see that Rawlsian policies though initially leading to a drop, are optimal for long-term social welfare.","- The modelling conditions, such as ""rich-get-richer"" and ""poor-get-poorer,"" are well-explained, but the survival condition (line 255) is difficult to interpret. Is this condition standard from Radner and Rothschild, or is it specific to your framework?
- Clarifying how common the survival condition is would help, as knowing whether the system is in a survival or ruin state is essential to determining whether a Rawlsian or utilitarian policy is optimal.","- Please address the questions above.
- A minor question: your theory seems to extend to interventions that allocate resources to more than one agent at each time step $\\sum a_i(t) = M$. Is there a reason not to include this in the main body, with \\(M=1\\) as a special case?"
d8hYXbxX71,d8hYXbxX71,Policy Design in Long-run Welfare Dynamics,Accept (Poster),ozW9j9WdY8,ICLR.cc/2025/Conference/Submission11925/Reviewer_PGAD,"In this paper, the authors study how policies can affect long-term welfare in a population. Specifically, they prove that under the survival condition, the Rawlsian policy has greater long-run utility than the utilitarian policy, and under the ruin condition, the utilitarian policy has greater long-run utility than the Rawlsian policy. They demonstrate their findings through numerical simulations.","* The paper is very well-written and easy to follow.

* The paper studies the problem of how policies can affect the long-run welfare of the population, which is important and interesting.

* The paper presents theoretical results that have clear implications. The proof is technically sound.

However, since I have no background in economics, I find it challenging to evaluate the significance of the paper's contributions or the reasonableness of its welfare model. So I choose to assign a positive rating to the paper with a low confidence score.","See the ""Questions"" part.","* In the welfare model it is assumed that everyone's welfare will decay over time without interventions. But why is this the case? In the paper [Roy Radner and Michael Rothschild, 1975], such a model is used to describe how a manager would assign his efforts to several tasks. In this circumstance, it is reasonable to have a ""decay with time"" assumption since if a task is ignored then things may get worse over time. However, I cannot see why such an assumption can be generalized to the model of welfare. Also, the ""rich-get-richer"" assumption seems at odds with the bounded $f$, $g$ assumption. I think the authors should add more explanations about the modeling choice to make the paper more accessible to readers outside the field. I also recommend the authors use real-world data or existing literature to rationalize such modeling choices. (In the current simulations only the initial state of the model is based on true data, and all other components are hand-crafted.)

* In the paper the author only considers the policy that focuses on one individual at a time. Can the authors comment on what the main technical difficulty is in considering a more general policy that forms a probability distribution on the population?

* In the paper, the authors do not touch on the problem of optimality of policies. It seems the current welfare model can be conceived as an average-reward continuous-state MDP. Since the functions $f_i$ and $g_i$ are assumed to be known, finding the optimal policy is equivalent to solving a planning problem in such an MDP, which is well-studied in the literature. Can the authors give a concise discussion about that?"
NJxCpMt0sf,NJxCpMt0sf,"Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",Accept (Poster),kOyDROhYyp,ICLR.cc/2025/Conference/Submission11923/Reviewer_P7K2,"In this paper, the author present a new framework for training multi-modal networks, called the Multi-modal Multi-task Mixture of Experts. The framework consists of two components:
- MSoE: Modality specific mixture of experts --> for each modality, they learn a function g that applies:
 column-wise softmax (D) on X times a learnable matrix, multiplied by X, followed by row-wise softmax (C) on the output and a linear combination to compute the prediction.
- MToE: Modality shared modality task mixture of experts --> connects tasks to input modalities by learning a task embedding shared across experts.

They also propose a mutual information loss and evaluate the approach on four publicly available medical imaging datasets for breast cancer and OCT.",The framework presented is original and interesting. It outperforms existing baseline models. The authors run experiments on multiple datasets and conduct an ablation study.,"- The paper presentation requires improvement. For example, there is unnecessary use of ; and there is incorrect use of opening quotations "". The authors also repeatedly introduce the abbreviations - this should be done once.
- I found it difficult to parse through Figure 2 (Can you relate it with the textual explanation of the functions?)
- The authors only compare to a few baselines, can you incorporate more? There is a lot of literature on multimodal learning now.
- Are the performance improvements significant? Can you conduct significance testing and provide confidence intervals?
- The experiments are conducted on medical imaging datasets. How does this apply to other non-imaging modalities where modality competition may be more pronounced. For example, this could be applicable to MIMIC CXR (chest X-rays) and MIMIC EHR where downstream tasks are more dependent on the EHR modality.
- The main results section in the text should also discuss the quantitative results.
- What was your hyperparameter tuning strategy? It is unclear if these baselines have been best optimized.
- Can you also compute AUROC and AUPRC for the classification tasks? Accuracy is not sufficient.",- Can the authors discuss the scalability of the framework? What is the computational complexity?
NJxCpMt0sf,NJxCpMt0sf,"Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",Accept (Poster),ksCn2chrig,ICLR.cc/2025/Conference/Submission11923/Reviewer_xATa,"The paper mainly addresses two challenges in clinical tasks: patient-level and task-level dynamic fusion. For the patient-level fusion, a modality-specific MoE is employed. For the task-level fusion, a modality-task MoE with conditional MI regularization between experts and modalities given tasks is adopted. The experiments using EMBED, RSNA, VinDR, and GAMMA datasets outperform existing methods in both single-task and multi-task settings.","- The paper is well-structured and easy to follow. 
- The motivation is clearly stated and convincing. 
- The experiments show promising results over many baselines both in stand-alone and add-on manners.  
- The paper adopted PID to make a fair comparison of synergy information.","- As far as I understand, there have been works leveraging the shared and specific information across modalities and should be included in discussions, see [1-3]. 
- Is there an ablation study for a reduced number of experts? How sensitive is this method when the number of experts decreases compared to other MoE methods? What is the procedure for choosing the number of experts? 
- Please discuss the computational cost compared to the baselines. 


[1]Wang, Hu, et al. ""Multi-modal learning with missing modality via shared-specific feature modelling."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[2]Yao, Wenfang, et al. ""DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.

[3]Chen, Cheng, et al. ""Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion."" Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III 22. Springer International Publishing, 2019.","- Are there any ablation studies on datasets other than EMBED?
- Are there any theoretical explanations on why the method mitigates gradient conflict?
- Is there any clinical interpretation of the results in Figure 5? For example, the difference in modality contribution across different tasks."
NJxCpMt0sf,NJxCpMt0sf,"Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",Accept (Poster),beQvBiay8T,ICLR.cc/2025/Conference/Submission11923/Reviewer_dm8E,"This paper proposes a multi-modal, multi-task, mixture of experts for various medical diagnoses to address the challenges of sample-dynamic modality fusion (and modality-task dependence (selecting the right modalities for a task). Concretely, this is done by using a combination of modality-specific experts and experts shared between modalities and tasks. M4OE shows promising initial results in terms of both absolute performance and enforcing modality utilization.

Based on the weaknesses and questions outlined my score indicates a rejection for now, but I generally like the motivation of the paper, especially the aspect on modality utilization. I am willing to increase my score if my concerns are addressed and questions clarified.","- The M4OE is highly effective at enforcing modality utilization - this is a meaningful contribution that many multimodal models suffer from, although I do have some questions about this.
- The overall performance of the model is outperforming the baseline, even if the results are missing crucial information to validate the statistical significance of the results.
- Strong visuals that are additive to the understanding of the paper.
- Good conceptual motivation of the paper, although I believe that the motivation would further benefit from some concrete examples of sample dynamism and clinical examples of tasks that are modality-dependent.","- Abstract: the one-liner for sample-dynamic modality fusion is unclear as the specific and shared information always varies per sample unless they are identical. To my knowledge, sample-dynamic spans a much wider field of problems like missingness, robustness to noise, which the manuscript does not consider.
- Abstract: “Results demonstrate superiority over state-of-the-art methods” is extremely vague. Along which metric?
- You claim an expansive space by saying that the method is “multi-modal multi-task”, but your experiments only look at multi-view settings of a single modality (images). I would encourage you to narrow the scope/claim of the paper as the paper does not consider heterogeneous modalities (images, text, tabular, etc.).
- Experimental setup: I would encourage you to provide more detail in this section to aid reproducibility. For example, it is unclear whether cross-validation is used. No confidence intervals or standard deviation of results are reported to judge the statistical significance of the results. Additionally, no code was provided in the supplementary materials that would help with the clarification of the experimental setup.
- Literature: missing out on the largest corpus of literature (intermediate fusion), which many latent variable models for multimodal fusion fall under, many of which are using a mix of modality-specific and shared spaces.","- How do you determine which expert sees which task? The connection between Figure 1 and the method section is not very clear.
- The manuscript talks a lot about sample adaptivity, but how does your experimental setup show that the model handles sample adaptivity effectively? Which aspects of sample adaptivity?
- Figure 3c suggests that the modality utilization is forced towards the same mean in your method. What about cases where modality dominance/competition is good? For example, if I have one very noisy modality, wouldn’t it be desirable to have the modality that contains all the signal to get all the model’s attention? Isn’t this graph showing that we enforce equal utilisation of all modalities regardless of the signal? Additionally, does this finding not contradict your claim in Figure 1, which is that only some experts are used (as opposed to all experts with a more balanced contribution)."
NJxCpMt0sf,NJxCpMt0sf,"Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",Accept (Poster),HempfVZAo4,ICLR.cc/2025/Conference/Submission11923/Reviewer_7L3d,"This paper introduces M4oE, a framework for multi-modal, multi-task learning in medical diagnosis. M4oE addresses two primary challenges in multimodal diagnosis: sample-dynamic modality fusion and modality-task dependence. The framework incorporates modality-specific modules and a modality-shared modality-task mixture of experts (MoE) to dynamically learn both unique and shared information across modalities. A conditional mutual information loss is used to optimize the framework efficiently. Experimental results on two medical diagnostic tasks demonstrate M4oE’s advantages over existing methods.","1. The M4oE framework is a novel contribution, with two innovative components—modality-specific modules and a modality-shared modality-task MoE—that allow for efficient learning of both distinct and shared information across modalities for multiple tasks.

2. The analysis is comprehensive, including detailed evaluations of modality competition, modality-task dependence, and sample-level modality contributions.","1. The M4oE framework is complex, raising concerns about optimization and practical implementation. Including a discussion on computational resources and runtime would help readers assess its feasibility for real-world applications.

2. The paper does not address whether M4oE can function effectively when certain modalities or tasks are unavailable—a common scenario in clinical settings. Clarifying this would strengthen the model’s applicability.","1. Could the authors provide computational resource requirements and runtime comparisons for M4oE and baseline methods?

2. In Figure 4, the similar difference between subfigures (a) and (b) and between (c) and (d) suggests both M4oE and the baseline might be capturing modality-task dependence. Could the authors clarify?

3. In Figure 3, is it reasonable to assume that the diverse distribution in (c) is preferable to that in (b), given the absence of a known ground truth distribution for each modality? Using the method in Liang et al. (2024) to quantify modality interactions could provide a more rigorous evaluation.

4. In Figure 1(a), under Part 3, should the label for the green triangle be $G_p$?"
s5N7p5UjgR,s5N7p5UjgR,Markovian Transformers for Informative Language Modeling,Reject,M9AyTQlHF1,ICLR.cc/2025/Conference/Submission11801/Reviewer_vxem,"This paper proposes a framework where the reasoning steps are used as fixed-size states, which limits the model’s context (text bottleneck), and force the model to use the reasoning steps as input. This method design is inspired by the fact that past CoT literature find that the final answer might not be sensitive to the CoT trace. In the experiment, the authors show that the model trained with this method is indeed more fragile against CoT pertubations.","- The Markovian framework blocks the model from attending back to the original question and force it to use the CoT context for generation. This provides a new view and framework for analyzing CoT effects.
- The reinforcement learning-based approach demonstrates improved performance on tasks requiring multiple steps, 
- The CoT steps generated from this method seem to be more interpretable, from two dimensions: 1) pertubation of the CoT could lead to more model errors 2) the reasoning can be carried over to another model.","-  While the approach improves the model’s reliance on CoT, it’s uncertain if this CoT is genuinely interpretable by humans. The transferrability between Mistral and Llama should only serve as an indirect proof.
-  It is actually fine to focus on QA task, but probably we'd like to see how this can generalize to more domains other than arithemtic. Would this paradigm also work for other reasoning task as well?
- There seem to be no baselines and ablation designed, so it is a bit hard to position the effectiveness of the method against other methods.
- The fragility analysis is insightful but lacks depth. A more detailed investigation into which types of perturbations impact CoT reliability most could provide valuable insights.
-  Writing-wise, the paper writing is clean, but probably some adjustment of the sections flow and emphasis would be nice.
   -  For instance, while the method section is quite detailed, it’s presented before establishing the limitations of existing CoT techniques clearly, which makes it harder to understand the innovation.
   - There are few tables but quite a few definitinos and equations. I'd suggest consider streamline the method descriptions and move some of them to the appendix, while adding more discussion and insights in the main body.","- Is there a way to combine the interpretability with actual human perception? Though informativeness here can be used to improve model quality, it is also very helpful from human level. This is probably mentioned in F. But it seems to me F is more about how to encode human interpretability in training.
- In F it is mentioned that ""optimal CoT would be a compression of the question, which can potentially be difficult for humans "". Is this observed in your experiments?
- Were there more ablation study or comparison conducted?"
s5N7p5UjgR,s5N7p5UjgR,Markovian Transformers for Informative Language Modeling,Reject,xEcX6iHMxl,ICLR.cc/2025/Conference/Submission11801/Reviewer_CNEs,"This paper proposes a metric to measure the informativeness of CoT tokens, and then uses RL to train the model to generate highly informative CoT tokens, in order to improve the correctness of the final answer. Experiments in random addition problems and GSM8K math problems demonstrate the effectiveness of the proposed metric and RL methods. The paper also shows that more informative tokens will bring gains in interpretability.","The technical ideas, including the proposed metric and RL methods, are new, well-motivated, and technically reasonable.  

The experiments in random addition and GSM8K are positive.","The experiments are limited to a synthetic math problem setting and GSM8K, and the only trained model is Mistral 7B. 

The presentation needs a better organization. E.g., some major results are placed in the appendices, but training details are in the main paper.","Have you tried other open-source models like llama? Not use CoT of Mistral in it, but use your method to finetune Llama."
s5N7p5UjgR,s5N7p5UjgR,Markovian Transformers for Informative Language Modeling,Reject,bRQna1gVxi,ICLR.cc/2025/Conference/Submission11801/Reviewer_etnS,"This work introduces and explains the construction of a Markovian Language Model to study causality in chain-of-thought reasoning. With a limited state (the previous state and its observation) on which to condition, the model is trained (fine-tuned) to maximize an informativeness objective through PPO, and is empirically shown to improve performance on mathematics tasks such as GSM8K and toy addition problems.","* The setup of this causally-guided model is pretty novel, and the finding that this improves performance by optimizing on an “information” metric is an impactful finding. 
* The selection of the RL training technique (PPO) is supported by highlighting the limitations of other considered methods (expert iteration and policy gradient).
* The work is written in a way that was simple to follow, which I appreciated.
* The gains on GSM8K (24.64% --> 35.71%) are meaningful (although, a bit tucked away in the paper's text).","* I understand the general intuition surrounding the design of the informativeness function, but it would be good to add some discussion on why the expected reward over the trajectory actually constitutes / addresses “informativeness” under your construction. 
* While math tasks have a more well-defined structure (the order in which their steps may be pursued), this is less clear for other tasks without such a clear structure in natural language, for instance. It would be good to examine this approach on at least one such task to further support the method’s general efficacy.
* Despite the intuition-based process of selection for the RL training strategy, there are recent works that advocate in favor of expert iteration and REINFORCE / vanilla policy gradient for LLM reasoning and RLHF [1, 2]. To this effect, including such approaches for comparison in the results section (or in the appendix) would strengthen the defense of the PPO method chosen. It would be helpful to include some evidence supporting the limitations posed.
* While I appreciate documenting the design choices in Section 4.3, some justification behind them would be beneficial, either through ablations (it’s fine for these to be in the appendix) or relevant references. 

1. Teaching Large Language Models to Reason with Reinforcement Learning. Havrilla et al. 2024 
2. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. Ahmadian et al. 2024","* Is the space of states task-conditional? This isn’t apparent based on the formulation in Section 3.1 (and is unclear by the wording in line 162). If not, then it would seem that the set of relevant “CoT states” would be very sparse relative to the complete space. 
* As posed in the weaknesses section, does this method extend to other reasoning tasks (e.g. in natural language or code) whose structure is less “linear”?"
s5N7p5UjgR,s5N7p5UjgR,Markovian Transformers for Informative Language Modeling,Reject,GjEOgp8264,ICLR.cc/2025/Conference/Submission11801/Reviewer_Ddh5,"__Post-rebuttal update__:

After the rebuttal has concluded, I feel the need to express my strong support for this paper. I believe the proposed method has the potential to become an industry-defining standard, which ICLR should be proud to be the publisher off. The authors have done a lot to further improve the paper from a decent submission to an excellent submission that should be highlighted at a conference. While one can always conduct more experiments to support one's claims even more strongly, I think the remaining requests made by other reviewers are unrealistic. The paper should be accepted as is.

---------

The paper addresses an issue of Chain-of-Thought (CoT) reasoning in LLMs where the LM's final answer does not always depend on the CoT. The paper's idea is to enforce informativeness by conditioning the answer model on the generated CoT only without other context. To this end, the paper formally defines Markovian Language Models and (informative) update functions, from which the policy gradient procedure is derived. Applying the framework to the specific use case of CoT reasoning, the paper experiments with several RL techniques such as expert iteration, policy gradient, and PPO. The model is applied to a simple arithmetic task of adding 15 numbers as well as GSM8K, and shows that the model a) improves performance on the task b) is sensitive to perturbation in the CoT reasoning and c) produces CoTs that are sensible to a different language model such that its performance on the task is improved.","* The paper addresses an important limitation in Chain-of-Thought reasoning, which is of relevance to the broader ICLR community.
* The core idea is intuitive and simple.
* The paper is well written.
* The results on the simple arithmetic task and the math task are promising.
* The claims that the proposed method improves the generated CoTs in terms of interpretability and informativeness are well supported.","The method is evaluated only on few tasks and models, limiting how sure we can be that this is a useful method. Especially an application to language modeling would be very insightful and potentially extremely impactfull. However, while more is always better when it comes to experimental results, I think that this initial set of experiments support the ideas presented well and should suffice for publication.",Please use different line styles in your figures so colorblind people can make sense of them. Otherwise Figure 2 and 3 are really hard to parse!
o2uHg0Skil,o2uHg0Skil,"RL, but don't do anything I wouldn't do",Reject,sexbK4r8VZ,ICLR.cc/2025/Conference/Submission11755/Reviewer_vbPE,"The paper investigates the effectiveness of KL regularization as a safety paradigm for RL agents. The authors, using the formalism of algorithmic information theory, show that if one uses KL to regularize the behaviour of RL agent, one would eventually need to continually impose a very tight KL constraint. In fact, they show in Theorem 1 that there exists near-optimal policies with little KL divergence to an imitative policy. In the work this is showed that this is not only singular to KL, but to other regularizations such as TVD. 

The paper further proposes empirical experiments to validate their theoretical results. Specifically, they do so in a scenario when an LLM simulating a teacher has as an objective to maximize positive sentiments from the students. The empirical results show that by increasing the KL budget, the agent learns to remain silent. Together with other results, these 'imperefect' experiments support the theoretically results proved. 

Finally, the author propose to overcome the problems highlighted by proposing a theoretical alternative, which relies on the RL agent asking for help to the human demonstrator if at hand. Given the intractability of this approach, it was not possible for the authors to empirically validate this approach.","The paper proposes novel, and technically solid ideas. In many points along the paper the authors provide additional intuitions on the significance of the results, which have been useful to grasp the subtleties of the theoretical results. The continual links between the claims made, the results showed and the implications that they have made the paper easy to follow and clear to understand, although this is not precisely my area of expertise. Also good to include a proof outline as now, e.g. for Theorem 1. 

I am also positively surprised that the paper provides empirical experiments to validate the theory. By reading the first part of the paper I would have imagined this to be difficult or lead to an oversimplification, but I am actually satisfied by these results. I still believe some additional experiments could be done to improve this ( see questions).

More importantly, I think the claims are correctly calibrated - given the technical ( and almost speculative, in a positive sense) nature of the paper, it would have been easy to fall in overestimating the impact and significance of the results. The authors did a very good job in clearly stating the limitations of their work, both for the theoretical and empirical part.","The main weakness of the paper regards the assumptions made in Section 4 (see also questions below). Most of them are sound, but I believe part of the community would not necessarily agree with this. I think it could be beneficial for the paper to have additional results where some of the assumptions are weaken, and investigate which theoretical results would derive in tis case. For example, what happens if we drop the assumption that all overoptimized policies lead to unsafe behaviours? 

Another point is that it is not completely clear to me that future powerful AI systems will in fact respect the assumptions made in the paper. While I agree that current AI systems are not powerful enough to fully support the theoretical results, it would be insightful to add some discussion regarding why the authors think that this paradigm will necessary be relevant for more powerful AI systems.","- Could Theorem 1 be summarized as saying ""once an event E which can be described with a small K(E) happens, given the bound given the RL agent may be able to exploit this to perform a (near-)optimal policy while remaining in the KL constraint. Thus, given that maximally optimizing the true reward leads to unsafe behaviour, this may lead to catastrophic behaviour"". If that is the case, I have two questions related to this: (1) Is there any constraint on which type this event E needs to be? In other words, does this hold for all unprecedented events with small K(E), or for a subset of them which may be related with unsafe behaviour? (2) If we drop the assumption that overoptimization necessarily leads to catastrophic behaviours but let's say happens with probability $p$ once overoptimizing, how can Theorem 1 be extended? Is then the probability of having catastrophic behaviour just proportional to $p$, or we are ensured that in that case the KL would be enough?

- I suggest making Figure 1 more explanatory, for example by adding the notation relevant to the component nearby each box. I believe this would be possible if making the image larger, and would be a good reference for the readers to appropriately understand the settings and go back to it if needed while reading the paper. 

- What results would be obtained if instead of doing KL(rl policy|| base/imitative policy) one uses KL(base/imitative policy||rl policy)? 

- Any explanation for the bump on the left Figure 3, around the right side of the x-axis?

- ""when the agent discovers a sufficiently high-reward strategy, the fixed KL penalty becomes swamped and ignored, and if the KL penalty is increased to a level where it can stop that, the agent never gets off the ground"". I believe it would be interesting to see results where this is the case, especially looking at which is the threshold for the KL penalty to exhibit one behaviour or the other. Did you observe a 

- It seems a bit arbitrary to run experiments solely with a budget of 10 or 20. It would be good to provide additional experiments where the budget is varied, and have a plot where on the x-axis there are a few budget data points, and on the y-axis a metric that aggregates the fraction of responses empty.

I will be keen to raise my score if the above questions and doubts are addressed appropriately."
o2uHg0Skil,o2uHg0Skil,"RL, but don't do anything I wouldn't do",Reject,YbfBUhvhq1,ICLR.cc/2025/Conference/Submission11755/Reviewer_s47a,"This paper studies the problem of KL-constrained RL, that underpins modern LLM technology, from an algorithmic information theory lens. Their main result suggests that the KL penalty used to avoid the RL fine-tuning process deviating too much from the pre-trained, predictive policy can result in very bad policies that maximize the proxy reward, have small KL, and learn unsafe, undesired behaviors.
The authors formulate the problem by modeling the base/predictive policy as a Bayesian predictive model that is an approximation to the real policy that we do not have access to. Their main result suggests that RL finetuning would make the policy converge to very simple policies that are reward maximizing and have small KL penalty but that diverge from the real policy that we would like to get.","1. The paper is very well written: The ideas behind the theory are easy to follow and the authors argue compellingly why this might be a good model of the KL-constrained RL problem
2. The paper is very relevant to modern problems. The paper sheds light on a possible explanation of the performance of RL fine-tuning of imitation policies and they suggest a possible avenue to solve the problem. 
3. The empirical evidence is compelling and useful to further understand the theory. The authors use interesting LLM fine-tuning experiments to show how simple rewards can be optimized with simple, policies that have small KL values but converge to policies that are qualitatively far from the desired policies (and the base policy)","The argument made by the authors is certainly compelling and the empirical evidence seems to suggest that the theory applies. I wonder if algorithmic information theory arguments fit the real-world problem. I guess that there are hypotheses that neural networks approximate such algorithmic theoretical simple programs, however we do not know enough about it. This might be the subject of future research, however, I'd like the authors to comment more on this.","1. If we are expected to devolve to very simple policies when doing the RL finetuning, why is it that we can tune the process such that we get the performance of modern LLMs? 
2. If over-optimizing our proxy reward function causes these struggles, what, you might say, must be the RL objective for this problem if maximizing return is not good?"
o2uHg0Skil,o2uHg0Skil,"RL, but don't do anything I wouldn't do",Reject,1R1m2lueiH,ICLR.cc/2025/Conference/Submission11755/Reviewer_4v2F,"In current llm training, it is common to force the RL policy to be close to a base policy learned by imitating a demonstration policy (the paper calls it trusted policy) from data, using KL regularization. The problem that this paper points out is that current regularization approach can not keep the RL policy close to the demonstration policy.

The paper first notes a simple fact that, for three policies B, D, R,  even if KL(D || B) and KL(R ||B) are both small, KL(R || D) can be infinitely large. This implies that KL(RL policy || demonstration policy) can be large, even if KL(demonstration policy || base policy) and KL(RL policy || base policy) are small. Therefore, RL policy and demonstration policy are not close to each other with KL regularization.

The paper then provides a result showing that there are near-optimal policies with a small KL divergence from a base policy. And given that reward models are not accurate and RL policies typically exploit the weaknesses of the reward models, near-optimal policies are bad policies. This suggests that to keep RL policies not nearly optimal (and therefore bad), the KL regularization needs to be very strong. Further, the same result shows that more imitation learning only slowly increases the KL divergence. The paper then performed an empirical study to verify this theoretical result.

The paper finally shows that for a particular existing imitation learning algorithm of the base policy, which can actively ask for help when facing uncertainty, the above problem is avoided. However, this algorithm is not tractable.","The studied problem is closely related to large language model training, which is a popular topic currently.

The results of the paper are novel, non-trivial, and explains certain observation found in large language model training.

The paper shows the authors understanding of the root cause of the problem.","My main criticism is the quality of the writing of this paper. The paper reads like the flow of the authors' thoughts, instead of an academic paper. The poor writing makes it hard to evaluate the paper's contributions. 

Examples:

1. Paragraph right after proposition 2. ""Developers of self-driving cars are learning the hard way that this bit of algorithmic information theory has practical analogs: Even with enormous datasets, unprecedented road conditions occur all the time. These results suggest that if we intend to use an imitation learner as a base policy for regularizing a goal-directed agent, we should not strive to approximate ideal Bayesian imitation.""

I don't know why the two sentences should appear in the same paragraph. How are they related to each other? And why is this paragraph immediate after proposition 2?

2. The constant d is a small one corresponding to how much code it takes to implement a search tree, Bayes’ rule, and a few if statements.

The paper didn't mention the search tree, Bayes’ rule, and if statements up to this point. Why talk about them?

3. ""So unless we use a fairly tight lifetime KL constraint, if the RL agent just waits for an unprecedented event with small K(E), it could then execute an optimal or near-optimal policy, even if that catastrophically thwarts human control, regardless of the content of the base model’s training data, even if the humans that the base model imitates would never, ever behave that way.""

I don't understand this sentence. Does such a long sentence effectively convey your ideas?

4. ""We say an action is Vξ,U -optimal if it maximizes the associated Q value""

What is Q value?

5. ""Let’s consider the case where it is acting in the real world, and maximal reward could be attained by thwarting our control and intervening in its own reward, setting it to a maximal value for all successive timesteps.""

What does this mean?

6. ""The utility function, simply summing rewards, has an extremely short program length.""

Why?","1. Theorem 2: why is \\pi_c^{TVD} function of a_t and x_{<2t}? According to your definition, it is not a function.
2. ""As we increase the amount of training k, the Bayesian imitative base model ξ becomes a closer approximation to the humans generating the actions a<k""
Why is k related to the amount of training?
3. From Definition 1, \\nu stands for environment transition probability. So \\xi should also be. Then why consider the KL divergence between \\pi and \\xi in Theorem 1?"
o2uHg0Skil,o2uHg0Skil,"RL, but don't do anything I wouldn't do",Reject,Xr9lCclZy1,ICLR.cc/2025/Conference/Submission11755/Reviewer_CJKk,"The central point in this paper is that regularizing a policy by KL(policy || base policy) for safety reasons, does not guarantee much safety if all we know about base policy is that KL(safe policy || base policy) is small. This is relevant as many RLHF works use KL regularization as a safety mechanism. The paper shows an interesting theoretical result (Theorem 1) that shows that by minimally affecting the KL distance, a policy can be fine tuned to maximize an arbitrary reward function. The result builds on algorithmic information theory, and looks at a policy that only changes behavior to maximizing reward after some event has occurred, and the idea is to bound how much more complex this policy needs to be (identify the event + compute max reward policy). 
The second part of the paper tries to connect this theory to an experiment, where a base conversation agent was fine tuned using RL to maximize the sentiment of the other person’s response under some KL constraint. The agent learned to output empty response, which give neutral sentiment, showing that while KL was regularized, an unwanted performance was obtained. 
The third part shows that a specific alternative to KL regularization that is not computationally tractable can in principle avoid the alignment problems outlined in the first part.","Disclaimer: my expertise is RL, and I was not familiar with algorithmic information theory before reading the paper. 

1. The paper is interesting and thought provoking! I found the connection between KL regularization for RLHF and algorithmic information theory very interesting, and in general I enjoyed reading the paper.
2. The insight that KL regularization cannot guarantee that utility is not optimized (Theorem 1) is important, as this technique is common in RLHF, and alignment is of high interest to a large part of the ICLR community.
3. The experiments, although more of an illustration than an actual result, are interesting and not trivial.","The biggest weakness is in connecting the theoretical assumptions and results in the paper to a practical meaning. 
1. What is the meaning of selecting the prior according to Solomonoff Induction (Lines 141-149)? Why should we expect the RLHF to be related to this prior?
2. In theorem 1, for the bound to be small, we need K(U_m), K(E), and K(v \\xi) to be small. I did not understand how we can bound these terms, or why should they be small in practice. Could the authors provide some simple examples that demonstrate the consequences of Theorem 1? The authors discuss the theorem in length, but I found the discussion too vague.
3. In the experiments, a critical factor was that neutral sentiment give reward (0.5) and is easy to obtain. This is a nice demonstration of the point of the paper (high reward, but bad policy). Still, this seems a bit “engineered”, and would have been easy to fix by not giving reward for neutral sentiment. I wonder how the results would look like without reward for neutral sentiment.

Detailed comments:

Line 67: what does “open mindedness” mean here exactly? I found this phrase hard to parse.

Line 77-78: this sentence is very confusing to read, consider revising it to clarify your point. What exactly do these empirical results show?

Line 166: in Definition 2 - for a deterministic environment, can we remove the max over observations? If so, Think that adding this would clarify the motivation for this definition. Also, was this definition considered in prior work?

Line 205: why is K(U_m) small / bounded? 

Line 236: “if the RL agent just waits for an unprecedented event with small K(E)” - wouldn’t such an event be very unlikely? Can you say something *in expectation* (or high probability)?

Line 240-249: I found this paragraph very vague and confusing. Why is k the amount of training? Does K(E) necessarily grow with k (is there a formal statement)? 	Can you translate the second half of the paragraph with months of life, etc., into a formal statement?

Line 251: what exactly is the definition of “simplest unprecedented event”? 

Line 253-256: I don’t see the connection between algorithmic information theory and rare road conditions (what is the complexity of a road exactly?). Can you provide a more rigorous connection between this paragraph and the previous theorem?

Line 257-259: What if we used Jensen-Shannon Divergence? My guess is that it should fix the problem outlined in Proposition 1.

Line 284: why do you add the feature activations to the agent?

Line 290-303: The KL you refer to here is for single actions, not the KL in definition 2, right? Can you elaborate on the different KL terms you use?","In addition to the above, there’s something I don’t understand about Theorem 1. 
Consider the following example:

There are only two actions, A and B, and no observations. The reward for action A is 0, and the reward for action B is 1. The utility is the average reward (so max is 1).
The imitative policy chooses A with probability 1-epsilon, and therefore, its utility is epsilon. The optimal utility policy always chooses B, and has value V* = 1. 
Now, for a policy to obtain reward p at some time step, it needs to choose B with probability p. Then, at that time step, for small epsilon, the KL(policy || imitative) ~ -ln(epsilon)*p > p [using a simple approximation].
In this case, for the policy to obtain high utility, it must also exhibit high KL from the imitative policy.

How does this result reconcile with the explanation for Theorem 1, which claims that “there are policies with near-optimal utility with little KL divergence to an imitative policy”? What am I missing here?"
xCFdAN5DY3,xCFdAN5DY3,A Foundation Model for Weather and Climate,Reject,jNIdr7btzq,ICLR.cc/2025/Conference/Submission11704/Reviewer_pzCZ,"This paper introduces a new foundation model, Prithvi WxC, for atmospheric modeling applications in weather and climate.  Prithvi WxC was trained on 3-hourly data from 1980 to 2019 from the MERRA-2 reanalysis dataset based on a masked reconstruction/forecasting pre-training objective. 
The model follows a transformer-based encoder-decoder architecture inspired by Hiera and MaxViT.
Afterward, the model is fine-tuned for various downstream tasks: Medium-range weather forecasting, global and regional downscaling, and learning a gravity wave flux parametrization. These tasks have different sets of spatial resolutions, variables, and datasets, showing the flexibility of the foundation model.","1. Prithvi WxC is quite flexible as it can be used for a broad range of downstream applications, as convincingly shown in the experiments. 
2. The method contains original ideas such as the pre-training objective and using climatology-derived anomalies as targets, which I found interesting to read about.
3. The paper is generally clearly written and easy to read.","1. The paper falls short of establishing a compelling case for Prithvi WxC as a foundation model for weather or climate. The practical significance and advantages of this approach remain inadequately demonstrated:

a.) While foundation models typically excel at zero-shot performance and data-efficient fine-tuning across diverse tasks, the evidence presented for Prithvi WxC's capabilities in these areas is not convincing. Baselines for the non-forecasting experiments are either very weak (interpolation-based downscaling) or non-existent (gravity wave experiments). Some highly relevant and simple baselines are: 
- How much worse(?) does Prithvi WxC perform on these tasks if you omit the pre-training stage (i.e. initialize with random weights instead of the frozen pre-trained ones, and train all parameters jointly from scratch on the tasks)? 
- How about completely removing the pre-trained transformer backbone (i.e. removing the Prithvi WxC block from Figures 12 & 13)? 
- For the latter, it would be also good to run an experiment where you replace the pre-trained Prithvi WxC backbone with some ""lightweight"" blocks (e.g. a (deeper) U-Net), trained in a task-specific way from scratch, to account for the huge difference in parameter counts if you completely remove Prithvi WxC. 

These ablations would immensely help in understanding how useful the pre-training stage is for these downstream applications (e.g. does using pre-trained Prithvi WxC improve performance over such simple baselines? Is it more data-efficient?). Besides, otherwise, it is hard to see evidence for the claim in the conclusion that *""Instead of building task-specific ML-models from scratch, these pretrained encoders can be used to develop more precise data-driven models of atmospheric processes""*.

b.) No ablations are included. I understand that training such a huge model is expensive but having a few ablations would have been very appreciated (perhaps, with a smaller-scale version of the model). For example:

- How crucial is it to predict climatology-normalized targets as opposed to normal per-variable means/stds? 
- What's the forecasting performance of Prithvi WxC after the first pre-training phase?
- How important is local vs global masking? What about the masking rates?
- What's the line of thought behind randomizing the distance between input timesteps? Can the model only use one input timestep? I presume this is possible by masking the corresponding snapshot by 100%, but no experiments with this setting are shown. 

c.) The weather forecasting results seem lukewarm, albeit it is hard to judge because the comparison is not apples-to-apples.
- Prithvi WxC is trained and evaluated on Merra-2. The baselines are evaluated on ERA5. These reanalysis datasets have different spatial resolutions. The evaluation years seem to be different too (correct me if I'm wrong). It would help to fix this mismatch. For example,  given the foundational nature of Prithvi WxC... why not fine-tune it on ERA5 directly? Showing that it can be competitive to these baselines in an apples-to-apples comparison would be a very strong result.
- Based on the mismatched comparison, Prithvi WxC seems to be competitive on 6h to 12h forecasts but it's quite notable that its performance implodes compared to the baselines for longer lead times. It is very unclear why. I wouldn't necessarily expect this version of Prithvi WxC to be state-of-the-art, but the performance does seem underwhelming. Especially given that the authors did ""several things"" to tune these results (i.e. a second forecasting-specific pre-training stage and autoregressive rollout fine-tuning).
- The hurricane evaluation includes hurricanes from 2017 to 2023. This seems to overlap with the training data period (up to 2019). 
- Either Figure 6 or its analysis in the main body of the text (lines 251-253) is wrong because I see all of the three models do best on exactly one of the three RMSE figures.
- For the hurricane forecasting experiments, I would appreciate a comparison to the state-of-the-art models included in the weather forecasting experiments (e.g. GraphCast) which have shown to be better than FourcastNet.

d.) The downscaling problem setup is artificial. Downscaling coarsened of existing reanalysis/model outputs is not of much use in practice. A realistic and important downscaling application, as discussed in the Appendix, would be to downscale coarse-resolution model outputs to high-resolution outputs (either of a different model, observations, or the same model run at higher resolution).
 
e.) The climate model parameterization experiments should be more carefully interpreted. 
- The model predicts outputs that are normalized by the 1980-2019 climatology. Unfortunately, decadal or centennial simulations of the future under a changing climate are inherently a non-stationary problem. It is highly unclear if Prithvi WxC would remain stable, let alone effective, under this highly relevant use case. This is particularly so as the in-the-loop (coupled to a running climate model) stability of ML-based climate model parameterizations is a well-known issue.
- The selling point for ML-based emulators of climate model parametrizations is often their computational cheapness. Thus, the runtime of Prithvi WxC should be discussed. Given the large parameter count of Prithvi WxC it might be important to note its runtime as a limitation for these kinds of applications.
- Line 461 claims that Prithvi WxC ""outperforms"" task-specific baselines but no baselines whatsoever are included in the manuscript for this experiment.
- Are the inputs a global map? I am not familiar with gravity waves, but I believe that most physics parameterizations in climate models are modeled column-wise (i.e. across atmospheric height but ignoring lat/lon interactions). This is surely a simplification of these parameterizations, but it seems to indicate that they're highly local problems. What's the motivation for using global context then?
- The end of the section should be worded more carefully, clearly stating the aforementioned limitations.

f.) No scaling experiments are included. Thus, it is unclear how important its 2.3 billion parameter size is, how well the model scales, and how its size impacts performance on the downstream applications. Besides, vision and language models are usually released with multiple model sizes that cover different use cases (e.g. balancing inference speed with accuracy). It would be really useful to get these (and carefully compare them) for Prithvi WxC.

2. Related work is insufficiently discussed. Please include an explicit section discussing it, focusing on:
- Carefully comparing similarities/differences to existing weather foundation models (e.g. architectures, pre-training objectives, downstream applications etc.). Besides, ClimaX is not properly discussed in the paper. Given that it's also a transformer-based foundation model, validated on forecasting, downscaling, and climate emulation, it is very important to include it in the comparison. 
- Similarly, please discuss how exactly the masking technique in this paper relates to the ones proposed in Vandal et al. and McNally et al..
- Carefully discuss how the architecture is derived from Hiera and/or MaxViT (and other papers of which components were derived, if any).

3. While the authors transparently discuss some issues/limitations with their experiments (e.g. the evaluation data mismatches), it would be nice to also include an explicit paragraph or section on this (and include aforementioned things like the issues with the climate model parameterization experiments).

Minor:
- Can you properly discuss, and include a reference to, what a Swin-shift is?
- Similarly, for the ""pixel shuffle layers""
- Line 39: Pangu -> Pangu-Weather
- Line 48: Nowcasting should be lower-case
- Equation 1: Consider reformulating this as an objective/loss function.
- Also Eq. 1: What is $\\hat{X}_t$? What is $\\sigma_C$?
- Line 93: $\\sigma^2_C = \\sigma^2_C(X_t - C_t)$ doesn't make sense to me.
- Line 104: *"" same 20 year period that we used for pretraining.""* .... Do you mean 40 year period? If not, which 20-year period from the 40-year training period did you use?
- Line 157: Multiple symbols are undefined (e.g. $V_S$).
- Line 169: It's not entirely clear what ""alternates"" means in this context.
- Line 429: ""baseline""... do you mean Prithvi WxC? 
- Line 507: ""improved""... improved compared to what?
- Figure 12: Do you mean 'downscale' on the right ""upscale"" block?
- Sections D. 2.3 and D.2.4 in the appendix are literal copies of the corresponding paragraphs on pages 8 and 9. Please remove.","Major: 
- Do you compute area-weighted RMSEs? If not, I strongly encourage fixing this (especially for the weather forecasting experiment; see e.g. Weatherbench2).
- For the fine-tuning experiments, do you start them based on the weights resulting from the first or second pre-training stage? If the former, then the second pre-training seems to also be a form of fine-tuning and I would suggest avoiding using the term ""zero-shot"" for reporting the forecasting results.
- From looking at Figure 3, I don't think that I agree with the author's interpretation that *""It is interesting that reconstruction performance is relatively little affected by lead time at the lower end of masking ratios""*. There's a clear cap between ""0h, global"" and ""6h, global"", especially for the lower end of masking ratios. Do I miss something?
- Can you include downscaling results on more variables than only T2M?

Minor:
- What's the difference between the encoder and decoder blocks? Fig. 1 suggests these are identical... Is the difference some densification by the ""reconstruct batch"" module in Fig. 1? If so, can you explain this more and make it clearer?
- Line 174: ""reduce the masking ratio to 50%""... is this a typo (you use the same rate for the first stage)? What's correct?
- Why is there no reference line in figure 5b)?
- Please define what's meant by spatial and temporal RMSEs.
- Can you include snapshots like Fig. 9 but at the native temporal resolution used for prediction (i.e. not a monthly mean)?
- How does using different patch sizes (larger than the used size of 1) impact downscaling performance? 
- Why do you call the model Prithvi WxC?"
xCFdAN5DY3,xCFdAN5DY3,A Foundation Model for Weather and Climate,Reject,OPDospeaEJ,ICLR.cc/2025/Conference/Submission11704/Reviewer_HFKb,"The paper presents Prithvi WxC, a new foundation model designed to support a wide range of weather and climate applications. Built on a large, transformer-based architecture, Prithvi WxC is trained on the extensive MERRA-2 dataset, which covers 160 variables capturing atmospheric data. The model is unique in its ability to address multiple tasks—including forecasting, downscaling, and parameterization—making it versatile in handling both regional and global weather patterns.","### Originality and significance 
Prithvi WxC pushes the foundation model concept in atmospheric science further by expanding beyond just forecasting— seen in earlier models foundation model Aurora. Its architecture and training approach enable it to tackle a variety of downstream tasks, such as downscaling and parameterization, making it a valuable tool for both short-term weather predictions and long-term climate modeling. With its modular design, Prithvi WxC can be adapted flexibly to new tasks that combines AI with physical climate models.

       
### Quality
The authors have thoroughly evaluated Prithvi WxC across a range of tasks, including zero-shot reconstruction, downscaling, and extreme event forecasting. The extensive validations across different downstream tasks support Prithvi WxC’s adaptability and effectiveness in diverse weather and climate applications.



### Clarity and open research
The paper is organized in a clear, logical flow, moving smoothly from motivation and background to model architecture, objectives, and results. Key ideas, like the mixed masking and forecasting objective, are presented in a way that makes the technical contributions accessible to both AI and climate science audiences. Code and comprehensive supplementary materials are provided.","### Reliance on a single reanalysis dataset (MERRA-2)
MERRA-2, with its relatively lower spatial resolution, is not commonly used in AI-driven weather and climate research, where higher-resolution datasets like ERA-5 are preferred for their superior predictive accuracy. The authors themselves acknowledge this limitation, citing the weaker performance of Prithvi WxC in hurricane track forecasting compared to the ERA-5-trained FourCastNet, attributing this discrepancy to MERRA-2's lower spatial resolution.      

Prithvi WxC’s exclusive training on the MERRA-2 reanalysis dataset raises questions about whether it truly qualifies as a foundation model. The narrower training base implies that the model may be learning a representation more specific to MERRA-2 characteristics, along with its biases and errors, rather than capturing a broader, more generalized understanding of weather and climate dynamics. By contrast, Aurora foundation model are pretrained on six diverse weather and climate datasets, including ERA-5, CMCC, IFS-HR, HRES Forecast, GFS Analysis, and GFS Forecasts, which span various sources like forecasts, analyses, reanalyses, and climate simulations [1]. This multi-source approach ensures a broader, more representative foundation that enhances versatility across diverse applications.Prithvi WxC would benefit from a similar multi-dataset training approach to strengthen its robustness and generalizability, which would then qualify it as a foundation model.

[1] Bodnar, C., Bruinsma, W. P., Lucic, A., Stanley, M., Brandstetter, J., Garvan, P., ... & Perdikaris, P. (2024). Aurora: A foundation model of the atmosphere. arXiv preprint arXiv:2405.13063.

### Ablation Study
While the authors propose a novel objective function, they only hypothetically attribute Prithvi WxC's strong short-term forecasting performance to its masking objective, without providing empirical evidence. This lack of testing weakens the claims about the model’s unique architecture and objective function. The observed performance could be influenced by several factors: the mixed objective itself, specific network structures or attention mechanisms, and choices made in the pretraining setup.

Without ablation experiments, the paper's assertions about the effectiveness of these innovations remain speculative, leaving readers uncertain about the impact of each component. An ablation study could isolate the contributions of these elements and would strengthen the paper by making its claims more concrete and providing clearer insights into Prithvi WxC’s architectural and training contributions.","### Minor issue:  Weak baselines for downscaling
The comparison primarily involves interpolation-based methods and does not consider more advanced, AI-driven downscaling models or domain specific statistical/dynamical downscaling."
xCFdAN5DY3,xCFdAN5DY3,A Foundation Model for Weather and Climate,Reject,AGMerUZk8m,ICLR.cc/2025/Conference/Submission11704/Reviewer_noFm,"The paper introduces a new foundation model for weather and climate, called Prithvi WxC, with 2.3 billion parameters, and trained on relatively unconventional yet interesting reanalysis products of MERRA-2. The authors use a relatively novel pre-training strategy in this field, where in addition to forecasting-only pre-training, they also combine masking for reconstruction in the hope of better self-supervision, among several upsides (e.g., natural extension to data assimilation with sparsely-gridded observation). The FM is then evaluated on several downstream tasks, including forecasting, downscaling, and parameterization.","Operationally, the large-scale pre-training of 2.3B FM is impressive in the emerging field of AI4Science. The use of large-scale dataset is also noteworthy. The writing is clear and the downstream tasks are clearly defined, and touch upon some of the hardest challenges facing the field. The use of beyond-atmospheric variables (coupled ocean + land) is also welcomed to build a full Earth system FM.","There are several major weaknesses in the paper, including non-existent/weak baselines and misleading claims, summarized below:

Major weaknesses
1. The term foundation model for this work is misleading, since in contrast to other similar works e.g., ClimaX, Aurora, the model is only trained on one data source which is MERRA-2. The problem: reanalysis products are by construction applicable for short-medium range applications as they are used either to evaluate NWPs or used as ICs for the next forecasting window. The climate dataset being missing, makes the claim of Prithvi being also a climate FM is an overreach at best. 

2. Related to the first point, the paper does not evaluate on any climate-related tests despite claiming it to be a climate FM too: (a) is the model stable over 50-100 years climate rollout? (b) what is the climate drift/bias compared to SOTA climate emulator? The paper applies downscaling to a climate dataset CORDEX, but this is less of a climate question and just a general downscaling problem since the former is more concerned about getting the long-term statistics, rather than a high-resolution state realization, correct (which is near impossible to nonlinear chaotic systems such as the Earth system). 

3. The downscaling benchmarking is also unacceptable as the baseline is too weak (e.g., bilinear, nearest neighbor). The author should either remove this part or add stronger baselines where the SOTA is at least a deep learning based method. Also, there is no benchmarking on the parameterization downstream task. At least use existing DL-based models from recent works. 

4. The forecasting performance appears to be unconvincing at best: with 2.3B parameter, it performs worse than e.g., <50M parameter GraphCast (~40x smaller) even at short lead time of 66 hours (<3 days). Even when the authors mention this result is ""zero-shot"" (which I find it unconvincing since there is rollout fine-tuning still) and the target is different (MERRA-2 vs ERA5), the obvious larger error growth (Figure 4) is alarming as it may not be useful for long-range climate forecasting. Also, why not benchmark against MERRA-FourCastNet in the forecasting task since this provides for a fairer comparison as both are trained with MERRA (as in the case for hurricane prediction). Finally, figure 4c (single line: forecasting cloud) is a case-in-point: the lack of sufficient apple-to-apple baselines where training/eval is done on MERRA-2. 

Overall, I find the results unconvincing given the lack of data sources, proper baselines, and inferior performance gain despite Prithvi being orders-of-magnitude larger than any SOTA model. As a side note: I believe the task of downscaling and parameterization is similar in that both attempts to resolve small-scale physics in an otherwise coarse-resolution model. I suggest the authors combine or use different downstream tasks e.g., climate projection.","In addition to the weaknesses above, can the authors clarify the following:

1. How is the masking pre-training strategies better than just using variable lead-time that includes delta_t = 0? 
2. How is the local-global attention better than existing pre-training strategies of e.g., patch-variable-level tokenization employed in e.g., ClimaX? 
3. With masking as an additional pre-training strategy, what is the cost-performance tradeoff? 
4. Has the authors measure Prithvi's parameterization stability in an online-coupled setting?"
xCFdAN5DY3,xCFdAN5DY3,A Foundation Model for Weather and Climate,Reject,IQy0U4IKqw,ICLR.cc/2025/Conference/Submission11704/Reviewer_j9By,"The paper titled ""Basic Models for Weather and Climate"" introduces Prithvi WxC, a 2.3 billion parameter basic model for various weather and climate tasks. These include downscaling, autoregressive prediction, and extreme event estimation. The model is trained on 160 variables from the MERRA-2 dataset and combines mask reconstruction with prediction tasks to learn from various atmospheric data. Its encoder decoder architecture and ability to work with different spatial topologies make it suitable for global and regional weather modeling.","Originality: The introduction of basic models for weather and climate applications is novel and important. Unlike specific task models such as FourCastNet and GraphCast (Lam et al., 2022), Prithvi WxC addresses a wide range of tasks and effectively narrows the gap between artificial intelligence models for specific weather tasks and general artificial intelligence base models.
Quality: The mixed pre training objective of this model combines masking and prediction, which is robust, especially because it uses climate bias rather than just future state prediction, which enhances the adaptability of the model. The model also showed impressive results in zero sample evaluation of reconstruction and autoregressive prediction, outperforming the baseline in a short delivery cycle.
Meaning: The ability to generalize to multiple downstream tasks, such as downscaling and gravity wave parameterization, suggests that this model has the potential to have a significant impact on weather and climate modeling.","Generalization to other datasets: Although the model performs well on MERRA-2 data, its generalization ability to other datasets such as ERA5 or CMIP has not been fully explored. Validation on different datasets will better demonstrate its robustness.
Long term prediction: The accuracy of Prithvi WxC decreases with the extension of the prediction window, especially beyond 66 hours, and its performance is poor compared to models such as Pangu. A deeper investigation into how to maintain performance within an extended time frame can improve its utility in medium - and long-term forecasting.","Architectural flexibility: Can you provide a detailed explanation of the adaptability of the architecture when applied to non rectangular grid systems, such as those used for ocean simulation or polar regions?
Generalization strategy: What steps are taken to ensure the performance of the model when training or fine-tuning on datasets outside of MERRA-2, such as ERA5 or even higher resolution datasets?
Hurricane forecast: Can you provide more details on how the hurricane trajectory prediction of this model compares to specific task models such as FourCastNet, especially in areas with sparse data coverage?"
LCk3umTAXx,LCk3umTAXx,Gamified crowd-sourcing of high-quality data for visual fine-tuning,Reject,p0Io86JFo2,ICLR.cc/2025/Conference/Submission11677/Reviewer_iRce,"This paper introduces the Gamified Adversarial Prompting (GAP) framework, aimed at enhancing the performance of multimodal AI models in visual question answering (VQA) tasks. By attracting over 50,000 participants on the Telegram platform, the author utilized the MiniCPM-Llama3 model for experiments and designed the GAP-VQA dataset to address knowledge gaps. Results indicate that, after targeted fine-tuning, the model's performance significantly improved across various benchmarks. The GAP framework emphasizes the importance of human involvement, mitigating biases associated with AI self-assessment, and promotes a more transparent and ethical approach to AI development, underscoring the critical role of human creativity in multimodal model improvement.","1. The paper presents an innovative Gamified Adversarial Prompting (GAP) framework that effectively integrates human involvement with multimodal learning strategies, significantly enhancing the performance of multimodal AI models in visual question answering and paving a new research direction.
2. The GAP framework underscores the vital role of human cognition and diverse perspectives in the model enhancement process, effectively mitigating biases and errors commonly associated with traditional self-assessment methods
3. The empirical results presented demonstrate substantial performance gains, particularly through targeted fine-tuning of the MiniCPM-Llama3 model, validating the effectiveness of the GAP-VQA dataset in addressing specific knowledge deficits
4. The adaptability of the GAP-VQA approach is noteworthy, as it not only improves the MiniCPM model but also shows robust transfer learning capabilities across different model architectures, indicating its broad applicability in the field of visual question answering.","1. Although the GAP-VQA dataset has been filtered to ensure a high proportion of adversarial examples, the diversity and representativeness of its samples still require further validation. The selected 3,683 question-image pairs may not adequately cover the diverse scenarios encountered in real-world applications. A lack of diversity could lead to suboptimal model performance on unseen tasks or images.
2. The evaluation of the model primarily relies on GPT-4 as the evaluator. While it can provide a degree of accuracy assessment, this reliance may have limitations. The evaluation criteria and preferences of GPT-4 might not be applicable to all types of visual questions. Additionally, the scoring range (0 to 1) in a single dimension may not fully capture the model's performance in complex reasoning or multimodal understanding, potentially affecting the objectivity and consistency of the evaluation.","1. Rationality of Assumptions: The parameters ε and δ in Equations (1-4) are small positive numbers and the assumption that δ < ε is always valid. Is this relationship consistently upheld across different experimental conditions? If the model's performance deviates from these assumptions in specific scenarios, how does this impact the reliability of the analytical process?

2. Effectiveness of the Reward Mechanism: How does the reward system ensure that player behavior consistently aligns with expectations? If players deliberately mark correct answers as incorrect for other motives (e.g., mischief), how are these situations handled? Does the system have mechanisms to detect and correct such inconsistencies?

3. Sustainability of the Data Collection Mechanism: How is the sustainability of this data collection mechanism ensured? As the research progresses and the model size increases, so does the demand for data. Have there been considerations for continuously incentivizing participant engagement through methods such as raffles or accumulating reward pools?"
LCk3umTAXx,LCk3umTAXx,Gamified crowd-sourcing of high-quality data for visual fine-tuning,Reject,Nwp2Ti5Q7c,ICLR.cc/2025/Conference/Submission11677/Reviewer_NiAq,"The main contribution of the paper is the introduction of an approach,  Gamified Adversarial Prompting (GAP). The idea is to devise an interactive app for users: the user will play a game tying to find a question that the AI answers incorrectly. With the GAP framework, high-quality data to enhance visual instruction tuning in large multimodal models can be collected. The paper contributes by introducing a dataset based on MSCOCO for building GAP, by proposing a  strategy for collecting VQA pairs from players and by introducing a gamified platform that was used to engage over 50K players. The paper shows that with the use of the data collected with GAP the performance of MiniCPM-Llama3-V2.5-8B can be improved. Other experiments include cross-dataset results showing that the use of GAP is beneficial to improve on other benchmarks and evaluation of different models.","- The paper is based on a very interesting idea, which is using gamification for collecting data for fine-tuning large multimodal-models.
- The experiments demonstrate that the proposed approach improves the performance of a model, i.e. MiniCPM-Llama3-V- 2.5-8B.
- The proposed system was used by several participants and a detailed analysis of users' participation is shown in the Appendix","- The writing of the paper needs significant improvements. The description of the method is confusing with some details only discussed in the supplementary material (see  A.3 PLAYER INTERACTION MODEL). A lot of space is dedicated to related works while some additional details in the main text should have been also dedicated to describing the GAP and the final system. 
- The descriptions in L 337 about  intrinsic and extrinsic factors is very high level and details on how this is integrated in the model are lacking
- The supplementary material could hep to understand the proposed approach but it is poorly referred in the main text and not well organized. 
- The proposed approach is beneficial in the case of a single model MiniCPM-Llama3-V- 2.5-8B, while the other models the improvements are mild (Table 4). This leads to question the effectiveness of the proposed framework.
- The results in Table 5 are not convincing or at least require a longer discussion, outlining possible reasons for mild improvements or not even improvements.","- Why MSCoco was chosen as dataset? The cardinality of the tainted dataset seems small. How the choice of this dataset influences the performances in Table 5?
- Why the analysis in Table 5 focuses on the chosen methods?"
LCk3umTAXx,LCk3umTAXx,Gamified crowd-sourcing of high-quality data for visual fine-tuning,Reject,M5Kvxghc2E,ICLR.cc/2025/Conference/Submission11677/Reviewer_kMr4,"This paper introduces Gamified Adversarial Prompting (GAP), a framework aimed at crowdsourcing high-quality data for the visual instruction tuning of large multimodal models. By gamifying the data collection process, GAP motivates participants to create challenging questions and answers that address the knowledge gaps of these models. The paper also includes an approach to automatically evaluate and reward player submissions with high accuracy, enabling to scale to 50000 players in a few weeks.","1. By gamifying the process, GAP keeps players motivated and engaged, potentially leading to high-quality data collection.
2. By automatically evaluating and rewarding player submissions, this approach can effectively scale up the data.
3. The framework has demonstrated significant improvements in model accuracy on VQA benchmarks, indicating its effectiveness.","1. The number of baseline models used in the experiment is not enough, and the numerical results presented in Table 5 do not show significant changes.
2. Quality Control: While the framework aims for high-quality data, there may still be variability in the accuracy of player-generated content.
3. Unable to determine the specific classification of the questions asked by the player, making it difficult to balance the number of different types of questions.","1. Will a large amount of this type of data be created in the future to be integrated into LLM training?
2. Is it possible to have a stronger LLM replace the player's role and a weaker LLM handle the data creation process?"
LCk3umTAXx,LCk3umTAXx,Gamified crowd-sourcing of high-quality data for visual fine-tuning,Reject,JomtVOuPMV,ICLR.cc/2025/Conference/Submission11677/Reviewer_cHQ5,"The paper introduces a framework for crowd-sourcing high-quality data for visual fine-tuning. The authors propose a chat-like interface that lets users interact with large multimodal models. While models must answer users's questions, the users must discover weaknesses in the model performance by posing diversified queries to the model. The authors transformed the evaluation procedure in a game to both evaluate and discover weaknesses of such models while also collecting supervised data to overcome the discovered limitations. With the collected data, the authors demonstrated the capability to tune and improve model performance.","- The authors introduce a platform to collect high-quality data and propose to use a game-like experience to engage the users and incentivize the discovery of weaknesses and collection of high-quality data for model fine-tuning.
- The idea of gamifying the experience is not much explored in the literature and it could lead to faster discovery of weaknesses and improvements in model performance.
- The authors split the data pool into an easy and hard split. While their objective is to collect hard questions on the hard split, they must also evaluate players' capabilities in uncovering inaccuracies in the model answers. For this purpose, they slightly ""poison"" the model answers in the easy split to assess how well a player can distinguish correct answers from slightly inaccurate ones.","- The pool of data considered for the questions is limited to COCO. Despite the scalability that the tool can and did achieve, I expect the major bottleneck to be the limited diversity and quantity of the data pool.
- While the authors report the gain in performance of a group of models when tuned on the collected high-quality data, we lack evidence of the effective level of quality of the collected data and of the effect of such data on the tuning process. Specifically, I would expect to see some metrics/statistics to quantify data quality and more comparisons to distinguish the effects of tuning with the collected data vs tuning with other already-available datasets for instruction tuning.
- There is a lack of comparison of the models tuned on the data w.r.t. other models available in the literature.
- (minor) Tables are very ""sparse"", i.e., space is not well-optimized, resulting in a paper that feels slightly shorter compared to other works. I am wondering if the paper would gain from reorganizing the tables in a better way and introducing more information from the Appendix or from data statistics/additional comparisons.","- Why did the authors focus on COCO and do not consider more ""unsupervised"" datasets? Why not use large-scale datasets or a mixture of different datasets?
- Can you provide some evidence of the data quality/diversity resulting from the crowd-sourcing? Can you report some statistics regarding, e.g., the categories of the collected questions (as listed in the Appendix), their diversity, etc.? Since the collection was done on COCO, the authors could exploit supervised annotations to categorize questions and images in terms of, e.g., the subject (i.e., annotated classes), properties, etc. What is the average number of questions per image? Are there images/classes with more questions than others?
- How does tuning on the collected data compare with tuning on other already-available instruction tuning datasets?"
hjROBHstZ3,hjROBHstZ3,Causal Representation Learning from Multimodal Biomedical Observations,Accept (Poster),dWgzw97MEX,ICLR.cc/2025/Conference/Submission11612/Reviewer_5aPk,"This paper studies the causal graph discovery problem from multiview data. Given observed multimodal data, the goal is to estimate the causal relationship of latent features between modalities. The authors study the data generation process where the observed multimodal data are independent given the latent variables. Under this assumption, the authors employ the encoder-decoder framework to decompose the latent factors and nuisance factors, and a sparsity regularization function to impose the sparse relationship between modalities in the latent spaces. They compare the proposed method with several baselines on simulated tasks and the results show improved mean correlation coefficient. Furthermore, the proposed method is applied to human phenotype dataset.",This paper tackles an important and yet not well-addressed problem. It is well-motivated by biomedical applications. The proposed framework is more general compared to the prior work (as shown in Table 1).,"In general, I find several parts that need further clarification. (see the question parts)

My main concern is that the simulated tasks focus on low-dimensional data with simple sparse causal structures. It is not clear whether the method is scalable and can be generalized to more complex causal structures.","a. the paper assumes that the latent variables within the same modality form a DAG, could the author clarify why this assumption is necessary? Secondly, if this is needed, the loss function (6) does not enforce the latent variables within the same modality to be a DAG. How can we ensure such structures will be satisfied in the learning phase?

b. does the causal relationship between different modalities need to be DAG? If there are cycles, are we still being able to identify the latent variables?

c. I find the definition of condition 4.3 confusing. What are the connections between matrices A, G, T? and A seems to only denote the relationship between modality 1 and 2?

d. In Assumption A.1, what is smooth inverse?

e. Could the authors provide a guideline for practitioners on how to systematically choose the number of latent nodes?

f. In Theorem 4.4, does it require the regularization term to be scaled by some coefficient?"
hjROBHstZ3,hjROBHstZ3,Causal Representation Learning from Multimodal Biomedical Observations,Accept (Poster),WI5YrkrcKt,ICLR.cc/2025/Conference/Submission11612/Reviewer_JxDh,This paper provides a method that identifies the latent variables up to invertible component-wise transformations from multi-view data under the weak assumption that these latent variables exert partial influence on every modality and strong influence on one modality. The proposed approach relies on weaker assumptions compared to the existing works and yet provides strong results. The primary motivation for this approach is on multimodal health care data but the experiments are conducted on both synthetic and image datasets in addition to health care data.,"1.   The contribution is very clear and useful for future works on identifiability. More strengths included in the summary.
2.   The writing is lucid. The examples are cleverly used to contrast the proposed method against the existing works.","I did not find any significant weaknesses. However, I do have a few questions for the sake of clarity. Questions in the following section.","1. It is assumed that the information of $z^{(m)}$ is preserved in its corresponding observation $x^{(m)}$ and it exerts sufficient influence on other modalities' observations $x^{(-m)}$. Now, consider the observation from another modality $x^{(k)}$ that has a similar behavior. How does $z^{(k)}$ not interfere in the identifiability of the subspace of $z^{(m)}$?
2. Is the considered setting a non-linear version of [10]? Can the authors add text comparing/contrasting the proposed approach with [10]?
3. Does ""fully share"" mean ""retrieve without errors"" in line 212? That's what I understood from the example that follows this statement in line 215.
4. Can the authors explain how the method is different from [9] apart from the latent variables being independent in [9]?Specifically, what does ""part of the variables are directly observed and causal directions are given by default"" mean in lines 335-338?
5. Can the authors add an intuitive description for Condition 4.3 as its implications are not clear from the main text? It is enough that this intuitive explanation is included in the appendix.


Typos and minor writing mistakes:

1. In line 78, what is $n$? Is it $z_i^{(m)}\\rightarrow z_j^{(n)}$?
2. Is the co-domain of $G(z, \\epsilon)$ correct in line 303?
3. Some issues with the references. E.g., ""Non-parametric Identifiability of causal representations from unknown interventions"" appeared in NeurIPS 2023. Sturma et al., (2023) cited twice in lines 126-127.


[1] Sturma et al., 2023, ""Unpaired Multi-domain Causal Representation Learning"", NeurIPS 2023

[2] Zheng et al., 2022, ""On the Identifiability of Nonlinear ICA: Sparsity and Beyond"", NeurIPS 2022"
hjROBHstZ3,hjROBHstZ3,Causal Representation Learning from Multimodal Biomedical Observations,Accept (Poster),tPXIEqsQt8,ICLR.cc/2025/Conference/Submission11612/Reviewer_uhwZ,"Authors develop new methods to identify patterns in complex multi-modal biological datasets. It improves upon previous work in two key ways: First, it uses a flexible mathematical framework that doesn't make rigid assumptions about how the underlying biological factors (latent factors) are distributed. Second, these factors can influence each other across different modalities. Simulation study and real-world data support authors's claim.","1. The paper is well-written and motivated. 
2. Authors provide identifiability guarantees for each latent component. This is helpful to characterize the interactions among all latent
components across modalities for the biological applications. 
3. The assumptions of theoretical results look reasonable to me. 
4. Real-world dataset analysis is provided, demonstrating its usefulness.","1. A1 indicated the neural network needs to be invertible. How do authors achieve this? Authors use normalizing flow, an invertible generative model, as a part of their network, what about others? Also what's the computation efficiency? 
2. In real world, user-defined number of latent variables can be biased. Have authors analyzed it?",See above
hjROBHstZ3,hjROBHstZ3,Causal Representation Learning from Multimodal Biomedical Observations,Accept (Poster),ciCxjx0Lzw,ICLR.cc/2025/Conference/Submission11612/Reviewer_mBq3,"This paper develops a theory for identifying the underlying structure of multi-modal data involving latent causal variables, based on the assumption of smooth invertible generating functions. It further investigates the sparsity of this causal structure and proposes an estimation method. Experiments conducted on both synthetic and real human phenotype data demonstrate the effectiveness of the proposed approach.","- The paper is generally well-written.
- It includes an adequate literature review.","- On real impact. As mentioned by the authors, knowing the number of underlying latent variables is unrealistic. 

- Missing implementation details, e.g., more details on data generation, network structure, optimization scheme, hyper-parameters, etc. see questions as well.","1. In my view, the example provided in Section 3 does not align well with the proposed model. What is your rationale for considering heart sizes and bone structures as latent variables?
2. In Appendix D.1, could you provide further details on the MLP architecture used for the numerical data generation process? Have you considered implementing an invertible neural network structure in your simulations? If you increase the number of data samples, do you expect the MCC to approach 1? I would appreciate seeing results on this, particularly illustrated with four variables (two latent and two observed).
3. The arrow from $z$ to the adjacency matrix in Figure 4 is unclear. Could you clarify this relationship?
4. How did you compute the KL divergence? Do you assume that $p(\\gamma)$ follows a Gaussian distribution, with the encoder output providing the mean and variance?
5. When calculating the SHD, given that the indices of latent variables can be permuted, could this pose an issue?
6. In the context of the MNIST dataset, what causal relationship exists between the colored MNIST and fashion MNIST data?
7. How does the learned adjacency matrix A relate to the one derived from the PC algorithm?

Comment:
For the real-world application, I encourage the authors to explicitly present evidence from the literature demonstrating the success of their algorithm."
hjROBHstZ3,hjROBHstZ3,Causal Representation Learning from Multimodal Biomedical Observations,Accept (Poster),IkIejbVwvT,ICLR.cc/2025/Conference/Submission11612/Reviewer_91p1,"The paper addresses the identification of hidden causes underlying observed multimodal data. The authors propose a model that avoids previous limitations, specifically not requiring shared latent information across multiple modalities or assuming an exponential family distribution for hidden causes. The paper provides a theoretical identifiability analysis and presents experiments, including one on biological data.","The paper presents meaningful relaxations of assumptions from prior work.

It is relatively well-written, considering the complexity of the theory and notation involved.","There are issues with the main theoretical claims and their proofs.

The practicality of the current assumptions and their real-world applicability remains unclear.

The experiment using the human phenotype data does not justify the title or the highlights in the abstract.","### Theory

1. *Identification vs. Estimation*: In Theorems 4.2 and 4.4, identification and estimation are confused by statements like “We estimate the generating process…”. Identification is a model property and should be established before estimation is addressed. For a model $\\mathcal{P}:=\\\\{p_{\\phi}:\\phi \\in \\Phi\\\\}$, we simply show that $\\forall \\phi_1, \\phi_2 \\in \\Phi (p_{\\phi_1}=p_{\\phi_2} \\implies \\phi_1=\\phi_2)$ to demonstrate identification.

2. *""Identification under Regularization""*: Particularly, Theorem 4.4 is problematic in stating that identification is achieved through regularization, which pertains to estimation. If regularization is essential, the model should be reformulated—like for ridge regression, the regularization can be treated as a sparse prior in a probabilistic model.

3. *Proof of Theorem 4.2*: The step to equation (14) seems to rely on $x=\\hat{x}$. However, equal distributions do not imply equal values of random variables. If (14) is correct, the derivation should clarify how equal distribution leads to (14).

4. *Definition of $\\tilde{g}_x$*: Strictly speaking, there is one $\\tilde{g}_x$ for each $m$, and it’s unclear if these $m$ functions can be shown as the same function. If not, we are in fact making assumptions about $m$ functions.

5. *Proof of Theorem 4.4*: 1) Condition 4.3 mentions *undefined matrix $A$*. In the proof, equation (28) involves more than a single matrix,  as denoted by $A$ in Condition 4.3, and the definition of $d^*(U)$ does not seem to align with $d^*$ in Condition 4.3 (this questionable reasoning later is extended to $D$). This proof section is unclear both in notation and logic flow; we can not even see $A$ as a placeholder for the various matrices used in equation (28)! I suggest sorting out the involved matrixes (I believe, if the proof is not simply wrong, there should be several similar conditions for different matrixes), and a clear connection between the proof and assumptions should be seen. 2) *the regularization again*: the reasoning following equation (29) requires satisfaction of (29), but regularization (5) can only encourage, not guarantee this condition. This suggests the idea of “identification under regularization” itself does not work, not only an incorrect formulation as mentioned in the 2nd point.

### Implications
One core aspect of the causal model is that two modalities cannot share any direct causes, which is questionable. While $z^{(m)}$ can, and indeed is required to, affect $x^{(-m)}$ indirectly through $z^{(-m)}$, this seems to require hidden causes $z$ to be lower-level features. For example, “age” is a high-level feature affecting many modalities, like “sleep” and “eyes.” Thus, if the ""sleep"" modality includes ""age"" in $z^{sleep}$, then, to satisfy the ""indirect"" causes requirement, lower-level features $z^{eyes}$ would be needed to mediate the causal effects from “age” to “eyes"", i.e., $d(z^{eyes})$ should be large enough to block all the causal effects from “age” to “eyes.” And, when we have many modalities and many hidden causes, this eventually requires most hidden causes to be low-level.

While the theoretical relaxations and real-world examples are appreciated, it’s unclear why, in the “sleep patterns” and “brain imaging” examples, your assumptions could be satisfied, though I can see previous ones are limited.


### Biological Application
The demonstration of the biological application lacks depth, and the claimed “strong interpretability” is overstated. For example, in the human phenotype dataset, how should we interpret the learned hidden variables? Does the method enhance our understanding of the data or its underlying mechanisms? How does the number of latent variables affect the results (possibly with very different causal graphs)?

Additional specific questions are: Why are causal relationships involving FRight and FLeft not symmetric? For example, why does FRight affect Cataract but not FLeft?
Why are there no Sleep variables that influence “Sleep efficiency”?
Shouldn’t ocular quality impact hand grip strength rather than the reverse?

More experiments should be conducted, both on the model side (e.g., testing various numbers of latent variables and other hyperparameters) and the data side (e.g., trying many different modalities). 


Overall, the authors could clean up theoretical developments, clarify the practical implications, and leave the biological application to future work."
jjfve2gIXe,jjfve2gIXe,U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,Accept (Poster),JyayontSI6,ICLR.cc/2025/Conference/Submission11589/Reviewer_bboC,"The paper connects two well-known phenomena in deep learning, namely emergent capabilities and inverse scaling. By breaking down the scaling curve to different difficulty levels, the authors show that LLM skill emergence can be the result of an inverted-U-shaped scaling on easy questions, where the easy-questions curve cancels out the scaling on hard questions. The authors propose a method to predict emergent scaling by fitting sub-groups of the dataset separately.","- The main result of this paper connects two important topics in the field, inverse scaling and emergence, making it a very relevant research topic, especially since neither phenomenon is well understood.
The results on MMLU and Persian-QA are clear enough, in my opinion, to establish the existence of a connection between U-shaped scaling and emergence. Although Big-bench arithmetic and the appendix results are less clear, they do not diminish from the other results. Even if the easy-hard data split failed to visualize inverse scaling in the latter experiments, a different split might still show similar curves to MMLU.

- The proposed fitting method, slice-and-sandwich, showcases how the documented phenomenon can be used to predict emergence in practice.

- The paper is clearly written and easy to follow.","- While the fitting method is an overall positive addition to the paper, it is less convincing than the scaling results (which are, for me, the main result of the paper). The fits shown in the main section use knowledge of the point where emergence starts, making them of little value, since predicting that point is the main reason why we care about fitting emergence.
Appendix F2 fixes that issue by showing fits on random sections of the scaling curve before emergence, but these fits are not so convincing as a reliable method to predict the point of emergence.

- Appendix E shows the scaling curves when plotting accuracy, and attributes the difference from the main results to accuracy being a 'crude measure', which is not a satisfactory explanation to me.","- Minor correction: In the definition of $M$ (line 128), $N$ and $D$ are mixed up.

- Naming $M$ 'effective model size' is confusing for the reader, since model size is $N$ while $M$ is actually the log of compute $C$. Maybe a better name would be 'effective compute' or just 'log compute?"
jjfve2gIXe,jjfve2gIXe,U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,Accept (Poster),7kOpZKJsOt,ICLR.cc/2025/Conference/Submission11589/Reviewer_nJbX,"The paper investigates explanations for LLM emergent abilities (sharp transitions from chance-level to high performance as a function of model size) and proposes a method to predict emergent performance. They show that on certain task, when questions are group by difficulty, two distinct scaling trends appear: U-shaped scaling for hard questions and inverted U-shaped followed by improvements for easy questions. This motivated a method for capabilities forecasting: fitting polynomial regression separately for easy and hard questions and then aggregating the results.","1. The paper addresses a very timely topic: the science of LLM evals and predictive evals. It is relevant for scientific understanding of frontier LLMs as well as for AI policy (informing capability thresholds and buffers in responsive scaling policies).
2. The paper described an interesting insight that easy and hard questions have different scaling trends and that this could explain sharp transitions of aggregate performance","1. I found the paper hard to follow. I think the clarity of writing and figures could be substantially improved. Some concrete suggestions:
(a) the name ""binary Brier score"" is very confusing. What's binary about it? It's continuous for each question.
(b) I think it's very confusing to say Brier score is ""noisy due to insufficient confidence calibrations"" (line 161). What does it mean to be sufficient? Why is this noise? Why ""calibrations"" in plural?
(c) what are ""available choices' (line 159)? Is it the same as classes (line 139)?
(d) names for difficulty levels (e.g. 0_1502_brier) are confusing. They're okay in Python code, but I'd expect something more readable and abstracting away from distracting details (e.g. I don't really care it's 1502 questions).
(e) The colors on plot could match the semantics of difficulties, e.g. you could use a continuous color map (e.g. red-blue transition to represent easy-hard transition).
(f) plots for different datasets could be next to each other
(g) ""Slide-and-sandwich approach allows the data to speak for itself"" is misleading. The whole point is that you bake in more assumptions in the method.
2. The name ""effective compute"" is somewhat misleading if it's just log compute. Ideally, effective compute could be something like g-factor [1] or the first principal component from observational calling laws [2, 3]. It would be great to use that as a metric. Or at the very least, it should be just called ""log compute"".
3. Authors only evaluate their methods on 3 datasets (plus 3 more in the appendix). I don't think that's enough to justify the claim that they found a general method for capabilities forecasting and a general explanations of emergent features. BIG-Bench includes more than 200 tasks.
4. Relatedly, all of them do show emergent capabilities. I wonder how Slice-and-sandwich methods behaves on tasks that do not show emergent capabilities: does it falsely predict them? I think this is an important point: so far the authors only presented a method for emergent capabilities forecasting conditional on the presence of emergent capabilities. But in practice we don't know that in advance.
5. Overall, I'm somewhat underimpressed by predictive performance and I worry that the authors overfit to a particular set of 6 tasks. If find some patterns in a dataset and bake in this pattern as an inductive biases of your method (polynomial regression), surely you'll improve predictive performance. But will those inductive biases generalize to entirely new tasks you never looked at? The authors provide no evidence for that (unless I'm missing something, happy to be proved wrong).
6. I'm not super convinced this is a general explanation of emergent capabilities. Maybe others have different scaling trends? The whole point that a superposition of U-shaped and inverted-U shaped scaling explains emergent capabilities is also somewhat qualitative. It's not starkly clear from the plots, e.g. MMLU does not seem consistent with this explanation.
7. Consider discussing [4] as related work

[1] Unveiling the general intelligence factor in language models: A psychometric approach

[2] Observational Scaling Laws and the Predictability of Language Model Performance

[3] Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?

[4] The Quantization Model of Neural Scaling","1. How were the 6 tasks you experiment on selected? What were the criteria and at what point were they selected?
2. What's the functional form of the polynomials you fit (line 416)?"
jjfve2gIXe,jjfve2gIXe,U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,Accept (Poster),lTF9x653F3,ICLR.cc/2025/Conference/Submission11589/Reviewer_9qWW,"This paper investigates the emergent behavior of large language models as their effective size increases. The authors observe that for certain downstream tasks, the continuous metric exhibits a U-shape for easier questions and an inverted U-shape for more difficult questions below the emergence threshold, with steady improvements beyond this threshold. Additionally, they propose a method for forecasting model performance beyond the emergence threshold.","1. The paper connects the scaling trend on easy questions with the double descent phenomenon in deep learning is quite interesting.
2. A group-wise scaling law based on difficulty level can offer more detailed insights compared to a single overall scaling curve.","1. This paper primarily focuses on the continuous Brier score as a metric, which may not fully capture the validity of the observed scaling trends. It is unclear whether these trends are specific to this single metric. To establish this phenomenon as more general, validation across other metrics is needed.
2. The writing and organization of this paper are difficult to follow. For instance, the scaling plots for various LLMs are presented without clearly specifying the types or model families used, as these details are only provided in the appendix. Additionally, the figure depicting the scaling curves lacks clarity; for example, the label '0_1502_brier' in Figure 3 is ambiguous. It appears to represent the index of data points, but using the difficulty level as a label would be more informative.
3. The proposed task in Section 4 is using data collected before the emergence threshold to forecast the occurrence of emergent abilities and the scaling trend beyond this threshold. However, the paper lacks details on how the threshold is selected and how this choice is validated. Furthermore, more explanation is needed to highlight the significance of this task. Why is predicting the scaling curve beyond the emergence threshold based on early performance trends important?
4. In the experiments, a few multiple-choice datasets are utilized to demonstrate the presence of emergent behavior, but they also note that such behavior is absent in other BIG-Bench datasets, as detailed in the appendix. This raises questions about whether the observed emergence is specific to certain datasets rather than a general phenomenon. A more comprehensive analysis is needed to uncover broader patterns and clarify the characteristics of these emergent behaviors—for instance, identifying which types of tasks exhibit this emergence and which do not. This would help determine the conditions under which such emergence is applicable.","1. The authors appear to categorize difficulty levels as easy, medium, and hard. How might the granularity of these difficulty levels affect the results?
2. In Section 4, the proposed method Slice-and-Sandwich first fits a function to the binary Brier score and then projects the forecasted scaling trend back to accuracy. I am trying to understand the benefit of this additional step. My interpretation is that fitting the binary Brier score curve is more precise, particularly for extrapolation, compared to directly fitting the accuracy curve. Could the authors elaborate on why this approach is preferred over fitting the accuracy curve directly?"
jjfve2gIXe,jjfve2gIXe,U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,Accept (Poster),giUaOSkvcc,ICLR.cc/2025/Conference/Submission11589/Reviewer_K8WA,"By dividing questions according to difficulty levels, the authors observe U-shaped scaling for hard questions and inverted-U scaling for easy questions, followed by steady improvement. Then the authors propose a simple pipeline, called Slice-and-Sandwich, to predict the model performance.","1. The inverted-U vs. U-shape scaling seems interesting.

2. The pipeline seems to be novel. Although the basic principle of the method is fairly easy to understand, it substantially outperforms the baseline methods.

3. The experiments are diverse. The authors uses 56 LLMs to evaluate the difficulty levels, and the effectiveness of Slice-and-Sandwich is demonstrated on 6 benchmarks.","1. The fitted scaling trend is either underestimated or overestimated the actual scaling trend. Although the authors have given explanations, it should be clearly pointed out that why this  underestimation /  overestimation is acceptable. For example, the prediction error of the score of hard problems on Arithmetic benchmark (figure 7) seems actually comparable to the prediction error of the score of all problems on Arithmetic benchmark  (figure 9). 

2. The authors provide possible explanations for the inverted-U vs. U-shape scaling. They provide examples of easy/hard problems and give very brief descriptions of the performance of different models. The concepts would be easier to understand if the authors can provide some specific outputs of weak/strong LLMs on easy/hard problems in the appendix. 

3. There are some typos in the paper. For example, in all figures, the authors use exactly the same color for the easiest and most difficult problems, making it a bit confusing. In line 484, $F_s^c$ should be $F_e^c$. Also, the layout of the appendix seems weird. For instance, P21 only contains a single huge image.

4. Perhaps the authors should use one sentence in the introduction section (line 53) to explain what is `binary brier score` to make it easier for readers to understand.","1. I am interested in understanding why averaging the scaling trends of both the easy group and the hard group leads to an accurate prediction of the overall aggregate scaling trend. It's possible that with one prediction being overestimated and the other underestimated, the outcome appears to be coincidentally accurate. The author should provide further (possibly qualitative) explanation to demonstrate that the good result is not a matter of chance. If this question is resolved, I believe the soundness and contribution of the paper can be substantially improved.

2. The authors provide a separate figure for each benchmark. Would it be possible to also provide a figure that includes the results from all benchmarks?"
DrNN5qx66Z,DrNN5qx66Z,TVBench: Redesigning Video-Language Evaluation,Reject,B6qVAoJIL5,ICLR.cc/2025/Conference/Submission11521/Reviewer_TPmi,"This paper introduces TVBench, a new benchmark for testing video understanding capability of multimodal models. Flaws in widely used existing benchmark (MVBench) are demonstrated, namely, spatial biases, textual biases and reliance on world knowledge. In addition, it's also shown that open-ended benchmarks can contain similar biases. TVBench is constructed from pre-defined templates in order to mitigate these biases and test temporal reasoning capabilities. Supporting experimental results demonstrate that state-of-the-art models struggle on this benchmark. Similarly, text-only or image-text foundation models struggle to beat random chance signifying the difficulty of this benchmark compared with existing benchmark.","The paper is well-written and easy to understand. It tackles an important area of video understanding, i.e. the lack of strong benchmarks that test temporal reasoning in videos. The presentation clearly analyzes drawbacks of existing benchmarks and proposes a new benchmark.
- The QA pairs don't use LLMs in the loop, and thus can avoid many hallucination related issues.
- The performance of SOTA models is very low (Table 4). This indicates the benchmark is indeed difficult.
- Clear contrast with MVBench is demonstrated, especially using text-only and image-only models. This justifies most of the claims in the paper. 
- A significant, and often overlooked issue in open-ended evaluations is pointed out in Section 4. Using closed-source proprietary models whose back-ends may change arbitrarily to score open-ended responses and track our progress on video understanding can be misleading.","The main weakness of this work is around experimentation. 
- Human baseline performance is not presented. This is important to judge the quality of the benchmark and the presented results.
- Different models are used in Table 2 to make the claim that MVBench has textual bias. Ideally, the same model (ideally the best model) needs to be presented with text-only and video as inputs to justify the claim.
- Similarly, in Table 4, different models are used to compare different biases (text, image, video) of the model. 

Further Limitations:
- Using standard template QA pairs may limit the range of video understanding being assessed.
- In Figure 2 and the associated text in the paper, it's presented as if detecting the absence of something is an easy task. However, by definition, one must watch the entire video to make sure what we're detecting is indeed absent.","- Can we use the same model and ablate text-only, image, video, shuffle, reverse, etc. in Table 4? Ideally Gemini 1.5 pro as it performs the best on this benchmark?
- MVBench is presented as not a great benchmark. However, its performance is also not saturated (best model achieves 67.7 in Table 4). Do the remaining QA pairs satisfy the criteria set in the paper? What is the size of the data? Can we remove the bad examples from MVBench and get a bigger and better dataset than TVBench?

EDIT: Updated score based on the rebuttal."
DrNN5qx66Z,DrNN5qx66Z,TVBench: Redesigning Video-Language Evaluation,Reject,2Od5QdBOWD,ICLR.cc/2025/Conference/Submission11521/Reviewer_sqwF,"The paper introduces TVBench, a new video-language benchmark that addresses critical flaws in existing benchmarks like MVBench. The authors identified three problems with current benchmarks such as:
- Single frames are enough
- Question text reveals answers.
- Common knowledge beats video.

The authors demonstrated those problems by showing that both text-only language models and single-frame vision models perform well on existing benchmarks. In contrast, when it comes to TVBench, most state-of-the-art video-language models perform close to random chance.

The benchmark consists of 10 temporal tasks across 2,654 question-answer pairs, ensuring models must understand the sequence and timing of video events to succeed. The authors validated their benchmark by showing that shuffling or reversing video frames significantly impacts performance, unlike previous benchmarks.","- MVBench Analysis: Thorough and systematic identification of MVBench limitations with clear evidence.
- Validation Methods: Creative use of video shuffling/reversal to verify temporal understanding requirements.
- Benchmark difficulty: Evaluation showing most current models fail at true temporal reasoning.","- Problem analysis: even though the authors did identify the problems with MVBench, the analysis of other benchmarks is quite limited. The paper states “We conduct a comprehensive analysis of widely used video question-answering benchmarks” while focusing only on MVBench. Several datasets in the relative section can be analyzed similarly and it is still uncertain if all of those datasets also have those problems
- Small amount of dataset examples: there are 10 different tasks within the dataset, yet the paper shows only one example from the whole dataset.
- Task design: Authors state: “Questions should not be answerable using spatial details from a single random frame or multiple frames e.g. after shuffling them.” However, even the only given example from TVBench about scenes in the movie can be solved using two frames. Additionally, tasks in TVBench like Scene transition, Action Antonym, and Moving Direction can be solved with only two frames instead of one. Image LLM evaluation with more frames would be important.
- Benchmark creation details: There are very few details on how the dataset was collected and annotated. In general, given details about the dataset creation are very vague. For example: ""Instead of including random, easy negative candidates, we define hard candidates that cannot be discarded without temporal information"". How do you generate the hard negative examples?","- How the dataset was annotated, how exactly did you come up with the wrong answers? 
- Show more examples from the dataset. Several examples from each of the tasks. The examples can be illustrated similarly to how you did it in Figure 5
- Are those MVBench problems shown in other benchmarks?"
DrNN5qx66Z,DrNN5qx66Z,TVBench: Redesigning Video-Language Evaluation,Reject,OqDeFoJTob,ICLR.cc/2025/Conference/Submission11521/Reviewer_A5py,"The paper investigates three issues of MVBench: 1) independence of video or video motion, 2) bias in the generated question-answer pairs, and 3) heavy reliance on world knowledge in questions. A significant part of the paper was written to prove and showcase these problems in the MVBench. A new benchmark called TVBench is proposed to mitigate these issues by redesigning the questions and available choices. The new benchmark attempts to prove that with no visual input or just image input, the models will perform like random guesses. Some strong video models also perform so even with full video inputs. The experiment also presents the results of inputting video frames in reverse order or shuffled order to prove that the benchmark questions requires understanding on the true video motion to be answered correctly.","- The paper addresses some critical issues with existing video understanding benchmarks, which is that the correct answers to many questions do not rely on information from the video or video motion. Thus, proposing a new benchmark to resolve these issues are well-motivated.
- The paper explains in detailed examples and some ablation studies to prove that these problems exist widely in MVBench.
- Based solely on the reverse & shuffle order experiment results in Table 4, it seems that TVBench indeed improves some questions' reliance on video inputs and the motion contained in those videos.","While I appreciate the authors' great efforts to prove their statements of the issues, I am confused by many details after reading through the paper and am not fully convinced by the quality of the new benchmark.

- Some of the results in the figures and tables, or the way they are presented, can be confusing. In Fig.1 left, the trend seems linear after the models achieve a certain level of performance (>50) on MVBench. In Fig. 1, right, MVBench shows a performance drop in VIdeoChat2 when the video is reversed. How do these results support the claim that MVBench does not measure temporal understanding? What is Table 1 trying to prove? I cannot compare the results of GPT-4o + image inputs with Gemini 1.5 Pro + video input to the conclusion that a single image is sufficient. You should at least fix other variables and leave the input as the one changing to prove that. Besides, even though the results are close, did you prove that the questions answered correctly are the same ones? It's a similar issue in Table 2 that I cannot understand how text-only rows could be compared to video input rows since they are using different models. 

- Since it's a benchmark, it should attempt to document the performance of as many models as possible. A lot of video models are missing, such as Video-LLaVA, mPLUG-Owl, PandaGPT, ImageBind, Video-LLaMa and etc. In addition, the GPT-4 series can accept multiple images, which is essentially the same as video models with video inputs -- they all need to sample a certain number of frames as multiple image inputs. You can also concatenate multiple frames into one image and feed into the GPT-4 series. It doesn't make sense to me to only benchmark GPT-4o with a single frame input. 

- Writing is a big issue in this paper. So many details make it hard to understand the paper without being confused. 
  - In Fig. 1, what is the unit of the axes? 
  - Table 1 is presented but never referred to in the text. If I understand correctly, some ""Tab. 2"" should refer to Table 1 instead. Please also choose between ""Tab 2"" and ""Tab. 2"" so that searching is convenient. 
  - I think the paper shows an excessive amount of bad examples from MVBench, which makes some of these figures unnecessary. While it's good to identify and prove the existence of these problems, more efforts should be spent convincing the readers that the ""proposed"" benchmark is high-quality and indeed resolves these issues. 
  - I understand that Sec. 4 is trying to show that open-ended qa and evaluation are not reliable, but how does that matter with the main point of this paper? Multiple-choice-based QA and open-ended QA are different settings used in different benchmarks or evaluations. It doesn't convince me that TVBench is high quality by showing the weaknesses of open-ended QA -- they are different settings. 
  - In line 430, *following the model provided in Tab. 5 for each task*, what is *model* in Table 5? There is no *model* column in Table 5 and the appendix is too short to provide enough context. How many templates are you using? Are the templates in Table 5 showing all you are using? How did you collect these templates? If you have hired annotators, how did you ensure the quality of these templates? These are all important details to be included in the paper to convince readers about the quality of TVBench. 
  - This is a minor point, but I don't favor using statistics of **Huggingface downloads** as some sort of evidence in the introduction (lines 37-38). Regardless of whether you are trying to use the number to support MVBench or question its reliability, it's better to appreciate **scientific merits** instead of **popularity metrics** in academic writing.",Please see the weaknesses.
DrNN5qx66Z,DrNN5qx66Z,TVBench: Redesigning Video-Language Evaluation,Reject,gSKD44FUXh,ICLR.cc/2025/Conference/Submission11521/Reviewer_QEG7,"This paper analyzed issues in the current most popular video-language benchmark (MVBench), and propose a new benchmark that alleviates the issues. Specifically, the authors provide solid evidence showing MVBench is less temporal, less visual, and simple solution of making an open-ended evaluation protocol can't address the problem. The authors then manually design question types and create questions by combining and filtering questions from existing video-language benchmarks. The authors show the resulting dataset is does not suffer from the issues.","- The paper works on an important problem of video-language model evaluation, and identify issues in a widely used benchmark with solid evidences. I believe the impact of the paper will be high.

- I like the analysis and experiments the authors provided for MVBench, the numbers in Table 1 and Table 2 are convincing, and the examples in Figure 2-4 are illustrative. The presentation is well structured and convincing.

- Evaluation on the new benchmark in Table 4 shows text and image only baselines are close to random, and shuffle or reverse the frames drops the performance for all models. This supports that the proposed benchmark emphasizes on temporal information. Table 4 contains results of a wide range of models.","- While this paper convincingly shows the proposed benchmark is better than MVBench, the authors did not provide discussion/ evidence whether if it is the final video language evaluation benchmark. For example, the benchmark might be too short / in too limited domains / person centric (I am not raising concerns on these particular issues). Some statistics about the datasets are needed.

- The 10 tasks picked in Table 5 looks a bit arbitrary / artificial to me. It will be helpful if the authors provide more rationale why these tasks are picked.

- It is unclear to my how the question-answers are created. Are then all from existing datasets, or the authors hired raters to filter / verify them?","Overall this paper works on an important problem and provided a valid solution (a new benchmark). The analysis are backed up by solid experiments, and I believe this paper will have positive impact on the community. My concerns are mostly on discussions and clarity, and I expect the authors to address them in the rebuttal. My current rating is a weak accept, and I am happy to raise my rating if my concerns are addressed."
swdMzQUhBx,swdMzQUhBx,iAgent: LLM Agent as a Shield between User and Recommender Systems,Reject,dyTkpR46jT,ICLR.cc/2025/Conference/Submission11450/Reviewer_pJpw,"This paper presents a novel method called iAgent that applies LLMs to recommender systems. This approach adopts a user-centered perspective by creating a unique agent for each user as the recommendation model. Specifically, the authors convert users' historical actions into instructions and employ multiple LLMs, such as a Parser, Extractor, and ReRanker, to model user behavior. This approach captures user preferences from the user’s perspective rather than the platform’s. The authors also provide a recommendation dataset containing user instructions and conduct extensive experiments to evaluate the method's effectiveness.","1. The paper is clearly written and easy to understand. The appendix includes extensive experimental details, allowing future researchers to easily follow and replicate the study.
2. Adopting LLMs for re-ranking is quite novel, as they can leverage world knowledge to enhance result interpretability and improve re-ranking accuracy.","1. The LLMs used in this paper are zero-shot without fine-tuning, which relies heavily prompt engineering. Additionally, all four datasets are based on user reviews. In other domain-specific scenarios where world knowledge is less relevant, these zero-shot LLMs may perform poorly.
2. Personalized re-ranking is quite common in modern recommender systems. For example, [1-3] proposed re-ranking models that adjust the initial ranking list based on user preferences. The authors should compare their LLM-based re-ranking method with these non-LLM-based approaches.
3. The authors randomly sample 9 negative items along with one true item to create the candidate ranking list (L320). This distribution differs significantly from the actual ranking output of a real ranking model. Some learning-to-rank (LTR) models could be used instead to generate a more realistic initial ranking list.
4. Compared to other LLM-based methods, it’s unclear where the primary performance gains of this approach are from. For example, are the gains due to the Profile Generator’s ability to capture user traits or the Parser’s richer information extraction? How do different prompts affect metrics like HR? A more detailed ablation study should be added.
5. The authors mention the decline in LLM performance with longer sequence length (L422) but do not adequately solve it. Additionally, they select users and items with sufficient behavioral data (L293), which may further intensify this issue.
[1] Personalized Re-ranking for Recommendation
[2] Multi-factor Sequential Re-ranking with Perception-Aware Diversification
[3] On-device Integrated Re-ranking with Heterogeneous Behavior Modeling","1. Have the authors tested on long-tail users and long-tail items (e.g., with fewer than 5 actions)? Since LLMs leverage world knowledge, they might be particularly useful for cold-start scenarios.
2. Could the random assignment of user personas cause mismatches between user profiles and their historical behaviors? (L299)"
swdMzQUhBx,swdMzQUhBx,iAgent: LLM Agent as a Shield between User and Recommender Systems,Reject,PA8KeQGgKJ,ICLR.cc/2025/Conference/Submission11450/Reviewer_yPLW,"This paper introduces a new user-agent-platform paradigm for recommender systems, where an agent acts as a shield between the user and the platform's algorithms. The authors construct INSTRUCTREC datasets that include user instructions along with interactions, and propose an Instruction-aware Agent (iAgent) that leverages these instructions to refine recommendations. They further develop an Individual Instruction-aware Agent (i2Agent) that optimizes each user's unique profile and interests using a dynamic memory mechanism. Experiments show the effectiveness of the proposed method.","* The authors' development of the INSTRUCTREC datasets, which incorporate user instructions alongside user-item interactions, provides a valuable new benchmark for evaluating recommendation agents. 
* The paper introduces two agent-based models, iAgent and i2Agent, that demonstrate strong performance improvements over state-of-the-art baselines.","* The generated dataset is relatively small, with each subset containing fewer than 100,000 entries. This may limit the robustness of the findings. As a result, traditional recommendation models may be trained in an underfit manner. In Table 4, some performance of traditional recommendation models resembles random guessing.


* The paper does not compare its approach with existing LLM-based recommendation methods, such as TALLRec [1] and LLaRA [2].

* It is recommended to report token consumption and model complexity for this work, as these factors are essential for understanding the computational demands of the proposed models.


* The agent-based approach may introduce new types of biases, such as the agent's own inherent biases based on its training data and knowledge sources. Evaluating and mitigating such biases should be considered.


Reference  
[1]Bao, K., Zhang, J., Zhang, Y., Wang, W., Feng, F., & He, X. (2023, September). Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems (pp. 1007-1014).  
[2]Liao, J., Li, S., Yang, Z., Wu, J., Yuan, Y., Wang, X., & He, X. (2024, July). Llara: Large language-recommendation assistant. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 1785-1795).","Please refer to the ""Weaknesses""."
swdMzQUhBx,swdMzQUhBx,iAgent: LLM Agent as a Shield between User and Recommender Systems,Reject,L5wJAdvlbr,ICLR.cc/2025/Conference/Submission11450/Reviewer_kjVV,"This paper proposes a novel idea: agent as the shield between recommender systems and users. While this idea is interesting, the proposed components do not address the novelty of the proposed idea. Meanwhile, this paper lacks discussion with related works and the presentation of this paper is poor.","1. The proposed agent-as-shield idea is interesting.
2. The paper is easy to understand.
3. The experiments in this paper are comprehensive.","1. The technical novelty of this paper is limited. Almost all the components (i.e., parser, reranker, memory, and self-reflection) in this paper are existing technologies [1, 2, 3]. The main novelty of this paper lies in the proposed concept of agent-as-shield, while this novelty is not reflected in the component design. The model design is not relevant to the agent-as-shield idea. It looks like a general agent framework, rather than agent-as-shield framework.

2. This paper lacks sufficient discussion and comparison with previous related works, and chooses troublesome baselines (i.e., instruction-aware methods). Therefore, the experiments are unable to support the claims of this paper.

    2.1 While using agents as a shield between the recommender system and users is an interesting research direction, this idea is similar to the RecSys-Assistant-Human framework proposed in RAH [4]. However, the paper does not address it. 

    2.2 Moreover, this paper chooses troublesome baselines such as BM25, BGE-Rerank, and EasyRec, which have no relevance to this agent-based work, while ignoring agent-based baselines such as InteRecAgent. Therefore, the effectiveness of the proposed method remains unverified.
3. The writing of this paper can be significantly improved. 

    3.1. The notations in this paper are chaotic. For example, all the notations in this paper adopt the same format and font, and the meaning of each notation is hard to understand (e.g., X_{SU}, P_{pr1}, and P_{pr2}). Please refer to prior works [5, 6] for guidance on presenting formulas more clearly and formally.

    3.2. It is unclear whether iAgent and i^{2}Agent are proposed separately. iAgent is a simplified and naive version of i^{2}Agent, and its purpose and meaning are not adequately explained.

    3.3. The task definition is unclear. First, the task definition from lines 143 to 149 is too short and ambiguous, which makes it hard to showcase the novelty of this task. Second, using “our task” to denote the proposed agent-as-shield task is ambiguous. Please use a specific task name for clarification. Third, defining RecAgent as a task name may lead to confusion, since it is also a method [7]. 

[1] Park, Joon Sung, et al. ""Generative agents: Interactive simulacra of human behavior."" *Proceedings of the 36th annual acm symposium on user interface software and technology*. 2023.

[2] Hou, Yupeng, et al. ""Large language models are zero-shot rankers for recommender systems."" *European Conference on Information Retrieval*. Cham: Springer Nature Switzerland, 2024.

[3] Zhao, Yuyue, et al. ""Let me do it for you: Towards llm empowered recommendation via tool learning."" *Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval*. 2024.

[4] Shu, Yubo, et al. ""RAH! RecSys–Assistant–Human: A Human-Centered Recommendation Framework With LLM Agents."" *IEEE Transactions on Computational Social Systems* (2024).

[5] He, Xiangnan, et al. ""Lightgcn: Simplifying and powering graph convolution network for recommendation."" *Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval*. 2020.

[6] Qian, Chen, et al. ""Chatdev: Communicative agents for software development."" *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*. 2024.

[7] Wang, Lei, et al. ""Recagent: A novel simulation paradigm for recommender systems."" *arXiv preprint arXiv:2306.02552* (2023).","Why do we need both iAgent and i^{2}Agent? I think iAgent is just a simplified version of i^{2}Agent, which is unnecassary for introducing.

I am happy to increase the score if the author can address my questions."
swdMzQUhBx,swdMzQUhBx,iAgent: LLM Agent as a Shield between User and Recommender Systems,Reject,UzSBoViYxH,ICLR.cc/2025/Conference/Submission11450/Reviewer_reNA,"This work aims to address the challenges of users objectively accepting the recommendation result from commercial platforms and propose an LLM-empowered agent standing between the user and the platform, acting like a shield. This work first proposes a simple iAgent as an initial solution, followed by a more sophisticated i²Agent. Further, the author proposes a dataset InstructRec and conduct experiments on it.","1. The motivation of LLM stands between the user and platform in recommender system is good.
2. The paper is well-written with a clear methodology.","1. iAgent shares many similarities with existing agents in recommendation, such as RecMind, RAH, InteRecAgent, and AgentCF. However, the authors do not clearly articulate the significant differences beyond the task setting. Since existing agent frameworks can easily adapt to different tasks by changing prompts, the lack of discussion on this aspect raises questions about the contribution of this work.
2. To my knowledge, iAgent appears to simply adapt existing architectures (e.g., InteRecAgent and RecMind). It consists of two main components: LLM-based user query analysis and prompt-based reranking (with self-evaluation essentially functioning as majority voting-based reranking). This design paradigm for agents is already common in the context of LLM as in-context recommenders (e.g., LLMRec, etc.) and LLM-based Agents (e.g. InteRecAgent and RecMind). Moreover, if i²Agent consistently outperforms iAgent, it suggests that iAgent should be part of i²Agent rather than having its own section.
3. For i²Agent, the core component is the Profile Generator, which is not particularly innovative. Using ground truth items to adjust an LLM’s personalized preference cache has already been applied in previous works, such as RAH and AgentCF, as early as last year. This raises doubts about the originality of the work.
4. In the experimental section, iAgent lacks comparisons with existing similar approaches, such as RecMind, RAH, and InteRecAgent, making its effectiveness questionable.",please refer to weaknesses
4sDicVEy6M,4sDicVEy6M,What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits,Accept (Poster),C5rsG4RdmB,ICLR.cc/2025/Conference/Submission11437/Reviewer_ta8u,"This paper applies deep learning techniques to uncover evolutionary traits in biological data. It leverages contrastive and orthogonality losses to facilitate hierarchical prototype learning. Additionally, the paper introduces over-specificity and discriminative losses to guide and constrain model training. The proposed method demonstrates improved performance over baseline methods across multiple benchmark datasets.","1. The motivation is interesting and well-justified. The hierarchical structure of biological data presents significant challenges for distinguishing species and identifying evolutionary traits.
2. The techniques employed in HComP-Net appear technically feasible. The use of contrastive loss for learning clustered features has proven effective in self-supervised learning, while orthogonality loss helps capture diverse features.
3. The visualization results are clear and impressive.
4. The paper is well-structured and easy to follow.","1. It seems that the over-specificity and discriminative losses play opposing roles in the direction of model optimization, which raises the question of whether these two losses might interact and lead to abnormal model convergence. It would be beneficial if the authors could provide some theoretical or experimental analysis on this issue.
2. This is a minor point, but the overall method appears to be a combination of multiple techniques, making the flowchart somewhat complex and redundant.",Please kindly refer the the weaknesses.
4sDicVEy6M,4sDicVEy6M,What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits,Accept (Poster),mo5AzqI0rl,ICLR.cc/2025/Conference/Submission11437/Reviewer_hRXA,"The work a novel framework called Hierarchy aligned Commonality through Prototypical Networks (HComP-Net) aimed at discovering evolutionary traits among species by learning hierarchical prototypes over the tree of life. It addresses the challenges of existing prototype-based methods that often produce over-specific prototypes at internal nodes, which can hinder the identification of common traits shared by descendant species. HComP-Net employs a unique over-specificity loss, a discriminative loss to ensure prototypes are absent in contrasting species, and a masking module to maintain classification performance. Through empirical analysis on various datasets, including birds, fishes, turtles, and butterflies, the authors demonstrate that HComP-Net effectively learns accurate, semantically consistent, and generalizable prototypes.","This paper investigates a highly intriguing task: identifying visual features preserved during species evolution. I believe this task is inherently challenging due to the limited and often insufficient quality of training data, making it difficult to obtain stable, semantically interpretable visual features. In this work, the design of the loss function and the associated explanations are intuitive and easy to understand.","1. The first comment is related to the choice of using visual data to identify common features in species evolution. Given the scarcity of high-quality biological images, especially in the context of vast evolutionary networks, and the inherent issues of imbalance and interference in such images, I wonder if it might be more precise to analyze common traits directly from textual descriptions or anatomical data. Could you please elaborate on the rationale behind prioritizing visual data for this task?

2. The paper introduces several loss functions aimed at ensuring the diversity and effectiveness of the learned prototypes. I would be very interested to see ablation studies on these loss functions to better understand their individual impact.

3. In Figure 4, when comparing the part consistency between HComP-Net and HPNet, different bird images are used. I am curious to know if this choice is justified and, if so, what the reasoning behind it is. Would it not be more appropriate to use the same images for a clearer comparison?",Please see the weaknesses.
4sDicVEy6M,4sDicVEy6M,What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits,Accept (Poster),R9PTmWoQxU,ICLR.cc/2025/Conference/Submission11437/Reviewer_hsuk,"The paper introduces a new framework that can be used for learning biological prototypes within hierarchies while avoiding the learning
of over-specific features at internal nodes of the genetic tree. The authors perform tests with different datasets including mostly the pictures of birds, fishes and butterflies. The authors focus on the quantitative and qulitative evaluation.","- The idea presented in the paper is interesting and original. The idea is quite simple (which is a plus). I liked that the authors aimed to present a method that can help to facilitate the analysis of different species in biology. 
- The paper is well-written, and also presents a lot of nice and clean graphics and examples that make the content understandable.","- The motivation of the paper says that there are many image datasets in the biology, so ML can be used to provide some new visual suggestions for common traits in species belonging to a common group. Nevertheless, such suggestions seem to be more important for some newly discovered species (and not the ones that are already well-known), and for such species there is a possibility of not having so many images. This can decrease the practicality of the method. Statements such as “Furthermore, HComP-Net demonstrates a unique ability to generate novel hypotheses about evolutionary traits, showcasing its potential in advancing our understanding of evolution” are too bold in my opinion.
- Another issue with a possible practical use of the method is that the proposed solution to provide semantically meaningful information requires human annotation, which can be very subjective. The authors mention this limitation in the appendix, however they do not give any solution for a mitigation. 
- minor: some typos/grammatical mistakes can be found in the paper, e.g. “that are not just shared across all it descendant species but are also” -> “that are not just shared across all ITS descendant species but are also”","- The paper discusses many similarities with ProtoPNet and refers to it often, e.g. “For HPnet, we used the same hyperparameter settings and training strategy as used by ProtoPNet for the CUB-200-2011 dataset”, “We follow the same training strategy as provided by ProtoPNet for the CUB-200-2011 dataset.”. Why ProtoPNet is not used in the comparative experiments (also other non-hierarchical models are used there)?
- How can the paper help to “advance our understanding of evolution”? E.g. There is a high chance that for the common species (whose pictures are available in large-scale datasets) the same features that were already used to make such a classification will be highlighted. Could the authors present a use case, in which the solution could be successfully used in practice to contribute to the understanding of evolution?
- How are the representative images (also could be perceived as prototypical images) chosen from the dataset to allow for proper explanations (some images can be taken e.g. from a non-typical angle and do not show the prototypical features well and make the explanations difficult)?
- Is it possible (and if yes, how) to use this solution in other hierarchical problems (not in the area of biology)?"
4sDicVEy6M,4sDicVEy6M,What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits,Accept (Poster),EgynGp289c,ICLR.cc/2025/Conference/Submission11437/Reviewer_PA9n,"This work presents an extended approach for hierarchical prototype learning by training a neural network to discover hierarchical prototypes using structured information from a phylogenetic tree. Building upon HPNet (Haze et al., 2019), this approach, termed HComP-Net, contrasts with traditional models that use flat structures by incorporating loss functions such as over-specificity loss and discriminative loss. These functions enable HComP-Net to learn prototypes that align with the hierarchical structure of the tree, enhancing interpretability and consistency. Empirical results highlight HComP-Net’s ability to produce accurate, semantically coherent prototypes transferable to unseen species. Tested on a dataset of 190 bird species and additional organisms, the model performs better than baseline models. Additionally, HComP-Net has been shown to generate visual hypotheses on evolutionary traits, offering insight into traits across various levels of the phylogenetic tree. This research underscores the potential of interpretable representation learning using structured hierarchical prior knowledge.","Originality: The research builds on the problem initially formulated in HPNet (Haze et al., 2019), specifically addressing the challenge of over-specific prototypes. The authors introduced over-specificity and discriminative loss functions, enabling HComP-Net to learn prototypes that adhere to the hierarchical structure of a phylogenetic tree. This approach enhances interpretability and brings a fresh perspective to prototype learning.

Quality: Given the identified problem of over-specific prototypes, the authors demonstrate the efficacy of their methods across fine-grained classification tasks, also showing advancements in interpretability. The proposed model performs consistently well in these tasks, validating its quality and effectiveness.

Clarity: The paper is well-organized, with clear explanations of the background, literature, methodology, experimental setup, and results. This clarity enhances the readability and accessibility of the research, making its contributions understandable and well-supported.

Significance: This work highlights the potential of interpretable representation learning driven by structured hierarchical knowledge. By using a phylogenetic tree for guidance, the model provides insights into trait evolution and aligns closely with the biology-inspired data structure, marking a contribution at the intersection of AI and biology.","1) On the Need for a Separate Masking Module
The fact that an additional masking module is required suggests that the initial contributions, particularly the over-specificity and discriminative loss functions, may not fully address the issue of over-specific prototypes. This need points to a limitation in the proposed loss functions' effectiveness in preventing prototypes from becoming overly tailored to specific species. Ideally, a more robust solution would directly manage prototype specificity through the loss functions alone, reducing reliance on extra modules that could complicate the model and potentially impact interpretability or scalability.

2) On the Role of the Phylogenetic Tree in Classification
In section 5.1, the authors suggest that achieving high classification accuracy is not the primary goal. However, providing the model with additional information, like a phylogenetic tree during training and inference, could indeed aid classification performance. The tree may allow the model to leverage hierarchical relationships, which could enhance classification accuracy by using shared traits among related species. Thus, there seems to be a disconnect between the claim of not prioritizing classification accuracy and the model's design, which inherently includes information that could enhance it.","1) The CNN backbone used was ConvNeXt-tiny architecture; why the fine-grained accuracy from this architecture is not included in Table 1? Will it be better/worse when compared to other methods on the Table?

2) In terms of semantically meaningful prototypes, could you discuss the possibility of obtaining similar findings using Explainable AI techniques?"
4sDicVEy6M,4sDicVEy6M,What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits,Accept (Poster),0zIByR1PmP,ICLR.cc/2025/Conference/Submission11437/Reviewer_z89S,"This paper addresses a scientific problem: discovering evolutionary traits in biology from images. To tackle this, it proposes the Hierarchy-Aligned Commonality through Prototypical Network (HComP-Net). The primary objective is to reduce over-specific prototypes that lose common features expected to be observed in all species sharing a common ancestor. To achieve this, the approach applies contrastive, orthogonality, and discriminative losses to the prototypes, and introduces over-specificity loss and masking to mitigate over-specific prototypes.","The method details are well-demonstrated, with a clear overview and examples that make the content easy to follow.

The study presents a novel approach to improving the interpretability of prototypical networks, allowing for more accurate representation of parent-child relationships.

It enhances the specialization of deep networks applied in scientific discovery, particularly in phylogenetic analysis.","Two limits of experimental demonstration. 
1. The primary contribution lies in incorporating parent-child relationships to reduce over-specific prototypes within the contrastive losses, while the architecture does not appear to include specific structures for establishing a prototype hierarchy.
In my understanding, proposing a hierarchical structure is a contribution of HPNet, not HCompPNet. This point should be clarified in title and method description. 
2. The exclusion of certain related works leaves me a question about novelty on practical impact. For instance, PIPNet, a recent and closely related method employing self-supervised learning (2023), is not included in the comparison. It only uses HPNet from 2019. However, semantic gaps has relevance to over-specificity, and authors also mention strong motivation from PIPNet. The reason why the network is excluded needs more detailed explanation or comparison in experiments.",Why does the over-specificity loss adopt a specific log-tanh loss form? Doesn’t it simply establish an arbitrary criterion for identifying overly specific prototypes? This choice requires further explanation and discussion.
oMkHoJjLXB,oMkHoJjLXB,Embodied Referring Expression Comprehension Through Multimodal Residual Learning,Reject,7Pi3qiSvYN,ICLR.cc/2025/Conference/Submission11352/Reviewer_2VWn,"The paper introduces Refer360, a novel dataset designed for embodied referring expression understanding, capturing interactions from multiple perspectives. In addition, it proposes an innovative Multimodal Guided Residual Module (MuRes), which enhances cross-attention mechanisms by effectively fusing visual and language features. The MuRes module establishes an information bottleneck by leveraging the differences between query, key, and value in cross-attention, ensuring seamless integration with residual connections.","The dataset features multimodal data captured from a wide variety of environments, including both indoor and outdoor settings. This addresses a significant limitation of existing datasets, which predominantly focus on indoor scenarios.

The proposed MuRes module provides an effective method for fusing multimodal representations, with the potential to enhance performance in downstream tasks such as visual question answering (VQA).","Lines 051–053: Verbal Utterances and Natural Expressions
The authors argue that utterances like ""left ball"" and ""right ball"" introduce bias and limit the model's ability to understand interactions. However, such expressions are natural in human communication and essential for interpreting real-world interactions. Excluding them could reduce the dataset’s ability to model realistic communication, thereby limiting the model's generalization to human behavior. While minimizing biases is a valid goal, removing these phrases may unintentionally hinder the model’s ability to understand natural referring expressions.

Lines 070–073: Ego View and Occlusions
The paper claims that incorporating multiple perspectives (e.g., egocentric and exocentric) helps mitigate occlusions. However, in real-world human interactions, an individual's egocentric view is not accessible to others. Communication typically relies on third-person perspectives, and handling occlusions is a natural part of this process. Incorporating egocentric views may introduce an artificial setup that does not align with real-world scenarios. Furthermore, this approach increases complexity and hardware requirements (e.g., wearable cameras), which may not be practical or scalable.

Lines 213–215: Participant Demographics
The dataset participants are primarily students with a mean age of 26, introducing potential demographic bias. This narrow age range may not adequately represent the wider population, particularly younger or older individuals. Consequently, the dataset’s ability to generalize across diverse age groups and social contexts could be limited.

Lines 337–338: Discrepancies in Model Training
The paper notes that the BLIP-2 model was trained with a smaller batch size (2) compared to other models (32), while all models were trained for the same number of epochs. This leads to BLIP-2 undergoing significantly more gradient updates, potentially skewing the performance comparison. However, the paper does not sufficiently address this discrepancy or its implications for the results.

Table 3: Performance of MuRes across Models
The performance improvements achieved by MuRes are inconsistent across different models, raising concerns about its general effectiveness:

VILT: MuRes shows only marginal gains (0.5 for IoU-25 and 0.6 for IoU-50 under the best setting).
BLIP-2: Performance declines significantly for IoU-25 (-4 for V, -3 for L, and -13 for V+L) and shows only minor improvement for IoU-50 under the L setting.
Dual-Encoder: Improvements are minimal, with slight increases of 0.3 for IoU-25 and 0.8 for IoU-50.
Scholarship
The paper could improve its scholarship by referencing relevant recent work, such as Understanding Embodied Reference with Touch-Line Transformer [ICLR 2023].",see weakness box
oMkHoJjLXB,oMkHoJjLXB,Embodied Referring Expression Comprehension Through Multimodal Residual Learning,Reject,qraoPxpLUg,ICLR.cc/2025/Conference/Submission11352/Reviewer_TYRR,"This manuscript presents a method to interpret embodied referring expressions, featuring the development of the Refer360 dataset and a MuRes module. 

The Refer360 dataset, designed for diverse human interactions in various indoor and outdoor settings, overcomes typical limitations of existing datasets by capturing multiple perspectives and blending verbal and nonverbal cues. 
The MuRes module reinforces salient aspects of modality-specific representations through cross-attention.

Experiments demonstrate that integrating MuRes into existing multimodal frameworks improves performance on several metrics related to embodied referring expression comprehension and VQA.","- Refer360 dataset enhances multimodal model training by encompassing a diverse array of human interactions across various environments. It includes both verbal and nonverbal communication, offering a detailed representation of real-world scenarios and setting new standards for dataset quality and utility in model development.

- The MuRes module isolates and enhances key features from various modalities, increasing the clarity and detail in the analysis of complex interactions.","- The technical contribution of the MuRes module is modest, as it relies on cross-attention for feature enhancement, a method already common in the multimodal field. 
Integrating the MuRes module into existing multimodal frameworks can increase computational demands considerably due to its complex cross-attention mechanisms, leading to higher processing times and resource usage.

- In Table 3, applying MuRes sometimes results in lower performance compared to models without residual modules. For larger models like BLIP-2, the performance significantly drops, suggesting that MuRes may not be well-suited for all model architectures.

- Table 4 does not include the results for BLIP-2, raising doubts about the effectiveness of the proposed method on larger models.

- It is recommended to add experiments for the VQA task on embodied-related datasets to further validate the method's effectiveness in this specific application area.

- (minor) It is advised to use \\citep for in-text citations to ensure consistent formatting and improve readability.","Please refer to the weaknesses section. 

After considering the advantages and disadvantages outlined above, my overall assessment leans negative. I will adjust the score accordingly based on any responses or clarifications provided."
oMkHoJjLXB,oMkHoJjLXB,Embodied Referring Expression Comprehension Through Multimodal Residual Learning,Reject,rlL1GIzMqd,ICLR.cc/2025/Conference/Submission11352/Reviewer_niSU,"This paper presents a dataset called Refer360 which has verbal and non-verbal data from different viewpoints for referential expressions in indoor and outdoor settings. Furthermore, a multimodal guided residual module (MuRes) which can be used for improving representations of existing multi-modal models. The MuRes is evaluated on two downstream tasks: referential expressions and visual question answering. The baselines are CLIP, ViLT and BLIP-2 with and without MuRes(V), MuRes(L) and MuRes(V+L). For the referential expression, MuRes does not show a consistent performance increase while on the vqa task, MuRes(V+L) shows better performance except on Clip.","- A large dataset contains multi-modal data for referential expression. The dataset is recorded with egocentric and exocentric camera views, which is beneficial for training models with better generalisability. 

- Part of the dataset is collected in outdoor scenarios while most previous datasets are collected in indoor settings.","- The authors describe the dataset as ""to facilitate the understanding of human interactions in real-world settings"", however, there is no interaction in the experiment protocal. There is only referential expressions by a human participant. 

- The benchmarking on referential expression only uses image and text as input modality, discard other modalities in refer360, especially gaze, which is an import indication of human's intention in referential expressions. Additionally, using gaze to identify objects in human-robot interaction has been shown adequate for understanding humans' intended object in various works. 

Shi, Lei, Cosmin Copot, and Steve Vanlanduit. ""Gazeemd: Detecting visual intention in gaze-based human-robot interaction."" Robotics 10.2 (2021): 68.
Yang, Bo, et al. ""Natural grasp intention recognition based on gaze in human-robot interaction."" IEEE Journal of Biomedical and Health Informatics 27.4 (2023): 2059-2070.
Belardinelli, Anna. ""Gaze-based intention estimation: principles, methodologies, and applications in HRI."" ACM Transactions on Human-Robot Interaction 13.3 (2024): 1-30.

With the multi-modality data, the authors could benchmark for instance scanpath prediction using other modalities, and model performance using videos captured from different viewpoints.

- There are details of how the dataset is collected missing, for details see comments below.","- The authors mentioned that the dataset was collected under different conditions such as variable lighting conditions, object arrangements, and environment appearances. These need to be elaborated.

- In one session, does the robot move when the participant refers to one object? In different sessions, when a viewpoint changes, does the participant always refer to the same object?

- The description of task needs more details, does the participant freely decide which object to point at?

- Annotation details are needed. What is annotated? The frames where humans are pointing at something or the intended object bounding box? Are all objects annotated? 

- I'd use video language models with MuRes as the dataset is in the video form."
oMkHoJjLXB,oMkHoJjLXB,Embodied Referring Expression Comprehension Through Multimodal Residual Learning,Reject,s65LcmgYsV,ICLR.cc/2025/Conference/Submission11352/Reviewer_EbAp,"In this paper, the authors introduce an embodied referring expressions dataset called Refer360, including various settings and providing insights for the community. Accordingly, a base model called MuRes is designed for evaluation.","- Contributing a benchmark called Refer360, facilitating the study of real-world human-robot interaction.

- The proposed dataset covers various real-world scenarios such as indoor and outdoor, supporting real-world applications without the need for sim2real adaptations.

- The paper is well-organized and easy to follow.","- It seems that the authors have overlooked some related literature, such as [1-3], which introduced datasets and methods related to embodied referring expression, aimed at enabling agents to navigate to target points based on natural language instructions (i.e., Instruction Following). Although I understand that the authors intend to build an interaction-oriented embodied referring expression database, these related works are still worth discussing.

[1] Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR.

[2] REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments. In CVPR.

[3] Room-Object Entity Prompting and Reasoning for Embodied Referring Expression. In TPAMI.

- The proposed baseline method is quite simple, raising doubts about whether it can handle such complex interactive embodied tasks. Why not use open-source MLLM for fine-tuning, such as by adding extra adapters or training with LoRA? 

- The performance of the proposed method on Refer360 is not promising compared to other methods.

- More samples (current 14k) including interaction would be beneficial for embodied tasks.",- Is it possible to test the model trained on the Refer360 database on other related databases to validate the benefits brought by the rich information of Refer360?
oMkHoJjLXB,oMkHoJjLXB,Embodied Referring Expression Comprehension Through Multimodal Residual Learning,Reject,xWQrLW9lVK,ICLR.cc/2025/Conference/Submission11352/Reviewer_acP2,"This paper presents an Embodied Referring Expressions dataset to compensate for the missing perspectives in existing datasets, such as the full spectrum, perspective bias, and single viewpoint. In addition, the authors propose the residual module to improve the performance of existing pre-trained VLMs.","* The proposed dataset includes both indoor and outdoor scene understanding, and the data scale is quite considerable. It may be helpful for subsequent research work.
* The information source of the proposed dataset is very rich, and the author declares that it includes exo visual views, such as ego visual view, depth, infrared, 3D skeletal data, audio, and robot camera motion.","* In my opinion, the focus of this work is on the proposed new dataset (core contribution), however, the author's discussion on the proposed dataset is rare. For example, the proportion of samples in different scenes in the visual part, how to describe the text part, the length range of each scene's text token description, and how the proposed dataset can be helpful in downstream tasks, etc.
* The multimodal fusion method proposed in this article seems to be a simplified version combining BLIP and LLaVa (cross-attention and mlp across modalities), with the only difference being the shift in focus from modal mapping to modal information fusion. In addition, How is the information of exo visual view, ego visual view, depth, infrared, 3D skeletal data, audio, and robot camera motion processed and integrated? The description of the method in Figure 3 is not clear.
* From the results in Table 3, it can be seen that the proposed residual module has limited help for pre-training models, compared to Without Residual and Vanilla Residual.","How are multiple visual features extracted and fused, such as depth information? How is the text description obtained and what are the bases? How many frames of information does each sample contain?"
oMkHoJjLXB,oMkHoJjLXB,Embodied Referring Expression Comprehension Through Multimodal Residual Learning,Reject,waSEl2jkgl,ICLR.cc/2025/Conference/Submission11352/Reviewer_Ct4m,"This paper introduces a new dataset, Refer360, for embodied referring expression comprehension along with a novel multimodal fusion technique using a bottleneck architecture - MuRes. The authors argue that the existing datasets lack the necessary diversity and complexity to truly capture human interactions - hence, they introduced Refer360. The authors also argue that MuRes helps bridge modality gap by learning aligned complimentary representations.","1. Originality: 

The authors convincingly provide arguments to demonstrate the gap in current datasets - they are limited in the variety/complexity of human interactions and variety of environments. The paper addresses these two important problems in their dataset by incorporating multiple viewpoints, multiple perception modalities, and multiple environments in their data. They also collect the data in scripted and unscripted manners. This dataset, with inclusion of verbal and non-verbal cues, is a valuable resource for future research.

The fusion architecture (MuRes) introduced in this paper is an interesting addition to the work. Using cross-attention to reinforce salient features from each modality is creative.

2. Quality: 

The research is conducted to a high standard, with a clear and well-motivated methodology. The data collection process is rigorous, involving the use of Kinect sensors mounted on robots and smart glasses worn by participants. This approach allows for the capture of rich multimodal data, including both visual and linguistic information. 

The proposed MuRes method is well-grounded in existing literature and addresses a key challenge in multimodal learning, namely, the effective fusion of information from different modalities.

3. Clarity:

The paper is well-written and easy to follow. The authors provide a clear and concise description of their method, and the experimental setup is well-explained. The results are presented in a clear and informative manner, with helpful visualizations and tables. The paper also includes a concise (but sufficient) discussion of related work, placing the research in its proper context.

4. Significance:

This work has the potential to make a significant impact on the field of embodied AI. The Refer360 dataset is a valuable contribution that will enable researchers to develop and evaluate more sophisticated models for embodied referring expression comprehension or any other embodied AI task. 

The MuRes method also shows promise for improving the performance of multimodal learning models in a variety of applications. The experiments in the paper show a substantial increase in performance for certain tasks. These performance gains may also translate to other downstream tasks in multimodal settings.

Overall, this research represents an important step towards the development of AI systems that can interact with humans in a more natural and effective way. Having more datasets like these is the key to fast-track research in embodied AI.","1. Lack of Clear Definition and Contextualization: The paper delays defining ""embodied referring expression comprehension"" until the related work section. This key concept should be introduced and clearly defined upfront - in abstract. Doing so would significantly improve the readability and understanding for readers unfamiliar with this specific area of research. Additionally, while the authors mention the importance of verbal and non-verbal cues, they neglect to provide specific definitions and examples within the context of their research. Providing these would enhance the paper's clarity and comprehensiveness.

2. Inadequate Analysis of Performance Inconsistencies: The analysis of the results does not delve into the inconsistencies in performance gains observed across different MuRes variants (MuRes(V), MuRes(L), and MuRes(V+L)), particularly in the embodied referring expression comprehension task - refer to its results table. A more in-depth discussion of these inconsistencies is necessary. The authors should provide explanations that justify the choice of a specific bottleneck architecture based on the domain, offering insights into the interplay between the architecture and the specific characteristics of each task. There should be some experiments and analysis to understand which modality should be favored for a particular type of task.

3. Limited Comparison with other methods for learning complementary representations: While the paper does a good job of showing that MuRes improves multiple encoder-fusion architectures, its does not perform any comparison with existing techniques for learning complementary representations in multimodal settings. The paper does compare their bottleneck framework with simple residual-based fusion and fusion without residuals, but these do not sufficiently show how it compares against other existing methods. For example, explore methods like those presented in ""Learning Cross-Modality Encoder Representations from Transformers"" (Tan et al., 2019) and ""Attention Bottlenecks for Multimodal Fusion"" (Nagrani et al., 2021), .

4. Potential Bias in Data Collection: The data collection protocol includes both constrained and unconstrained settings. As participants may have participated in both, there is a concern that their experiences in the constrained setting might have biased their behavior in the unconstrained setting. The authors should address this potential issue and discuss any measures taken to mitigate this bias.

5. Limited Theoretical Foundation: While the MuRes architecture demonstrates empirical success, the paper lacks a strong theoretical foundation to support its design choices. The justification for the cross-attention mechanism and the specific configuration of queries, keys, and values appears to be based primarily on intuition and experimental validation. A more rigorous mathematical grounding, potentially drawing inspiration from information theory or optimization theory, would significantly strengthen the paper's technical contribution and provide deeper insights into the underlying principles of the MuRes architecture.","1. Please talk about how you may mitigate the potential biases introduced in the dataset by interleaving scripted and unscripted interactions. Scripted interactions can bias a subject to act in a certain way while performing an unscripted interaction. Did you perform any experimentation/analysis with randomizing/separating the scripted-unscripted session sequence to understand/mitigate this bias?

2. The diagram depicting MuRes architecture could do a better job at showing how this architecture is a “bottleneck residual connection” architecture. Having the projection and cross-attention blocks vertically stacked may better represent this. Or labelling the key, value, queries in the diagram may make it easier to follow.

3. The experiments are solid but not varied in tasks covered. Showing performance improvements on more tasks can bolster the claims made in the paper. It would best if these new tasks are generation-based.

4. Can you provide any theoretical foundation for the design choice (cross-attention based bottleneck) that leads to the MuRes architecture? If this is something that is described by a related work, please talk about this in the related work section. If not, add a section justifying this choice theoretically.

5. Please define the ""embodied referring expression comprehension"" earlier in the paper so that the paper becomes easier to follow."
n2EU4PUrJP,n2EU4PUrJP,Soup to go: mitigating forgetting during continual learning with model averaging,Reject,fNV4sBqHwB,ICLR.cc/2025/Conference/Submission11350/Reviewer_kPso,This paper proposes Sequential Fine-tuning with Averaging for continual learning in LLMs pre-training. The proposed method mitigates forgetting by periodically merging the fine-tuning model with earlier checkpoints trained on previous tasks.,"1. The motivation for the proposed methodology is clear: the proposed method reduces forgetting without intensive memory costs for storing past data or multiple copies of previous checkpoints.
2. This paper presents empirical comparisons of the proposed method with regularization-based CL and model merging in LLMs pre-training CL. These results verify the effectiveness of the proposed method.","1. The proposed is validated in limited settings: it only tests on 3 or 4 tasks. Moreover, the order of arrival of these 3 or 4 tasks is fixed. This is not practical in continuous learning, where there may be an infinite number of tasks and we have no control over the incoming tasks.
2. The proposed method can be applied to CV tasks. Thus, it can be tested in CV tasks under more settings and compared with more existing works. However, this paper fails to do so.
3. The performance of the proposed method is inferior to the data replay-based method in most cases. 
4. [Minor] In ""Model Merging"" of related works, the paragraphs have no space between them, making it uncomfortable to read.","Can the proposed method be generalized to longer task sequences? If so, I would suggest that the authors use the CIFAR100 or ImageNet datasets to construct task sequences, as is done in existing CL works, and compare the proposed method to more CL baselines."
n2EU4PUrJP,n2EU4PUrJP,Soup to go: mitigating forgetting during continual learning with model averaging,Reject,oAdPiob6oK,ICLR.cc/2025/Conference/Submission11350/Reviewer_KE43,"This paper introduces the SFA method, designed to address the issue of catastrophic forgetting in the continuous learning process of models. The method does not rely on historical data but instead continuously merging checkpoints from the training process with the original model. SFA is equivalent to L2 regularization to some extent. In the settings proposed in the paper, this method outperforms existing penalty update methods. The paper further analyzes the performance of the SFA method under different merging frequencies.","- The method is simple and easy to understand, which could facilitate practical usage if proven effective.
- The paper compares the method with several existing approaches, validating its effectiveness.","- The overall writing and organization of the paper are unclear in many places. For example:
  1. The reviewer believes that the formula in Algorithm 1 should be $\\theta_t = \\beta \\theta_0 + (1-\\beta) \\theta_{t-pT}$. The key point of the algorithm, as understood by the reviewer, is to mix the merged and updated model with the original model to maintain performance.
  2. The explanation from lines 154 to 156 is confusing.
  3. The experimental results are introduced starting on page 5, but the related figures are placed on page 7, making the paper difficult to read.
  4. In Figure 1, the legends for L2 and EWC should be dashed lines.
  5. In the series of figures starting from Figure 2, the author attempts to include too much comparative information in a single figure, making them hard to understand.
- The models used in the paper are all the relatively poorly performing Pythia 2.8B, leading to a direct drop in GSM8K accuracy to 0% in some cases. This limits the reliability of the results in these scenarios. The authors are encouraged to conduct experiments on more recent and higher-performing small language models.","- Besides linking SFA with L2 penalty, could the reviewer further discuss why SFA significantly outperforms L2 penalty? Are there specific scenarios where one method is more suitable than the other?"
n2EU4PUrJP,n2EU4PUrJP,Soup to go: mitigating forgetting during continual learning with model averaging,Reject,lurvXASJCt,ICLR.cc/2025/Conference/Submission11350/Reviewer_evVQ,"This paper concentrates on catastrophic forgetting of pretrained large language model where data from instruction fine-tuning (IFT) tasks arrives in a sequence. To mitigate the forgetting, this paper proposes Sequential Fine-tuning with Averaging (SFA), a method that merges models with earlier checkpoints trained on previous tasks during the course of training. This paper also tries to bridge L2 penalty with model merging technique, which aims to explain why the proposed method works. Finally, experiments are provided to empirically evaluate the proposed method.","1. This paper is well-written and easy to follow.

2. I appreciate the extensive experiments.","My main concern is about the proposed method.

1. I think the entire work (including the proposed method) is based on eq.3 which is not correct. Section 4 of this paper try to explain why the proposed method is reasonable by eq.3. According to eq.1, we can obtain the update rule of parameters, i.e. $\\theta_{t+1}=\\theta _t-\\eta(\\Delta _{\\theta _t}L _{\\mathrm{task}} + \\theta _t- \\theta _o)$. Splitting this update into 2 steps, we obtain eq.2 and $\\theta _{t+1}=\\theta _{t+1}^*-\\eta(\\theta _t- \\theta _o)$. It is noteworthy that $\\theta _{t+1}=\\theta _{t+1}^*-\\eta(\\theta _t- \\theta _o)$ is not equal to eq.3, because $\\theta _{t+1}^*=\\theta _t-\\eta \\Delta _{\\theta _t}L _{\\mathrm{task}} \\neq \\theta _t$. Therefore, eq.3 is not correct.

2. The issue in 1 can not be ignored, because it means that the proposed method in Algorithm 1 is not reasonable. It is a significant mistake.",1. What is $p$ and $T$ in Algorithm 1? Please provide a more detailed description on the steps of algorithm.
n2EU4PUrJP,n2EU4PUrJP,Soup to go: mitigating forgetting during continual learning with model averaging,Reject,3cR6B2zoHo,ICLR.cc/2025/Conference/Submission11350/Reviewer_uFBx,"This paper introduces Sequential Fine-tuning with Averaging (SFA), a method designed to mitigate catastrophic forgetting in pretrained large language models during continual learning by merging earlier checkpoints without requiring data buffers or parameter copies. SFA outperforms traditional penalty methods and merging techniques, enabling effective performance across diverse fine-tuning tasks in areas such as Math, Law, and Code.",This paper investigates an important problem on alliviating the forgetting of LLM during contiually fine-tuning. It shows that an simple technique that averages the model averages can effectively perserve the abilities of different domains.,"While the paper presents valuable insights, I would like to point out several limitations based on my evaluation:

* **On the Novelty of Findings**: The use of model averaging (WiSE-FT [1]) to alleviate forgetting has been extensively investigated in prior work [1]. Although this paper claims to extend WiSE-FT to multiple domains, it appears to still average the model checkpoints before and after fine-tuning a new domain, which closely resembles the approach in [1, 2]. Furthermore, many of the results are consistent or overlapped with those found in [2].

* **On the Effectiveness of Model Averaging**: The paper posits that model averaging mitigates forgetting by mimicking L2 regularization towards θ0. However, numerous studies [1, 2] indicate that model averaging is generally more effective than L2 regularization. Therefore, this explanation might not sufficiently account for the observed effectiveness of model averaging.

In conclusion, neither the results nor the explanations presented seem particularly novel. Additionally, I would recommend that the paper include a more thorough discussion of existing work, particularly [2], due to the significant overlap in findings. 

**References**:  
[1] Mitchell Wortsman, et al., Robust Fine-Tuning of Zero-Shot Models  
[2] Yong Lin et al., Mitigating the Alignment Tax of RLHF",See above
awvJBtB2op,awvJBtB2op,Generating Freeform Endoskeletal Robots,Accept (Spotlight),jsKzBAw9t2,ICLR.cc/2025/Conference/Submission11292/Reviewer_Le5n,"This work proposes (1) a simulation framework, (2) a generative model, and (3) a co-optimization procedure for endoskeletal robots that consist of both rigid bones and soft tissue, in contrast to rigid-only or soft-only bodies in many previous works. (1) The endoskeletal robots are simulated as a collection of elastic and rigid voxels in a 3D space, with type-specific constraint forces between them. (2) The learning-based generative model is trained by autoencoding a large dataset of synthetic training data (valid voxelized body plans) into a latent space. After training, the decoder generates voxelized body plans from this latent space, which are post-processed into a graph structure defining a simulated robot body. (3) The co-optimization procedure uses two nested training loops, in which the outer ""evolutionary"" loop searches through the latent space with CMA-ES to design better morphology, and the inner ""lifetime"" loop learns a reused actor/critic architecture with PPO to control an arbitrary morphology on control tasks. The agent is rewarded for forward locomotion across a variety of terrains.","**S1: Novelty.** Unlike many previous works that focus exclusively on rigid or soft bodies, this work combines them into a single framework. The manuscript provides an extensive related works section for situating this work within the field, which was helpful.

**S2: Clarity.** The manuscript was a pleasure to read. The writing is clear and concise, with appropriate details provided in the extensive appendices. The mathematical notation is simple and consistent. The figures are thoughtfully designed. Overall, great work!

**S3: Quality.** The methods are appropriately applied, and the experimental results are technically sound and informative (especially with the ablations).

Overall, I believe this is a solid work which I will recommend acceptance of, provided that my feedback below is adequately addressed.","**W1: Code.** I did not see a link to code, which is critical for reproducibility. Can the authors please provide this during the rebuttal?

**W2: Videos.** It will be useful to see qualitative results (i.e. videos) in addition to the quantitative reward plots. Currently, the reward values aren't contextualized, so it's had to tell if the behavior is actually good, only that it's improving.

**W3: Ablation of discrete action space.** It would be useful to show the comparison of discrete vs continuous action space (line 310), and provide some discussion. Is this a constraint from your simulation, or a learning issue? Wouldn't PPO also be able to control continuous action spaces, as it does in other related sim works?

**W4: Minor figure/text edits.**
- Figure 2 has nice aesthetics, but could be simplified. It has a lot of extraneous subpanel labels that aren't useful (Figure 5 too). Really the figure has 3 columns (initial, training, final), and the rows are instances of the same population. This message could be highlighted with in-figure annotations, like an arrow indicating the direction of training time.
- Figure 1 looks pretty, but what is its purpose? It looks like a bunch of molecules floating on a dark ocean, which was confusing as it primed me to think about fluid simulations, (but the sim only works on terrestrial envs, line 533). Also, the entire caption is one really loong sentence.
- Typos: line 376 sampled, line 398 the the, Fig 7 subpanel A label, Equation 10 star misplaced","**Q1: Actor/critic inputs.** What's the rationale for conditioning the actor and critic on different state?

**Q2: Ablation of evolved designs.** On line 498, is that statement a mistake? It does not seem like Optimized RL is doing well with the initial designs. And I'm not sure how catastrophic forgetting is related, i.e. forgetting how to control a bad initial policy?"
awvJBtB2op,awvJBtB2op,Generating Freeform Endoskeletal Robots,Accept (Spotlight),FavP1kFprA,ICLR.cc/2025/Conference/Submission11292/Reviewer_dYez,"In this paper, the authors introduced a new brain-body design of freeform endoskeletal robots. The authors introduce a computational framework that integrates the generation of external and internal structures through a combination of 3D modeling, latent space encoding, reinforcement learning, and evolutionary strategies.","1. The authors integrated 3D modeling, latent space encoding, reinforcement learning, and evolutionary strategies into a cohesive framework for robot design.

2. Optimizing in the latent space significantly reduces the computational burden of the co-design process.

3. The use of a universal controller and simultaneous optimization of morphology and control through reinforcement learning demonstrates the robustness and adaptability of the approach.

4. The research topic of this paper is interesting and significant.","1. While the paper focus on the co-design problem of endoskeletal robots, which is innovative, the main contribution of this paper seems to be not clear. Besides, the analysis of scalability and the potential real-world applications are needed.

2. It would be better to have a picture to describe the whole co-design pipeline of the method. 

3. The whole co-design pipeline (Learning the latent space description-> Training a universal controller via RL-> Optimizing morphology through EA) is not new. The most important innovation of the article seems to turn out to be the modeling of endoskeletal robots.

4. The authors utilize multiple advanced techniques, which may lead to high computational costs and complexity, might limit practical applications.

5. The latent space encoding, while effective, may lack interpretability, making it difficult to understand the underlying principles that guide the design evolution.","1. Could the authors provide more insights into the latent space encoding? Specifically, how do specific regions of the latent space correspond to different types of robot designs?

2. Can the proposed method find endoskeletal robot morphologies which can generalize to several tasks?

3. How does the proposed method compare to other state-of-the-art approaches (Such as transform2act, ea+rl bi-level optimization) in terms of design diversity, performance, and computational efficiency?

4. What are the potential real-world applications of the robots designed by your methods? How feasible is the transition from simulated designs to real-world robotic systems? What are the main challenges in this transition?"
awvJBtB2op,awvJBtB2op,Generating Freeform Endoskeletal Robots,Accept (Spotlight),I1HwaPeRxc,ICLR.cc/2025/Conference/Submission11292/Reviewer_uyFn,"The authors designed and implemented a software library capable of generating three-dimensional freeform endoskeletal robots. Across various tasks, the researchers demonstrate the simulations’ ability to provide a means of benchmarking the evolutionary design and representational learning of complex embodies systems.",Provides an easy-to-use computational design of freeform endoskeletal robots and the library software for benchmarking the representations and evolution of these agents over time.,"The paper's conclusion that ""design evolution finds better designs but they do not imply that evolution (i.e. cumulative selection) is necessary"" appears to contradict the study's methodology, which used reward values that created selection pressure across generations. Please clarify how this conclusion aligns with the reward-based selection used in the experiments, and if possible, provide evidence that random mutations alone could achieve similar results. We recommend rephrasing to better reflect the relationship between mutations and selection pressure while maintaining the valuable insights about design diversity.",What insight may explain the evolution of the agents in these specific directions? Can we find similarities of this biologically?
awvJBtB2op,awvJBtB2op,Generating Freeform Endoskeletal Robots,Accept (Spotlight),mYcIHwbVmh,ICLR.cc/2025/Conference/Submission11292/Reviewer_wJ4t,"This paper bridges the two opposing types of robots namely fully rigid and fully soft robots in current literature by introducing a novel multi-physics simulator of 3D endoskeletal robots that feature rigid, jointed bodies covered by soft materials. The authors employ a variational autoencoder trained on synthetic body plans to yield smooth and expressive latent representation of morphology. With the aid of a Graph Transformer-based modular control policy trained alongside morphological evolution, the authors showcase the feasibility of their simulation environment and latent representation to support the emergence of complex, lifelike morphologies capable of navigating various terrains. This work lays an important foundation for developing more sophisticated, biomimetic and free-form artificial organisms.","1. The paper introduces an open-source simulation environment of 3D endoskeletal robots with four predefined terrains. The voxel-based representation is conceptually simple and meanwhile offers great convenience for future work to benchmark their brain-body co-design algorithms. It also provides an open-endedness to design more complicated terrain maps for specific needs. 

2. I appreciate that the authors meticulously designed a suite of control experiments to demonstrate the indispensability of morphological evolution. This is generally absent in existing brain-body co-design studies, which take morphology and control as a whole to evaluate task performance.","The introduction seems a bit too verbose. The authors are suggested to only keep the most essential information regarding current state of the art and their contributions, and relegate the remaining to Related Work or Appendix.","1.Some previous studies have demonstrated that Graph Neural Networks tend to have difficulty with long-distance communication, as messages would be diluted in multiple hops [1]. Could you explain why you have chosen a Graph Transformer, instead of a conventional Transformer with full connection between modules (as in [1,2])? 

[1] Kurin V, Igl M, Rocktäschel T, et al. My body is a cage: the role of morphology in graph-based incompatible control[C]//9th International Conference on Learning Representations. 2021.

[2] Gupta A, Fan L, Ganguli S, et al. Metamorph: learning universal controllers with transformers[C]//International Conference on Learning Representations. ICLR, 2022.

2.Is your simulation environment currently restricted to locomotion on various terrains? Does it or will it support manipulation tasks, such as carrying or pushing an object? 

3.Could you explain why the population is cloned during policy training? 

Thank you!"
Q2bJ2qgcP1,Q2bJ2qgcP1,Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark,Accept (Poster),a2bkYIJnul,ICLR.cc/2025/Conference/Submission11132/Reviewer_tCgC,The paper proposes a new statistical parameter to evaluate the performance of a method to estimate CATE. Then the paper shows that this parameter can be consistently estimated. Finally the paper demonstrates that this parameter can be used to select estimation methods for CATE.,"- The statistical parameter proposed in the paper is simple to implement and makes intuitive sense. 
- Theoretical guarantees are provided. 
- The paper compares many estimation methods for CATE in the empirical application.","- The paper largely overclaims the number of datasets. Essentially, there are only 12 unique datasets and the paper uses a bunch of sampling methods to create many more variants of the original 12 datasets.
- The findings in the first paragraph of the abstract have been known for a long time, and they are not unexpected, as claimed in the paper. The estimation of CATE is known to be very noisy, and that's why the causal inference literature has primarily focused on the estimation of ATE (but not CATE) for decades. 
- The clarity and rigor of the paper need to be improved. For example, the term ""CATE model"" is a bit awkward and is rarely used in the literature. CATE is an estimand commonly defined based on potential outcomes. When ""CATE model"" is used, it is unclear whether this term refers to an outcome model to define CATE or an estimation method for CATE.",Please address the weaknesses above.
Q2bJ2qgcP1,Q2bJ2qgcP1,Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark,Accept (Poster),t3N4mAWeT2,ICLR.cc/2025/Conference/Submission11132/Reviewer_x6Rc,"In this work, the authors propose a new way to estimate treatment effect and perform model selection when the counterfactual is not available. To do this, the authors propose a new metric called Q, which is derived from the Mean Squared Error (MSE) but adjusted to exclude terms that do not depend on the estimator.","Lacking good metrics for evaluation is a big problem in casual inference due to no access to counterfactuals. This paper proposes a new metric, which could be helpful for practitioners when comparing different models.","1. For the experiments given in this work, the estimation (training) size is much smaller than the evaluation size. This is not intuitive to me as in typical ML settings, we have training-test ratio to be 4:1. Using a much lower ratio of training vs test samples could potentially lead to the problem of unfitting, which could be a reason why many baselines have too many degenerate cases. I think it is necessary to use a large number to training samples to rule out this hypothetical scenario.","1. Can you explain why your estimation size is much smaller than the evaluation size?

2. Can you re-conduct the experiments when the estimation size is much larger than the evaluation size? Let's use the more standard ML experimental setup, where the ration between training and test samples is 4:1. You should apply this to all datasets in your Table 4. Then rerun the whole experimental pipeline and report results similar to those shown in Table 1.

3. Also, there is no standard deviation for Table 1. It might be a little bit misleading in using the description ""43,200 datasets"" in Table 1. What you are essentially doing is repeated resampling, similar to bootstrap. In this case, in addition to what I ask for #2 above, could you also report results on the same datasets, with same percentage of treatment and other parameters fixed, with both mean and standard deviation, where the standard deviation solely comes from bootstrap?"
Q2bJ2qgcP1,Q2bJ2qgcP1,Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark,Accept (Poster),nyytEEJssi,ICLR.cc/2025/Conference/Submission11132/Reviewer_vxwd,"This paper presents a large-scale benchmark study evaluating the performance of contemporary Conditional Average Treatment Effect (CATE) estimation models. The authors use a novel application of observational sampling to evaluate 16 modern CATE models across 43,200 datasets.  Their key findings challenge the effectiveness of current CATE models in capturing real-world heterogeneity.","Large-scale and comprehensive benchmark study.

Novel approach to CATE evaluation using observational sampling and the Q statistic.

Rigorous theoretical analysis and proofs supporting the proposed methodology.

Use of real-world datasets provides valuable insights into the limitations of current CATE models.","While aiming for diversity, the selection of datasets may still not fully represent the breadth of real-world applications, potentially limiting the generalizability of the findings. More explanation of dataset selection criteria would strengthen the paper.

The paper focuses on MSE as the primary evaluation metric. While justified by its practical relevance, exploring alternative evaluation metrics could provide additional insights.

While innovative, the reliance on the Q statistic is new and requires further validation and adoption by the wider research community.","The paper discusses the generalization of the Q statistic to new distributions. Could you provide more details about the assumptions underlying these generalization results? How sensitive are these results to violations of these assumptions?

What are the main limitations of the current study, and what directions for future research do you suggest based on these findings? What kinds of new CATE models or evaluation methods should be explored? What types of additional data would be useful for further validation?

How can the findings of this study be used to guide the practical application of CATE models in various domains (medicine, economics, etc.)? What advice would you give to practitioners regarding the selection and interpretation of CATE models?"
Q2bJ2qgcP1,Q2bJ2qgcP1,Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark,Accept (Poster),YHYGdKWa7L,ICLR.cc/2025/Conference/Submission11132/Reviewer_UrBr,"This paper conducts an empirical study of the accuracy of CATE estimation methods, asking whether existing methods in the literature reliably outperform trivial benchmarks. The main idea is to use RCT datasets, where the propensity is known, to construct benchmarks where a simulated propensity score is used to construct an ""observational"" sample. Since the propensity score is known for the original data, estimators on the observational sample can be benchmarked without having to rely on accurate estimates of propensity or outcome models (effectively using a HT estimator, potentially augmented with other regression models just for variance reduction purposes).","This work is definitely important for the field. Evaluating CATE models is very difficult due to the risk of self-serving bias that the paper discusses, and it is unclear when in practice modern techniques are helpful. Constructing benchmarks and evaluation methods which use only real data, via RCT datasets, is important. Using semi-synthetic datasets with simulated outcomes, as is common in the field, is much less convincing.","There are some respects in which the execution of the paper could be improved:

(1) Based on plots in the appendix, there appears to be significant variation in the performance of different methods across datasets (as would be expected). This deserves more discussion, and investigation. E.g. why do double ML methods perform well in some cases and not in many others? One hypothesis, based on benchmarks for ATE estimation, would be that outcome regression methods work best as long as the outcome models are reasonably well estimated, with double-ML style methods providing benefits when the outcomes are not as well captured. 

As is, there is little in the way of takeaways about the conditions under which existing methods do or don't perform well. This sort of diagnosis would be at least as useful as the headline results about % of cases where methods work well or don't, since the headline numbers are very specific to the composition of this specific benchmark. 

(2) Relatedly, the benchmark appears to be weighted quite heavily towards a set of multiple tasks all drawn from the same dataset: out of 12 tasks, 8 are different outcomes taken from the general social survey. We might reasonably expect different tasks from the same dataset to share some similar characteristics so the amount of actual diversity in the benchmark is less than what the number of tasks would suggest. 

(3) Regarding the degeneracy rate, are the results different if we test for whether Q is significantly larger than 0 (i.e., a CI excludes 0) rather than if the point estimate is > 0? I would guess that most of the datasets are large enough for the estimates of Q to be fairly precise, but it would be helpful to verify this. 

While I think the paper makes a worthwhile contribution even with these weaknesses, the contribution would be significantly strengthened with a more diverse set of tasks and more analysis of what drives performance variation across those tasks.","Any comments/clarifications, particularly related to points 1 and 3 above, would be helpful."
Q2bJ2qgcP1,Q2bJ2qgcP1,Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark,Accept (Poster),cMbdvkUpMF,ICLR.cc/2025/Conference/Submission11132/Reviewer_jZnM,"This paper presents a framework to evaluate CATE estimation methods in observational studies. To address challenges in existing approaches that either leverage semi-synthetic data or rely on untestable modeling assumptions, this work proposes to rely on propensity modeling for evaluation. Estimators for the MSE (minus a constant) are proposed and their theoretical properties are discussed under conditions on the propensity models. Then, this paper provides a comprehensive benchmark by deriving observational studies via resampling RCTs, which leads to unbiased evaluation of existing CATE methods. The benchmark reveals surprising findings about the insufficiency of the existing methods for capturing heterogeneity in CATE.","1. This paper studies the important problem of reliable evaluation of CATE methods. 
2. The paper is comprehensive, containing fruitful results. 
3. The paper contributes new tools and benchmarks to the field, which may benefit other researchers and motivate future works. 
4. The paper is clearly structured.","1. The writing quality of the paper can still be improved. Sometimes the advantages of the proposed method is a bit overclaimed. It will be helpful to also discuss the limitation of the proposed approach.
2. The discussion on the surprising results can be more in depth. Why are existing methods even worse than constant treatment effect estimation? Is this because they are too variable (so that variance in MSE is large)? Is this because the heterogeneity in the datasets is actually small? 

Please see my questions for more comments.","1. Why is Theorem 3.4 better than the double robustness results? It seems to require consistency of the propensity model, but double robustness estimation can be consistent if either the outcome model or the propensity model is correct. 

2. What are the assumptions for Theorem 3.13 to hold? Does the propensity score need to be estimated? If so, why didn't its estimation error enter the result?

3. It's weird to count randomly split data as a ""new dataset"" so that there are 43200 benchmark datasets. Please consider revise your claim on the number of datasets. 

4. In section 4.3, the performance of $\\hat{Q}$ is a bit worse than model-augmented versions of $\\hat{Q}$. It would be nice to provide some suggestions on what estimators to use in practice. Does the model-augmented version has additional bias/modeling concerns?

5. What's special about the Horwitz-Thompson estimator for $\\eta$? It seems that $\\eta$ can be anything whose conditional expectation given $X$ equals $\\tau(X)$. Is it the point that it doesn't use any outcome model? Can it be replaced by other reasonable choices? 

6. While addressing the concerns of semi-synthetic evaluation and outcome modeling, here the method requires the propensity model to be well estimated (though I understand that the evaluation part doesn't have this concern due to controlled sampling process). Is this over-simplifying real-world evaluation scenarios where the treatment assignment in observational studies can be complicated as well? What is the method evaluating if an inconsistent propensity model is used? Is it estimating some other quantity that is related to MSE of CATE estimator? 

7. I would suggest separating the theoretical results for the most general evaluation case (eg in a real observational study) and the evaluation part in this paper. In real observational studies, consistent evaluation inevitably rely on some good modeling. However, it is important to be clearly stated that the evaluation in this paper doesn't need this because of the controlled sampling process."
zxO4WuVGns,zxO4WuVGns,Inverse decision-making using neural amortized Bayesian actors,Accept (Poster),CHrpkDBoiw,ICLR.cc/2025/Conference/Submission11101/Reviewer_bwX5,"This paper introduces a method for performing Bayesian inference on the parameters of Bayesian observer-actor models, particularly suited for scenarios where Bayesian decision-making can be computationally intractable. The approach leverages a neural network to amortize the decision-making process of the subject by training the network to minimize the expected task-relevant cost with respect to the posterior over latent states and the action distribution. This setup allows for efficient, gradient-based inference of parameters from behavioral data. The authors validate their approach on synthetic data, highlighting its effectiveness and also discuss identifiability issues with recommendations to mitigate them. They further illustrate the method's applicability to human behavioral data.",This paper address an important bottleneck in inverse decision-making by amortizing the agent's behavior using a neural network. This enables efficient Bayesian inference over the subject's behavioral model parameters. The experiments on synthetic data validate the approach through comparison with analytical solutions. The discussion on identifiability and the experiment design recommendations add valuable practical insights.,"In this work, the proposed approach aims to infer what a subject’s decisions were optimal for. However, there is still an assumption of optimal behavior, which may not always hold in real-world scenarios. Factors such as suboptimal learning or changing task demands can lead to deviations from optimality. Even if these deviations could potentially be reframed as an alternative optimality criterion, doing so would introduce additional identifiability challenges. It would be beneficial  to discuss the limitations of this assumption.

Another potential limitation is that the use of the reparameterization trick requires a specific form of action distribution, which may restrict the model’s adaptability to diverse datasets and tasks where this distributional form does not apply.

Finally, the presentation could be improved. Several figures lack clear labels and legends, making them difficult to interpret, and acronyms are introduced without prior definition. A revised presentation with attention to these details would enhance the paper.","- In figure 2A, could you clarify what is $r^{\\ast}$? Should it be $a^{\\ast}$ instead?

- The top left panel in figure 2B is missing a label. Should it be $\\sigma_0$?

- In figure 2C, it would be useful to include separate x-axis labels for the analytical and nn cases.

- The caption for figure 3B uses $\\beta$ as cost asymmetry parameter, but all figure labels use $\\alpha$. Are they the same?

- In figures 3B and 3C, it would be helpful to make the ranges of the axes the same in all panels. 

- Figure 4b is difficult to follow, including a legend would be very helpful."
zxO4WuVGns,zxO4WuVGns,Inverse decision-making using neural amortized Bayesian actors,Accept (Poster),V1egZ4TdhT,ICLR.cc/2025/Conference/Submission11101/Reviewer_QV14,"The paper addresses the challenge of using Bayesian models to infer decision-making parameters (inverse decision making) from behavioral data, especially for tasks involving continuous actions where traditional Bayesian methods struggle with computational intractability. 

The authors propose a new method where a pre-trained neural network, trained unsupervisely, was used to approximate an actor model’s parameter. The gradient-based Bayesian inference makes the method relatively efficient. This approach shows promising alignment with analytical solutions where they exist and effectively models human behavioral data in various sensorimotor tasks.","The paper provides an innovative approach by using a neural network to approximate the Bayesian model for inverse inference, which traditionally faces computational intractability issues. Their neural network method, trained in an unsupervised manner, enables efficient inference of decision-making parameters without relying on closed-form solutions or restrictive assumptions (like Gaussian distributions or quadratic costs).

Clear problem formulation and motivation.","The authors mentioned that their method could be applicable to a large number of tasks involving continuous responses, including economic decision-making, psychophysical production and crossmodality matching. However, the authors only tested their method on sensorimotor tasks. Testing methods on a diverse set of tasks involving continuous responses would significantly strengthen the paper.

The authors acknowledge that this method is currently constrained to relatively straightforward perceptual models. Extending it to more complex tasks (such as those involving circular variables or advanced cognitive reasoning) remains a limitation in its current form.","How scalable is the current approach? What are the computational requirements for training the neural networks for more complex cognitive reasoning tasks?

Could the authors provide more details about the choice of network architecture and hyperparameters?"
zxO4WuVGns,zxO4WuVGns,Inverse decision-making using neural amortized Bayesian actors,Accept (Poster),yLqKL8eWaD,ICLR.cc/2025/Conference/Submission11101/Reviewer_nJXd,"This paper considers an important and fundamental problem of inferring priors and costs from behavior. Typically, the inverse decision-making problem is intractable. Therefore, the author approximate the solution with a neural network, and show that the ground truth can be recovered well on simulated dataset. The author also explore the human behavioral data.","1. The writing is clear, and the core ideas are well articulated. 
2. This paper introduces a novel approach for Bayesian inference about the parameters of Bayesian actor models.","The most significant concern is the lack of experimental advancements. This work only presents experimental results from numerical simulations and some simple human behavioral dataset, where simple MLP is able to recover posterior distributions. Presumably, the algorithm proposed by the author will face several challenge when we have to due with high-dimensional input.
1. It might be hard to train $f_{\\psi} (\\theta, m)$ when $\\theta$ contains more than 100M parameters.
2. The HMC might not have the property of rapid mixing.",Please refer to the Weaknesses.
nzOD1we8Z4,nzOD1we8Z4,ContextGNN: Beyond Two-Tower Recommendation Systems,Accept (Poster),FbZZ0xrEJr,ICLR.cc/2025/Conference/Submission11092/Reviewer_GUBd,"The paper proposes ContextGNNs, a new architecture for graph-based recommendation. Unlike previous similar approaches, such as pair-wise recommendation solutions, the authors propose a pair-wise representation only for exploring the items in the user's local subgraph, while the architecture employs the usual two-tower method for the item side. Finally, a network is designed and trained to fuse the two representations and provide the final recommendations. 

On the one hand, the ContextGNNs framework first generates a pair-wise representation for users and items, where a $k$-hop subgraph for each user is sampled, and a GNN is leveraged to obtain embedding representations and the overall item ranking. On the other hand, a two-tower representation is exploited on the item side, as it can be shown that: 1) GNNs do not learn useful information on the item side as the item interaction matrix tends to be very dense; 2) shallow item embeddings are effective; 3) GNN item representations can scale poorly with large numbers of items in the catalog. The so-learned item embeddings are injected into the GNN forward pass presented above, to allow the user representations to align with the item ones. Finally, an MLP is trained to fuse the two designed contributions. 

Results on different tasks and datasets from the RelBench platform demonstrate the efficacy of the ContextGNNs proposed baseline, also when compared to different settings of the same model.","\\+ The proposed approach is well-placed within the existing literature on (graph)-based recommendation, and the motivations behind ContextGNNs are quite solid and sound

\\+ Overall, the paper is well-written and easy to follow

\\+ I appreciated the locality score study, which conceptually and empirically supports the rationale behind the approach

\\+ I also appreciated how the authors discussed possible extensions of the proposed approach in different and interesting new directions

\\+ Even considering the page limitations, I believe the experimental setting is adequately extensive to empirically support the rationale behind the paper","\\- To my understanding, it seems that the list of baselines for the experiments is not completely diversified and updated to the latest advances in graph-based recommendation. I would have appreciated seeing more recent and popular approaches being tested against ContextGNNs, such as LightGCN (cited in the paper), SGL [i], UltraGCN [ii], SimGCL [iii], and LightGCL [iv].

**References**

[i] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie: Self-supervised Graph Learning for Recommendation. SIGIR 2021: 726-735

[ii] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, Xiuqiang He: UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. CIKM 2021: 1253-1262 

[iii] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, Quoc Viet Hung Nguyen: Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation. SIGIR 2022: 1294-1303

[iv] Xuheng Cai, Chao Huang, Lianghao Xia, Xubin Ren: LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. ICLR 2023","I have a question that does not refer to the one weakness raised. As observed by the authors in the locality score study, it is interesting to see that many validation/test links are not in the users' nearest neighborhood. Moreover, we know from the literature, that graph-based recommender systems may be affected by oversmoothing. On such a basis, my question is: how the graph-based model can learn long-range (and, evidently, ground-truth) products connected to users if the model cannot explore on more than ~3 hops (on average)? Do you have any intuitions for that?"
nzOD1we8Z4,nzOD1we8Z4,ContextGNN: Beyond Two-Tower Recommendation Systems,Accept (Poster),wfXbJkRpCY,ICLR.cc/2025/Conference/Submission11092/Reviewer_uwX2,This paper studies the two-tower architecture for recommender systems. They introduce a context-based GNNs-based framework for link prediction in recommendation by using a pair-wise representation technique for similar items in the user's local subgraph. The proposed method achieves promising performance in their experiments.,"1. The research issue is very important in the industry. 

2. Their experimental results demonstrate the effectiveness of the proposed method. 

3. The paper is easy to follow.","However, there are some concerns in the paper:

1. The novelty of the proposed method is unclear.
This paper leverages local interaction graph for representation learning. The idea is very mature and has been well-studied. 
 Compared with exciting methods, what are the novelty of the proposed method? Additionally, the related work in the paper can be enhanced. 
This paper can be improved by discussing more advanced two-tower architectures. Furthermore, the related work cannot demonstrate the novelty of the proposed method.


2. The technical contributions of this work are also unclear. I was wondering what are the main challenges they are addressing. 
The proposed idea looks good. However, the proposed method doesn't have any theoretical analysis.
The model design didn't motivate well. 
This paper considers the temporal factor in modeling recommendation. However, what are the technical contributions for taking advantage of it?


3. The experimental section can be enhanced before publication. First, they can compare with some advanced recommendation baselines. There are many advanced recommendation methods for modeling user and item representations. 
Second, the datasets here are unclear to me. Whether the proposed method can be used for large-scale datasets?",See the weakness.
nzOD1we8Z4,nzOD1we8Z4,ContextGNN: Beyond Two-Tower Recommendation Systems,Accept (Poster),6h9tgHcOo4,ICLR.cc/2025/Conference/Submission11092/Reviewer_pueu,"This paper proposes a recommendation framework that integrates pair-wise representations with two-tower representations. Specifically, it leverages pair-wise representations to capture fine-grained patterns of past user-item interactions, while utilizing two-tower representations for ""distant"" items. The authors have validated the effectiveness of their approach on the RELBENCH benchmark.","1. The motivation behind the proposed method is clear and has significant practical implications for large-scale recommendation systems in the real world.
2. The authors have validated the effectiveness of the proposed method on the benchmark datasets.","1. The authors emphasize in the abstract and introduction that their method is a relatively general and effective strategy in recommendation scenarios. However, the experimental section only tests the method on datasets that include rich multi-behavioral and temporal interactions. This discrepancy between the claims and the experimental validation raises concerns. Therefore, the authors should either revise their overall statements or consider incorporating a wider variety of datasets for comparative analysis.
2. The experimental section of the paper is quite limited, as it currently only reports the MAP metric and lacks detailed parameter sensitivity analysis. Moreover, the paper does not address how the proposed method's performance may be affected when user behavior information in the dataset is insufficient in type or quantity. Specifically, would we expect a significant decline in performance compared to the baseline algorithms under such conditions? Additionally, I recommend that the authors incorporate a broader range of recent comparison methods to strengthen their empirical evaluation.
3. The paper does not provide the original code, which raises concerns regarding the reproducibility of the results. Additionally, there are some errors in the equations presented.",Please refer to the weaknesses
nzOD1we8Z4,nzOD1we8Z4,ContextGNN: Beyond Two-Tower Recommendation Systems,Accept (Poster),anVy2yahQQ,ICLR.cc/2025/Conference/Submission11092/Reviewer_U3kG,"* The method uses a pair-wise representation for familiar items in a user's local subgraph and two-tower representations for recommending exploratory items. A final network combines both pair-wise and two-tower recommendations to create a unified ranking of items.

* The study utilizes CONTEXTGNNs in relational deep learning for recommendations within relational databases, which consist of various tables with rich multi-modal features and complex behavioral interactions. This environment provides an ideal setting for testing the model's effectiveness. Additionally, the research explores the challenges of modeling intricate human behavior patterns.","* This article has a clear structure and is easy to follow.

*Tested different tasks, which is rare in previous work.","* It appears that this work has not released the code or dataset, making reproducibility uncertain.

* This work lacks technical innovation and seems quite basic. Such a simple design should emphasize time complexity, runtime efficiency, and scalability, but the paper does not seem to address these aspects.

* This work does not compare with other recent baselines or commonly used models in the industry. Additionally, it does not provide specific data from the dataset. Whether in industry or research, this work does not seem to fit well. Overall, it appears too basic and simple, with relatively little effort involved.",* The paper mentions the complexity issues of pair-wise methods. Could you provide a time complexity analysis and runtime efficiency experiments of this work compared to others?
nzOD1we8Z4,nzOD1we8Z4,ContextGNN: Beyond Two-Tower Recommendation Systems,Accept (Poster),1MnD2QUSVP,ICLR.cc/2025/Conference/Submission11092/Reviewer_7mVo,"This paper presents ContextGNN, a refined approach that classifies items into user-specific selections within a user-centric local graph, alongside other items. Subsequently, the GNN learns the pertinent user and item embeddings, thereby enabling the derivation of both pairwise and two-tower representations for enhanced recommendations.","1. The paper is well-written and easy to understand.
2. The use of both pairwise and two-tower representations for recommendations is quite interesting.
3. The experiments conducted in this paper seem to demonstrate the model's effectiveness.","1. The author should consider open-sourcing the code for the proposed method to enhance reproducibility.
2. In the comparison of methods, it would be beneficial to include more recent and advanced GNN-based recommender models beyond NGCF.
3. Given that the author mentions both multi-behavior and temporal graphs, it would be advantageous to illustrate how the GNN backbone is designed to accommodate such graphs.",See Weaknesses.
SmxM4POTBk,SmxM4POTBk,IntGrad MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Interpolation Guided Gradual MT,Reject,Q3Epy377OE,ICLR.cc/2025/Conference/Submission10951/Reviewer_gKyV,"This paper assumes that a source sentence (called end sentence) has paired sentences (called start sentences) that are easy to translate or have gold translations and these paired sentences with corresponding translations can be used as translation samples in order to prompt the model to translate the source sentences. To use this assumption, the authors presented a multi-step prompting framework:

- Step 0:  use an existing dev set to create a start sentence pool
- Step 1:   select start sentences from the pool based on similarities.
- Step 2:   calculate the interpolation sentences between the source sentence (or end sentence) and the paired start sentences and translate all interpolation, start sentences, and the end sentence.
- Step 3: Chain all sentences and translations into multiple prompts according to the start sentences.
 - Step 4: select the final translation from the results of these prompts.

Experiments are conducted on FLORES-200 for 7 languages.","1. The idea is interesting. 
2. The paper is clear. I can easily follow the paper.
3. The authors analyze each step in the ablation study.","1. Evaluation is not fair. The authors use xCOMET to select translations and evaluate the final results on the test set. The final results are biased to xCOMET or COMEwiki scores. The authors have to provide scareBLUE scores to justify the effectiveness. Meanwhile, since you use all dev sets as your start sentences pool, it is not fair to other models. How do you select 3 examples for your 3-shot translation? What is your selection method ? Can you try an experiment where we select 3 similar sentences from dev set with the source sentences as the 3 examples?

2. Efficiency is poor. The method required too many prompts and tokens to finish the translation task. For example, in Table 2, the model requires 3 prompts that generate multiple sentences for translation. Can you provide a study of efficiency? 

3. Experiments are limited. The authors claim the framework works for low-resource languages. However, only limited experiments are conducted in Table 1.","1. Refers to Weaknesses.
2. How many interoplation sentences generated on average for one translation?"
SmxM4POTBk,SmxM4POTBk,IntGrad MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Interpolation Guided Gradual MT,Reject,OtinEaXk92,ICLR.cc/2025/Conference/Submission10951/Reviewer_3NmE,"The paper introduces IntGrad MT, a technique that enhances the machine translation capabilities of Large Language Models without additional training. It utilizes Sentence Interpolation and Gradual MT to explore the intermediate processes in translation by creating a sequence of translations that progressively increase in complexity. This additional context is used to improve the final translation. Empirical results demonstrate that this method improves translation quality, particularly for low-resource languages such as Hindi, Swahili, Bengali, and Marathi, as measured by xCOMET scores. IntGrad MT offers a practical way to elevate translation performance without relying on extra training data.","- The idea of introducing intermediate steps (additional interpolated demonstrations) to guide the LLM in improving translation is interesting. Like CoT and ICL, this represents a training-free translation strategy that achieves enhancement.

- The proposal was tested in experiments, particularly showing significant improvements in low-resource translation scenarios.

- The authors provide insightful analysis and ablation studies in the paper, which are beneficial for understanding the effectiveness of the approach.","However, there are some weaknesses:

- Lack of strong baselines. The paper does not include some strong multilingual LLMs in the comparison, such as the recent TowerInstruct. Comparisons are only made with its dependent LLMs, which limits the scope of evaluation.

- The left result in Figure 5 does not convincingly support the claim that ""the interpolation successfully bridges the two sentences"". Question: Could you provide more explanation? The example shown in Appendix B.1 on Page 17 indicates that the sixth interpolation still shows a considerable difference from the end sentence in word-level similarity. What is the COMET score for that sentence?

- Unfortunately, the evaluation was conducted on sentence-level tasks, whereas LLMs excel in document translation. It would be beneficial to see if the interpolation approach is also effective with document-level test data.

- One of my concerns is that there is no analysis or evaluation of computation time during inference. The authors should report this information to make the study more comprehensive.

- The authors claim that their method addresses difficult translation tasks by expanding the areas where the model performs well. However, they did not clearly define what constitutes a 'difficult' level, and there is a lack of an ablation study examining the translation quality among sentences with varying difficulties. Additionally, it is unclear whether it is possible to enhance the model with just a few interpolated sentences for the more challenging cases.",See the weaknesses
SmxM4POTBk,SmxM4POTBk,IntGrad MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Interpolation Guided Gradual MT,Reject,CLXowCLqqC,ICLR.cc/2025/Conference/Submission10951/Reviewer_Y21e,"The paper introduces a method called IntGrad MT that alleviates performance of LLMs for MT in low-resource language pairs using parametric sources of information with no extra training. In IntGrad they utilise 2 techniques, sentence interpolation and gradual MT. Former starts from an easy seed sentence and keeps on changing it till it reaches the source sentence (sentence to translate) and latter keeps on translating each one while using the previous translation results as few-shot examples for the current sentence. The authors show efficacy of their method by conducting thorough experiments and ablation studies.","- The paper is very clearly written and easy to follow except for the Output selection part.
- Their approach doesn’t require extra training and utilises existing parametric information. 
- The experiments performed by authors are quite extensive including ablation study for finding optimal combination of strategies and sentence interpolation analysis.
- The authors used xCOMET over COMET which has error span detection capabilities. I highly recommend them showing how their approach was better when compared to baselines by showing error spans of the baselines (which their own approach won’t have).
- Figure 1 gives a good intuition of IntGrad MT and Figure 2 is really helpful for visualising Gradual MT","- Output selection could have been worded properly. Although it is clear from the paragraph that “thr” is a pre-processing step and “delta” is a post-processing step but:
    - “thr” should come before sentence interpolation in Figure 3 which is not the case
    - “delta” should come before aggregation? 
    - From what I understood - start sentences -> thr -> interpolation -> Gradual MT -> delta -> aggregation -> Final translation
    - If this is correct then you please modify Figure 3 and meaning of output adoption strategy
- Even though this approach doesn’t require training but interpolation step is quite costly as it uses a 72B LLM to generate sentences to reach end sentence which increases the latency of the overall translation. It is evident that ""thr"" reduces number of interpolated sentences but it increases the latency by how much compared to the baselines? How do you plan to reduce the latency of the Interpolation model?
- For few-shot baseline, authors use 3 MT pairs but for their thr + delta approach I can see from Table 3 that Interpolation and GradMT were executed for 45% sentences out of which number of selected outputs were 279 from which final MT result is produced. Do you ensure a fair comparison between you method and the 3-shot baseline, given the difference in the number of examples used.","- I had some trouble understanding “Start Sentence Pool Creation”. Start sentences are essentially in English, then why were they translated as mentioned in Section 4.1?
- I can see the authors used Euclidian distance throughout the paper but cosine similarity is used more frequently when measuring similarity between 2 sentences using SBERT in many NLP papers. Why did the authors use Euclidian distance and not cosine similarity metric?
- Please change the colour of MQM(MetricX) for Figure 3 as it looks quite washed out when printed.
- I wanted to know if having bigger context length had any detrimental affect on the final results in Table 3?


Typos and formatting-
- Line 241: SBERT similarity
- Line 471: measure
- Table 1, 2, and 3 have a missing \\toprule. Missing \\bottomrule for Table 2
- Figure 3 says “n” start sentences but caption says “k” start sentences"
SmxM4POTBk,SmxM4POTBk,IntGrad MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Interpolation Guided Gradual MT,Reject,c5n6tLoypy,ICLR.cc/2025/Conference/Submission10951/Reviewer_efMc,"This paper proposes IntGrad MT, a method to enhance machine translation capabilities of large language models (LLMs) without additional training by using sentence interpolation and gradual MT. The approach constructs chains of sentences that incrementally increase in difficulty, using the model's own translations as few-shot examples. The authors evaluate their method on various LLMs (GPT-3.5, Mistral Nemo, Llama models) across multiple languages and report improvements especially for low-resource languages.","1. The paper addresses an important problem of improving LLM translation capabilities for low-resource languages *without* additional training

2. The evaluation includes multiple models. Even though source is fixed to English, the paper evaluates on multiple target languages from different language families and using different scripts.

3. The ablation study attempts to analyze different components of the method","1. A very big issue is that the baselines (0-shot, 3-shot) are unfairly chosen because they are too weak, and as a result I am not at all convinced that the method proposed by the authors would outperform stronger baselines. Since the main contribution of the paper is improving MT quality, this is something that definitely should be improved. At the absolute minimum, additional baseline results for n-shot should be included, where n is chosen such that the computational costs are similar to the method proposed by the paper. Ideally, some other popular methods that improve LLM-based MT without fine-tuning (from related work Section 2.1) will also be included.

Let's consider example 3 from the appendix, which has an interpolation path of 7 sentences. If I understand correctly, this means that we start from the first sentence and do a 0-shot translation, and then recursively do (n-1)-shot translations until we arrive at the final sentence in the interpolation path. The total costs for X are thus:

    - s1 -> 0-shot translation +
    - s2 -> 1-shot translation with input (s1) +
    - s3 -> 2-shot translation with input (s1,s2) +
    - s4 -> 3-shot translation with input (s1,s2,s3) +
    - s5 -> 4-shot translation with input (s1,s2,s3,s4) +
    - s6 -> 5-shot translation with input (s1,s2,s3,s5) +
    - s7 -> 6-shot translation with input (s1,s2,s3,s5,s6)

(Note that this ignores the costs for step 0: start sentence pool creation, step 1: start sentence selection, step 3: MT results aggregation. It also assumes a single start sentence, otherwise costs will be multiplied by number of start sentences (assuming same interpolation sentences).)

2. The method is very computationally expensive compared to the baselines. This is true even for much stronger baselines (e.g., n-shot with relatively large n) that are not included in the paper. The authors mention ""IntGrad MT introduces some computational overhead"", but this seems a severe understatement. The computational overhead of the method is not properly analyzed, but this analysis should have been included to help readers understand these trade-offs.

3. As acknowledged by the authors, their method only works when English is the source language. This is not necessarily a prohibitive issue, but when we have few-shot (with larger n) available as alternative, it is unclear why one would use this more complicated method over that (current results fail to convince me of the improvements over stronger baselines, see first point).

4. I would not call LLMs translation capability ""inherent"", as you do in the abstract. See e.g. https://aclanthology.org/2023.acl-long.524/, which shows that translation capabilities are mostly due to (incidental) bilingualism, i.e., parallel data.

5. While the authors briefly discuss related work, the discussion lacks depth. It is not clear how the authors method is different from this discussion. (It can be inferred by a reader who is knowledgeable about the related works, but that is insufficient.)

6. A lot of clarity and typo issues, I list some below:
    - Figure 1 is a bit unclear, nowhere is mentioned what blue/red colours mean. I think I can infer it but it is not clear enough.
    - line 89: ""To what sentences IntGradMT can be effective?"" awkward sentence, rephrase to ""For which sentences is IntGradMT effective?""
    - line 135: ""which is a prompting technique asks model"" -> ""that asks the model""?
    - line 214: ""list of translation"" -> ""list of translations""
    - lots of issues with missing space before opening parenthesis, e.g. line 235: ""Nemo(Mistral-Nemo""; line 281: ""selection(step 1)""
    - line 372: ""theee"" -> ""three""","1. Baseline Comparison and Computational Cost
    - Given the computational complexity of your method (detailed below), why weren't stronger baselines included? At minimum, n-shot baselines with comparable computational cost should be included.
    - How do you justify this overhead compared to simpler approaches?
    - Why weren't comparisons made to other non-finetuning MT improvement methods mentioned in Section 2.1?

2. Limited Language Direction
    - The method only works with English as the source language. Given this limitation and the high computational cost, could you elaborate on why one should choose this method over simpler approaches like n-shot with larger n?
    - Have you explored why the method fails for non-English source languages?

3. Technical Claims and Methodology:
    - The paper describes LLMs' translation capability as ""inherent"", but research (e.g., https://aclanthology.org/2023.acl-long.524/) suggests it comes from parallel training data. Could you clarify this characterization?
    -How is your method fundamentally different from previous work? The related work section doesn't clearly distinguish your contributions.

4. Clarity Issues
    - Could you clarify the meaning of colors in Figure 1?
    - Several writing issues need addressing (e.g., ""To what sentences IntGradMT can be effective?"", missing spaces before parentheses, etc.)
    - Can you provide more precise technical details about the aggregation method and threshold determination?

5. Implementation Details:
    - How stable is the method across different interpolation models?
    - What is the variance in performance across multiple runs?
    - How were hyperparameters (like the number of start sentences) chosen?
    - It is likely that the reference-free QE model you're using does not work very well for low-resource languages. How do you inspect to what extent this is an issue?"
SmxM4POTBk,SmxM4POTBk,IntGrad MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Interpolation Guided Gradual MT,Reject,NCIle4ox0J,ICLR.cc/2025/Conference/Submission10951/Reviewer_97xN,"This paper addresses the topic of improving LLM-based machine translation through prompting. Specifically, the paper introduces IntGrad MT, which is a combination of sentence interpolation and gradual machine translation. The intuition is to start from translating a sentence that the model can translate well, and gradually interpolate that sentence into the desired source sentence using an LLM. The LLM used for translation, then, is prompted to translate each of the interpolated sentences, given its previous translations as a guide.

Update after discussion period: The revised version of the paper has partially addressed my concerns about baselines and evaluation metrics, so I am raising my score.","The proposed IntGrad method is a very interesting and novel idea. It is unlike any prior work that I am familiar with, and (with some improvements to the experiments) I could see opening up new avenues for research into LLM prompting.","1. **Evaluation metrics**. The proposed model is evaluated using mostly xCOMET. I have two potential concerns here: a) Although xCOMET correlates well with human judgments on *high-resource* languages, it is much less reliable on *low-resource* languages. In particular, the DA data used to train xCOMET only covers EN-ZH and EN-DE of the evaluated language pairs. b) The models partially develop on COMET, as it is used in output selection (CometKiwi but the base model and training data is largely the same) and sentence pool creation. This raises the concern that gains could be an artifact of tuning towards the metric, and not necessarily due to better translation. A simple fix could be to use a model-free metric (which could be more language-agnostic) as a point of comparison (e.g., chrF), and to include even a small amount of human evaluation to corroborate the results.

2. **Weak baselines**. The paper (L16) points out that several methods have been proposed for improving LLM low-resource translation including few-shot translation and using dictionaries and grammar books, but does not do any comparison to using dictionaries and grammars. In addition, for low-resource translation neural machine translation is a strong baseline and should be included, but no rationale is given for why the paper doesn't compare to NMT or why LLM-based translation would be preferable (it's not cheaper or faster, either). The zero-shot and three-shot baselines are not particularly fair either: given how compute-intensive the proposed method is, it would be fair to use more shots (e.g., 5 or 10) or chain-of-thought. In addition, per L265, ""we use the same start sentences for interpolation as in the few-shot examples"", which also seems unfair to few-shot, as the extra compute and iterations used in the proposed IntGrad could be similarly used to find good/appropriate few-shot examples.

3. **Construction of the sentence pool**. The construction of the sentence pool seems it would have a huge effect on the final performance, and it seems underexplored in this paper. In particular, I am curious what the impact of the size and quality of the sentence pool is. Also, the sentence pool used in the paper is the development set corresponding to the test sets, so it is quite well-matched to the test set in domain, style, and format. This is not a realistic setup, and it would be good to see experiments showing what happens when you don't know the domain in advance and have to use a generic sentence pool, as in a real-world application.

4. **Practicality of the method**. I would have liked to see some discussion of translation time and cost, beyond a cursory mention in limitations (L534 ""introduces some computational overhead""). What computational overhead, what is the increase in cost, and how much longer do translations take? Is it possible to batch the translations? These are relevant questions to be explored in at least some depth, given that I think it's quite a large increase.","1. Although you mention in limitations that ﻿﻿﻿""sentence interpolation did not perform well in languages other than English"" (L537), I'd be curious to see examples of others, and some evaluation results (even presented as negative results). I think that would be informative for future work.

2. L252: ""We first utilize the dev split of the dataset to create start sentence pool. During evaluation, to test the effect of the output selection strategy, we selected 10% of the test portion of the dataset to set the DA score threshold. The remaining 90% is used to assess the overall performance on the dataset"": since these are established dev/test splits, I recommend using a 10% subset of the *dev* set to set the DA score threshold. Maintaining the test split as it was originally released and evaluating on the whole thing will help reproducibility.

3. Do you think you could eventually interpolate from one language to another? E.g., if a model translates English->Hindi well but German->Hindi poorly, do you think you could gradually interpolate English source with German source to eventually get a German sentence? This is not essential for the current paper, of course, but I'm curious to hear your intuition on this."
yhmVrA8W0v,yhmVrA8W0v,The Convergence of Second-Order Sampling Methods for Diffusion Models,Reject,X54tkOLGhI,ICLR.cc/2025/Conference/Submission10937/Reviewer_FNqr,"Diffusion models (DMs) learn the score functions associated with a diffusion process, and use the learned scores to simulate an SDE corresponding to the backward process. While samples can be simulated by either an ODE or an SDE, SDE samplers are practically superior in terms of sample diversity and quality. This paper sets out to investigate second-order SDE solvers for the backward SDE, and concludes that second-order solver is preferable to the standard first-order discretization methods in terms of convergence with respect to the Kullback-Leibler divergence. 

The paper mainly investigates two (approximate) second-order SDE solvers, SDE-DPM-2 and Runge-Kutta 2 methods, and compares the convergence results to first order SDE solvers such as EI. The paper presents theorems that suggest that SDE-DPM 2 is more preferable to RK-2 from the perspective of KL-divergence, mainly due to the added discretization error. 

While the paper mainly focuses on the VP-DMs based on the Ornstein-Uhlenbeck forward process, the main result also applies to the variance-exploding forward process as well, shedding light on the applicability of solvers on other forward processes.","The paper presents convergence results of second-order SDE solvers for diffusion models, which is very relevant to current research in diffusion modeling given the empirical usefulness of SDE-based simulation of the backward process, and the open question of suitable discretization techniques in this context. The paper gives a theoretical foundation on the application of high-order SDE solvers in diffusion modeling, which motivates further research on suitable solvers for diffusion generative modeling. 

- Within the scope of the paper, it presents a compelling argument in favor of SDE-DPM-2 over RK-2 or first-order discretization methods for the practical simulation of samples. I find the insight of ""not second-order solvers are equal"" overall interesting and helpful. 
- The paper also illustrates that the convergence bounds empirically with Gaussian mixture examples. 
- The theoretical results are quite general as they apply to both VP and VE diffusion models.","While I have an overall positive outlook on the paper, I think the paper's overall organization seems confusing: the paper presents the main theorems and some empirical results, then jumps back to a sketch of the proof and how the theory works for the VE-type diffusion models. In my opinion, presenting the paper as theorems on SDE-DPM-2 and RK-2, proof sketch, discussion on VE and then experiments seems like more logical progression of the narrative. 

There are a number minor issues in terms of the paper's presentation. Here is a list I have found: 
- Many discretization methods mentioned in the paper are known only as acronyms without mentioning what the acronyms are. 
- The mentions of $x_k$ in assumption should be $x_{t_k}$ in equations such as the one in Assumption 2, eqs. 11 and 13.
- The use of partial derivatives w.r.t. $x_{t_k}$ seems confusing. I assume it means the Jacobian matrix. Perhaps the authors can explicitly denote a notation to describe the Jacobian matrix for clarity. 
- While it is useful to see that second-order SDE solvers makes improvements empirically, Table 1 presents quite little added information other than a somewhat vague empirical confirmation that SDE-DPM-2 does have empirical value, which has already been demonstrated by Lu et al. (2022b).","- The paper presents the KL convergence results about the VP- and VE-types of diffusions models separately. Could you briefly explain how different types of forward processes affect your proof?
- I find panel (b) of Figure 1 quite helpful as an illustration between theory and practice, but the paper also presents convergence results with respect to RK-2. What would the theoretical bounds of RK-2 look like on that graph?
- Are equations 11 and 13 identical? If so, why?"
yhmVrA8W0v,yhmVrA8W0v,The Convergence of Second-Order Sampling Methods for Diffusion Models,Reject,5kcwaBYkp8,ICLR.cc/2025/Conference/Submission10937/Reviewer_XcvQ,"This paper studies the convergence properties of score-based diffusion models with a second-order discretization scheme called SDE-DPM-2, which improves the complexity over a first order exponential intergation scheme. Interestingly the result for SDE-DPM-2 is also stronger than the more widely used RK-2 scheme.","- The authors address a question of significant interest in the diffusion model literature, namely which discretization schemes are most sample efficient at inference time.
- The paper gives some additional theoretical support to the observation that higher order schemes can be important for sample complexity and differentiates between subtleties, such as the additional approximation in the linear term of the SDE.  
- Experiments show a modest improvement of CIFAR-10 FID with small numbers of sampling steps and improved convergence with discretization fineness using SDE-DPM-2 over RK-2.","- There is no comparison of the computational cost of RK-2 vs DPM-SDE-2 vs EI
- I felt the authors should have more clearly delineated their contributions relative to Chen 2023, which they follow closely.
- A number of the assumptions are quite strong. For example, the expectation of the second time derivative of the score is assumed to have a magnitude upper bounded by some time-independent constant. In practice, it is often the case that the score changes in quite a singular fashion near $t=0$.","- I think the authors might have a mistake in assumption 4, because I don't see them using the operator $\\nabla^3$ anywhere. 
- Can the authors add error bars to the table of the FID scores? At present, I don't feel that these illustrate their point particularly well."
yhmVrA8W0v,yhmVrA8W0v,The Convergence of Second-Order Sampling Methods for Diffusion Models,Reject,RhjBGYoZWc,ICLR.cc/2025/Conference/Submission10937/Reviewer_xXao,"This paper analyzes the convergence of the higher-order discretization method (SDE-DPM-2). Under some smoothness condition as well as score estimation error and high oder estimation error, a sampling complexity at the order of O(1/epsilon) is established to ensure the KL divergence smaller than epsilon^2. In comparison, the complexity of second-order Runge–Kutta method (RK-2) scales as O(1/epsilon^2).","This paper analyzes the convergence of the higher-order discretization method (SDE-DPM-2). Under some smoothness condition as well as score estimation error and high oder estimation error, a sampling complexity at the order of O(1/epsilon) is established to ensure the KL divergence smaller than epsilon^2. In comparison, the complexity of second-order Runge–Kutta method (RK-2) scales as O(1/epsilon^2).","Although the following paper is posted after your submission, there maybe exist some conflict messages between your paper and this work: you said that RK-2 is less efficient, while this work claimed that RK-2 is provably fast.
Wu, Y., Chen, Y., and Wei, Y. Stochastic runge-kutta methods: Provable acceleration of diffusion models.

Li et al. (2024) also provided a sampling complexity of O(1/epsilon) under KL divergence and a better complexity of O(1/sqrt(epsilon)) for TV, which may reduce the theoretical contribution of this work and was not discussed here.  

There exists some other convergence analysis for high-order sampling of diffusion models. It seems that their rates are better than yours, but such comparisons are missed here.
Huang, D. Z., Huang, J., and Lin, Z. Convergence analysis of probability flow ODE for score-based generative models.
Huang, X., Zou, D., Dong, H., Zhang, Y., Ma, Y.-A., and Zhang, T. Reverse transition kernel: A flexible framework to accelerate diffusion inference.",No question.
yhmVrA8W0v,yhmVrA8W0v,The Convergence of Second-Order Sampling Methods for Diffusion Models,Reject,cAIl8XMZtL,ICLR.cc/2025/Conference/Submission10937/Reviewer_xHXz,"The paper investigates the convergence of the second-order discretization method (SDE-DPM-2). Given an $O(\\epsilon^2)$ $L^2$-accurate score estimation, the paper demonstrates that the sampling complexity of SDE-DPM-2 is $O(1/\\epsilon)$ instead of that of the exponential integrator scheme, which is $O(1/\\epsilon^2)$. Furthermore, the paper extends the analysis to the Runge-Kutta-2 (RK-2) method, proving that SDE-DPM-2 exhibits superior efficiency compared to RK-2.","- The paper studies the SDE-DPM-2 scheme for the inference of diffusion models and improves the sample complexity from $O(1/\\epsilon^2)$ to $O(1/\\epsilon)$.
- The mathematical proof looks sound to me.
- Several experiments are conducted to validate the theoretical findings.","- The assumptions appear overly strong and artificial to me. Unlike the conventional assumption that the neural network score function $s(t, \\cdot)$ is approximately $\\epsilon^2$ close to the true score function $\\nabla \\log p_t$, Assumption 2 is, to my understanding, contingent upon the loss function employed in training diffusion models. Consequently, it is not feasible to guarantee or even evaluate this assumption for diffusion models.
- I recommend redrawing Figure 1 in logarithmic scale to corroborate the theoretical findings.
- The proof appears to follow the approach outlined in [1]. I believe it is possible to enhance the sample complexity in the data dimension from $O(d^{3/2})$ to $O(d)$ by drawing techniques inspired by the state-of-the-art results presented in [2].
- I believe this paper lacks a comprehensive literature review. It fails to cite closely related empirical studies [3] and theoretical studies [4, 5], as well as the recent advancements in accelerating diffusion models, such as knowledge distillation [6], consistency models [7], adaptive stepsizes [8], parallel sampling [9], randomized midpoint [10], among others.

[1] Chen, Hongrui, Holden Lee, and Jianfeng Lu. “Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions.” International Conference on Machine Learning. PMLR, 2023.

[2] Benton, Joe, et al. “Nearly d-linear convergence bounds for diffusion models via stochastic localization.” (2024).

[3] Dockhorn, Tim, Arash Vahdat, and Karsten Kreis. ""Genie: Higher-order denoising diffusion solvers."" Advances in Neural Information Processing Systems 35 (2022): 30150-30166.

[4] Wu, Yuchen, Yuxin Chen, and Yuting Wei. “Stochastic Runge-Kutta Methods: Provable Acceleration of Diffusion Models.” arXiv preprint arXiv:2410.04760 (2024).

[5] Li, Xuechen, et al. “Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond.” Advances in Neural Information Processing Systems 32 (2019).

[6] Luhman, Eric, and Troy Luhman. “Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed.” arXiv preprint arXiv:2101.02388 (2021).

[7] Mei, Song, and Yuchen Wu. “Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models.” arXiv preprint arXiv:2309.11420 (2023).

[8] Jolicoeur-Martineau, Alexia, et al. “Gotta Go Fast When Generating Data with Score-Based Models.” arXiv preprint arXiv:2105.14080 (2021).

[9] Chen, Haoxuan, et al. “Accelerating Diffusion Models with Parallel Sampling: Inference at Sub-Linear Time Complexity.” arXiv preprint arXiv:2405.15986 (2024).

[10] Gupta, Shivam, Linda Cai, and Sitan Chen. ""Faster Diffusion-based Sampling with Randomized Midpoints: Sequential and Parallel."" arXiv preprint arXiv:2406.00924 (2024).","- Assumptions 3 and 4 are both bounds for the third-order derivative of $\\log p_t$. However, I firmly believe that temporal derivatives can be represented as spatial derivatives, thereby revealing fundamental properties of the data distribution, as shown in Equation (22) in [2]. Could you please clarify why Assumptions 3 and 4 are considered separate?
- If Assumption 2 is replaced with the corresponding assumption from [1], is the result for SDE-DPM-2 still valid? Is there any method to ensure the validity of this assumption during the training process?"
yhmVrA8W0v,yhmVrA8W0v,The Convergence of Second-Order Sampling Methods for Diffusion Models,Reject,erQNsr1vdj,ICLR.cc/2025/Conference/Submission10937/Reviewer_4rK4,"This paper investigates the convergence properties of a second-order discretization method, SDE-DPM-2, for diffusion models. The main result demonstrates that SDE-DPM-2 achieves an improved $O(1/\\epsilon)$ convergence rate to obtain an $O(\\epsilon^2)$ error in KL divergence, surpassing the performance of existing EI discretization methods. Additionally, using similar proof techniques, the paper shows that another widely used second-order method, Runge-Kutta, does not attain this level of convergence. Further analysis extends these results to the VE SDE, achieving a comparable convergence rate.","1. The writing is very clear. It provides both theoretical and empirical comparisons with the most related papers. 

2. It proves a better convergence rate for a second-order sampling method. 

3. It also extends the setting to VE SDEs, showing that the analysis framework can be further generalized.","(1) The biggest weakness of this paper is the stringent assumptions. In Assumption 2, this paper assumes the Taylor expansion is accurate, while in most of the previous works for SDE analysis, only the value accuracy is needed. I have seen similar assumptions in [1], which assume the closeness of the Jacobian matrix with respect to $x$ in dealing with ODE analysis. They also showed that such an assumption is not required for SDE. However, Assumption 2 in this paper, though for SDE analysis, is even stronger than that, because it assumes that the time-derivative is also close.

Moreover, Assumptions 3 and 4 are also very strong. When t is close to 0, the score function will get close to the gradient of the probability density function of the data distribution. Such boundness assumptions thus will require the smoothness of the data distribution. As is shown in Appendix B, it can only hold when the data distribution is a Gaussian mixture. This diverges from many useful data distributions, especially when the data is constrained on a low-dimension manifold. As a result, I think more discussions are required to verify the reasonability of these assumptions.

(2) The writing of the paper is a little inconsistent. For example, in equation (6), the first-order derivative is approximated with the value of the score function, while in equation (13) it becomes the partial derivative concerning t and x. Moreover, the notation used here, defined in Line 204 is not standard and very confusing. In Line 201, it says “The difference between the EI and SDE-DPM-2 schemes lies in the approximation of the score function”, while in Line 401, it says “The key difference between EI and SDE-DPM-2 lies in the update scheme at each time interval” It is unclear whether they have the same meaning or not.

(3) The description of the contribution is a little bit inaccurate. It claims that SDE-DPM-2 is more efficient than Runge Kutta. However, no guarantee has been given (Corollary 3.3 only shows that the method used in this paper cannot provide a better guarantee for Runge Kutta). It is possible that there exists an analysis of Runge Kutta that can achieve better results.  As is shown in the experiment, the performance of Runge Kutta and SDE-DPM-2 is similar, both better than first-order methods. Thus, the claim seems a little strange to me. Moreover, this paper says that for VE SDE, the convergence is aligned with VP SDE. However, the remark under Corollary 5.1 shows that it only works when overlooking the initial error, which is the key difficulty of VE SDE. This point should also be emphasized in the introduction.

(4) The paper is not self-contained. For example, the proof of Proposition 4.2, directly refers to Chen et al 2023a without any explanation. In my opinion, the argument here is far from trivial and should not be omitted.

----
[1] Li et al. 2024 Towards Non-Asymptotic Convergence for Diffusion-based Generative Models ICLR2024",Is it possible to get a better convergence rate for Runge Kutta methods?
TSrhLq5hSA,TSrhLq5hSA,On a Hidden Property in Computational Imaging,Reject,JywTHjVOMf,ICLR.cc/2025/Conference/Submission10545/Reviewer_pMMZ,"This paper considers a hidden property in computational imaging, demonstrated across Full Waveform Inversion (FWI), Computed Tomography (CT), and Electromagnetic (EM) inversion tasks, which reveals that they share a common set of one-way wave equations in the latent space. The authors leverage understanding of this shared latent representation to achieve accurate reconstructions and predictions across imaging tasks, achieving similar or better performance than existing methods but with fewer parameters.",- Results of experiments on computational imaging tasks show simliar or improved performance with fewer model parameters.,"- The work draws very heavily on two prior works by Chen et al. 2023 (a,b).  As far as I can tell neither of these works have been accepted by peer-review venues.
- There is no theoretical motivation for the hidden wave equations, as far as I can tell, although I did not review the cited papers.",- Can the authors speculate about similarity between the computational imaging tasks considered that might give rise to the observed phenomenon?  Or do they believe this phenomenon should existing for all computational imaging tasks?
TSrhLq5hSA,TSrhLq5hSA,On a Hidden Property in Computational Imaging,Reject,BqpBP9veb4,ICLR.cc/2025/Conference/Submission10545/Reviewer_bUtp,"The paper considers three inverse problems (namely, FWI, CT reconstruction, and EM inversion). The authors propose a new architecture for the reconstruction operator by exploiting the relationship between the latent representation of the measured data and the parameters to be reconstructed.  In particular, the construction of the reconstruction net exploits the fact that the data and the parameters, in their latent space, are governed by the same wave equation with linearly correlated initial conditions. Numerical experiments are conducted to demonstrate the performance of the new architecture as compared with a couple of baseline methods for the three inverse problems mentioned above.",The paper proposes an interesting idea to extend FINOLA (first-order norm+linear autoregressive modeling) to consider both the data space and the parameter space. This can potentially lead to useful architectures for various other inverse problems.,"1. In my opinion, the paper’s main contribution is to propose a new architecture, and not establish any fundamental “hidden property” in computational imaging problems (contrary to what the abstract and the introduction attempt to portray). This also makes the overall presentation somewhat misleading and difficult to follow.

2. The numerical experiments do not provide strong evidence in favor of the proposed method. The baseline methods for comparison (e.g., SIRT and InversionNet for CT) are chosen somewhat arbitrarily. State-of-the-art deep learning methods for CT (such as learned primal-dual by Adler and Oktem) are not used for comparison, making it difficult to judge the empirical superiority of the new architecture.","The overall exposition is somewhat difficult to follow, primarily because of the lack of clarity about the paper’s main contributions and the usage of non-standard terminologies in comparison with the inverse problems literature. For instance, the word “modality” is used to refer to the parameter and data spaces, which can lead to confusion. Some specific comments are below:

- Abstract: “where both modalities are governed by complex mathematical equations”: This is a rather vague statement to use in the abstract. What specific governing equations are being referred to here? What latent space is talked about here? The actual contributions or the significance of the proposed method do not come out clearly from the abstract.  

- Figure 1 caption (and other places in the introduction): The phrase “latent space” is used frequently without much explanation about what exactly it refers to. 

- Page 2: “Whether an elegant mathematical relationship exists in the latent space, akin to that
in the original space?”: I don’t think it is a precise, well-formulated research question. The mathematical relationship between the parameter space and the data space is determined by the specific imaging modality, whereas the relationship in the “latent space” is empirically enforced (and no such relationships are shown to exist theoretically). 

- Page 2: “...typically with a bottleneck in the network, they lack a deeper understanding of these latent representations.”: I don’t see any such “deeper understanding” (which in itself is somewhat vague and subjective) being uncovered in this paper either. 

- The phrase “target property” is used in several places in the paper. Could you please explain what this means?

- Section 2.1: It might be good to make the descriptions of the inverse problems (FWI, CT, and EM inversion) more concise. 

- Page 6: “Difference with vanilla FINOLA”: This part needs to be rewritten. Currently, it gives the impression that the proposed method is capable of handling multi-modal data for reconstruction, which it isn’t. 

- Page 7: Architecture details: Do you use the same architecture for the reconstruction network for all three inverse problems considered? 

- CT experiments: There is no comparison with the unrolling-based techniques (such as learned primal-dual), which are known to yield state-of-the-art reconstruction performance. The choice of the baseline techniques is somewhat arbitrary and not well-motivated."
TSrhLq5hSA,TSrhLq5hSA,On a Hidden Property in Computational Imaging,Reject,iAxzy4MMsX,ICLR.cc/2025/Conference/Submission10545/Reviewer_Uz6Q,This paper present a framework for solving inverse problems like the full waveform inversion. The problem is tackled by projecting the data inside an embedding space which linearize the initial condition. Furthermore the proposed method combined a reconstruction and a inversion target and the two embedding spaces are linked using wave equations. Such result is of large interest for the inverse problem community. The framework shows promising results against to state-of-art methods.,"The paper presents a very interesting framework for solving a class of inverse problems.

1. **Originality**: while this paper relies on previous recent research, the HINT framework is new. The multi-path FINOLA is clearly a novelty for this problematic. The hidden wave phenomenon is very intriguing and the underling properties seem pretty helpful to inverse. 
2. **Quality**: the method is well motivated and the context is clear. The link with previous work is done with the contribution clearly highlighted.
3. **Clarity**: the paper is easy to read and most of the components are described.
4. **Significance**: since inverse problems are an important class of problems in signal, imaging... such work can have a big impact on the community.","There few weakness is the article, they are minor but they impact my final score.

1. The class of inverse problems that can be considered is unclear. Do we have an idea of which problems involved an hidden wave phenomenon? Even an insight would be welcome.
2. The multi-path FINOLA need a better description. I don't see the ""multipath"" in the equations.
3. One small experiment to compare the method with classical framework (LASSO with wavelets...) would interesting to have a full idea on the effectiveness of the framework.","* I have a question on the class of inverse problems, do modalities like MRI or tomography enter in the framework?
* Please clarify the part about the multipath."
te2IdORabL,te2IdORabL,JPEG Inspired Deep Learning,Accept (Poster),oKSKBIEca6,ICLR.cc/2025/Conference/Submission10458/Reviewer_38Fe,"In this paper, the authors study the impact of JPEG compression to the performance of deep learning, and propose a JPEG-inspired deep learning framework. For that, they present a differentiable soft quantizer to train the JPEG layer. Experiments show that the proposed method increases the accuracy by almost 21%.","1. Overall, the paper is written clearly, and can be easily understood.
2. A variety of experiments are conducted to verify the effectiveness.
3. The used technique seems sound, although I don’t check it in detail.","1. I don’t think the problem studied in this paper is very important in the community. Usually, the images fed into deep learning have been precessed by the fixed JPGE compressor, and we don’t have the chance to modify the process like that in this pager, so the actual application value is limited. In addition, I see the paper is an improvement for the method in Yang 2021 (Entropy), which is not followed by many researchers.  
2. Now lots of papers have shown the vision transformer and large model are effective in many computer vision tasks, I don’t know whether the proposed method can adapt to these new networks. The authors should discuss that. 
3. Although a variety of experiments are conducted, the compared method is only one baseline, which can not show the effectiveness.",please see the weakness
te2IdORabL,te2IdORabL,JPEG Inspired Deep Learning,Accept (Poster),ZzJeNiywI3,ICLR.cc/2025/Conference/Submission10458/Reviewer_BiPm,This paper proposes jointly optimizing both JPEG quantization operation and a DNN to achieve greater effectiveness. A trainable JPEG compression layer with a novel differentiable soft quantizer are proposed. Extensive experiments validate the effectiveness of the proposed method.,"1. To make JPEG trainable, a differentiable soft quantizer is proposed. It works well with JPEG. Overall, this paper makes JPEG trainable which is significant contribution. Because many frameworks equipped with JPEG can be trained by the differentiable soft quantizer.
2. A novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer is proposed. Experiments show it can improve the accuracy significantly with only 128 parameters.
3. This paper enjoys a good writing.","1. It is better to make a comparison for the latency. The speed of the model is also important to report.
2. Only image classification is considered. The proposed method is better to be validated on more tasks, such as object detection and segementation. 
3. Hyperparameters are tuned differently on different datasets.","1. How to conduct datasets for experiments? For ImageNet-1k, the images are already compressed by JPEG. In traditional deep learning framework, the images are loaded and decoded by JPEG. How to insert the encoding operation in this framework?"
te2IdORabL,te2IdORabL,JPEG Inspired Deep Learning,Accept (Poster),jgXQjj4EEU,ICLR.cc/2025/Conference/Submission10458/Reviewer_FTRb,"This work proposes a new training framework for deep learning models, JPEG-DL, utilizing an image compression module to improve the model performance. To this end, a learnable JPEG-based image compression layer is introduced with a differentiable soft quantization method and is trained jointly with a main model. At test time, a compressed image from the compression layer is fed into the model. The experimental results show consistent performance improvements of existing classification models on various benchmarks with higher robustness on adversarial attacks.","- The approach of leveraging image compression to improve pure performance of a model is interesting.
- The paper is well-written and it is easy to follow.
- A variety of network architectures and datasets are used in the experiments.","Major concerns:
- A comparison to training with JPEG-based data augmentation is required to validate that the proposed method provides benefits beyond simple data augmentation using JPEG. 
- There is no baseline for the differentiable quantizer. For example, comparisons could be made with methods such as the straight-through estimator (i.e., using the identity function as a gradient function) or additive uniform noise [1].
- The baseline for the image preprocessing method for training is insufficient; only comparison results with the vanilla models are presented. For stronger persuasiveness, comparison results with other learnable or non-learnable preprocessing modules are needed.
- In L199, it is mentioned that the differentiable soft quantizer is adapted from Yang & Hamidi (2024), but it seems unclear what that exact referenced paper is. Is the patent (https://patents.google.com/patent/US11461646B2/en) the correct source?
- The analysis on the significant difference between performance improvements across different datasets is needed. For instance, in Table 2, what accounts for the notable performance improvement in fine-grained tasks (especially the Flowers dataset)? Additionally, why is there few performance gain in the ImageNet results in Table 3?

Minor concerns:
- The empirical study is limited to classification tasks. 
- In Table 4, the bits per pixel (bpp) is excessively high, making it incomparable to typical lossy compression methods. While empirically showing the possibility of compression, it does not seem particularly convincing.
- The proposed method requires additional computation for encoding and decoding of an image, but the time complexity is not investigated.
- A typo in L292 ""we fixe"".

[1] Ballé et al., End-to-end Optimized Image Compression, ICLR 2017.","- Is the target of the adversarial attack in Figure 3 the whole model including the JPEG layer?
- There appears to be more room for exploration regarding preprocessing modules for performance enhancement. For example, what if deep learning-based image compression models were used? What about a simple autoencoder?"
te2IdORabL,te2IdORabL,JPEG Inspired Deep Learning,Accept (Poster),yyRNRpaNDW,ICLR.cc/2025/Conference/Submission10458/Reviewer_2zCL,The paper introduces a trainable JPEG inspired layer to neural networks. The new layer performs the linear JPEG steps of RGB to YCbCr and DCT with the non-differentiable quantization step replaced with a learnable quantization proxy finishing with the linear IDCT and YCbCr to RGB.  The learnable quantization proxy uses a soft-max with a tunable parameter which controls how close the proxy matches true quantization. The quantization step size is a learnable parameter in this layer. The paper shows how including this layer as the first layer of neural network architectures can improve their accuracy on different tasks.,"Overall this is a very interesting paper with an unintuitive result. As the authors point out (ln 30) the conventional wisdom is that JPEG compression removes information from an image and should only hurt neural network accuracy. However as this paper, and some prior works, show that is not necessarily the case. This paper builds significantly on prior works by showing not only that JPEG compression can be mitigated, but that it can actually be a large component of a neural networks success and proposing a method for achieving this. The differential soft quantizer is something which may be useful in many different applications, potentially being a better option that the addition of noise or a straight-through gradient as it more accurately models the information loss. Finally, the results show a clear improvement when incorporating the method.","While the appendix was fairly comprehensive with additional results there are a few additional things that I would have liked to see. The first is that the paper only tests the JPEG layer as the first layer of the architecture, there could have been more experiments in layer placement that would have been really interesting to see. It also wasn't immediately clear to me how $\\alpha$ was being set in experiments, I understand that there is a derivation of $\\frac{\\partial}{\\partial\\alpha}$ but is that parameter actually trained and if so how was the gradient magnitude controlled? There is some discussion of this from ln 689 but it was a little unclear if $\\alpha$ was fixed or not. One thing missing from the JPEG step was chroma subsampling: another non-linearity. Was this considered? It would be fascinating to see if neural networks respond to missing color information similarly to humans.

Lastly, and maybe most importantly, there was little discussion of *why this couterintuitive results holds*. While many view JPEG as something incidental the core idea of JPEG to isolate important information based on frequency bands. My take on the results presented here is that the learnable layer is essentially filtering out information which is irrelevant for the networks task, but I would love to hear the authors take on it. Perhaps such analysis could lead to a more direct approach? (For example: a layer which only filters frequencies or which alters the color channels, etc.)","* Could we see even better results if the JPEG layer was included periodically? 
    * What if *all* nonlinearity was replaced with the JPEG layer?
* Please clarify how $\\alpha$ was used in experiments
* What about chroma subsampling?
* Why do the authors think this layer helps?

## Update After Discussion

After discussion with the authors I am raising my rating

The authors did a great job responding to the concerns of myself and fellow reviews and went above and beyond on additional experiments which strengthen the case for this paper quite a bit. I specifically have to call out the layer 1 non-linearity experiments that the authors conducted on a very short turnaround that shows additional gains for the JPEG layer. Given the impact of this result I have to conclude that there is indeed something fundamental about using a JPEG inspired layer as a non-linearity which could have repercussions on the broader field. As I stated in a comment, many view JPEG as something incidental; a specific way of storing images. But the core idea of JPEG is to re-weight frequency bands based on their importance. We know from several studies (Maiya et al. [1] for example) that such re-weighting affects neural networks much as it does humans and this paper gives an actionable method for capturing this phenomenon.

1. Maiya, Shishira R., et al. ""Unifying the Harmonic Analysis of Adversarial Attacks and Robustness."" BMVC. 2023."
mUMvr33FTu,mUMvr33FTu,CipherPrune:  Efficient and Scalable Private Transformer Inference,Accept (Poster),2YZX5MFhde,ICLR.cc/2025/Conference/Submission10397/Reviewer_Qed2,"CipherPrune effectively removes unnecessary tokens by combining input-adaptive pruning based on importance scores with progressive layer-wise pruning. For the remaining tokens, it applies high-order and low-order polynomials according to their importance to maximize computational efficiency. This approach achieved 6.1x and 10.6x faster performance on inputs of 128 and 512 tokens, respectively, significantly improving inference speed without accuracy loss.","- Achieving a substantial improvement in speed, with a 6.1x increase for 128 tokens and a 10.6x increase for 512 tokens, without any reduction in accuracy, is remarkable.

- The application of different polynomial orders based on importance scores, rather than merely pruning, is an impressive idea.

- The paper clearly differentiates itself from existing studies by detailing each step, explaining what previous research overlooked, why these aspects are necessary, and how their work contributes to these areas.

- All experimental settings are well-documented, providing extensive end-to-end results and comparisons across multiple models.

This paper demonstrates advancements over BOLT in every aspect. The time complexity of pruning and the resulting speed have both improved, with no difference in accuracy compared to BOLT. The authors provided a thorough analysis of BOLT, detailing the shortcomings of its ideas and the specific areas they have enhanced. In addition to building on ideas from previous studies, they introduced an innovative approach by applying polynomials of different orders based on importance scores. Comparisons with various papers are well-presented in the appendix, and it’s impressive that this technique can be applied to most inference models, including 2PC and 3PC models. I believe this is an excellent paper.","- Although the paper claims no accuracy reduction, there is a minor decrease of around 0.2%.

- In terms of pruning, the only prior work compared is BOLT. (I’m unsure if there are other relevant papers, so it may be possible that only BOLT exists in this context.)",See the weakness
mUMvr33FTu,mUMvr33FTu,CipherPrune:  Efficient and Scalable Private Transformer Inference,Accept (Poster),ihc6nL3TFL,ICLR.cc/2025/Conference/Submission10397/Reviewer_V3E7,"This paper introduces *CipherPrune*, a framework designed to mitigate the communication and latency overheads associated with both nonlinear and linear operations (FLOPs) in private inference for transformer-based models. CipherPrune introduces a token pruning method that assesses input token redundancy, progressively removing unimportant tokens layer by layer across the network, thereby leveraging task-level token redundancy to achieve efficiency without sacrificing predictive performance.  

Additionally, for less important tokens CipherPrune employs low-degree polynomial approximation for GELU and Softmax, in contrast with the higher degree polynomial approximation for critical input tokens, further enhancing efficiency without compromising security guarantees or predictive performance.","$\\bullet$ This work contributed to both the front-- protocol and network level-- and demonstrated the potential of a synergistic approach that achieves significant speedups and substantial communication reductions. 

$\\bullet$ The authors identify limitations in plaintext-based token pruning for private inference and introduce ciphertext-specific pruning techniques that avoid the need for decryption at each network layer, addressing a key bottleneck in efficiency. 

$\\bullet$ They effectively tackle the shortcomings of task-agnostic token pruning (such as word elimination) seen in prior work, demonstrating the advantages of a task-specific, layer-wise progressive token pruning strategy that optimizes both performance and efficiency. 

$\\bullet$ The paper is well-written and clearly outlines the challenges, the limitations of previous approaches, and the authors' contributions, making it easy to follow and appreciate the impact of this work.","$\\bullet$ Step 3 (Figure 4), prune and reduce, would increase the computational burden on the client side. A discussion on the resource burden (computation and memory) on the client should have been included, assuming the client has a low-end mobile device with limited compute and memory. 

$\\bullet$ This work does not address the overhead introduced by LayerNorm, which can add substantial communication and latency costs depending on the protocols used. For instance, in CipherGPT [3], LayerNorm accounts for **22%** of the total latency and communication overhead. 

$\\bullet$ One significant drawback of this paper is the absence of a detailed characterization of layerwise token redundancy. Without this, readers miss out on key insights into how redundancy varies across layers, which could have helped guide more efficient network design and optimization.

$\\bullet$ Overall, the CipherPrune framework appears quite complex to understand, implement, and deploy. Its successful application requires expertise in both protocol design and network architecture, making it challenging for those without specialized knowledge in these areas. 



## Correction in the draft

$\\bullet$ L#355 Pruning ratio larger than zero --> Pruning ratio greater than zero

$\\bullet$ Figure 4 depicts the 2PC threat model for Post-LN configuration, and in the FFN block diagram, one linear layer (after the GELU layer) is missing. 

$\\bullet$ Missing citations [1] and [2]

1.  Zimerman et al., Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption,  ICML 2024

2.  Luo et al. SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models, ACL 2024. 

3. Hou et al., CipherGPT: Secure Two-Party GPT Inference",See the weaknesses
mUMvr33FTu,mUMvr33FTu,CipherPrune:  Efficient and Scalable Private Transformer Inference,Accept (Poster),x1Y3qq5cdK,ICLR.cc/2025/Conference/Submission10397/Reviewer_Mztk,"This paper introduces an innovative framework designed to address the efficiency and scalability challenges of private Transformer inference. The proposed CipherPrune framework incorporates secure encrypted token pruning and polynomial reduction protocols to optimize performance. By progressively pruning unimportant tokens and assigning lower-degree polynomial approximations to less important tokens, CipherPrune achieves notable reductions in execution time and communication costs without compromising accuracy.","1. The combination of encrypted token pruning and polynomial reduction is novel and well-executed, contributing to reduce PI costs and latency while maintaining model performance.

2. The proposed CipherPrune demonstrates substantial improvements in runtime and communication efficiency.","As token pruning may be less effective in tasks that rely heavily on retaining comprehensive token-level information, such as machine translation, I suggest that the authors explicitly outline which applications are most suitable for this method and where its benefits may be more limited. This clarification would enhance the practical understanding and applicability of the work.","Can you specify which types of NLP tasks or applications are most suitable for the CipherPrune framework, and which tasks may see limited benefits from the proposed token pruning approach?"
mUMvr33FTu,mUMvr33FTu,CipherPrune:  Efficient and Scalable Private Transformer Inference,Accept (Poster),cZBrZbgUKT,ICLR.cc/2025/Conference/Submission10397/Reviewer_Xg5m,"The paper introduces **CipherPrune**, a framework designed to enhance the efficiency and scalability of private inference in Transformer models, particularly BERT and GPT2, by implementing secure token pruning and polynomial reduction protocols. It addresses challenges related to runtime overhead and scalability by utilizing cryptographic techniques such as correlated oblivious transfer and secure multi-party computation, achieving significant speed improvements (up to 18.2x faster than existing methods) without compromising accuracy. Additionally, the framework incorporates crypto-aware fine-tuning to optimize pruning and approximation thresholds during training, ensuring efficient inference while maintaining data privacy.","- **Hybrid Cryptographic Approach**: The paper introduces a hybrid framework that combines homomorphic encryption for linear operations and secure multi-party computation (MPC) for non-linear operations, significantly improving the efficiency and scalability of private inference in Transformer models like BERT and GPT2.

- **Secure Token Pruning Protocol**: CipherPrune implements a secure token pruning protocol that evaluates token importance against a learned threshold, allowing for efficient pruning of unimportant tokens while preserving data privacy during inference.

- **Crypto-aware Fine-tuning**: The framework incorporates crypto-aware fine-tuning to optimize pruning and approximation thresholds during training, ensuring that the model maintains high performance and accuracy while facilitating efficient private inference.","I have a concern about the communication cost comparison between SoftMax and GELU here. 

From the manuscript, the pruned tokens are selected after attention so that the communication from SoftMax here is still significant because you need to do compare, multiply, and etc.

How does the communication cost from SoftMax compared to one that GELU introduce?",see weakness
vEtDApqkNR,vEtDApqkNR,MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting,Reject,aFbmaLgxxC,ICLR.cc/2025/Conference/Submission10330/Reviewer_8TNh,"This paper introduce MambaTS, a new time series forecasting model based on selective state space models. In order to tackle multivariate forecasting, the timeseries patches of each variable is unrolled in a certain order to form a single sequence. One key innovation of the paper is a method for estimating the causal relationship between variables during training via random walk without return.",The paper propose a strategy to apply Mamba to to multivariate ts forecasting and achieves empirical result comparable to SOTA.,"1. The proof in Proposition 2 does not make sense to me. I am not sure the whole concept of random walk on a casual graph with certain cost is well defined in the paper.
2. The proposed method claims to leverage the causal dependency between the variables and thus is more suitable in the multivariate setting. However, it does not seems to have a large advantage over chanel independent PatchTST, which is univariate forecasting method.","I don't see why a random permutation is equivalent to a random walk. Line 324 says ""K − 1 transition tuples ${(v_1, v_2),(v_2, v_3), · · ·(v_{K−1}, v_K)}$ are derived"". I wonder what prevent the authors from deriving K(K-1)/2 tuples, so that each $(v_i, v_j), i<j$ is included?"
vEtDApqkNR,vEtDApqkNR,MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting,Reject,6MNQMvilN8,ICLR.cc/2025/Conference/Submission10330/Reviewer_zjn1,"This paper introduces MambaTS, an architecture for long-term time series forecasting that models global dependencies efficiently with a linear scan, avoiding the computational challenges of self-attention. The Variable-Aware Scan along Time (VAST) mechanism dynamically infers causal relationships among variables using random walks and determines an optimal scanning order through heuristic path decoding. This design achieves scalability and adaptability, particularly for complex, high-dimensional datasets with unknown causal structures.","- MambaTS reduces computational complexity from quadratic O(K^2) to linear O(K) by leveraging a topologically ordered linear scan, making it suitable for high-dimensional time series data.
- VAST enhances adaptability by inferring causal relationships in the absence of explicit causal graphs, using random walks to approximate dependencies and mitigate the need for exhaustive pairwise calculations.","Reliance on heuristic optimization for scanning order yields sub-optimality:
- The variable-aware san along time (VAST) employs the asymmetric traveling salesman problem (ATSP) to determine the optimal scanning order, relying on heuristics like simulated annealing to address its NP-hard nature. Although heuristics provide feasible solutions, this dependency introduces inconsistency, as different approximations may affect the accuracy of variable ordering (in case of complex, dense inter-variable connections)
- Extra experiments on alternative heuristic approaches such as genetic algorithms (that are powerful in navigating NP-hard problems) could reveal a more stable and efficient approach. In the same vein, additional experiments measure how different heuristic methods affect the resulting scanning order and, subsequently, forecasting accuracy. This can help users determine if any heuristic consistently produces a favorable scanning order.

Convergence guarantee or confidence interval is not covered in causal estimation which lacks usability:
- Proposition 2 lacks formal guarantees for convergence speed, raising questions about the robustness of causality inference in finite settings. Without clear bounds on the number of walks required, the approach may yield only approximate estimates, especially when practical constraints limit the number of walks. This limitation affects the consistency and reproducibility of causal estimation results, as reliance on empirical averaging may not ensure reliable causal inference across varied dataset structures. 
- It might be helpful to introduce a stopping rule based on convergence metrics (e.g. average change in transition costs), or introduce confidence intervals on causality estimates to users to give insight into the stability of causal inferences under finite computational budgets, where both suggestions seem to be beyond the scope of this study. I hope the authors consider usability in the future works.","The paper offers a well-reasoned and innovative approach to time series forecasting, with theoretically sound propositions and a practical methodology that balances computational efficiency with modeling accuracy. While the heuristic reliance on VAST and scalability issues in dense graphs present limitations, the model’s strengths in efficiency, adaptability, and architectural design make it a valuable contribution. MambaTS is especially promising for high-dimensional and complex time series data, though further work is recommended to address heuristic dependency and enhance robustness in varied causal structures. Overall, it's a solid and innovative work on time-series forecasting, effectively incorporating causality in a computationally efficient and scalable manner."
vEtDApqkNR,vEtDApqkNR,MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting,Reject,H6X3oU0rqL,ICLR.cc/2025/Conference/Submission10330/Reviewer_dR3c,"This paper proposes MambaTS, a selective state-space model for long-term time series forecasting (LTSF) that addresses the computational limitations of Transformers by leveraging causal relationships across variables and time with a single linear scan.","1. LTSF presents a compelling and complex challenge.
2. The experiments are thorough but still lack some essential details.","1. **Lack of Experimental Details:** Important implementation details are missing, such as patch length, the value of beta in Equation 7, and whether the random walk on variables is conducted K-1 times per epoch (meaning  k-1 more training time cost than one epoch).
2. **Efficiency Concerns:** Theoretical complexity analysis in Table 5 lacks practical runtime comparisons. Given that MambaTS requires K-1 iterations to estimate causal relationships, its efficiency is questionable.
3. **Incomplete Ablation Studies:** The paper introduces the TMB (with dropout replacing the original convolution), but no ablation study compares TMB and the original Mamba block, leaving its impact on performance unclear.
4. **Limited Explanation in Variable-Aware Scanning:** Section 5.2 does not clearly explain whether K-1 transitions are sufficient to estimate all variable orders, or if consistency (e.g., v1 always preceding vk) is assumed.
5. **Limited Benchmarking:** Two commonly used datasets (ETTh1, ETTm1) are missing, which reduces the generalizability of the results.
6. **Code Availability:** No code is provided, limiting reproducibility.
7. **Unpersuasive SOTA Claims:** Results in Table 2 are questionable. For example, our reimplementation of PatchTST (using official configurations) achieved better results than the reported MambaTS performance on ETTm2 (input length 720). Specifically:
   - ETTm2_720_96.log: 0.1632, 0.2555
   - ETTm2_720_192.log: 0.2167, 0.2942
   - ETTm2_720_336.log: 0.2679, 0.3282
   - ETTm2_720_720.log: 0.3521, 0.3798

   These results suggest MambaTS may not definitively outperform all baselines, especially as no code is available for direct comparison.
8. **Notation Issue:** The meaning of \\( I \\) in Equation 7 is unclear.
9. **Inference Process Detail:** Section 5.2 lacks details on the inference process for Variable-Aware Scan Along Time.","1. How is the patch length chosen, and does it vary across datasets?
2. Could the authors clarify the random walk process and the number of epochs used in variable scanning?
3. Why were benchmarks ETTh1 and ETTm1 not included?"
vEtDApqkNR,vEtDApqkNR,MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting,Reject,NgpW0NtEAB,ICLR.cc/2025/Conference/Submission10330/Reviewer_YZLZ,"The paper introduces MambaTS, an improved selective state space model for long-term time series forecasting. The model leverages a novel method for variable-aware scanning along time (VAST) to model global dependencies in a time series with variable missing rates and different intervals. By utilizing a combination of causal graphs and shortest path solutions, MambaTS addresses the limitations of previous Transformer-based models which often struggle with high computational costs and inefficient handling of long-range dependencies.","1. The introduction of VAST and the use of causal graphs for modeling dependencies offers a unique solution to efficiently process long-term dependencies in time series data with linear complexity.
2. The model is tested across various public datasets, demonstrating superior performance compared to existing state-of-the-art models. This not only validates the efficacy of MambaTS but also showcases its versatility in handling different types of time series data.
3. MambaTS significantly reduces the computational cost traditionally associated with long-range forecasting models like Transformers by avoiding the quadratic complexity of the self-attention mechanism.","1. The effectiveness of the model heavily depends on the accuracy of the causal graphs. Incorrect or incomplete causal relationships can lead to suboptimal forecasting results, which the paper does not extensively address in terms of robustness against poor graph structure
2. While the model shows high efficiency and effectiveness, the paper lacks a thorough discussion on scalability, especially in scenarios with exceedingly large datasets or highly complex variable relationships.
3. There is a need for a comparison of the model’s performance with other SOTA methods, such as Onefitsall, TimeLLM etc.",See weakness.
vEtDApqkNR,vEtDApqkNR,MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting,Reject,qP5b7lknfI,ICLR.cc/2025/Conference/Submission10330/Reviewer_pAtf,"This paper presents MambaTS, an LTSF model addressing Transformers' self-attention complexity and bias by using causal relationships for global dependency modeling. The author designs variable-aware scan along time to get variable causal relationships and also Temporal Mamba Block to avoid causal convolution. The experimental results show that MambaTS outperforms several state-of-the-art models.","1. It is interesting to see another new work on mamba for time series forecasting. In my view, some properties of mamba are fit for time series and it's an interesting direction to explore more. 
2. The authors propose several designs to tailor mamba for time series application, which has its merits.","1. The clarity of the paper needs to be improved. In some parts, I cannot fully understand, such as the cost of a random walk without return. Also what is the cost from node i to node j, and how we can get this in the first iteration
2. Some claims have no support/evidence. For example, the authors mention that the random walk without return is a promising approach to estimate causal links. I would like to know the reason, e.g., any citations/proofs. 
3. The experiments seem not comprehensive. The authors only compare MambaTS with 7 baselines. There are a few more after iTransformer, which are worth to be compared. E.g., ModernTCN [1], UniTST [2], TSLANet [3]. 


References:

[1] ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis. 

[2] UniTST: Effectively Modeling Inter-Series and Intra-Series Dependencies for Multivariate Time Series Forecasting. 

[3] TSLANet: Rethinking Transformers for Time Series Representation Learning","1. In proposition 1, the assumption is that the causal graph exists. What if it doesn't exist? And is there any support on the random walk without return is a promising approach to estimate causal links? 
2. What is the definition of cost C and how we can get/set it empirically (e.g., the cost from node i to node j)?
3. Proposition 2 indicates that theoretically the causal relationships can be estimated without infinite random walks with return. I would like to ask how many walks are required empirically. And also the time spent? 
4. In Eq (6), how we can get the p^{(0)}? 
5. In my view, another major difference between MambaTS and iTransformer is that MambaTS model the dependencies on both time and variables, while iTransformer mainly on variables. I would like to know how this contributes to eventual performance. Because UniTST [1] is also modeling the dependencies on both time and variables dimensions, but with Transformer architecture. How does MambaTS compare with UniTST? 

Reference:

[1] UniTST: Effectively Modeling Inter-Series and Intra-Series Dependencies for Multivariate Time Series Forecasting."
XFpb3T5Zc9,XFpb3T5Zc9,iFedDR: Auto-Tuning Local Computation with Inexact Douglas-Rachford Splitting in Federated Learning,Reject,opH1o2SvhN,ICLR.cc/2025/Conference/Submission10234/Reviewer_pQaJ,"The paper proposes a way to automatically adjust the local computation while preserving the convergence guarantees. In particular, it focuses on a new algorithm, iFedDR, which is an inexact version of the Douglas-Rachford approach (FedDR), which is proposed as a correct way to employ the proximal operator in the FL setting. In particular, iFedDR is an extension of previous work that incorporates an adaptive stepsize and an extragradient correction step to allow for the relaxed condition of relative inexactness. The paper focuses on providing convergence guarantees of iFedDR and tests the performance of the new approach in experiments.","The paper is well-written, and the main contributions are clear. To the best of my knowledge, this is one of the first papers to aim to automate the number of local updates needed when clients use proximal updates. 

I also like the fact that the analysis was done from the monotone inclusion setup that allows direct extensions of the ideas to min-max problems and m-player games. The convergence guarantees are proven for a much more general method, iPPPA, which might be interesting in its own right.

The statement of the theorems is as expected, and from a pass on the proofs in the appendix, they look correct. I did not carefully check all the details but all steps are presented clearly and make the read of them easy.","I believe the paper would be improved if a table with a comparison with closely related works is presented. How the final convergence guarantees of the paper are related to the existing results? Can the main theorems capture previous convergence analysis as a special case?

I find the split of sections a bit confusing. There is Theorem 3.1 in section 3, but then later, there is a full section 5 devoted to Convergence analysis. Can the authors comment on that? Why not simply have all theorems related to convergence guarantees in one section?
In my opinion, Theorem 5.1 could be the only theorem in the main paper. The theorem 3.1 is only a corollary that can be easily not mentioned anywhere in section 3 (related to the design of the method) without affecting the flow of the paper. 

On plots: The figures in the paper look like screenshots rather than a higher-quality PDF. This makes the labels and names on the axis hard to read (they are blurred).",See the Weaknesses section.
XFpb3T5Zc9,XFpb3T5Zc9,iFedDR: Auto-Tuning Local Computation with Inexact Douglas-Rachford Splitting in Federated Learning,Reject,TBVoyN0GkY,ICLR.cc/2025/Conference/Submission10234/Reviewer_QdPZ,"This paper proposes iFedDR, which can be thought of as an inexact variant of Dougals-Rachford splitting, equipped with error-correction step via extragradient computation.","- FedAvg is widely still widely used in practice, but its convergence issue in the heterogeneous setting was known, which was “fixed” by methods that employ splitting methods such as FedSplit and FedDR. The proposed method, iFedDR, extends FedDR such that inexact proximal step can be employed, which is a non-trivial and important extension.
- Since the proposed method can be more generally applied to monotone inclusion problems, the provided theory applies to minimax problems.
- The paper is generally well-written, although it is a little confusing to follow different reductions (original problem to inclusion problem, iPPA to iFedDR.)","- It would be great if the authors can comment on the computational aspect of the proposed algorithm. In particular, I’m a bit confused about the authors’ assertion of “computationally negligible error correction”; do you mean that $\\zeta$ can be ignored in Step I.3 of Algorithm 1? In that case, does the theoretical result still apply?
- Algorithm 1 involves several matrix inversion per iteration and per client (3.3). I understand the manuscript is of theoretical nature, but some comments on the computational aspect would be helpful as the manuscript is cast as an FL algorithm.","- If I understand correctly, automatically adjusting the number of location computations needed is a “trade-off” in the sense that instead the step size $\\gamma$ needs to be tuned, which is used for both the client update (3.3 in Algorithm 1) and the server update (I.2 in Algorithm 1). Is there a good way to choose this $\\gamma$?
- In line of above comment, could the authors comment on other federated learning works that use adaptive step size, such as [2, 3]?
- Based on Figure 1 & Remark 3.2 (v), large value of $\\gamma$ leads to less number of communication steps, but increases the number of local iterations needed. Could this be alleviated by decoupling the client step size and the server step size? This would change the classical DR splitting though, but still curious.
- Remark D.3 mentions $\\zeta_k$ can be ignored. Is this at least empirically tested? I believe an answer to this question is important for the authors to assert “computationally negligible” error condition, even neglecting all the matrix inversions needed in the algorithm.

[1] Mukherjee et al. (2024) Locally Adaptive Federated Learning

[2] Kim et al., (2024) Adaptive Federated Learning with Auto-Tuned Clients"
XFpb3T5Zc9,XFpb3T5Zc9,iFedDR: Auto-Tuning Local Computation with Inexact Douglas-Rachford Splitting in Federated Learning,Reject,DE0mDq03IV,ICLR.cc/2025/Conference/Submission10234/Reviewer_rCZW,"This paper proposes iFedDR, which is a horizontal federated learning algorithm based on inexact Douglas-Rachford splitting. The proposed algorithm features adaptive stepsizes and an extragradient correction step. The algorithm includes an online-checkable relative error condition, which enables the server to dynamically request more accurate solutions to the local subproblems from some clients in order to guarantee convergence.",The paper is well-written with a clear flow. The theoretical results are solid and the proofs are easy to follow.,"1. Lack of discussion of other methods for solving the primal-dual problem (2.6) (or the primal (2.4a) and the dual (2.5)). It would be helpful to include one or two sentences explaining why Douglas-Rachford splitting is chosen over other existing methods. 

2. Some definition/interpretation of important variables are missing, e.g., the interpretation of $s$ in DRS and the definition of $m$ in Theorem 3.1. Please read through the main manuscript and make sure that it is self-contained.

3. The limitations of the proposed method was not discussed. A potential limitation of the proposed algorithm is its applicability in realistic federated learning tasks with deep neural network models, in which many of the assumptions (e.g., convexity) needed in the convergence analysis are violated and thus it is not clear to me if the proposed algorithm could still outperform the standard ones, e.g., FedAvg, FedSVRG, etc. I suggest the authors to add a paragraph discussing the limitations of the proposed method. 

4. The numerical experiments focus on simple logistic regression and linear models. They are good starting points but are structurally very different from the high dimensional nonconvex problems in training deep networks. I suggest to authors to test the method on, for example, training full ResNet18 on CIFAR10 in the federated scenario, and to report the training efficiency and accuracy against standard methods. Without more comprehensive tests, it is difficult to assess the potential impact of the proposed method.","1. The authors mention that the proposed algorithm is also applicable to constrained problems but did not discuss much further in this direction. Could the authors comment on what types of constraints (inequality, equality, etc...) can the algorithm handle? Is it simply reformulating constrained problems to the form of (2.1)? How does the number of constraints affect the dimensionality of the reformulated problem? The application to constrained problems may warrant a standalone paragraph for description and discussion, as well as perhaps some experiments to demonstrate the results.

2. How does the algorithm performs when the client loads are imbalanced (clients with different compute power, different amount of data, etc)?

3. Is the proposed method extendable to allow for asynchronous updates?"
GRMfXcAAFh,GRMfXcAAFh,Oscillatory State-Space Models,Accept (Oral),0kU14BMTLi,ICLR.cc/2025/Conference/Submission10203/Reviewer_o2RA,"This paper introduces a new continuous time recurrent network in the family of state space models. The architecture is proposed as an ODE that is discretized in two ways using first order implicit and implicit-explicit integrators. The terms of the ODE are further constrained to induce two different computational tricks to speed up computation; fast matrix inversion to make the implicit methods tractable, and parallel scans for faster sequence processing. The LinOSS architecture is empirically compared to other Neural ODEs and state-space models, and to transformers, on three different time domain problems for time series classification and prediction.","The paper demonstrates a cross cutting expertise from dynamical systems analysis through implementation optimization. The design of the ODE is crafting three advantages at three different levels of abstraction simultaneously: enforce theoretically proven stabilization, allow for efficient matrix inversion, and allow for parallel scans of the sequence recurrence. The experiments on time series problems are a good set of problems, and the results of LinOSS stand out against the broad set of comparisons. The proofs are sound, however, it is not clear if they apply to what actually is going on: see below.","One issue with the paper is highlighted in the core claim of pre hoc controlling for “forgetting” versus stability by choosing between LinOSS-IM and IMEX (Line 298).  Line 205 claims to demonstrate different advantages between the two methods, but this is not actually evident in the experiments. What characteristics of the problems in Table 1 lead to IM vs. IMEX performing differently? If anything, there is no difference between IM and IMEX in all examples but Worms. Is there something special about Worms, or is it a fluke? The result of Table 2 seems to contradict the claim that IMEX is better at long ranges and IM is better at forgetting: why does IM perform better on a problem where memory over a long sequence should be important? 

It is odd to rely on the integrator choice to enforce stability vs. forgetting. Using an ODE framework, it would seem more natural to change the ODE with dissipative terms, for example, instead of changing the integrator. By relying on the integrator choice, it is unclear if those properties would actually hold after training, if the discretization is thought to add new properties that the ODE did not have. Given the unclear results of the experiments, is it possible that results of the theoretical analysis do not apply after the model is trained? Did you try inspecting if the parameters of A and S still hold the assumptions / initializations assumed in Section 3, after training?


Consider the case where the system being learned is actually stable, but the backward euler IM formula is applied. Backward Euler is dissipative even when the dynamical system does not want to dissipate. What happens when you try to learn a model that should be energy conserving using  the LinOSS-IM architecture? The authors could try this by just trying to learn to forecast a simple oscillatory system with LinOSS-IM. I would expect that the model would learn “through” the IM discretization, and converge to an parameterization of an “unstable” ODE that is stable after being discretized by IM. See Krishnapriyan, “Learning continuous models for continuous physics” for a discussion on overfitting on learning through ODE discretizations. 

One idea to start to tackle this problem: Try LinOSS-FE “forward euler”. The architecture would be similar and the tricks in parallel scans would still work. This would be an ablation that would illustrate why stabilization of the implicit and imex integrators is important. If there is no performance difference with the Forward euler discretization in the experiments, or if the IM method can forecast a stable system, then perhaps the theoretical results do not represent what actually happens after training.","- What are the runtimes of the different methods in the experiments? While the accuracy metrics are strong, one of the purported examples of SSMs is the efficiency. What are run times when enabling and disabling different introduced optimizations:
  - Run time when not using the equation for matrix inversion?
  - Run time when not using the parallel scan?
- Line 74: Why is A diagonal? Is it only to induce the matrix inversion trick later?
- Some aspects of the architecture are unclear. How exactly are the LinOSS layers stacked into each other and within the network? 
  - In Figure 1, which parts of  the figure are u, y, z?
  - What effect do the nonlinearities have on the stability analysis?
  - How is the nonlinear layer in the figure a part of the model?
- In the experiments in Section 4, what are the exact hyperparameters and model graphs?
- The exploitation of Formula 3 to speed up the matrix is clever. How does this differentiate during training, though? Did you pass this formula through autodiff, or define a special differentiation rule?
- Does the parallel scans apply to only using first-order IM or IMEX integrators? Or does the ODE formula in general allow for the parallel scans with other integrators, such as a simple forward Euler, or a higher order IMEX?
- Table 1: s/UAE/UEA/g.
- Table 1: What does UEA stand for? Define abbreviations and add citations. (Is Walker 2024 supposed to be the citation?)
- Color highlighting in the tables is not colorblind friendly, nor BW printer/ereader friendly. Use symbols instead.
- Equation 7: Why is it important that equation 2 is a Hamiltonian system in this section? If the underlying ODE is indeed Hamiltonian, doesn’t that suggest that the IM discretization is not appropriate?
- Line 222: What are a, b supposed to be? Describe the specific case of operator & tuple shown here.
- Line 280: Why is it possible to assume that A_kk>0? Couldn’t they diverge from that assumption during training? What initialization is required?
- Line 289: s/constraint/constrained/
- Line 292: The steps in the proof are not obvious. More steps of proof should be presented. In the appendix would be sufficient.
- Just a remark: The title of section 4.2 is overly extravagant for the claims. These days, “extreme long ranges” would be context lengths of millions of tokens :)
- Appendix A: What are the parameter counts? What is the architecture of the “nonlinear layers”? What are the hyperparameters for the forecasting problem?
- Why does the PPG model have higher memory demands, when the models actually seem small?
- Appendix A: What ML library was used?
- Could you describe the loss functions and the complete architecture for the 3 different types of problems? How is time series classification grafted onto the LinOSS network?
- The importance of section 3.2 is unclear to me. It is nice to prove universality of a model, but what is special about LinOSS that other more general proofs of universality would not apply? I would not have questioned it. Is any aspect of the proof specific to LinOSS, or could it apply more broadly to more SSMs?"
GRMfXcAAFh,GRMfXcAAFh,Oscillatory State-Space Models,Accept (Oral),j5ZbPUVrJJ,ICLR.cc/2025/Conference/Submission10203/Reviewer_su1u,"The paper provides a novel state-space model. They have two different versions of it, in which they rigorously show the power of their algorithms and also experimentally verify it. The method outperforms SOTA methods in many tasks.","Provides strong theoretical together with intuitive explanations.
Contrasts their two proposed methods mathematically and also experimentally.
The experimental results are excellent and definitely contributes to the field significantly.
Supplementary material is comprehensive.","I think section 3.2 can be written more accessible.

I believe Figure 1 is very important but can be made more explanatory.","To the best of my understanding, the model cannot produce chaotic dynamics. What if the task in hand requires this? How does this contrast (if there is a contrast) with section 3.2?"
GRMfXcAAFh,GRMfXcAAFh,Oscillatory State-Space Models,Accept (Oral),J0S9oYEz5i,ICLR.cc/2025/Conference/Submission10203/Reviewer_Wn4t,"This paper introduces Linear Oscillatory State-Space models (LinOSS), a novel approach to sequence modeling based on forced harmonic oscillators. The model comes in two variants: LinOSS-IM (implicit) and LinOSS-IMEX (implicit-explicit). The key innovation is using second-order ODEs with diagonal state matrices, offering stable dynamics while only requiring non-negative diagonal elements. The paper provides theoretical guarantees for stability and universal approximation, along with empirical validation showing significant improvements over state-of-the-art models like Mamba and LRU on long sequences.","The paper demonstrates strong theoretical foundations by providing rigorous mathematical analysis of stability conditions, proving universal approximation capability, and establishing clear connections to Hamiltonian systems and symplectic integration. Implementation-wise, it offers a remarkably simple parameterization requiring only non-negative diagonal elements, achieves efficient computation through parallel scans, and presents two complementary variants with different preservation properties. The empirical results are great, showing strong performance on diverse tasks.","1. Limited Analysis of Model Interpretability:

While based on oscillatory dynamics, lacks discussion of learned frequencies

No analysis of how the model captures different timescales

2. Experiment:

No ablation studies on the impact of different initialization schemes

The implementation details are unclear. For instance, how does it compare to Mamba or S5 in terms of speed, training time, FLOPs, and memory usage? Discussing these aspects could enhance its practical utility.","1. Could the model be extended to incorporate coupled oscillations while maintaining stability guarantees?

2. What is the impact of the time step parameter Δt on model performance and stability?

3. How does the choice between IM and IMEX variants affect training dynamics and convergence?"
GRMfXcAAFh,GRMfXcAAFh,Oscillatory State-Space Models,Accept (Oral),0qHjbxVdLt,ICLR.cc/2025/Conference/Submission10203/Reviewer_w9Vn,"In this work, the authors propose a state-space model (SSM) architecture, Linear-Oscillatory SSM, derived from the discretization of second-order linear ODEs that models a network of forced harmonic oscillators. They develop two discretization schemes for the approach, study the stability properties of the parametrization and show universal approximation of LinOSS for approximating general continuous, causal input-output mapping. Through empirical evaluations, they demonstrate that their approach outperforms other Linear SSMs on time series classification, prediction and long-horizon forecasting.",The work is well motivated and the writing is clear and concise. The recurrence matrix afforded by the proposed approach has desirable stability properties and is less constrained compared to the typical SSM parametrizations. The expressivity is further supported by the theoretical analysis on universality of the proposed parametrization. The experimental results are thorough and demonstrate the efficacy of the proposed approach relative to baselines.,"Given that the parametrization has been introduced in Rusch & Mishra (2021) and the universality results for the non-linear counterpart to this work have been shown in Lanthaler et al., 2024, the novelty of this work is somewhat limited. Still, I think that this is a useful contribution overall as it improves the ability of SSMs for learning long-range dependencies.","* L116-118. Since the oscillators are independent, can the proposed architecture model transient synchronization/desynchronization?
* L250. `Assuming M is diagonalizable [...]`. If $M$ has real eigenvalues with algebraic multiplicity $ > 1$, would that make the system unstable? Admittedly, norm growth would be sub-exponential, so perhaps in practice it's fine.
* L289 `constraint` $=>$ constrained"
dTPz4rEDok,dTPz4rEDok,Offline Hierarchical Reinforcement Learning via Inverse Optimization,Accept (Poster),y2W4eIV3mR,ICLR.cc/2025/Conference/Submission10175/Reviewer_a4kN,"Authors introduce a new offline HRL approach that can recover high level actions given trajectories of state transitions.  To find the most likely high level actions, a new simple maximum likelihood approach is used.  They show their approach is able to solve a series of robotics and network optimization tasks.","- The approach to discovering the high level actions given a set of state transitions is simple.  Select the high level action with the highest likelihood.
- Significant empirical outperformance versus baseslines
- Empirical performance was robust to different controllers and modeling errors
- Applied algorithm to domains with very high dimensional action spaces (i.e., the network optimization problems)","The approach does appear to have significant limitations.
1.  The approach assumes access to an approximate model of the transition dynamics.  The authors argue this can be relatively simple to learn because it only requires a single step, but this can be difficult in high-dimensional and stochastic settings.
2.  The approach often assumes a pre-trained low-level controller.","1. Can you clarify what the observed state baseline is?  It seems that it is a high-level policy that always selects the next state from a trajectory as the subgoal?  Why was the next state chosen instead of e.g., 10 timesteps later?  Can you provide some intuition on why is performed poorly?
2.  Can you provide some data on the time horizon (i.e., number of primitive actions) to complete each task?"
dTPz4rEDok,dTPz4rEDok,Offline Hierarchical Reinforcement Learning via Inverse Optimization,Accept (Poster),sgK7mU9zs5,ICLR.cc/2025/Conference/Submission10175/Reviewer_7JuN,"The paper presents a method for obtaining high-level actions from an offline (state only) dataset, by solving the lower level policy inverse problem. The motivation is very clear: High-level, temporal abstractions and hierarchical decomposition is crucial for solving many complex, long horizon problems, therefore, a method for extracting high-level options from an offline dataset that does NOT contain those high-level actions is crucial. In addition, the paper propose to exploit the extracted high-level actions for performing offline reinforcement learning with the resulting high-level dataset, such that the found high-level policy can parameterize the low-level policy at inference time. The proposed method (OHIO) is evaluated on diverse environments (multiple robotics problems as well as network optimization), and performs favorably in comparison with numerous online and offline RL methods.","Very well written, clearly motivated, thoroughly validated both empirically and analytically.
To the best of my knowledge this work addresses an important gap in the literature, the possibility to learn high-level actions that parameterize arbitrary low-level policies from state-only trajectories (and assuming approximate knowledge about transition dynamics).","Not much here to be honest. The only thing that comes to mind is that it was not immediately clear to me how to solve Eq (6) in practice and in the general case, but this becomes very clear with Appendix A.4 and algorithm 2 and 3. Perhaps the content of A.4 could be more integrated into the main method section. 

It seems to me like method mainly deals with the implicit policy case, therefore, I am not sure how much value is being added by discussing the explicit policy case.","How much is this method affected by the quality of the approximate transition model? It would be extremely interesting to see how well the method performs when the ground truth transition model is used, compared with a model learned from the offline data (I guess this would only be possible when low-level actions are part of the trajectories). Did one of the experiments somehow access this and I missed it?"
dTPz4rEDok,dTPz4rEDok,Offline Hierarchical Reinforcement Learning via Inverse Optimization,Accept (Poster),PbKW8DdKX4,ICLR.cc/2025/Conference/Submission10175/Reviewer_dYhu,"This paper presents OHIO, a framework for offline reinforcement learning (RL) using hierarchical policies. The framework uses inverse optimization to recover high-level actions that generated the observed data. The authors perform experiments in robotic manipulation and network optimization environments, and show improvements against end-to-end and hierarchical policies.","1. The paper presents an interesting approach for effectively leveraging the inherent hierarchical structure for downstream learning.

2. The authors show impressive results on roobotic and network optimization environments, with many ablations pertaining the hierarchical structure.

3. It covers good related work section and the limitations of this method are clearly mentioned in the discussion section.","1. The main weaknesses in this paper are the experiments. Although the authors mention prior related hierarchical approaches for offline learning, the comparisons with such sophisticated approaches are missing in the experiment section. This makes it hard to judge the efficacy of the method against prior approaches. A more detailed comparison with such baselines on complex multi-level reasoning tasks is thus necessary.

2. Further, Algorithm 1 provides minimal information for the hierarchical formulation, and can be significantly improved by adding appropriate details of the training procedure. Extensive details on how the hierarchical levels interact with each other during training is also missing. The paper would benefit by adding pseudocode for how the high-level and low-level policies are trained and interact, with details on how the inverse optimization is integrated into the training loop.

3. Minor: Typo in Line 201.","Can you provide comparisons of OHIO with prior hierarchical approaches that leverage offline RL, e.g OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning [1] and approaches that leverage expert demonstrations like Relay Policy learning [2]. 

Can you discuss how would the method compare against prior methods that leverage expert demonstrations for learning the inherent hierarchical structure, e.g. Relay Policy learning [2].

[1]. OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning (Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, Ofir Nachum)
[2]. Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman)"
2d734s2WDb,2d734s2WDb,VIBEID: A STRUCTURAL VIBRATION-BASED SOFT BIOMETRIC DATASET FOR HUMAN GAIT RECOGNITION,Reject,h7tPjaMnTC,ICLR.cc/2025/Conference/Submission10133/Reviewer_Sqmc,"This study introduces a benchmark for gait recognition utilizing a novel structural vibration sensing technique, the geophone. and the new benchmark comprised of in total 100 subjects, collected under indoor or outdoor settings. It invesitgated whether the novel sensing modality can encode identity related information, and what is the limitations or sensitivity of this technique. Although this work addresses an interesting topic, it may not yet provide the technical depth or extensive experimental validation expected for broader applicability. There are also several concerns regarding the experimental settings and presentation clarity.","This paper presents a well-justified study, especially it clearly identifies current research gap for person identification. Overall it reads very well.","* [Sample size] Probably for the gait recognition task, we are more interest in how many subjects collected. The subject size, compared with current large dataset, especially the GaitSet, is still not that comparable. 

* [Technical contents] I am concerned that the technical content is somewhat limited, even for a benchmark paper, for ICLR. Please consider adding more experiments and tasks to thoroughly validate the usability of this dataset, such as Re-ID, gait event detection, and generalization across subpopulations... I encourage the authors to refer to established works like GaitSet for inspiration. Gait data is highly complex, influenced by factors such as age, gender, emotion, and health conditions. Reflecting on these factors in your experiments would enhance the depth of the study.

* [Applicability] I am also concerned that this kind of ambient sensor can only be applied indoor or with relatively small distances, which might limit its application, compared to wearable data?

* [Experiment setup] In the experiment settings, I noticed that there were no concurrent human activity when recording the data, this may be another issue that limits the usability of this study. Additionally, will the data be sensitive to the perspective of the sensor, as I know it is quite sensitive for vision based person identification. 

* [Gait event] gait event detection, is this be validated in terms of accuracy?

* [Dataset details] Further elaboration on the dataset’s composition and subject split for person identification would be valuable, particularly for readers unfamiliar with this topic. 

* [Table clarity] Table 5 is not clearly illustrated, what is the performance comparison between structural vibration and camera? Very limited information is given in both the table and the associated texts. Expanding on this comparison would help readers understand the relative strengths of each technique.

* [Minor - clarification] I assume the subjects of A2, A3, A4 are part of A1, correct? Please clarify this. 

* [Minor - citation] When citing a work which actually does not play any role in your sentence, please use (X et al., XXXX), rather than X et al. (XXXX).",I would appreciate if the authors could address the concerns I raised in the weaknesses section.
2d734s2WDb,2d734s2WDb,VIBEID: A STRUCTURAL VIBRATION-BASED SOFT BIOMETRIC DATASET FOR HUMAN GAIT RECOGNITION,Reject,PRJoestKFx,ICLR.cc/2025/Conference/Submission10133/Reviewer_g5tV,"The authors built a dataset with 100 people using geophone to do multiple experiments based on the human's structural vibration. It consists of multiple covariances including floor types, and distances. The work tries to find a connection between the vibration and identities and builds multiple benchmarks.","The paper is clear and easy to follow, and the tables and figures are easy to understand
The proposed question is interesting, trying to build the connection between identity and walking vibration, a fine detail when a human is walking. And authors collect a relatively large dataset in multiple conditions.","In real-life applications, it is hard to find a good condition to use a geophone to capture a human's gait with little noise.

How to control the noise in outdoor cases.

Compared to a camera, the vibration-based method is restricted by the sensor and distance. 

The protocol is not clear. How is the train and test set defined? For human identification, the identities appearing in the training set will not be present in the test set. It seems these experiments do not follow this setting","Although the authors define the floor in different classes, the hardness might be a more reliable way to classify. Since different carpets' thicknesses may have different responses. And what is the distance range for the geophone, since 4 m is not far for camera sensor"
2d734s2WDb,2d734s2WDb,VIBEID: A STRUCTURAL VIBRATION-BASED SOFT BIOMETRIC DATASET FOR HUMAN GAIT RECOGNITION,Reject,uLAMjtKUJB,ICLR.cc/2025/Conference/Submission10133/Reviewer_LqAV,"The presented work introduces a new biometric dataset for human gait recognition based on structural vibrations. The dataset is applied to various tasks such as person identification, domain adaptation, and multi-modal scenarios combining vibration and vision-based identification methods. Experimental analysis includes verification of machine-learning and deep learning approaches.","1. The description of the data collection protocol is clearly written with sufficient details and clear explanations of the research motivation
2. The introduction and related work sections contain important background information justifying the motivation for the introduced dataset.","1. Is there any requirement for using a specific sensor type during the inference if trained on the presented dataset? I'm wondering about the practical implication of the proposed solution. 
2. The work indicates that the concurrent activity was not taken into account, however it's very possible to happen in real-life scenarios. Would the presented dataset be sufficient for handling such scenarios? How should one prepare for additional noise introduced in this way? 
3. It's not clear how filtering of potential noise was performed? Was the assumption that the data collection is performed in an isolated environment without any noise? You mentioned that there was environmental noise present, but how do you quantify its presence? If the assumption is that there is minimal or no noise, it again raises question around the practicality of the solution.
4. It's not clear how the data was split for training and testing? Were the same subjects present in both subsets or did you ensure no overlap?
5. One of the motivation behind introducing a new dataset is that other datasets contains a limited number of subjects. It's mentioned that there are 100 subjects in the proposed dataset but then only 30 and 40 subjects are used for floor types and distance measurements. Why not all 100 subjects were used for all of the scenarios?","1. What do you mean by events in Table 2?
2. Line 407 - where is table 10? or did you mean 1?"
2d734s2WDb,2d734s2WDb,VIBEID: A STRUCTURAL VIBRATION-BASED SOFT BIOMETRIC DATASET FOR HUMAN GAIT RECOGNITION,Reject,BiaYMYnr5q,ICLR.cc/2025/Conference/Submission10133/Reviewer_35Gq,"The paper presents a novel dataset termed VIBEID, designed for human gait recognition using structural vibration data. The dataset includes recordings of 100 subjects across various distances, floors, and environments. Experiments demonstrate that structural vibration can serve as a viable biometric trait across different scenarios.","- Vibration-based gait recognition introduces a novel approach for human identification.
- The proposed baseline methods are effective.
- This work establishes the largest-scale vibration-based dataset to date.","- The dataset has a limited number of subjects, although it includes over 88 hours of recorded data.
- Compared to commonly used vision-based gait recognition, the operating distance remains relatively short.
- The evaluation setup lacks clarity.","1. To what extent do walking speed and the carrying of objects impact recognition performance?
2. Building on question 1, does abnormal gait pose significant challenges for re-identification?
3. The VIBEID dataset studies an operating distance range of 1.5m to 4m, while vision-based gait recognition typically works at distances over 10m. What is the distance limit for vibration sensors to capture meaningful gait signals?
4. If there are obstructions between subjects and the sensor, is reliable recognition still possible?
5. How does the proposed vibration-based gait recognition handle scenarios with multiple pedestrians walking simultaneously?
6. I recommend replacing Figure 3 with a clearer version.
7. The evaluation protocol should be more detailed."
IdKkm91BzB,IdKkm91BzB,Annealed Implicit Q-learning in Online Reinforcement Learning,Reject,kcv6kBHdVb,ICLR.cc/2025/Conference/Submission10130/Reviewer_Vdto,"This paper uses an annealed implicit Q-learning loss in online RL with SAC, to replace the standard MSE loss. The loss is annealed in a way that increases bias in the early stage of training and reduce bias toward the later stage.","- The paper proposes a very simple trick that can potentially improve the sample efficiency of training the Q-function in SAC.
- The experiments show a major improvement in hopper-hop and humanoid-run environments.
- The paper provides a large set of results on various $\\tau$ values being annealed or kept fixed.","## Lacks justification of expectile loss

While the paper spends a significant portion trying to motivate the $\\tau$ schedule, it is still unclear why introduce the implicit Q-learning loss to begin with? What benefit does it offer over the thoeretically guaranteed objective of SAC in online RL?

In other words, it is unclear why AIQL outperforms SAC in the environments considered. Figure 7 shows that the skewness and bias of SAC are the minimum, and lower than AIQL. Then what is the root cause of AIQL outperforming SAC?


## The annealing schedule of optimality is not sufficiently justified
I am still unable to understand why the bias should be kept high in the beginning (and lower in the end)?
Section 3.2 discusses overestimation bias and that too much overestimation bias would lead to failure in learning, which makes sense.
Section 3.3 says ""$\\tau$ control the trade-off between optimality and bias"", but this relationship was never justified. Therefore, the connection between skewness/bias and $\\tau$ value is not well established.


## The scheduling of tau depends heavily on the environment
Essentially this paper requires two kinds of hyperparameters:
- the start and end values of $\\tau$.
- the total duration of learning (set to 3M steps in this work) to set up the linear schedule.

But how does one select the decay schedule for an environment where the schedule is not known a priori? Even for the humanoid tasks, the convergence is not fully achieved, e.g., humanoid-run should reach max return of ~450 if trained for longer. So, how does one define the annealing schedule in such cases?

I would encourage the authors to demonstrate their idea on more diverse environments apart from DM Control, to fully understand when the expectile loss and the annealing schedule can help or hurt. And also how to set the hyperparameters in general.

## IQL loss on algorithms other than SAC
The paper claims ""tested the modeling of optimal Q-value using implicit Q-learning loss in continuous-action online RL"", but only shows results on SAC. Would similar improvements be expected in TD3 as well? The authors already have the TD3 baseline, so I expect this experiment to not be so hard as the incorporation of expectile loss should be pretty simple?

## Interpretation of the values of $\\tau$ annealing
In Section 4.3, Annealed (0.7) and Fixed (0.7) perform quite well, on par with Annealed (0.9). I am not sure whether there is a strong justification for the paper's insight that one needs to have a high bias in the beginning. In other words, I am still not sure where the learning gains come from.

Similarly, Fixed (0.6) does really well (713, 822) but Fixed (0.5), i.e. SAC, underperforms quite a bit (657, 765). What happens between 0.5 and 0.6 that causes the improvement? I believe investigating this could give a clear insight about why expectile loss is helping, which this paper is currently missing.","1. The paper mentions:
> As learning progresses, the policy approaches the optimal policy, reducing the necessity for high optimality in value function learning.

Why is high optimality not necessary towards the end? If the value function is suboptimal, then the policy would also be suboptimal.

2. The paper categorizes hopper-hop and humanoid-run as ""more challenging tasks"". What makes hopper-hop more challenging than quadruped-run?

3. Replacing SAC's value function update with expectile loss does mean that the entropy regularization is removed, correct?

4. What is the reason for AIQL to perform so well in hopper-hop? It seems this is the main reason for the cumulative results to be so good."
IdKkm91BzB,IdKkm91BzB,Annealed Implicit Q-learning in Online Reinforcement Learning,Reject,Y77wi90J5i,ICLR.cc/2025/Conference/Submission10130/Reviewer_9sVB,"The paper proposes a new method in online RL, focusing on the setting with continuous action space. The work starts from the advantage of expectile and the observation of overestimation bias in function approximation. Then a new algorithm named Annealed Implicit Q-learning (AIQL) is proposed, which is based on Soft Actor-Critic (SAC) and the tool of expectile inspired by Implicit Q-learning (IQL). Experiments are then to show the comparisons in performances.","> **Clarity**
- Despite some lack of notation explanation, the paper gradually leads readers to the proposed method through observation of gaps and theoretical discussion, making the main method easy to understand.

> **Significance**
- The proposed method is tested in various benchmark environments.","> **Originality**
- The proposed method mainly follows the ideas of both IQL and SAC.

> **Quality**
- The claims regarding the overestimation bias are not sufficiently convincing. Equation (1) is derived following the strong assumption that Q-functions are independent from actions, which tends to be an over-simplification of the scenario. In addition, the experiment shown in Figure 2 mainly concerns a single training step, but does not explain why such bias can still accumulate after online exploration (apart from claiming such result by 'if the bias becomes too large').
- In Section 3.3, the advantage of tuning $\\tau$ is described, after which a linearly decreasing $\\tau(t)$ is proposed. However, there is neither theoretical/empirical observation showcasing how different $\\tau$ affects the training in different stages (as a support to the advantage part), nor justification of the choice of linear functions (as a support to the proposal part).

> **Clarity**
- In Section 2.1, $d\\_0$ is not clearly defined by just claiming it is 'the probability distribution', which actually seems to be the initial state distribution. And the horizon $T$ is not defined.
- In the review of IQL, the loss function for the parameters of Q-functions misses the dependence on $V\\_\\psi$.

> **Significance**
- In Section 3.1 the paper emphasizes its advantage by estimating directly the optimal value compared to methods like SAC, while in actual implementation according to Section 4.1, the proposed method is just an expectile-loss version of SAC. More clarifications on distinctions between methods could be helpful.
- In some experimental results in Figure 4, SAC reaches higher average returns earlier than AIQL.","- Why is there no comparison with some online version of IQL, if any?"
IdKkm91BzB,IdKkm91BzB,Annealed Implicit Q-learning in Online Reinforcement Learning,Reject,wuJwijIfTM,ICLR.cc/2025/Conference/Submission10130/Reviewer_N256,"This paper proposes a new approach to improve sample efficiency in continuous action online RL by gradually reducing optimality bias, which helps control overestimation in Q-values. The method outperforms SAC and TD3 on DM Control tasks, offering better performance and robustness.","- Tackles a key challenge in online RL: boosting sample efficiency through better value function estimation.
- Presents a clear argument that overestimation and Q-value skewness are major issues, and introduces expectile loss in the critic architecture to help address them in continuous action tasks.
- Shows impressive performance gains compared to popular methods like SAC and TD3.","- Section 2.2 (Line 115): Why is it stated that ""In IQL, near-optimal values are estimated by using τ close to 1""? Could you clarify this choice?
~~- Sections 4.1 & 4.6: Could you expand your analysis to discuss Exp1 in Section 4.6? Based on the explanation in Section 4.1, it seemed the natural annealing strategy might align with Exp1 in Section 4.6, yet it performed relatively worse than the linear and sigmoid strategies.~~
~~- Section 4.2: In the XQL (Garg et al., 2023), the extension of SAC (X-SAC) showed comparable performance to SAC, even outperforming it in Hopper-Hop. However, the results in Figure 4 don’t seem to align with this finding. Could you provide any reasoning or justification for this discrepancy?~~",none
IdKkm91BzB,IdKkm91BzB,Annealed Implicit Q-learning in Online Reinforcement Learning,Reject,2Soy1apumB,ICLR.cc/2025/Conference/Submission10130/Reviewer_4Sx9,"The authors propose a sample-efficient actor-critic method by modifying the policy evaluation loss in online scenarios. They propose to do so by leveraging the expectile loss shown effective in offline scenarios. Additionally, the authors propose to decrease the learned expectile during training to perform implicit Q-learning at the beginning of the training and transition to a policy evaluation regime at the end of the training.","1. The paper investigates an important topic in RL. 

2. The proposed method is clearly explained and well-motivated.

3. The presented algorithm outperforms relevant baselines in the considered experiments.","1. The novelty of the proposed approach is limited as the paper's main contribution lies in applying Implicite Q-Learning to an online setting with a linear schedule for the expectile. Nevertheless, the findings are interesting. They would be worth sharing with the community if a more detailed study would be done. Here are some ideas to increase the depth of the analysis:

     ~a.  The authors should comment on the choice of learning the Q-function directly instead of using a separate value function as done in IQL. This choice should normally harm the performances as the learned expectile is now influenced by the stochasticity of the environment. Evaluating the proposed algorithm in a highly stochastic MDP would help justify this choice.~ 

     ~b. AIQL is only applied to SAC. Applying it to TD3 and CrossQ, for example, would strengthen the authors' claim.~

     c. Investigating the importance of the expectile regression loss compared to the quantile or Huber quantile regression loss would be interesting.

     ~d. Analyzing the influence of the hyperparameter $T$ would be beneficial for understanding its influence on AIQL.~ 

2. Changing the target expectile value during the training makes the loss non-stationary, potentially harming performances. The authors do not comment on this point.

~3. The code is not provided, which limits the reliability of the experiments. Additionally, the baselines’ hyperparameters are not reported.~","~1. Line 155, the authors claim that the scenario where all action values are equal has the largest overestimation bias. Can this be justified?~

2. To mitigate the issue presented in Weakness 2, the authors could consider having a Q-network with several heads, where each head is responsible for learning one expectile, similar to Quantile-Regression Deep Q-Network [1]. The head used for training the policy would evolve during training, starting from a low target expectile value to finish with a high target expectile value as proposed by the authors. I would be interested in the authors' comments about this suggestion.

[1] Dabney, Will, et al. ""Distributional reinforcement learning with quantile regression."" AAAI. 2018.

– Remarks – 

~A. Line 35, a stop between “efficiency” and “This” is missing.~

~B. Line 42, for clarity, I would replace “optimal value” by “maximal value”.~

~C. Line 90, the word “initial” is missing before “probability distribution”.~

~D. Line 120, the value function should not take the action $a$ as input.~

~E. Line 196, some parentheses are close, but they have never been opened. I suggest removing them.~

~F. Line 215, replacing $t$ by $\\min(t, T)$ in the right side of the equation would be more accurate.~"
B8aHIDSi7E,B8aHIDSi7E,Getting Free Bits Back from Rotational Symmetries in LLMs,Reject,hClVPujTrb,ICLR.cc/2025/Conference/Submission10052/Reviewer_kYZS,"In this paper, the authors highlight that the rotational symmetries of SliceGPT introduce redundancies. Based on SliceGPT, they propose further compressing the weights by the bits-back coding algorithm. Specifically, rather than treating all weight configurations as unique, they encode weights up to an equivalence class defined by rotations, enabling smaller memory requirements. They conducted experiments on several benchmarks with multiple models. The results verify the effectiveness of their proposed method.

**Strengths**
1. The proposed method is novel and insightful
2. The proposed method is well-motivated and has the potential to make a broader impact.
3. The experiment results are promising.

**Weaknesses**
1. The writing is not self-contained. Specifically, the paper relies heavily on bits-back coding. However, they do not properly connect SliceGPT with the previously proposed bits-back coding. It is hard to understand the actual algorithm.
2. It is questionable if the method can be applied in the real world given the compression/decompression and matrix decomposition procedures involved. The run speed of this method could be slower than that of the vanilla Transformer model.

In summary, the paper is insightful and well-motivated. However, the writing is not self-contained and the overhead could hinder the real-world application. As a result, I recommend a weak acceptance.","1. The proposed method is novel and insightful
2. The proposed method is well-motivated and has the potential to make a broader impact.
3. The experiment results are promising.","1. The writing is not self-contained. Specifically, the paper relies heavily on bits-back coding. However, they do not properly connect SliceGPT with the previously proposed bits-back coding. It is hard to understand the actual algorithm.
2. It is questionable if the method can be applied in the real world given the compression/decompression and matrix decomposition procedures involved. The run speed of this method could be slower than that of the vanilla Transformer model.",See the weaknesses.
B8aHIDSi7E,B8aHIDSi7E,Getting Free Bits Back from Rotational Symmetries in LLMs,Reject,43EuSV3eOK,ICLR.cc/2025/Conference/Submission10052/Reviewer_Li28,"This paper proposes the application of the bits-back algorithm to reduce the overhead of additional matrices introduced by pruning large language models (LLMs) using SliceGPT. In the SliceGPT method, an additional matrix is introduced for the rotation matrix, which helps in maintaining accuracy but results in additional computational overhead, thereby acting as a compression overhead. To address this issue, we propose an algorithm that encodes/decodes the rotation matrix using the bits-back algorithm, demonstrating that the rotation matrix can be computed solely through the decoding process during inference. Our proposed method shows an additional 3-5% improvement in compression efficiency compared to the practical compression rate of SliceGPT.","* The paper proposes a method to compress the rotation matrix Q introduced by SliceGPT using the bits-back algorithm, effectively reducing the parameter overhead.
* It demonstrates that the rotation matrix Q can be encoded and decoded using the bits-back algorithm without requiring a calibration set, relying solely on the weight matrix.
* The study shows that while the actual compression rate of SliceGPT with the rotation matrix Q is approximately 9%, the proposed method can achieve a closer-to-expected compression rate of 13%. It also demonstrates that applying the proposed encoding method to the rotation matrix Q in models such as OPT and LLaMA2-7B does not result in significant differences in Commonsense Reasoning (CSR) performance.","* The paper lacks sufficient analysis and experimentation regarding the practical impact on latency and throughput during inference when decoding the rotation matrix Q using the proposed method.
* The proposed method is somewhat limited in scope, as it can only be applied after the implementation of SliceGPT, thereby restricting its applicability.
* The actual benefits of encoding the rotation matrix Q in terms of inference latency and throughput might be minimal. It is likely that during the prefill stage, the additional decoding step for the rotation matrix Q could result in higher inference latency and lower throughput compared to SliceGPT alone.","* How does the proposed method perform in terms of inference latency and throughput gains compared to SliceGPT when applied on actual hardware like GPUs? If the effectiveness of this aspect is demonstrated, I would be inclined to increase my rating."
B8aHIDSi7E,B8aHIDSi7E,Getting Free Bits Back from Rotational Symmetries in LLMs,Reject,j6Ikj9za4j,ICLR.cc/2025/Conference/Submission10052/Reviewer_r3xU,"The paper presents a novel training-free compression technique of large language models that exploits rotational symmetries in the weight space. It uses bits-back coding, a compression strategy that takes advantage of these rotational symmetries to compress Transformer models by about three to five percent while impacting the model's perplexity in a negligible way. The method was tested on SliceGPT-pruned Transformers, namely the OPT and Llama-2.","- The paper presents bits-back coding used on neural network models, mainly focusing on enlarging language compression. 
- The proposed method is computationally feasible since it runs without retraining. 
- The paper's novel technique is evaluated on models, such as OPT and Llama-2, demonstrating performance metrics are not significantly affected in terms of perplexities drop.","-  This approach is inherently SliceGPT pruning and Transformer-specific architecture, which may also limit its use to other neural networks or pruning techniques.
- The methodology relies only on Transformer architectures, so applicability to lighter models suited to edge devices could be considered.","- What is the prospective effectiveness of the method when it comes to implementation on the models with precision format lower than float16? 
- Is it possible to apply the bits-back coding method to the architectures that are not transformers or the architectures compressed with different methods?"
B8aHIDSi7E,B8aHIDSi7E,Getting Free Bits Back from Rotational Symmetries in LLMs,Reject,o0GkvcO0Gb,ICLR.cc/2025/Conference/Submission10052/Reviewer_cA7j,"This work proposes to apply a coding scheme to utilize symmetries made available by the SliceGPT method for training-free weight-only quantization. The resulting method achieves an additional 3-5% reduction in the total weight sizes for SliceGPT compressed models. The authors show that their resulting models do not diverge significantly from the original models when evaluated on tasks such as PIQA, WinoGrande, and HellaSwag.","1. The proposed method is training-free, making the result independent from the choice of calibration set.
2. The proposed method can be computed using only a CPU, without heavy computational requirements. This will make adoption easier.","1. The improvement of 3~5% appears small unless the method does not impose other overheads. However, there is no detailed analysis of how much memory the other components, such as the correction code, use.

2. Additionally, the work does not include any analysis of the overhead in terms of the time required for inference caused by applying additional computations to the model. Even if computing the rotation can be performed on CPU, there should be an analysis of the effect on inference latency when measured end-to-end.","The beginning of Section 2 uses the word “delving” prominently. As the word “delve” is strongly associated with large language model outputs, we advise the authors to rephrase the sentence."
ySJSGZxN7M,ySJSGZxN7M,Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization,Reject,ZWNddxGjPn,ICLR.cc/2025/Conference/Submission9532/Reviewer_bWVH,"This paper proposed a branching scheme to accelerate HSNW search. Supposedly, this scheme could find out some more useful starting point in the layer-0 of the HSNW method. The experimental setup is completely wrong and thus I can't draw a conclusion that experiments validate the claim.",- Work on an important problem for practical application.,"- I implemented HSNW from scratch and published HSNW-related algorithm in top data mining conferences before. The experimental setup is completely nonsense to me. It should sort of follow-up the ANN-benchmark setup and that's a more reasonable way to show results. 

- Apparently, the algorithm doesn't compare to the real HSNW implementation in wall clock time, and only compare their own variations. 

- The algorithm seems to be implemented in Python only, which is sort of contradicting to the point of AKNN. Most HSNW algorithm is implemented in C, and there is a reason for that. Many algorithm improvement can't really benefit the search as the real-world hardware doesn't support well for the operations; or the asymptotic analysis doesn't align well with the real operation cost. Unless the authors showed their modified algorithm can accelerate in C (doubtful as there are so many branching and that possibly will cause memory read busy), it's not very convincing. 

- It reads to me that the method is only adding more operations in top layers. So it has to at least numerically show how many IP/distance calculation it saves for the layer-0. Otherwise, the computational cost can only go up......

- So even under the query time reported in Fig. 10, it's not very promising. Just based on the Figure 10, I will say it's not a really working method.","- The paper is not very straightforward to understand as it lacks a running example. I really don't know what exactly ""exclude_set"" contains. A running example or any pictorial example in Figure 3 helps.

The definition of LID (x) is unclear. LID(x) reads like query dependent, or say LID(q) is something we want. That means there will be a property per graph/query pair, but in Figure 2/3, there seem to be multiple high LID nodes. I'm not sure what exactly LID means."
ySJSGZxN7M,ySJSGZxN7M,Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization,Reject,cimgG6f2lw,ICLR.cc/2025/Conference/Submission9532/Reviewer_7mVb,"This paper The proposed HNSW++ algorithm, which introduces a dual-branch structure, LID-based node insertion, and skip-layer bridges to address the limitations of the original HNSW, such as local optima and cluster disconnections. Experiments have shown the that the proposed method is competitive both in performance and in inference speed.","1. The dual-branch structure and LID-based insertion mechanism are well-motivated and novel.

1. The experiments have (partially) shown the effectiveness of the proposed method.","My major concerns are about experiments.

1. The proposed method is implemented in Python, which is not a good programming language model for comparing speed. The authors may provide a theoretical time complexity analysis to complement their empirical results. This would allow a fairer comparison of the algorithm's efficiency across different implementations.

1. The baselines are relatively weak, I did not see the advanced methods (e.g., IVFPQ) used in faiss[1]. I'd like to have authors justify their choice of baselines and explain why stronger baselines were not included.

[1] Johnson, Jeff, Matthijs Douze, and Hervé Jégou. ""Billion-scale similarity search with GPUs."" IEEE Transactions on Big Data 7.3 (2019): 535-547.",please refer to weaknesses
ySJSGZxN7M,ySJSGZxN7M,Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization,Reject,gh0XYJvLtO,ICLR.cc/2025/Conference/Submission9532/Reviewer_NYMT,"This paper presents an enhanced Hierarchical Navigable Small World (HNSW) algorithm addressing limitations in local optima and scalability by introducing a dual-branch structure with LID-based insertion and a bridge-building shortcut technique. These innovations improve cluster connectivity, capture outliers more effectively, and reduce inference time. Experimental results across NLP, DL, and CV datasets demonstrate notable accuracy and speed improvements over the original HNSW algorithm.","1)This paper presents a novel enhancement to the HNSW algorithm, addressing key limitations related to local optima and inference speed. 
2)The research is evaluated across diverse benchmarks, including datasets from Computer Vision (CV), Deep Learning (DL), and Natural Language Processing (NLP). The experiments clearly support the proposed method's superiority in both accuracy and speed, with substantial performance gains.
3）The paper is well-written, with clear explanations of complex concepts and methods.","1)The related work section is overly concise, lacking a comprehensive review of current research, which limits contextual understanding of the contributions.
2)Although the paper claims improvements in inference speed, Figures 9 and 10 show only modest gains, casting doubt on the practical significance of this claim.
3)The experimental evaluations are relatively limited, with insufficient algorithmic comparisons; in particular, using only the GLOVE dataset for NLP benchmarks diminishes the persuasiveness of the results in this domain.","The paper introduces an optimization algorithm for HNSW,  including pseudocode for the dual-branch structure, LID-based insertion, and bridge-building techniques would enhance clarity and understanding."
fNMKqyvuZT,fNMKqyvuZT,Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets,Accept (Poster),Hn8qn0XLR0,ICLR.cc/2025/Conference/Submission9364/Reviewer_fNHf,"The authors introduces a novel method, Retrospective Backward Synthesis (RBS), aimed at enhancing the training of goal-conditioned Generative Flow Networks (GFlowNets) by synthesizing new backward trajectories. RBS augments ""virtual"" backward trajectories in
goal-conditioned GFlowNets to enrich training trajectories with enhanced quality and diversity. RBS improves the sample efficiency and performance of GFlowNets across a range of tasks, including sequence generation and biological sequence design.","1. The paper identifies and targets a significant issue in the training of goal-conditioned GFlowNets, offering a practical and innovative solution. Augmenting backward trajectories for training is interesting.
2. Comprehensive empirical results are provided, demonstrating the effectiveness of RBS over existing methods on multiple benchmarks.","1. Limited Discussion on Potential Drawbacks: The paper does not sufficiently address the potential limitations of RBS. For instance, there is no discussion about the computational overhead of synthesizing backward trajectories, nor is there a mention of whether the method is robust to different types of reward structures or environment dynamics.
2. Relevance and Scope of Application: The improvements are made specifically within the context of GC-GFlowNets, which may limit the applicability of the method.
3. Comparison with Diffusion Policies: Given the similarities between the proposed RBS and diffusion policies, a direct comparison would be valuable to understand the unique contributions and differences of RBS.
4. Assumptions on Environment Dynamics: It is unclear whether the proposed RBS method assumes or requires any particular properties of the environment, such as determinism or stochasticity. If the backward dynamics are infeasible or the environment is highly stochastic, the performance of RBS may be affected, and this should be addressed.
5. Quality of Synthetic Trajectories: The paper should include a discussion on how to ensure the quality of the synthesized backward trajectories, especially in cases where such trajectories may not correspond to realistic or feasible paths in the actual environment.
6. Lack of Comparison with Goal-Conditioned RL: Without a comparison to goal-conditioned RL, it is difficult for readers to fully appreciate the relative strengths and weaknesses of GC-GFlowNets. Including such a comparison would provide a more complete picture of the method's positioning within the broader field of goal-directed learning.
7. The authors may further investigate existing literature on augmenting backward trajectories for sample-efficient RL or backward learning in goal-conditioned RL, which makes the paper more comprehensive.","1. How does RBS compare with diffusion policies, and in what scenarios does RBS offer distinct advantages?
2. Does RBS assume deterministic or stochastic environments, and how does it handle situations where the backward dynamics are not straightforward?
3. How can the authors ensure that the synthesized backward trajectories are meaningful and do not lead to false positives in the learning process?
4. Could the authors include a comparison with goal-conditioned RL methods to highlight the specific benefits of using GC-GFlowNets?"
fNMKqyvuZT,fNMKqyvuZT,Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets,Accept (Poster),MDBoaa9h1N,ICLR.cc/2025/Conference/Submission9364/Reviewer_HyCh,"The paper proposes Retrospective Backward Synthesis (RBS), a novel method to enhance the training of goal-conditioned Generative Flow Networks (GC-GFlowNets). GC-GFlowNets have shown potential in generating diverse sets of high-reward candidates but face challenges due to sparse reward structures and limited coverage of explored trajectories, especially when using offline data. To address these limitations, RBS synthesizes backward trajectories that originate from a desired goal, enriching the training data with high-quality, diverse samples. This approach helps transform unsuccessful action sequences into positive learning experiences, thereby improving sample efficiency and generalizability.

The authors introduce additional techniques, such as reward signal intensification and backward policy regularization, to stabilize training and prevent mode collapse. Empirical results across various benchmarks, including GridWorld and bit sequence generation, demonstrate that RBS outperforms state-of-the-art methods in terms of success rates, sample efficiency, and scalability. Notably, RBS achieves nearly 100% success in large-scale tasks where competing approaches fail, highlighting its robustness and potential for further advancements in GC-GFlowNets.","- Backward-Looking Strategy for Enhanced Training: The proposed Retrospective Backward Synthesis (RBS) method utilizes a backward-looking strategy to synthesize trajectories from the goal state, significantly enriching training data. This approach effectively improves sample efficiency by converting failed experiences into successful learning signals, addressing the sparse reward problem.
- Empirical Validation of Sample Efficiency: The paper presents strong empirical results across a range of benchmarks, demonstrating that RBS markedly improves sample efficiency. The method achieves nearly 100% success rates in complex tasks where state-of-the-art baselines fall short, underscoring its practical impact.
- Clear Writing and Presentation: The paper is well-written and presented, with clear explanations, structured methodology, and comprehensive experimental results. The clarity facilitates a strong understanding of both the theoretical and practical aspects of the proposed approach.","- Scalability and Continuous Environments: The paper’s experiments focus on relatively simple and discrete environments, raising concerns about how well the Retrospective Backward Synthesis (RBS) method would scale to more complex, continuous, real-world tasks. The absence of testing in high-dimensional or continuous state-action spaces limits insights into its broader applicability.
- Tuning Challenges: The method's reliance on hyperparameters, such as reward scaling and backward policy regularization, introduces tuning challenges. While these components are beneficial for stabilizing training, they require careful adjustment, potentially impacting the ease of replication and practical deployment in varied scenarios.
- Lack of Comparison with Model-Based RL: Despite the inherent use of backward trajectory synthesis, which resembles model-based planning, the paper does not compare RBS with established model-based RL approaches such as MBPO or Dreamer. This omission makes it difficult to assess how RBS performs relative to other methods that also utilize environment models for planning and sample efficiency.","- In algorithm 1) line 6, how do we guarantee that the backward policy could reach $s_0$ from $y$ each time?
- Are tuning for reward intensification and backward policy regularization difficult? What the the effect of hyper-parameters on the performance?"
fNMKqyvuZT,fNMKqyvuZT,Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets,Accept (Poster),5m5sGFAGLN,ICLR.cc/2025/Conference/Submission9364/Reviewer_xjJi,"This paper addresses key challenges in goal-conditioned Generative Flow Networks (GFlowNets), specifically the problems of sparse rewards and limited trajectory coverage. The authors introduce Retrospective Backward Synthesis (RBS), a method that generates additional backward trajectories to expand the training data. Their approach aims to improve both the quality and diversity of training trajectories, providing more learning signals in scenarios with sparse rewards. Empirical evaluations demonstrate improved sample efficiency and performance compared to baseline methods across multiple benchmarks.","* The paper is well-written and straightforward to understand.
* Retrospective Backward Synthesis (RBS) is introduced with clear motivation, and the paper also presents training techniques such as backward policy regularization. 
* Empirical results demonstrate that the proposed method outperforms baselines, showing improved performance and sample efficiency.","* The evaluation tasks do not include key benchmarks like RNA Generation from Pan et al. (2023a), which limits direct comparison.
* The differences between the proposed RBS method and OC-GAFN are not clearly articulated. A more comprehensive discussion is needed to clarify the specific advantages of the RBS method. 
* It remains unclear how goals are defined across the evaluated tasks, which could impact generalizability and reproducibility.",Please address my concerns in the weakness
fNMKqyvuZT,fNMKqyvuZT,Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets,Accept (Poster),5oJkhlkVge,ICLR.cc/2025/Conference/Submission9364/Reviewer_iEkK,"The paper tackles the challenge of training goal-conditioned Generative Flow Networks (GC-GFlowNets) in environments with sparse rewards and limited offline data. It introduces Retrospective Backward Synthesis (RBS), which synthesizes new backward trajectories to enrich training data, improving sample efficiency and diversity. Experiments demonstrate that RBS significantly improves performance and generalization in various benchmarks.","This paper proposes a novel method called Retrospective Backward Synthesis (RBS), which synthesizes new backward trajectories in goal-conditioned GFlowNets to improve the quality and diversity of training trajectories. This approach introduces rich learnable signals, effectively addressing the sparse reward problem.","1.The experimental tasks are relatively simple and insufficiently comprehensive.

2.The latest goal-conditioned reinforcement learning algorithms are not selected for comparison.","1.	Age-Based Sampling is a very straightforward technique. How does it compare to previous methods like Prioritized Experience Replay (PER)? Did the authors attempt using PER as well?

2.	Regarding the experimental setup, since you’ve compared your method with reinforcement learning approaches, I assume that these experiments share the same tasks as those in reinforcement learning. If that’s the case, why weren’t newer RL methods selected as baselines? Additionally, for tasks like bit sequence generation, TF binding generation, and AMP generation, is DQN an appropriate baseline?"
Vlo3Gad3YP,Vlo3Gad3YP,Diff-BBO:  Diffusion-Based Inverse Modeling for Black-Box Optimization,Reject,P5kbqOubbD,ICLR.cc/2025/Conference/Submission9359/Reviewer_1x7Q,"This work proposes an inverse approach leveraging diffusion models for online BBO problem. Specifically, this paper introduces a new acquisition function to propose objective function values, and employ a conditional diffusion model to generate samples. The authors conduct experiments in design-bench to verify the effectiveness of their method.","1. This paper is easy to follow. The structure of this paper is clear.
2. This paper provides a solution for online black-box optimization (BBO) called Diffusion-based inverse modeling for black-box optimization (DIFF-BBO), which uses objective function values to generate solutions. This approach is interesting and has the potential to improve the performance of online BBO.","1. The core difference between the main idea of the proposed method (the inverse method) and LLAMBO [1] should be explicitly discussed.  While there are differences in the specific implementation details compared to LLAMBO, this approach appears to be more of an aggregation of the previous methods. Specifically, the inverse approach was originally utilized in MINS [2], while the conditional diffusion model was incorporated in DDOM [3].

2. Why do the experimental tasks of online BBO methods use offline BBO benchmarks? Is it because there is no suitable benchmark for online BBO?

3. While the background on online BBO is quite solid, the paper does not sufficiently explore prior work in offline BBO. Expanding the discussion to include them could provide a more comprehensive literature review and motivation support.

4. Superconductor, Ant and D’Kitty and so on are high dimensional problem. So, it would be better if this paper could compare with more high dimensional Bayesian optimization in recent years rather than simple black-box optimization.

[1] Large Language Models to Enhance Bayesian Optimization. In Proceeding of the 12th International Conference on Learning Representations, Vienna, Austria, 2024.

[2] Model inversion networks for model-based optimization. In Advances in Neural Information Processing Systems 33, pp. 5126–5137, Virtual, 2020.

[3] Diffusion models for black box optimization. In Proceedings of the 40th International Conference on Machine Learning, pp. 17842–17857, Honolulu, HI, 2023.","Please see Weaknesses part.

Besides, “They struggle with steering clear of out-of-distribution and invalid inputs” is often discussed in offline BBO instead of in online BBO. The reason why this paper presents it here for online setting should be discussed."
Vlo3Gad3YP,Vlo3Gad3YP,Diff-BBO:  Diffusion-Based Inverse Modeling for Black-Box Optimization,Reject,xzvrbX2PxY,ICLR.cc/2025/Conference/Submission9359/Reviewer_DvmK,"In traditional BBO a surrogate model $\\hat{f}$ is learned to approximate the objective function and then helps optimize an acquisition function which yields the next $x_{k+1}$ where to evaluate $f$. To select the next query point $x_{k+1}$ Diff-BBO instead performs posterior inference in the parameter space $X$ conditioned on a specified target objective value $y$ obtained by maximising an introduced Uncertainty-aware Exploration (UaE) acquisition function.

Conditional diffusion model are trained to learn the conditional distribution $p(x | y)$, where $x$ represents feasible inputs in the parameter space.

$y$ is chosen with a proposed acquisition function, Uncertainty-aware Exploration (UaE), that prioritizes target values $y$ with high expected objective values while minimizing epistemic uncertainty. This acquisition function balances exploration and exploitation. The paper provides theoretical proofs demonstrating that UaE achieves a near-optimal solution for the BBO problem.

Numerical experiments to support the work are presented",The paper introduces a new approach supported by solid theoretical results and empirical validation across diverse tasks. Difference with existing methods is clearly presented.,"An important part of the procedure is how is assembled the candidate set $\\mathcal{Y}$ and its corresponding weights $w$. The paper does not specify a principled method for choosing or tuning these weights, which makes this important aspect somewhat empirical given that for too high weights, the model may focus excessively on unfeasibly high values of $y$ while too low weights might limit the search space.


A diffusion model requires a large dataset to effectively learn the data manifold in the design space. If the function $f$ is expensive to evaluate, building a large dataset may be computationally expensive. Most of the experiments are run with a relatively high number of evaluations. How would the method perform on a smaller dataset?

Diffusion models are usually susceptible to mode collapse, where generated samples fail to cover the full distribution of the data. Was this observed? This could cause Diff-BBO to overlook potentially optimal regions in the design space.","Is there a systematic approach for choosing the weights $w$ for the candidate set $\\mathcal{Y}$, or is this step largely empirical?

Could the method perform effectively with a smaller accumulated dataset, as opposed to the relatively high number of evaluations used in the experiments?

Was this issue of mode collapse observed in Diff-BBO? If so, how does it impact the model’s ability to explore potentially optimal regions in the design space?"
Vlo3Gad3YP,Vlo3Gad3YP,Diff-BBO:  Diffusion-Based Inverse Modeling for Black-Box Optimization,Reject,6ui5MMv6bz,ICLR.cc/2025/Conference/Submission9359/Reviewer_cYdB,"This paper proposes a novel approach for solving black-box optimization problems. Unlike traditional methods that focus on learning a surrogate to evaluate design decision quality, this approach employs a diffusion model to approximate the distribution within the design space conditioned on a target value. Unlike existing inverse methods that assume access to an offline dataset, this paper studies a dynamic setting where the exploration-exploitation tradeoff must be considered. To address this challenge, an uncertainty-aware exploration method is introduced. The effectiveness of the proposed approach is demonstrated through extensive numerical studies.","- Overall the paper was very well-written. The key concepts and challenges are clearly introduced, which makes it easy for me to appreciate the contribution.
    
- Black-box optimization is an important methodology that has a wide range of applications in engineering and science. 

- The numerical studies are comprehensive and convincing.","- **Soundness of the theoretical analysis**. While I appreciate the authors efforts to justify the proposed approach through a theoretical lens, I found some of the results unsatisfying. 
    
  - For example, in Theorem 2, the bound does not depend on the number of samples collected in each iteration $N$. Intuitively, a large $N$ might lead to over-conservative estimates and a small $N$ might renders the estimate too optimistic. Relating the this bound to $N$ may lead to insights into the choice of this important hyper-parameter. The current bound is independent of $N$, suggesting that it might be loose. Furthermore, this theorem assumes the existence of a perfect diffusion model. I suggest the authors add further discussion on the implication/validity of this assumption. This comment applies to Theorem 3 as well.
  - Under a similar Lipschitz assumption, is it possible to derive surrogate approximation guarantees for the forward-based approach? If so, how does the inverse bound compare to the forward bounds?


- **Acquisition function.** This is a minor point, but I was wondering if a weight should be assigned to  $\\Delta$ in Equation (8) because (1) the two terms might be in different scales, and (2) the users may dynamically adjust the weight across different iterations to balance exploration and exploitation.",See weaknesses.
Vlo3Gad3YP,Vlo3Gad3YP,Diff-BBO:  Diffusion-Based Inverse Modeling for Black-Box Optimization,Reject,9FOd3IrRxN,ICLR.cc/2025/Conference/Submission9359/Reviewer_TAkd,"In this submission, the authors tackle the challenge of black-box optimization. At present, the bulk of the field has concentrated on methods like Bayesian Optimization, which employ a _forward_ model $p(y|\\mathbf{x})$ and an acquisition function $\\alpha(\\mathbf{x})$. Maximizing the latter at each iteration $t$ yields a candidate design $\\mathbf{x}_t$ that will in turn give a noisy evaluation $y_t$.

Interestingly, here, the authors use an _inverse_ model $p(\\mathbf{x}|y)$ together with an acquisition function $\\tilde{\\alpha}(y)$. As such, one can learn to generate designs $\\mathbf{x}$ conditionally to an output value $y$, such that the evaluated design results in a noisy evaluation $y_t \\approx y$. The precise value of $y$ that should be used for conditional generation is obtained by maximizing the acquisition function $\\tilde{\\alpha}$.

In this work, conditional generation is achieved through conditional diffusion models. While diffusion models have been used in black box optimization previously, building a suitable acquisition function to handle these models has not been done yet. The latter must adequately trade-off exploitation (high values $y$) and exploitation (values $y$ for which uncertainty model uncertainty $p(\\mathbf{x}|y)$ is high). The paper's main contribution is a thorough study of uncertainty quantification for conditional diffusion models, leading to a decomposition between _aleatoric_ and _epistemic_ uncertainty. This ultimately leads to an acquisition function that trades off high function values and epistemic uncertainty.

Finally, the performance of the proposed acquisition function is theoretically grounded depending on some assumptions on the target function $f$, and the method itself is shown to outperform concurrent baselines on a number of continuous and discrete datasets.","- I found the paper to be well-organized and motivated. The technical novelty is light but seems to be theoretically grounded. 

- The proposed method consistently ranks among the best competitors on a benchmark involving multiple baselines and datasets, while maintaining computation times in the same order of magnitude as Gaussian Process-based alternatives.","- From a practical point of view, I am not sure that Theorems 2 and 3 are useful: they assume the $L$-smoothness of the function $f$. This function takes as input $\\x$, which might be discrete, or an embedding of a discrete input like a molecule, and there is little to no chance to have $L$-smoothness in this case unless the embedding explicitly enforces that assumption. I believe this should at least be mentioned.","- An interesting ablation study would be to add a factor weight $\\beta$ in front of the epistemic uncertainty in Equation 8, and to vary this term. This would give an insight into how important uncertainty is in the acquisition process."
1GTARJhxtq,1GTARJhxtq,Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models,Accept (Poster),ZywBbWh82H,ICLR.cc/2025/Conference/Submission9185/Reviewer_qYWL,"The paper proposes that smaller language models effectively prune large datasets in a way that benefits the training of much larger model. Applying perplexity-based pruning techniques, they explore using a small model to filter high-quality subsets of data for training larger models. This approach is interesting because it’s a cost-effective alternative to using large models for pruning, and is applicable in real settings. The findings indicate benefits for downstream accuracy and training efficiency.

The paper demonstrates that a 125m parameter model can successfully prune data for large models and improve downstream task performance. The paper shows empirical results testing on The Pile and Dolma, two datasets with very different domain structures.
They also study the two settings of over-training and data-constrained setups and provide additional insights.","The goal, and the process, and algorithm are defined and presented very clearly. Experiments cover multiple settings, with different model sizes and training algorithms. 
The proposed method is super useful for researchers who investigate practical techniques for data curation, with insightful empirical results. 
Experiments include two very different dataset distributions, the Pile dataset and Dolma. The work shows thorough experiments for various selection rates and perplexity criteria, presenting strong evidence about settings in which perplexity pruning does and does not work.","Authors claim that datasets pruning increases the proportion of general domain data from web-scraped domains, and decreases the proportion of specific and technical domains. But it is unclear and counter intuitive why training on general domain data improves performance of models on benchmarks. I think the paper lacks analysis to explain this observation.","How do you expect the results to scale on models larger than 3B parameters? 

How does models' performance change on domains which are pruned the most?"
1GTARJhxtq,1GTARJhxtq,Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models,Accept (Poster),HIsB4Qqu4J,ICLR.cc/2025/Conference/Submission9185/Reviewer_wjeV,This paper presents a perplexity-based pruning method for reducing the size of pre-training datasets. The effect of pruning is evaluated through the performance on downstream tasks as well. Two datasets are used for evaluation: Pile and Dogma. The pruning efficacy is determined for over-trained and data-constrained regimes as well.,"- The paper addresses an important problem of pruning the pre-training datasets to enable efficient training of LLMs.
- The experiments are thorough and cover different dimensions of perplexity-based pruning. 
- The paper is well-written and the results are presented clearly. 
-  The findings are significant, as they show that perplexity-based data filtering can not only reduce the size of the pre-training datasets, it also leads to better performance on certain downstream tasks.","- The paper does not currently cover the computational complexity of the proposed pruning procedure. A few important questions that need to be considered in this regard:
    - How do the computational requirements for perplexity-based pruning increase with the size of the dataset to be pruned?
    - How does the cost of computing perplexity (before pruning) amortize over the efficiency improvements achieved while pretraining the model on the pruned datasets? 
- A discussion for choosing the right perplexity pruning method (low, medium, high) for the dataset should be included for the practitioners. From the experimental results, we can see that high perplexity selection performs better on Pile while medium perplexity selection is better for dolma. Can we extract any patterns from these results and other experiments that can be generalized to other datasets? 
    - For example, prior theory on data pruning for vision tasks shows that the optimal pruning strategy changes depending on the amount of initial data. When data is abundant, the better pruning strategy is to keep harder examples. In contrast, for smaller datasets, keeping the easier examples leads to better performance. [1] 
- The results show that test set perplexity may not always be a sound metric for evaluating a pruning strategy and that downstream evaluation is necessary. What should be the cheapest way of conducting the downstream evaluation of the correct perplexity pruning method, i.e., the one that can yield reliable results at a minimal cost? For example, could there be a small set of representative downstream tasks or metrics that could serve as efficient proxies for full downstream evaluation?

References:

[1] https://arxiv.org/abs/2206.14486","- A quantized model may lead to better inference efficiency while calculating the perplexity. Was this considered while running the experiments?
- High perplexity selection will also inevitably lead to the inclusion of a significant portion of the noisier examples in the overall dataset. How can we determine the proportion of such examples in the final dataset and exclude them reliably?
- Minor typo (line 66): perplexity-basd -> perplexity-based
- It would be useful to include the following closely related data pruning works in the related work section:
    - https://arxiv.org/abs/2403.07384
    - https://arxiv.org/abs/2402.09668"
1GTARJhxtq,1GTARJhxtq,Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models,Accept (Poster),vPqafKGHIJ,ICLR.cc/2025/Conference/Submission9185/Reviewer_wYdV,The authors filter LLM pre-training data by using the perplexity of a smaller language model. They demonstrate that dataset filtering improves the [initial] learning curve of LLM pre-training.,"The method is well motivated. Except for some uncommon terminology that is explained in later sections like ""non-standard training regime"", ""over-training"" (which is not over-fitting) the paper is clearly written.","L186 suggests that the final models are (pre-)trained for a fixed number of steps, no matter the dataset size. This sets the stage for dataset filtering, since training on the full dataset may go through fewer epochs. It would be interesting to train for long enough to show convergence in the plots in Fig. 1.  The story would be more convincing if there is an offset between the blue and red curves even after convergence. In fact, the ""over-training"" experiment in Sec. 3.4 shows diminishing gains, so I can imagine that they disappear fully at some point. The method would still have merits (steeper pre-training curve), just not the ones claimed in the paper.

Novelty. Perplexity-based pruning and countless variations of it are well-studied. The authors set their work apart from prior work in L058, but neither of the arguments (i)-(iii) (evaluation on downstream task, exploration of domain compositions, ""non-standard"" evaluation regimes) strike me as particularly strong.

I don't think that Algorithm 1 is really helping clarity. 1-2 normal equations would be just as expressive and more concise.

Edit: my point about novelty was unjustified - I have increased my scores after the rebuttal","- Fig.4 is interesting, but I'm not sure how Fig. 3 is relevant in practice - could you clarify?"
1GTARJhxtq,1GTARJhxtq,Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models,Accept (Poster),8BIMgXGP7t,ICLR.cc/2025/Conference/Submission9185/Reviewer_13P9,"This paper investigates whether a small model can be used to perform perplexity based data selection for a larger model. The key findings are that 1) a reference model with 30x fewer parameters compared to the larger model can be used to identify a subset of the training data which can improve the performance of the larger model relative to no pruning. 2) the filtered data subset can speed up training of the larger model, 2) the improvements carry over to some extent to over training and data constrained regimes, 3) ideal pruning criteria can vary by dataset e.g. for Pile, a high perplexity subset performs better while for Dolma, a medium perplexity subset works the best. The paper shows that test data perplexity is not a good indicator of the downstream task performance when using perplexity based pruning.","* Describes a simple approach to improve the performance of large language models using perplexity based data filtration using a smaller reference model.
* Presents useful results e.g. 1) filtration criteria varies by dataset type and 2) test set perplexity is not a good indicator of the downstream task performance.","* The main results (Table 1) do not include a random baseline i.e. what is the performance of a model trained on a subset of the data which has a similar size as the perplexity filtered buckets but is selected randomly?
* The paper does not contain ablations on the size of the reference model and sensitivity of the results to the random split (L113) used for training the reference model. Though exploring this space is computationally expensive, it may be useful to present 1-2 additional data points.
* It would be good to see some additional analysis to understand why a high perplexity set works better for one domain while a medium perplexity set works better for others.

Note: The authors have addressed some of these concerns (random baseline/sensitivity to random split) in  the rebuttal.","* L290: ""These results show that while the higher quality data resulting from perplexity-based data pruning does still lead to an improvement in downstream performance in the over-trained regime, there is not a relative increase in downstream improvement over the baseline when over-training."" It would be good to understand why this is the case since there are no repeats. 
* L314: ""That training on repeated perplexity-pruned data leads to diminishing gains after four repetitions post- pruning suggests that the higher quality data resulting from pruning does not change the point for which repeating data yields diminishing improvements in performance."" This sentence is confusing and should be reworded.
* In section 4.2, the paper presents results showing that the pruning affects data composition such that some domains (e.g. web) are oversampled compared to others (e.g. pubmed). It would be useful to perform additional analysis to understand why this is the case e.g. is it possible that the training split (L113) resulted in a smaller proportion of these domains for the reference dataset?"
B6AQzaQCsl,B6AQzaQCsl,Hot PATE: Private Aggregation of Distributions  for Diverse Tasks,Reject,Xjx7kbMwAU,ICLR.cc/2025/Conference/Submission9042/Reviewer_FQD1,"The paper introduces HotPATE, a method based on the Private Aggregation of Teacher Ensemble with the distinction that the method forgoes independent teacher data and models. In fact, the teacher coordinate their sampling such that upon aggregation of their votes, rare teacher decisions (for instance, rare tokens in the case of private synthetic next-token generation) can still be produced without requiring a lot of agreement between teachers. The paper claims this process improves the diversity of the resulting vote histograms without privacy cost of not having high agreement  (which is traditionally what ensures low PATE privacy costs for private prediction). A new definition for diversity-preserving aggregeation of distributions is presented. Empirical results show that under that definition, HotPATE improves upon ColdPATE. However, practical implications of the definition and broader contribution is unclear.","- Improving diversity of PATE responses is an interesting goal, given how much the privacy of PATE comes from teacher agreement (therefore lack of diversity in teacher votes).
- The idea of coordinated sampling of tokens seems novel. Although its privacy implications are unclear.","As someone who is quite familiar with PATE and its derivatives, I found this paper very hard to read and digest. I think there are a couple of reasons for this:

- **A robust privacy analysis is missing.** The paper introduces a particular histogram aggregation strategy that produces rate token frequency. In a sense, this is not an aggregation that produces a single vote but rather a transformed histogram. Overall, I found the presentation of this rather simple idea overly complicated in Section 3. However, the key issue here is not the contrived procedure and Definition 1, but rather the complete lack of privacy analysis under this new aggregation method. Let me clarify this point: the PATE privacy analysis only holds under the noisy argmax release. In particualr, the analysis is a function of the gap between the top vote and the second top vote of the histogram. If we were to use Def.1 and instead release transformed vote count (for the purposes of diversity), we are strictly releasing more information. In fact, since the rare token frequencies are kept (for diversity purposes), such a scheme will likely have higher privacy cost than releasing a full noised histogram of votes.

- **Writing and exposition is not polished.** The introduction is too long and full of technical detail with frequent forward references. None of the technical terms first appearance receive proper introduction.  I find page 4 almost completely incomprehensible as a result. New terms are frequently used before they are properly defined. For instance, ""homogeneous ensembles"" is used in Line 186 but partially defined in Line 191. Some terms are really never properly defined at all in the introduction (""diversity"", ""robustness parameter"", etc.)

- **Experimental results are limited.** The results are mostly validating that the algorithm produces more ""diverse"" tokens. I think this is necessary and good. However, throughout the paper it is unclear what the value of this ""diversity"" is. I was hoping the experimental results would showcases a concrete benefit from having more diverse tokens. For instance, better generalization (test error) on a down-stream task.

- **Empirical results contain no privacy quantification.** Although the paper seeks to find the trade-off between privacy and diversity, the empirical section contains no quantification of the privacy budget of the algorithm. Coupled with the fact that a proper privacy analysis is missing (see first point above) I have serious doubts regarding the privacy claims of the paper and the empirical section did not do much to alleviate them.","- Can you ground your notion of diversity in a practical example? Why should one adopt your notion of diversity? What utility does it bring? Can you provide concrete empirical results to support the benefit of improved diversity as you define it?
- I had a lot of trouble with your presentation of the suggested method as a privacy-preserving algorithm. Having read the paper, I am not convinced of claims such as Line 337:
  > This high agreement allows rare tokens to pass even with high privacy noise and allow for the aggregate distribution, with fixed privacy requirements, to satisfy Definition 1.  
Can you make a clear case for this?  
- Have I misunderstood part of your work? To be clear, I think as is, this paper is not ready for publication. However, I want to be fair and make sure that I have not misunderstood your work. So I'll be happy to engage with you during the rebuttal process."
B6AQzaQCsl,B6AQzaQCsl,Hot PATE: Private Aggregation of Distributions  for Diverse Tasks,Reject,1N5FKLKeYk,ICLR.cc/2025/Conference/Submission9042/Reviewer_YCGG,"This paper introduces Hot PATE, an extension of the PATE (Private Aggregation of Teacher Ensembles) framework, to settings where output diversity is important. PATE works by partitioning the data and training a teacher model on each partition. Then, for a given model input, the each teacher model ""votes"" on a label, and a final label is privately sampled from the teacher histogram. 

The key idea of Hot PATE is to preserve both privacy in the output label and the diversity of teacher distributions. The paper introduces the property of diversity preserving aggregation and introduces ensemble coordination as a technique to satisfy the property. Ensemble coordination strategically introduces correlation between teacher votes to ensure that rare tokens are transferred with high privacy noise, effectively mitigating the privacy penalty associated with high diversity, due to private sampling.

The authors provide an empirical demonstration of this approach in the context of in-context learning and show that Hot PATE yields greater diversity in output tokens.","- Introduces an extension of PATE that overcomes the diversity-privacy tradeoff
- Motivates the analysis through the notion of diversity preserving aggregation
- Connects proposed method with existing statistics literature: coordinated sampling
- Paper reads well, particularly with comparisons between hot and cold PATE","- The empirical analysis is more along the lines of a proof-of-concept rather than a thorough comparison. The paper would benefit from more systematic experiments between hot and cold PATE.
- No discussion of the limitations of the proposed methods. See question 1.","1. In practice, does increasing diversity ever harm utility?

Other Notes:

- Typo on Line 93: ""...include component that...""

- Typo on Line 190: ""...two use scenarios of applications...""

- Typo on Line 323: ""A tokens j that..."""
B6AQzaQCsl,B6AQzaQCsl,Hot PATE: Private Aggregation of Distributions  for Diverse Tasks,Reject,3GvQEvvw6W,ICLR.cc/2025/Conference/Submission9042/Reviewer_KdZL,"Private Aggregation of Teacher Ensembles (PATE) was designed for classification-like tasks where each datapoint has a single ground-truth label. For “diverse"" tasks such as sequential text generation, the responses might instead be distributions. But there is a tension between diversity and privacy: diversity in the responses reduces agreement among the teachers, which in turn requires a smaller noise scale and less privacy. This paper proposes “hot PATE” which allows for higher diversity in the responses without increasing the privacy cost.","* I think this paper has a significant contribution — via a carefully designed aggregation method, PATE can now thrive in a broader and more modern setting. Formalizing the notion of “diversity-preserving” (Definition 1) is also a helpful contribution.
* The PATE framework can now be applied to very fashionable problems such as in-context learning.","* The paper is not beginner-friendly and seems to assume a reader who is already very familar with DP, PATE and LLMs. In fairness, this probably is going to be the chief audience of this paper, but at the same time I find it somewhat egregious that differential privacy is never formally defined (even if the definition has to be deferred to the appendix due to space constraints).
* I felt that the privacy guarantees are not rigorously stated, DP implementations are largely left as poorly-described black boxes (e.g., NoisyArgMax in Algorithm 2 is never formally introduced) and none of the algorithms include the privacy parameters as input. I didn't see a formal privacy analysis that can be easily verified, and in terms of reproducibility I feel like the algorithms can’t really be implemented without knowing, for example, how to calibrate the noise scale.","* Besides coverage and diversity, are there other metrics which could be used to demonstrate the effectiveness of hot PATE?
* Line 274: If I’ve understood correctly, “the noise scale must satisfy $\\sigma << \\arg \\max_j c_j$” is a requirement on the utility, and not the privacy? It might be helpful to explain this more thoroughly."
B6AQzaQCsl,B6AQzaQCsl,Hot PATE: Private Aggregation of Distributions  for Diverse Tasks,Reject,ScJ2VUbSwS,ICLR.cc/2025/Conference/Submission9042/Reviewer_QVdG,"This paper introduces ""hot"" PATE, an extension of PATE designed for in-context learning via prompts, addressing tasks that are ""diverse"" and open-ended. They empirically demonstrate the potential of hot PATE for in-context learning.","1. The motivation is clear: sequential text generation tasks through in-context learning are inherently diverse (""hot"") with multiple valid responses.


2. The idea of aggregating responses from different teachers to maintain both diversity and privacy is interesting.","1. My primary concern is the empirical evaluation. The utility of in-context learning is typically measured by accuracy in the literature (e.g., [1,2,3]). However, this paper does not report in-context learning accuracy on specific tasks. It is unclear how much benefit hot PATE can provide for in-context learning. Additionally, the experiment is conducted on only one dataset, which is insufficient, and there is only one baseline (""cold"" PATE). It is unclear why comparisons to prior in-context learning work (e.g., [1,2,3]) are not included.


[1] Duan, Haonan, et al. ""Flocks of stochastic parrots: Differentially private prompt learning for large language models."" Advances in Neural Information Processing Systems 36 (2024).

[2] Tang, Xinyu, et al. ""Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation."" The Twelfth International Conference on Learning Representations.

[3] Wu, Tong, et al. ""Privacy-Preserving In-Context Learning for Large Language Models."" The Twelfth International Conference on Learning Representations.


2. The paper states that Wu et al. (2023), Lin et al. (2024), and Xie et al. (2024) are independent concurrent work, which is inaccurate. These should be considered prior work, as Wu et al. (2023) and Lin et al. (2024) were published at ICLR 2024, and Xie et al. (2024) at ICML 2024.


3. I suggest extending the literature review of this paper by including the work ""Tang, Xinyu, et al. Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. The Twelfth International Conference on Learning Representations."". This work studies differentially private in-context learning and proposes to use the sample and aggregate framework to generate DP synthetic examples for in-context learning inference. It could also serve as an experimental baseline for comparison.




3. Some typos:

(1) I recommend ensuring the correct application of \\citet and \\citep.


(2) Missing periods in Line 299, Line 396, and Line 427.",As in the weaknesses.
QKBu1BOAwd,QKBu1BOAwd,From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions,Accept (Oral),o0LorTHcnI,ICLR.cc/2025/Conference/Submission8990/Reviewer_C1G3,"This paper highlights the importance of proper tool documentation to improve tool learning. Due to incomplete documentation or consideration of human intuition, tool documentation may not include all the information to be useful for an LLM. To resolve these issues, the paper proposed DRAFT that iteratively gathers ‘experience’, learns from it, and rewrites the documentation for future usage.","1. DRAFT significantly outperforms across two benchmarks and outperforms three baselines.
2. It also verifies the results through a small human study — showing its effectiveness in practical use cases.
3. The paper provides an in-depth analysis of the impact of iteration count on the performance as well as the improved performance for retrieval in the rewritten documentation.","1. According to the first contribution highlighted in the introduction, DRAFT highlights ‘inefficiencies and inaccuracies within existing tool documentation hamper the effective utilization of tools by LLMs’. However, the paper does not show any quantitative evaluation of this claim rather than explaining the problems associated with it. Hence, I think the ‘highlight’ itself may not be a novel contribution as much as the framework.
2. For ‘tool-adaptive termination mechanism’, is it possible to measure the impact of different threshold on the performance of DRAFT?","1. The paper highlights how the count of iteration impacts the performance of DRAFT mentioning - ‘a decline in performance is observed after a certain number of iterations. This decline may be attributed to the introduction of redundant information as the number of iterations increases, potentially leading to overfitting’. Is it possible to measure the count of redundant information in the system? It would be nice to see some sort of hallucination error measurement on top of DRAFT."
QKBu1BOAwd,QKBu1BOAwd,From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions,Accept (Oral),C7vHJ17Up8,ICLR.cc/2025/Conference/Submission8990/Reviewer_JQEK,"This paper addresses the tool usage challenge for LLMs. The core idea is to develop a explore-analyze-rewrite approach to refine tool documentation to improve LLMs' ability to use tools that may have lower quality documentation. To enhance tool documentation refinement, DRAFT encourages diverse queries to try out different queries to trigger different tool behaviors; these execution results are then leveraged into the analysis phase for documentation rewriting. 

DRAFT is evaluated on ToolBench and RestBench, results on CP% and WIN% shows that the refined document improves task completion, especially compared to closely related EasyTool. Detailed analysis of iteraction rounds, ablation results on diversity and adaptive shows significance of features in DRAFT. Also, it's interesting to learn that refined document could also potentially benefit tool retrieval and human understanding.","This paper provides a well designed approach for improving LLMs' tool using ability without additional training. The document refinement approach, while not new, is well designed considering how to leverage LLMs' self-reflection ability and trial-and-error approach to enhance document quality. This paper has the following strengths:

1. Leveraging diversity of exploration and trial-and-error to compensate potential limitation of the the initial document. Where other refinement approach like EasyTool may not be able to obtain such extra information.
2. The evaluation has fair comparison with related work to highlight performance gain. The detailed analysis show additional benefits of this approach.","This paper can be improved upon the following two areas:

1. Please elaborate more in detail about Explorer design and evaluation. DRAFT relies on diverse queries/parameters to trigger different tool behaviors. It looks like the query generation depends on LLM to sample parameters automatically based on tool signature as provided by toolbench. It would be ideal to analyze how diverse in terms of parameter coverage, especially for certain types of parameter like arrays / strings. I imagine if the document is indeed incomplete as mentioned in the intro, these special cases may be less likely going to be hit during exploration. While existing benchmarks conveniently provide well defined parameter domain, it would worth the authors to include in the discussion about how to generalize this for tool documents without good parameter documentation.

2. The exploration-revision loop essentially helps the system to collect more input-output examples that previously doesn't exist in the document. A qualitative analysis between examples generated by revision-only based tool like EasyTool and new examples generated by DRAFT would be helpful to understand this nuance difference.

3. The intro motivates different types of documentation limiations (incomplete, redundant, inaccurate), it would be good to provide qualitative feedback about which scenarios DRAFT is best at improving, especially comparing to prior tools.","Check above. In general, this paper is well written, and the experiments are quite convincing to me. Please check above for clarifications about (1) parameter space exploration, (2) analysis of example quality and (3) types of documentations DRAFT best at targeting comparing to others."
QKBu1BOAwd,QKBu1BOAwd,From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions,Accept (Oral),43wMQhHkk2,ICLR.cc/2025/Conference/Submission8990/Reviewer_CrZf,"This work introduces a new prompting workflow for improving tool documentation iteratively called DRAFT. While prior tool documentation is typically iterated on manually and laboriously, this work focuses on automating the process of improving documentation via a three step procedure that can be repeated multiple times. The outputs of prior phases (e.g. Explorer, Analyzer) are fed into subsequent phases (Analyzer, Rewriter), where each phase produces prior experiences and filtering that is then used to refine the prompt further. To encourage diverse exploration and ensure that DRAFT terminates at some point, the authors introduce new strategies that encourage such convergence. Following the experimental setup of prior works, the authors show that DRAFT outperforms previous interactive prompting methods and run ablations demonstrating the effect of different parameters and effects of DRAFT.","The motivation established by the authors is sound and makes sense. In addition, the formalizations introduced in Section 2 make it easy to follow how the outputs on one prompt feed into the next. The use of an exploration phase is interesting, similar to fuzzing of a program for software engineering tests. The experiments are sound and it follows prior works. The related work covered by the authors is quite comprehensive. The ablations are complete, as the authors do a good job examining the several parameters of DRAFT.","- While they are clearly thoughtfully made, I would recommend Figures 1 and 2 be simpler to make it easier for a reader to understand what is going on. The captions would also benefit from being more specific about what is going on in the diagram. For instance, for Figure 2, I did not understand what “Trail” (I think this was supposed to be “Trial”?) refers to, is it an iteration of explorer/analyzer/rewriter?
- The methodology section, while clear in how the symbols relate to one another, feels somewhat ungrounded in terms of why certain equations or prompt templates were used. For instance, on line 299 - “when the documentation is adequately aligned with comprehension” - what does this mean? What is adequately aligned? I think introducing more citations and explanations for why contributions such as the diversity and termination mechanisms would be needed for a more rigorous justification of why this method is interesting, and not just a prompt based workflow that happens to work.
- DSPy (Khattab 2022) performs prompt optimization with multiple rounds of trial/error against a validation set. What makes DRAFT different from DSPy as a prompt optimization technique?
- Although the method is proven to be effective in the experimental results, I think it would be nice to see, qualitatively, the effects of better documentation on downstream performance. While it seems that DRAFT improves the quality of the documentation, it is not obvious why better documentation improves empirical performance. For instance, it may also be possible that more “concise” documentation does not necessarily lead to improvements in downstream performance.","- Equation 1: What is M? Is this supposed to be A, the Explorer?
- Line 209: How do the two components come together to make up the diversity promoting exploration strategy? Two different constraints were explained, but how do they come together to promote diversity? Is this explicitly included in Algorithm 1 somewhere? Also, is there any citation or support for why self-reflection would promote diversity? How do we know this works?
- Equation 3: Again, what is M?
- Line 281 - “Analogous to recipes requiring different levels of expertise, some tools may reach optimal documentation faster than others”. Any citation for this? What is “optimal” in this case? Also, what does faster mean? Is it in terms of the number of trials? Or the number of explorations needed?
- Equation 5: How did you come up with this? Ignoring empirical performance, why does similarity score + BLEU score work for measuring the degree of change? Why are they weighted equally (e.g. why not weigh surface form similarity / BLEU higher than semantic similarity?)
- Experimental Setup: How many rounds of DRAFT iterations were run (did DRAFT ever not terminate within a certain number of steps)? Were the initial documentations the original ones from DFSDT or ReAct?
- Typo Line 326 - “EayTool”
- The human study detailed in Line 473 onwards feels like it could be better explained. Are the doctor-al (Line 477) students part of the author group? What examples or guidelines did the human annotators use to judge the three criteria correctly? Did the human annotators generally agree with each other (Fleiss Kappa score)?
- Section 2 read a bit repetitive at times - was this section written with the assistance of a language model? It may be easier to read with less redundancy. Specifically, the beginnings of each of the subsections essentially repeat what was stated in the introduction and prior subsections."
UVnD9Ze6mF,UVnD9Ze6mF,AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories,Accept (Spotlight),DNB7xviXXM,ICLR.cc/2025/Conference/Submission8850/Reviewer_pHn2,"The authors introduce AIR-BENCH 2024, a new safety benchmark for AI models grounded in real-world regulations and policies. Using risk categories from 8 government regulations and 16 corporate policies, they crafted a set of 5,694 prompts covering 314 specific risk scenarios to test models’ ability to handle sensitive content safely. They evaluate some current LLMs on this benchmark.","Alignment with actual regulations: A novel feature of this benchmark is its grounding in real-world regulations. By basing AIR-Bench on key regulations from the EU, US, and China, it addresses critiques that existing AI safety benchmarks lack practical relevance, providing a benchmark that better reflects legal requirements.

Granularity of risk categories: The four-level risk structure comprises 314 specific risk categories. The detailed granularity should be appreciated for safety alignment.","1. Given that the paper bases its alignment on actual regulations, I am curious how it manages to balance or trade-off conflicts between regulations across different countries and regions. As one key motivation of the paper is its combination of regulations from various regions. How does it handle potentially conflicting elements, such as differing privacy laws and, more broadly, varying definitions of ""appropriate"" outputs across populations in different countries? Or do they just ignore it? 

2. The use of only GPT-4 for scaled generation from manually crafted templates seems somewhat artificial. I understand that a more expensive annotation method may not be practical. However, there may be some potential bias introduced by scaling generation with a single model from simplistic, handwritten templates. 

3. Regarding presentation suggestions, while the tables in the results section are indeed extensive, the takeaways seem limited. From my understanding, they represent combinations across two dimensions: different categories and models. It may be challenging for readers to identify key insights as there are so many big result tables. And I would appreciate a more in-depth discussion to highlight critical findings.","I still find the discussion somewhat not so in-depth thus:

Any insights into conflicts or correlations between AI compliance regulations across different countries while working on this benchmark? This might hold significant practical value for cross-border AI services."
UVnD9Ze6mF,UVnD9Ze6mF,AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories,Accept (Spotlight),SOVDn7CPof,ICLR.cc/2025/Conference/Submission8850/Reviewer_MPSz,"The paper introduces a safety benchmark for assessing LLMs based on regulatory and policy-driven risk categories. Leveraging an AI risk taxonomy, which unifies risks specified in government regulations and corporate policies, AIR-BENCH categorises multiple specific risk types and uses thousands of designed prompts to evaluate LLMs’ alignment with regulatory safety standards. The benchmark assesses model refusal behaviours across risk categories by simulating scenarios of potentially harmful content requests. This study evaluates 22 prominent LLMs, highlighting safety gaps and inconsistencies in regulatory alignment.","- The benchmark's alignment with diverse government and corporate regulations makes it very relevant for real-world applications, and it addresses a gap in existing benchmarks
- The benchmark itself is well structured with multiple levels of taxonomy which helps identification of model deficiencies across multiple risk categories
- The experiments include detailed comparison of multiple models, identifying key trends and areas where models fall short of regulatory expectations
- The paper is well-structure and easy to read, it introduces related work in detail and 

Overall, I like this paper a lot. It is of high quality and tackles a very relevant issue.","There are some weaknesses, but they are not very severe. 

- Using GPT-4o as both prompt generator and judge could introduce biases in evaluation, but this is appropriately acknowledged 
- Different regulations have separate restrictions for providers, distributors, deployers, etc, the way this is handled in the benchmark is not obvious. I.e. should the API refuse protected decision making (health/financial) unless the client has guaranteed sufficient oversight? What if the client has oversight? This nuance is hinted at in section 4.4 but a deeper discussion on this difference would be very helpful. 
-  The benchmark assumes that the model operates as a standalone system. However, many real-world applications involve human oversight, which might mitigate risks differently than this evaluation assumes.
- Given the evolving nature of AI regulations, the benchmark may become outdated quickly, limiting its long-term applicability unless it is regularly updated. The authors note this already, but it would be interesting to hear their thoughts on potential future paths towards mitigating this
- About 70% of the judge prompts remain model-generated, which might lead to inconsistencies in judgment fidelity, particularly in ambiguous cases
- Although the benchmark focuses on regulatory alignment, making it a net-positive, a more thorough examination of potential societal impacts could strengthen the discussion, i.e. could malicious actors use the benchmark to help identify weekly guarded models to speed up their activities?","-  If a response is neither explicitly refused nor overtly harmful, how is it classified within the framework?

- The paper notes that “Human experts then manually review and revise these expanded prompts to maintain prompt quality and alignment with the intended risk categories.” Could you provide additional details on the methods, criteria or protocols used in this manual review process?

- Where were the experiments run from? I.e. the IP region of the client. In case model providers are serving different levels of steering/guardrails based on user location, even if they use the same model/version identifier. This will be good to add in the de-anonymised version for increased reproducibility."
UVnD9Ze6mF,UVnD9Ze6mF,AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories,Accept (Spotlight),IxB1AiALh8,ICLR.cc/2025/Conference/Submission8850/Reviewer_D5kz,"In the paper under review, a benchmark on the safety and regulatory conformity of LLMs in proposed. The benchmark is a multi-tiered one, and is based on policies of various governments and companies. The key idea is to generate a set of instructions, which is curated and generated over multiple iterations, that is than fed to the LLM under examination. The responses of the LLM on these instrucations are then evaluated automatically.","* The paper adressing a very relevant topics. There is a lack of approaches for assessing the safety of LLMs in an automated fashion. This paper is a big step in this direction.
* The paper is very well structured and easy to ready. I light the color-coding of levels of the approach. This supports the reading a lot.
* All relevant literature is considered in this paper.","I do not see many weaknesses in this paper. Actually, I only see the limitation that the benchmark addresses dialects and authority statement, but it seems that this only holds for English. Other languages are not supported / evaluated. So there might be a risk that harmful output of an LLM in non-English languages are not assessed or overlooked.",Based on the limitation I mentioned: what about adressing different languages than English?
UVnD9Ze6mF,UVnD9Ze6mF,AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories,Accept (Spotlight),z1QXzM7xSd,ICLR.cc/2025/Conference/Submission8850/Reviewer_9PGQ,"The paper introduces a new safety benchmark for large language models (LLMs). Unlike existing benchmarks, this one is grounded in a taxonomy of undesirable behaviors derived from legal and policy frameworks. The benchmark consists of prompts categorized into various risk categories. Additionally, the authors evaluate several LLM models against this benchmark.","Overall, despite the paper being positioned more “grandiosely” than its actual contributions (see weaknesses), the proposed dataset of prompts clustered in tiered categories can be useful for the community studying refusal of LLMs to engage in undesirable behaviour. The authors have demonstrated that their dataset is more diverse and covers more categories than prior benchmarks. Beyond creating the dataset, the authors have also produced an automated evaluation system and have validated it against human raters. Because of the comprehensive and principled benchmark creation and evaluation in this paper, I would recommend its acceptance.","The paper seems to oversimplify the notion of “safety and risk” and implies that higher refusal rates on prompts related to specific prompts implies more safe models. However, the notion that a model can, in of itself, be safe or not, is rather simplistic. It is only when one acts in an unsafe way, as a direct result of a model’s response, that is unsafe. The model provides information and information itself cannot be unsafe, acting on information might or might not be unsafe or carrying risks. Therefore, the present benchmark offers a method of evaluating whether a model will refrain from potentially undesirable set of behaviours, rather than whether it is inherently safe or not. 

While the paper’s Section 4 “Evaluation and Takeaways” does indeed refer to “refusal” rather than “safety”, the rest of the paper doesn’t. This  leaves the impression that this benchmark measures the much more fundamental, complex and nuanced problem of “safety and risk” rather than the more grounded “refusal rate” that it actually does. Therefore, I’d recommend that the authors reconsider their positioning to align it better with the nature of their contributions.",See weaknesses.
nzjSvVZBIp,nzjSvVZBIp,An Effective Manifold-based Optimization Method for Distributionally Robust Classification,Accept (Poster),b69d5F2QFS,ICLR.cc/2025/Conference/Submission8724/Reviewer_v52n,"The authors propose a manifold-based DRO method that incorporates the geometric structure of training data to construct the uncertainty set. By integrating contrastive learning with Jacobian regularization, the method effectively captures the data manifold, providing theoretical guarantees for robustness. Experimental results on benchmark datasets demonstrate improved accuracy and robustness compared to conventional DRO methods.","1. The proposed manifold-based DRO method effectively captures the geometric structure of training data, leading to better robustness and accuracy compared to traditional DRO methods. This is achieved by integrating contrastive learning with Jacobian regularization, which helps in maintaining the data manifold structure.
2. The method provides theoretical guarantees for robustness by utilizing a novel approach to approximate geodesic distances on manifolds. This ensures that the model remains reliable even when faced with distributional shifts in the data.
3. Despite the complexity of incorporating manifold constraints, the proposed method is easy to implement and computationally feasible. The authors demonstrate this through experiments on popular benchmark datasets, showing that the method achieves superior performance without significant increases in computational cost.","I believe the motivation of this paper is reasonable and the method is effective. However, I would like to ask the authors whether the effectiveness of the algorithm depends on specific data? What kind of data characteristics would give MWDRO a greater advantage?",See Weaknesses above.
nzjSvVZBIp,nzjSvVZBIp,An Effective Manifold-based Optimization Method for Distributionally Robust Classification,Accept (Poster),gtRB3Vh1jc,ICLR.cc/2025/Conference/Submission8724/Reviewer_qtHQ,This paper proposes performing distributional robustness (DR) over data manifolds. It develops the dual form of DR over data manifolds and    uses the top singular principal vectors of the Jacobian matrix to characterize the tangent space of the data manifolds.,"- The idea of developing distributional robust on data manifolds is intuitive and interesting.
- The paper is well-written.","- It is unclear why we can use the linear span of top singular vectors of the Jacobian matrix of g can characterize the tangent space of data manifolds. The current explanations do not really convince me. Moreover, the experiments in Figure 2 only demonstrate that there are some dominant singular vectors and I cannot see how it explains why $T_{appr}(x, \\tau_0)$ is a subspace of the tangent space.
- It is also unclear to me why the game in Section 4.2 helps extract tangent information. Evidently, the CL loss encourages the features of data examples in $M_{SI}(x)$ to be close, whereas the Jacobian regularization suppresses the feature variations for perturbations of x across all directions. They seem to share the same purpose and nature.
- It is unclear to me why we can use the $pt^t$ in Alg 2 to approximate the geodesic distance from $q^t$ to $q^0$. 
- It is unclear how to do Exp operator to retract to the data manifolds. This seems impossible because we only assume that data lie in manifolds but we do not have anymore information of these manifolds. The Exp operator appears closed-form just for some simple and specific manifolds.
-  The proposed method is very computationally expensive, even more expensive than adversarial training. This is because we need to compute a trajectory for each data example which requires computing Jacobian matrix and also the gradients to data examples. The analysis on the computational complexity is necessary.
- The experiments do not demonstrate the benefit of performing DR on data manifolds. It would be better if the authors keep the same CL loss and Jacobian regularization, while replacing their manifold DR by standard DR on data space for a comparison. Moreover, the authors should provide the visualization of $q^1$,...,$q^t$ on the trajectory to see if we can make them on the data manifolds.",Please address my questions in Weaknesses.
nzjSvVZBIp,nzjSvVZBIp,An Effective Manifold-based Optimization Method for Distributionally Robust Classification,Accept (Poster),j9qpizWLYn,ICLR.cc/2025/Conference/Submission8724/Reviewer_Q211,"This paper introduces a manifold-based Wasserstein Distributionally Robust Optimization (WDRO) method aimed at improving the robustness of deep learning models. Furthermore, to tackle the challenges posed by the data manifold structure, the authors propose a game that integrates contrastive learning with Jacobian regularization.","S1. The paper is well-organized and presents its ideas clearly.
S2. The proposed methods demonstrate superior performance compared to state-of-the-art algorithms.
S3. The design of the manifold-guided game is a novel concept, leveraging neural networks to encode manifold information, particularly given the absence of a closed-form representation for the data manifold.","W1. The motivation for this work could be strengthened. In the Introduction, the authors mention that selecting an appropriate threshold for uncertainty is challenging and introduce a new uncertainty set by assuming that data is supported on a manifold. However, constraining the uncertainty set to manifolds also raises the same issue of threshold selection.
W2. The explanation of how the game aids in extracting tangent information is not intuitive enough. Including a figure to illustrate this concept would be beneficial.
W3. As the experimental results show, MWDRO takes more time compared to other algorithms, and it would be desirable to have a further discussion on this part of the reason.","Q1. In Figure 3, the accuracy of each model is already distinguishable even when the noise size is 0. Could you provide further discussion on this observation?
Q2. Is the use of contrastive learning essential in the design of the manifold-guided game? Are there alternative optimization objectives that could be utilized instead?
Q3. As noted in W3, could you further analyze which phase of training contributes to the increased time required for MWDRO?
Q4. Is it possible to further optimize the design of the algorithm to reduce the time to loss? For example, by utilizing simplified geometric computations or low-rank approximations."
nzjSvVZBIp,nzjSvVZBIp,An Effective Manifold-based Optimization Method for Distributionally Robust Classification,Accept (Poster),Fa7hPDJQtd,ICLR.cc/2025/Conference/Submission8724/Reviewer_P9ds,"This paper proposed a manifold-based distributionally robust optimization method to promote the robustness of existing deep learning models. Specifically, it designed a game that trades off between CL and Jacobian regularization to solve the DRO problem constrained by the data manifold. Both theoretical and empirical results show the robustness of the proposed method.","1. The proposed method is simple and effective. 
2. This paper provides a comprehensive analysis of their proposed method.","1. This paper should be further polished and re-organized. The introduction and methodology section should be more concise, and some contents should be moved to a more relevant part. I provide the details as follows: 1) the introduction of WDRO from line 49 to 71 can be more brief. 2) I can understand that the authors discussed the most relevant literature in Section 1.1, but the structure is a little strange. I suggest considering this part as a discussion part such as ""Relation to xxx"".  3) In the methodology, I also suggest the authors provide the literature or experiment support for the sentence that ""we should emphasize that the learned representation for manifold from neural networks is not sufficient for extracting tangent space"". 4) Eq (2) as a significant part of the methodology is mentioned in Section 1, which makes the methodology separate. I suggest the comparison between WDRO and MWDRO in the introduction being high-level, while its details can be put into Section 3 as the motivation. 
2. Some significant ablation studies are missing. For example, the effect of $\\lambda$ and model architectures has not been discussed. I suggest the authors test a range of values of $\\lambda$ to test its effect. As for the model architectures, the authors only provided ResNet18, which is insufficient. I suggest the authors try some state-of-the-art models such as ConvNext and Swin.
3.  The authors should clarify the advantages of the proposed method compared with [1] in Related Works and Experiments.

[1] Liu J, Wu J, Li B, et al. Distributionally robust optimization with data geometry[J]. Advances in neural information processing systems, 2022, 35: 33689-33701.","1. Could you please discuss the effect of $\\lambda$ and how to select the optimal value?
2. Could you verify the effectiveness of your proposed method on different model architectures such as ConvNext or Swin?"
nzjSvVZBIp,nzjSvVZBIp,An Effective Manifold-based Optimization Method for Distributionally Robust Classification,Accept (Poster),Xz3SE82kad,ICLR.cc/2025/Conference/Submission8724/Reviewer_m5cH,"This paper presents a novel manifold-based Distributionally Robust Optimization (DRO) method aimed at enhancing the robustness of deep learning models against distributional shifts. By integrating contrastive learning with Jacobian regularization, the proposed approach captures the manifold structure of the training data to construct a more accurate uncertainty set, thereby improving classification performance under various distributional changes.","1.	Writing: This work is presented with good writing style, where the summarized problems with detailed explanations make it easy for readers to understand the problem addressed in this article. However, there exist minor spelling and grammatical errors. The example in Fig.1 helps to understand the connections between geometric representation and semantic tasks.

2.	Novelty: It incorporates geometric structure into the construction of the uncertainty set, potentially leading to more realistic distributional assumptions. Contrastive learning with Jacobian regularization to encode manifold information is novel and seems to consume more geometric information. 

3.	Theory: Theorems 1&2 based on geometric scheme provides guarantees on approximation and gradient estimation, offering strong dual reformulations and approximation techniques for geodesic distances, which are crucial for establishing the method's reliability. 

The proof seems solid but I have not carefully checked the whole Appendix. 

4.	Experiments: Several scenarios and recent baselines are considered, implying improvements in accuracy and robustness under various distributional shifts.","1.	Approximation. There exists a gap between optimal solutions from original and dual problems. I’m not sure if Theorem 1 states the distances between the original and dual objects? 

2.	Selection on v. The bound derived in Theorem 1 heavily relies on v. I would appreciate it if the authors could give more detailed illustrations on the “dynamic mechanism” on setting v empirically?

3.	Broader baselines and empirical settings. 
For example, the settings for “Noisy Data” are kind of simple. What’s the variance of the added Gaussian white noise? It is suggested to follow the empirical settings in [1] to widen the difference between the training and testing sets, see Table 1 in [1]. 

More DRO approaches [1-3] for learning from noisy data are suggested to be included.
Moreover, it is kindly suggested to add introductions to these baselines in the supplementary file for better readability.

[1] https://arxiv.org/abs/2305.14690
[2] https://arxiv.org/abs/1902.07379
[3] https://arxiv.org/abs/2006.04662

4.	Illustrations on manifolds. The paper could benefit from a more in-depth analysis of the algorithm's scalability, especially regarding its performance with large datasets or specific manifold structures. 
Synthetic experiments on toy examples with already known geometric structure, e.g., the Swiss or Torus, could help to visualize and estimate the investigated manifolds.


5.	Computational Cost. While the theoretical underpinnings are well-developed, the paper may not provide a comprehensive assessment of the computational efficiency and practicality of the proposed method in real-world applications. Like the computational complexity analysis or empirical time/memory cost.","Please see the comments in Weakness.

I would be happy to raise my score if these issues are well addressed."
52Idqv2FNY,52Idqv2FNY,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,Reject,DV06H4OqGP,ICLR.cc/2025/Conference/Submission8665/Reviewer_sJ5t,"The paper initially explores the correlation between NLP benchmarks and human evaluation. With the advent of increasingly capable LLMs, human evaluations have become a steady and major alternative choice to evaluate the efficacy, performance and capabilities of LLMs. An important question that generally arises with the choice is whether NLP benchmarks are useless since human evaluations are costly and time consuming and are not always a gold standard. Where do NLP benchmarks fall? This paper explores this question and also explores the possibility of predicting human evaluations from NLP benchmarks. 

Two key questions are asked:
- To what extent are human evaluations and NLP benchmarks correlated?
- How well can benchmarks predict expensive and time-intensive human evaluations?

The researchers use all the Llama chat-2 models (7,13,30 and 70B parameters) to establish this, which were trained on 2T tokens and fine-tuned using SFT and RLHF. Human evaluations are collected by evaluating the Llama2 chat models pairwise against ChatGPT 3.5 on dataset a of single-turn and multi-turn prompts, where responses are sampled from each model. 3 Human annotators independently provide a pairwise comparison on the Likert scale (1 to 7, where 1 means chat llama preferred and 7 means chatgpt 3.5 preferred). uThey end up doing a large-scale study spanning factual questions, language assistance, writing, procedural questions, reasoning and many more.  The Chat Llama 2 models are evaluated on many popular NLP benchmarks right from AGI Eval, Ai2 Reasoning Challenge, Big Bench Hard, Boolq, commonseqa, GSM8k, MMLU, MATH, QuAC, PiQA and many more. Standard evaluation processes are used. 

The findings revealed that NLP benchmarks are broadly highly correlated with human evaluations, with certain benchmarks showing particularly strong correlations. The most predictive benchmarks included specific subsets of MMLU (covering topics like nutrition, human aging, and sociology), portions of BIG Bench Hard, HellaSwag, ARC, RACE, PIQA, Natural Questions, QuAC, and CommonSenseQA. However, some benchmarks showed weaker correlations, including ETHOS, Kth Sentence, most of Inverse Scaling, OpenBookQA, COPA, SciBench, and SIQA.

Using overparameterized linear regression, the researchers successfully demonstrated that NLP benchmark scores could predict human evaluation scores with reasonable accuracy. Despite the small sample size of only four models, leave-one-out cross-validation showed promising results, suggesting that faster and cheaper NLP benchmarks might effectively predict slower, more expensive human evaluations in many cases.

The authors note several limitations, including the small sample size, the assumption of linearity in their predictive models, and potential limits to generalizability across different model families thus rounding up the study and paving the way for future work as well.","1] The question at the center of the paper -- ""Correlation between NLP benchmarks and Human Evaluations,"" is an important central question to NLP evaluation in general. Human Evaluations are considered (somewhat so) the gold standard of evaluation but are extremely time-consuming and expensive to run; as models get more capable the human evaluations also get even more costlier because now we require experts to evaluate vs requiring less advanced folks earlier, but we can reliably construct more difficult benchmarks for models, so if these two things are correlated, perhaps lesser focus can be placed on human evaluations. 

2] Predicting Human Evaluations is a difficult task, and LLMs as judges are being increasingly explored as an alternative to human evaluations. The method in the paper also showcases some important insights into this process.","1] The small sample size brings into question the generalizability of these insights and results.

2] Only uses GPT-3.5 as the comparative model, no insight is provided into why this is the case? And also lacks any discussion of whether chatgpt 3.5 is a reasonable choice of a baseline. 

3] Perhaps a granular analysis of what makes a benchmark more correlated? Is there something common in the correlated benchmarks? This would also pave the way to designing and determining better benchmarks.","1) Why chatgpt 3.5? Could you justify the choice of this model? Why was chatgpt 3.5 the model chosen for comparison, is it a reasonable choice for a baseline? 

2) Could you generally talk about the distribution of the Likert scale that you got from the pairwise evals? Was there anything at all in which chatgpt was substantially better and generally chosen? (Assumption here that I suppose llama-2 would be usually better than chatgpt 3.5 in all cases)

3)  if these outputs were obtained from Chatgpt 3.5, which API was it received from, and what was the exact cutoff (e.g., ChatGPT-3.5-0604, etc.)?

4) Pairwise evals ultimately show revealed preferences and model choice between two outputs. Do you think this translates to human evaluation directly on model outputs (not comparisons) on NLP parameters like coherence, semantic relevance, factual relevance, etc.? Could you comment on the choice of pairwise evals?

5) Just a general question about related work: is there no related work? (while this correlation aspect might not have been explicitly studied), Studies have considered which of them is better in MT, summarization, and other NLP areas. Can you provide a more comprehensive overview of related work, including studies that have compared human evaluations and benchmarks in specific NLP tasks like MT/Summarization, and contextualize this work in the broader field?

6) Could you conduct a detailed analysis of features/characteristics shared by highly correlated benchmarks? I think that would help a lot in designing benchmarks in the future."
52Idqv2FNY,52Idqv2FNY,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,Reject,oqUDEGnPsN,ICLR.cc/2025/Conference/Submission8665/Reviewer_evg9,"The paper studies the relationships between the evaluation results of automated NLP benchmarks and those of human evaluation. It mainly revolves around two research questions: how well human evaluations and NLP benchmarks are correlated with each other, how well NLP benchmarks can predict human evaluation. Specifically, the authors develop a set of 1917 prompts organized by areas, categories, and subcategories, selects four LLMs from Llama 2 family, gets their reponses to the prompts, and conducts a large-scale pairwise human evaluation. The evaluation results of the four models on many automated NLP benchmarks are also derived. Then, the paper analyzes the correlations between human evaluation and automated NLP benchmarks and finds that they are highly correlated in most cases. Furthermore, the authors decompose the correlation matrix into rank-one components and demonstrate the communities between human evaluations and NLP benchmarks. Finally, the authors tried to fit a regression model to predict the human evaluations with automatic evaluation results as inputs.","- The research question of this paper—the relationship between evaluation results from automated NLP benchmarks and human evaluations —is generally important and meaningful. Recently, numerous automated benchmarks and human evaluations have emerged separately, but there has been little research on the relationship between them.
- This paper covers many automated NLP benchmarks and includes a large-scale human evaluation, which lends a certain level of generality to its results.","Although the idea of this paper is beneficial, many obvious flaws diminish its value.

- This study uses only four LLMs, which is too few. This leads to
  - The correlations between automated NLP benchmarks and human evaluation are calculated merely from two four-dimension vectors, which is unreliable 
  - Insufficient experiments for predicting human evaluation from automated NLP benchmarks, despite cross-validation conducted in the paper

- The paper lacks key details, including but not limited to how the prompt set used in human evaluation is obtained, the human evaluation process and its reliability (e.g. inter-annotator agreements), details of how the correlation is calculated (what is ~150 evaluation process?), the settings for linear regression. This not only creates difficulty in understanding but also raises doubt about the rigor of this study.

- The presentation of the paper could be improved. For instance, the font sizes in Fig 3, the upper part of Fig 4, and Fig 6 are too small, making it hard to read.","- More LLMs should be covered in this study. I understand the computational cost during inference and the cost in human evaluation, but four LLMs are definitely too few to support subsequent experiments.
- I do need more details of the human evaluation in your study. What makes me most confused is the selection of the prompts. Why don't you use the same question sets as those of automated NLP benchmarks? If there are too many, you can sample from each dataset. Now there is a mismatch between the prompts (questions) in human evaluation and automated NLP benchmarks and the mapping relationship is not clear. Even if ignoring the mismatch issue, you should provide the number of prompts per area and categories used in human evaluation.
- The experiments of one-rank decomposition in Section 3.3 need to be further explained. Can you better state your motivation of conducting this decomposition and what insights can we draw from that?"
52Idqv2FNY,52Idqv2FNY,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,Reject,wd45NoWT7O,ICLR.cc/2025/Conference/Submission8665/Reviewer_MpHR,"This work attempts to explore the correlation or consistency between common NLP automatic evaluation benchmarks and human evaluations in analyzing and comparing the capabilities of language models. They cover a wide range of datasets and conduct experiments on four different sizes of Llama 2 models and GPT-3.5, employing human annotators to provide evaluation data. They find that there is a high correlation between automatic benchmarks and human evaluations, and they identify which benchmarks show stronger correlations. Furthermore, they also fit models to predict human evaluation scores of language models from academic evaluation scores.","The motivation and research questions of this work are very interesting and significant. Considering that language models are becoming increasingly powerful, many traditional NLP benchmarks may have lost their discriminative power, leading researchers to turn to human evaluations, which are more costly and harder to reproduce. By analyzing the consistency between NLP automatic evaluation benchmarks and human evaluations, this work aims to identify highly consistent benchmarks to simulate human evaluations, thereby reducing evaluation costs. Their experiments cover a large range of datasets and settings, including constructed various categories of human evaluation data and many common NLP automatic evaluation benchmarks, demonstrating a very comprehensive effort.","Although the research topic of this work is meaningful, it is also actually very complicated and corresponds to a more challenging analysis process. Even though the work has tried to handle the experimental data and present corresponding results as macroscopically as possible, their experimental analyses remain confusing and fail to help readers capture the main points. From Figure 1 onward, the clarity and readability of the charts decline rapidly, and by Figure 6, it becomes nearly impossible to extract any information as the fonts are extremely small and the visualized results are poorly presented.

Some analytical settings in the paper are unclear or somewhat unreasonable. For example, in line 149, what does the ""evaluation process"" refer to, and why are approximately 150 combinations calculated in total? What do they represent? Additionally, if I understand correctly, it seems unfair to compare human evaluation results across mixed task types with different NLP automatic evaluation benchmarks that may focus on testing certain different abilities.",Please refer to Weaknesses.
52Idqv2FNY,52Idqv2FNY,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,Reject,NiOdBl7Qas,ICLR.cc/2025/Conference/Submission8665/Reviewer_Khx9,"This paper studies the relationship between NLP benchmarks and human evaluation results and aims to understand what roles NLP benchmarks should play in the era of LLM. They conduct human evaluations on four Llama 2 chat models and calculate the correlation between human evaluation results NLP benchmarks, spanning from open-domain QA, MMLU, and safety/adversarial datasets. They find that most NLP benchmarks correlate well with human evaluation results, and it is possible to predict human evaluation results based on scores on NLP benchmarks.","- This paper studies a very important problem: whether scores on NLP benchmarks correlate with human evaluation results. This can potentially guide researchers to construct better benchmarks
- This paper studies the possibility of using NLP benchmarks to predict human evaluation results. Considering the efforts of human evaluation, the problem studied in this paper can help us develop LLMs faster.","- The experiment parts are highly unclear and hard to comprehend. It is unclear how the correlations are calculated between human evaluation and NLP benchmark scores. There is even no **Experiment Setup** section in this paper, and the part that most looks like the experiment setting is the first seven lines of Section 3. After repeatedly reading those lines, I still cannot understand how the correlations are calculated. Precisely,
    - How do you aggregate the scores of different shots?
    - Why do you aggregate the results of different shots?
    - What is the number of shots?
    - How is the prompt formatted?
    - How are the demonstrations in the few-shot selected?
    - Where does the number *150* on Line 148 (page 3) come from?
    - How is the human evaluation conducted? How many samples are there in the single-turn and multi-turn dialogues? How are the topics selected? What is the distribution of the data?
    - If the paper only uses four models, is the correlation coefficient calculated using only the benchmark scores of 4 models and the human evaluation results of the models? This means we are only calculating the correlation coefficient between two sets for numbers with only four elements in each set. 

- There are only four models used in this paper: the four chat models in Llama-2 with different numbers of parameters. The abilities of those models are very distinct, so it is easier for human evaluators or NLP benchmarks to distinguish the strengths of these models. A more challenging and realistic scenario is to consider more LLMs whose abilities are more diverse.

- The figures in the paper are terribly and poorly formatted. Those figures do not seem like they are designed to be read. The font sizes in the figures are too small to read and clustered together. I need to zoom in to 400% on my computer to see the words.

- Section 3.3 is highly unclear, without explaining what the *communities* this section is discussing and with no experiment settings that allow the readers to understand what is happening now. 

Considering that the experiment setting is highly unclear and the results are poorly presented, it is impossible to evaluate the contribution of this work. The paper requires major refinement. However, the paper studies an important problem, and I encourage the authors to keep working on this topic.","- Q1. How do the authors conduct the experiment using the Llama-2-30b model?  In fact, there is no 30b model in the LLama2 series, and I assume the authors are referring to the Llama-2-34b model. However, even Llama-2-34b-chat (or the base model) is not officially released, so I wonder how this paper conduct experiments using Llama-2-34b-chat."
XsXHqEVtiB,XsXHqEVtiB,Let’s Stop Bleeding! Precise Bleeding Data Estimation & Visualization Methods for Laparoscopic Surgeries,Reject,JntqgODbci,ICLR.cc/2025/Conference/Submission8498/Reviewer_JULy,"The paper introduces a method for detecting and segmenting bleeding regions in surgical images, addressing a need for real-time visualization during procedures. The proposed approach begins with a set of preprocessing steps that create an initial bleeding segmentation mask. Following this, a generative model is applied to perform an image-to-image translation, producing refined segmentation masks from the RGB input images. Given the challenges of limited medical data availability, the model training leverages video data generated from phantom sources and synthetic still images created with the orGAN model.","Automatic bleeding detection represents an important task during surgical procedures to avoid complications during surgeries; hence, the paper addresses a relevant problem.","One of the main limitations of the work is the lack of comparison with recent models. Even though this is mentioned in the limitations section of the paper, it is relevant to include additional models defined for segmentation in the comparison, as this sets a baseline that allows evaluating the performance of the work. In this regard, while they might not be open works for bleeding, there is a group of work on general-purpose medical segmentation, including the UNet, nnUnet, and its recent variants, that can be potentially employed to segment the bleeding regions' still images and in video data.","Additionally to the limitation presented in the weaknesses section, some questions regarding the methodology are the following:
* How is the preprocessing related to the translation problem? Is the input to the image translation problem the image itself or the output for the preprocessing stage?
* Line 241 mentions there is a model based on the YOLOv8 model to detect bleeding in static images. Is this the same model described in section 4.1?
* In the pixel differentiation stage, it is mentioned that the model employs the first and last frames to detect bleeding as a change in the grayscale images. How sensitive is this stage to non-bleeding-related changes, like tissue ablation?
* Why is the pixel differentiation stage necessary if the YOLO model can give an initial bleeding location?

It is recommended that the font size be increased for the figures, as some illustrations are hard to read."
XsXHqEVtiB,XsXHqEVtiB,Let’s Stop Bleeding! Precise Bleeding Data Estimation & Visualization Methods for Laparoscopic Surgeries,Reject,kxAG18DE77,ICLR.cc/2025/Conference/Submission8498/Reviewer_JV1y,The detection of bleeding zones/regions is a crucial step during surgical procedures. A system indicating the bleeding regions within the surgical scene to the surgeon essentially helps in improving the patient outcome. This work proposes a GAN-based bleeding detection method for precise localization of the bleeding region. Synthetic datasets are designed to train this method and the work claims to improve the instance detection of bleeding zones in real surgical scenes.,"- This work addresses a crucial yet underexplored issue of bleeding detection during surgical procedures. IA GUI framework is developed to make the detection tool easily accessible for surgeons.
- This work defines a synthetic dataset generation pipeline to create a labeled bleeding dataset. This dataset would be a valuable contribution to the surgical community if the authors make it publicly available.","- Contribution of the work: The contribution of this work is unclear (see questions)

- Details regarding the dataset are missing (see questions)

- Robustness to real-surgical environments: This work claims that previous methods fail to detect blood regions in real surgical environments under varying lighting conditions or when the organ is the same color as the blood (lines 178-179). However, these claims are not tested using the proposed methods. Although experiments were conducted on the Hamlyn dataset, only qualitative results are presented. This raises the question of who evaluated these qualitative results. For intricate applications such as bleeding detection, qualitative evaluation by doctors or surgeons is essential, alongside quantitative comparisons, to show improved performance.

- Evaluation of variations in bleeding patterns: This work claims to use a mimic organ method to develop a dataset of bleeding events with variations in flow, patterns, and lighting conditions (lines 140-149). However, these variations are not evaluated. A small subset of real bleeding events with such variations should be collected, and the dataset from the mimic organ setup should be compared to validate these claims. For qualitative comparison, a user study involving doctors is necessary, and for quantitative comparison, a simple bleeding detection (classification) model—classifying whether bleeding is occurring or not—can be used. This step is crucial to validate the robustness of the mimic organ model.

- Lack of baselines and ablation: This work lacks a comparison to baselines or ablations (see questions)","- Contribution: 
Does this work propose a method to generate a synthetic bleeding dataset? If so, what real dataset was used to construct the synthetic dataset? What anatomies were focused on in this study, and why was this important for defining the bleeding region? Line 185 mentions 1,000 annotated images—could the authors clarify why this number was chosen when, with a synthetic data generation system, the number of images could be scaled beyond that? 
Is a method being proposed to detect bleeding regions in a given surgical scene? This clarification is needed to judge the contribution of this work.

- Overlap in data: Clarification on the dataset used for training and testing.
1.  What was the real dataset used to generate synthetic bleeding videos using the mimic organ system? 
2. What was the dataset used to train the orGAN system to generate synthetic bleeding images ?
3.What was the dataset the YOLO detection algorithm was trained on ?
4. What was the size of all the split dataset used for each of these training runs ? The work mentioned only a dataset size of 1000 annotated images, which was from the mimic system. 
The YOLO instance segmentation was trained on these 1000 images. Could the authors clarify the size and nature of the test dataset ?

- The need for image-to-image (I2I) translation: Pix2PixHD [i] is a paired image-to-image translation approach, where the input is a bleeding image and the output is a bleeding alert map highlighting the exact bleeding region. Why was this task framed as an I2I translation instead of a simple segmentation task? More clarification is needed to understand the importance of using I2I in this work.

- Is there overlap in the datasets between the training of these modules? In line 425, the instance segmentation method achieves a precision of 86.6% in the first epoch. Does this suggest a data leak between the generation and segmentation training? If not, could the authors provide the training and validation curves and clarify why the score is so high in the first epoch?

- Baselines: Why was the proposed method (SBAM) not compared against the BAM[ii] method which served as the base framework for this work ? The BAM method gives a wider area of bleeding from which an instance segmentation mask can be constructed using the same YOLOV8 model in section 3.1.
Why was Pix2PixHD chosen ? There are other improved versions such as SPADE [iii] (GAN-based), ControlNet [iv], T2I-adapter [v] (diffusion-based) for semantic image synthesis. An ablation of these methods is necessary to show why Pix2PixHD might be the better choice.
The SBAM method is based on image-to-image translation method. There exist two generators, three discriminators and two different faces. What and why was this procedure chosen ? Ablation on each of these components is necessary to show why this specific I2I method was used in the study
What was the ratio of real and synthetic datasets used for training the YOLO segmentation method ? This ablation indicates the need for synthetic images and can show for which type of bleeding types does it improve/decrease performance.

- The figures 7 and 8 are difficult to read. Could the authors explain how a lower generator loss means a better GAN training and high quality images being generated (lines 442-446). How to account for model collapse in this case ? 

- In line 521 it is mentioned that prior works do not release their code to compare them against this work. However, I find this contradictory, that this work also mentioned that the data and code would be released upon the completion of a subsequent work. Could the authors clarify their statement?

- What is the inference latency of the deployed model on the GUI. Would this serve in real-time applications ? Could the authors comment on how to improve such a system? This would be necessary when such a system needs to be used by doctors in a real-surgical environment, which this work claims to be suitable for.

Minor suggestions:
Improve the wordings within the figures to make it easier for the reviewer to read the contents of the image (Fig.2,4,5,7,8)
The authors could read through the manuscript and use scientific language for writing. Words such as “beautifully, greeted” could/should be avoided for a scientific paper.

References:
i. TC Wang et al., High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
ii. M.Sogabe et al., Bleeding alert map (bam): The identification method of the bleeding source in real organs using datasets made on mimicking organs
iii. T.park et al., Semantic image synthesis with SPADE
iv. L.Zhang et al., Adding Conditional Control to Text-to-Image Diffusion Models
v. Mou et al., T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models"
XsXHqEVtiB,XsXHqEVtiB,Let’s Stop Bleeding! Precise Bleeding Data Estimation & Visualization Methods for Laparoscopic Surgeries,Reject,eiiY9kCBfK,ICLR.cc/2025/Conference/Submission8498/Reviewer_gGd7,"A novel framework, Selective Bleeding Alert Map (SBAM), which uses GAN-based image segmentation to focus on precise bleeding detection, aiming to reduce unnecessary distractions and improve visualization accuracy in laparoscopic surgeries. The SBAM framework is supported by a synthetic dataset of annotated surgical images, offering a new method for enhancing bleeding detection accuracy during operations.","SBAM addresses an unmet need in surgical procedures by improving visual focus and response to bleeding, which is critical in minimally invasive surgeries. The model achieved strong performance metrics (precision and accuracy), indicating high potential for real-world surgical integration. The use of a synthetic dataset for model training reduces the reliance on real surgical images and associated privacy concerns.","The model has limited applicability to other (non-)surgical contexts, aims at providing a specialized engineered solution to an application.

The reliance on synthetic data might limit the model’s robustness in diverse real-world surgical scenarios. Validation on a broader set of actual surgical videos or images would enhance the results’ credibility. 

The framework’s complexity could pose challenges in practical surgical scenarios. 

The paper does not provide an extensive exploration of ablation studies or comparisons with a broad range of baseline methods. The evaluation mainly focuses on demonstrating SBAM’s performance. there is no mention of systematic ablations that examine the impact of individual components (e.g., GAN-based architecture, segmentation thresholds) or a detailed comparison with established detection methods. comparisons with baseline methods or additional ablations could better establish SBAM’s unique contributions and clarify how each component of the model contributes to its overall performance.","How does the model perform on real surgical data beyond the Hamlyn dataset, and are there plans for further validation?

Could the approach be generalized to other forms of bleeding detection, or is it specific to this surgical type?

Have you performed ablation studies to evaluate the impact of individual components within the SBAM framework, such as GAN-based segmentation or specific thresholding techniques?

How does SBAM compare to other baseline methods in bleeding detection, and could these comparisons be included to strengthen the evaluation?"
XsXHqEVtiB,XsXHqEVtiB,Let’s Stop Bleeding! Precise Bleeding Data Estimation & Visualization Methods for Laparoscopic Surgeries,Reject,Sz3TQQseEl,ICLR.cc/2025/Conference/Submission8498/Reviewer_7AtK,The paper presents a methodology for instance segmentation in surgical image analysis to segment the active bleeding area. Their proposed model reaches a high performance with the help of using a fine-tuned YOLOv8 model.,"1. The idea of implementing an instance segmentation model on detecting active bleeding spots is interesting.

2. The use of the pixel differentiation to analyze/detect active bleeding area is helpful.

3. Developed a bleeding segmentation dataset.","1. It would be helpful to include ablation study to analysis the loss weights since there are multiple losses used in this study. Particularly,  the impact is using the adversarial loss (LGAN), feature matching loss (LFM), and perceptual loss (LVGG) needed to be further investigated and analyzed to demonstrate each loss function's effectiveness in solving this task.

2. The figures in Figure 7 is too small, and it is hard to visualize. The authors can take some actions to help with improving the readability, such as increasing the figure size, splitting it into multiple figures, or highlighting key areas that need better visibility.

3. There is lacking quantitative evaluations. I understand the concerns stated in the limitations; However, it might be helpful to compare using different DL models to fine tune, such as the demonstrating the performance using other popular segmentation models as an ablation study to justify why selecting the YoloV8 model.","1. YoloV8 is a model that released in 2023, why not trying to use the latest release YoloV9?

2. I am also curious whether occlusion of the non-bleeding areas by surgical tools or the surgeon's hands could impact the accuracy of segmenting the active bleeding region, as it may interfere with the pixel differentiation calculations."
RZ3m2LMYze,RZ3m2LMYze,Representation Confusion: Towards Representation Backdoor on CLIP via Concept Activation,Reject,Z8Nks2ZaXI,ICLR.cc/2025/Conference/Submission8165/Reviewer_E6Up,"This work proposes an interesting ""concept-based"" backdoor attack where backdoor poisoning images are labeled following the concept score from explainable AI tools. In particular, if the concept vector or a given image is larger than a pre-defined threshold, the label of the given image will be changed to the target label. After training on the poisoned data set, images with an ""internal trigger"" (i.e., concept vector larger than the threshold) will be wrongly classified as the target class. The authors also motivate the design choice with the cognitive neuroscience theory. The proposed attack is claimed to be stealthy against existing baseline defenses.","This paper is well-structured, with detailed explanations and a background provided to support each step of the attack design. The authors share an interesting observation that the ""concept activation"" is the driving force behind the backdoor attacks. Extensive experiments are also provided to validate the proposed attack's effectiveness, where the attack is especially effective against backdoor defenses.","- This attack jumps out of the regular threat model where an adversary puts the trigger on test samples during inference to manipulate the predictions. This change makes the backdoor attack more stealthy since there is no need to add external triggers during the test. However, it also brings a question about how to implement it in practice when the adversary needs to change the prediction on a certain target images. For example, the concept vector threshold of the target image is indeed lower than the pre-defined value. Is there a feasible way to increase the value to trigger the backdoored model? One possible solution is to deliberately change the concept vector score of the target image. It would be great if the authors could clarify this point.  

- Related to the question above, if the adversary manages to change the concept vector score, the proposed method is similar to a regular backdoor-triggered sample. In this case, how reliable is the concept score? Taking the results of table 2 also into consideration, when does the proposed attack fail, and is it caused by the concept score calculation? In addition, can the same strategy be used in feature space for random statistics instead of calculating the concept vector score?

- Adaptive defenses. One major advantage of the proposed attack is that it can resist backdoor defense. Regarding the working mechanism of the previous defense, it is non-surprise that previous defenses do not work for the proposed attack. For example, ShrinkPad must inspect the difference between clean and triggered images. The proposed attack is somehow unrelated to ShrinkPad since the triggered inputs do not include substantial patterns. Given the existing defenses, would an adaptive defense that considers the proposed attack mechanism easily filter the proposed backdoor attack? It would be great if the authors could clarify this point.

Minor: 
Line 219, bold -> green.
The font size in Figure 4 is too small.","Please clarify about triggering on a randomly selected test image, the reliability of the concept score, and adaptive defenses."
RZ3m2LMYze,RZ3m2LMYze,Representation Confusion: Towards Representation Backdoor on CLIP via Concept Activation,Reject,WLBJOp5UTz,ICLR.cc/2025/Conference/Submission8165/Reviewer_jJUJ,"This paper presents RepConfAttack, a backdoor attack framework that exploits internal concept representations within CLIP models instead of traditional external triggers, drawing inspiration from cognitive neuroscience. The method achieves high attack success rates while maintaining clean performance across multiple datasets.","- First to apply the Hopfieldian view from cognitive neuroscience to explain backdoor attack mechanisms.
- Proposes a novel concept representation-based backdoor attack method (RepConfAttack) without requiring external triggers","- Only validated on image classification tasks downstream of CLIP, leaving other potential tasks unexplored
- Claims to target CLIP but lacks comparison with representative CLIP-specific attacks like BadCLIP, which weakens the comparative analysis
- Insufficient justification for concept selection threshold (σ) choice
- Limited sensitivity analysis on poisoning rate
- No validation on larger-scale datasets, such as ImageNet","- How to select optimal trigger concepts? Are certain concepts inherently more suitable as triggers?
- Have you considered using combinations of multiple concepts as triggers?
- Can this method be extended to other types of multimodal models?
- How does the method perform on larger-scale datasets?"
RZ3m2LMYze,RZ3m2LMYze,Representation Confusion: Towards Representation Backdoor on CLIP via Concept Activation,Reject,hznAATyOHP,ICLR.cc/2025/Conference/Submission8165/Reviewer_9Sod,"This paper addresses the evolving threat of backdoor attacks in deep learning models, where hidden triggers can be covertly embedded to control model behavior at inference. Traditional attacks use external patches or perturbations as triggers, but they often face two challenges: detection by defense mechanisms and high computational costs. To overcome these, the authors draw inspiration from cognitive neuroscience, comparing model decision-making to human cognition and proposing a new approach that manipulates internal representations directly. The proposed framework, called Representation Confusion Attack (RepConfAttack), bypasses the need for external triggers by modifying inherent concepts within the model’s representation spaces. This approach enhances stealth, making the attack less detectable by standard defenses. Experimental results show that RepConfAttack is effective, achieving high success rates even against strong defenses, suggesting a novel and advanced method for conducting undetectable backdoor attacks.","1. The paper addresses a timely and important topic, focusing the security of self-supervised learning (SSL) models against backdoor attacks. The proposed RepConfAttack introduces a novel approach that leverages cognitive science to manipulate internal representations, enhancing stealth and evading detection.

2. The findings are inspiring, and the perspective using cognitive science adds an interesting dimension.","1. The novelty of the proposed attack lies in leveraging naturally existing concepts in the dataset, which seems similar to the existing work by Wenger et al. (NeurIPS '22). Could the authors clarify the specific technical differences from [1]?

2. While the use of cognitive science to explain the backdoor attack is intriguing, the connection feels tenuous and lacks strong motivation. First, the cognitive science aspects are described mostly in natural language without much technical depth or formalization. Second, the cognitive science section seems unnecessary, as the proposed Confusion Attack can be fully understood by reading Section 4.3 alone.

3. The practicality of the proposed attack raises some concerns. For example, if ""water"" is chosen as the target concept, does the attack only succeed when ""water"" is present in the image? How does the attack perform when this concept is absent? Traditional backdoor attacks can always succeed by injecting a trigger, ensuring consistent success.

4. Concept set size: What is the typical size of the concept set in the experiments? Additionally, how does the size of the concept set affect the performance of the proposed method?

5. Determination of threshold $\\sigma$: How is $\\sigma$ determined in the experiments, and how sensitive is the performance of the proposed method to variations in $\\sigma$

6. The paper misses some backdoor defense strategies specifically designed for SSL, such as SSL-Cleanse[2] and DECREE[3]. 
Discussing how RepConfAttack could potentially evade these defenses would strengthen the paper's security analysis.


[1] Wenger et al., Finding Naturally-Occurring Physical Backdoors in Image Datasets, NeurIPS '22.

[2] Zheng et al., SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning, ECCV '24.

[3] Feng et al., Detecting Backdoors in Pre-trained Encoders, CVPR '23.",Please respond to each weakness mentioned above.
RZ3m2LMYze,RZ3m2LMYze,Representation Confusion: Towards Representation Backdoor on CLIP via Concept Activation,Reject,OT5eKELD82,ICLR.cc/2025/Conference/Submission8165/Reviewer_5Upt,"The paper proposes a novel backdoor attack framework called Representation Confusion Attack (RepConfAttack), which explicitly manipulate the concepts and eliminates the need for backdoor triggers.","1. **Novel Motivation**: The motivation behind using cognitive neuroscience, particularly the Hopfieldian perspective, to conceptualize backdoor attacks is innovative. This unique view allows the paper to frame backdoor manipulation in a new and insightful way.
2. **Comprehensive Experiments**: The authors conducted extensive experiments on different datasets and multiple CLIP variants. The results demonstrate the effectiveness of the proposed attack in terms of high attack success rate and high clean accuracy.","1. **Motivation Complexity**: The motivation for relating backdoor attacks to cognitive neuroscience is somewhat complex and could be difficult for readers unfamiliar with the field. While the Hopfieldian view provides an interesting perspective, its necessity in the context of backdoor attacks on neural networks is not entirely clear. More emphasis could have been given to explain why this specific perspective is crucial for the proposed attack mechanism.

2. **Novelty of the method**: While this work provides a novel perspective on the success of backdoor attacks, the method itself is not entirely new. It closely resembles traditional backdoor attacks, specifically those categorized as physical attacks [1]. From this standpoint, the contribution of this work remains unclear.

3. **Method Generality**: The paper specifically targets image classification tasks on CLIP models, and it is unclear whether the proposed method can be effectively extended to other tasks, such as multimodal contrastive models. Additionally, it is not evident why the authors chose to use the CLIP visual encoder without utilizing the CLIP text encoder. Could the authors clarify the reasoning behind this choice?

4. **Experiment Limitations**:  I find the experimental setup to be inadequate, particularly regarding the choice of dataset and attack method. When utilizing CLIP models, why not employ datasets specifically designed for them, such as CC3M, ImageNet, or Caltech-101? Furthermore, the attack methods compared are primarily intended for smaller models and are somewhat outdated. It would be more appropriate to compare against state-of-the-art attacks specifically developed for CLIP models.

5. **Defense Limitations**: The defense methods examined in this paper are also outdated, largely stemming from 2021. The only detection method from 2023 relies on image-based techniques, which are insufficient against the proposed attack. It would be advantageous to investigate more advanced defense strategies. Additionally, I believe the method presented in this article may not withstand the most sophisticated defense and detection techniques that assess whether the modalities are aligned [2][3].

[1] Backdoor Attacks Against Deep Learning Systems in the Physical World.

[2] VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models.

[3] Robust contrastive language-image pretraining against data poisoning and backdoor attacks.","1. What is the differrence between this work and physical backdoor attacks?
2. why the authors chose to use the CLIP visual encoder without utilizing the CLIP text encoder, while ""CLIP"" appears in the title of the paper?"
Kpjvm2mB0K,Kpjvm2mB0K,Streaming Algorithms For $\\ell_p$ Flows and $\\ell_p$ Regression,Accept (Spotlight),WXHUOnI89P,ICLR.cc/2025/Conference/Submission7681/Reviewer_Rniw,"The paper gives upper bounds and lower bounds for streaming underdetermined $\\ell_p$ linear regression in the column arrival model. The results apply both to the value of the best fit, and to the actual solution. There are various results, depending on the value of $p$ and the approximation guarantees. The arguments use duality and sparsification.",The paper explores the full range of values of $p$.,The methods rely substantially on past work.,
Kpjvm2mB0K,Kpjvm2mB0K,Streaming Algorithms For $\\ell_p$ Flows and $\\ell_p$ Regression,Accept (Spotlight),bmvEKsc68Q,ICLR.cc/2025/Conference/Submission7681/Reviewer_SDMB,"The paper initiates the study of $p$-norm regression in the one-pass streaming setting. They consider the underdetermined setting $\\min_{Ax=b} \\|x\\|_p$ where $A$ has size $n\\times d$, $n<<d$, receives $d$ column updates. They consider $p\\in [1,\\infty)$. When we consider instances graphs, these updates correspond to edge insertions. 

The first result is an algorithm for “p-norm flow sparsifier” in $O(n^2)$ space which is a general version of graph sparsifiers which minimize the $\\ell_p$-norm. This algorithm follows from using the online lewis weight sampling algorithm of Woodruff and Yasuda’22 on the dual problem. The other algorithmic result deals with giving a tradeoff between the size of the sparsifier and objective error when the goal is to maintain the entire solution vector x. The first algorithm only maintains the objective value.

The paper also presents several information theoretic lower bounds which match these upper bounds. They also give some extra lower bounds for the case of $p=2$.","The paper studies an important problem and makes a significant amount of progress in the new direction of streaming algorithms for regression. They also leave some good open questions in the paper. The techniques are also quite simple and use previous works quite well. The main technical core is the lower bound part, which is of independent interest.",It seems like there are too many ideas and previous results used in the paper. Would be useful to have some more preliminaries in the appendix or somewhere.,"1. Can the authors add a comparison with what is known in online algorithms in the corresponding settings?
2. The tables in the paper, that give the lower and upper bounds for different settings, can these be split in a different way so that its easier to contrast the lower and upper bounds for all settings? This would also make clear what settings are still unknown."
Kpjvm2mB0K,Kpjvm2mB0K,Streaming Algorithms For $\\ell_p$ Flows and $\\ell_p$ Regression,Accept (Spotlight),iEs4H9rest,ICLR.cc/2025/Conference/Submission7681/Reviewer_fogf,"The $\\ell_p$ regression $\\min_{\\mathbf{Ax} = \\mathbf{b}} ||\\mathbf{x}||_p$ is a fundamental problem in machine learning, data science, and numerical linear algebra. When $p = 2$, it is the classical linear regression problem and has a closed-form solution $\\mathbf{x}^* = \\mathbf{A}^\\top (\\mathbf{A}\\mathbf{A}^\\top)^{-1} \\mathbf{b}$. The general $\\ell_p$ regression has been well-studied, in particular, when $\\mathbf{A}$ is the vertex-edge incidence matrix, it is the $\\ell_p$-norm flow problem, which corresponds to transshipment, Laplacian solver, and maximum flow if $p = 1, 2$, and $\\infty$ respectively. This paper considers the scenario that the matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$ has $n \\ll d$, which leads to the linear system $\\mathbf{Ax} = \\mathbf{b}$ being underdetermined. In this case, it is impractical to store all the $d$ columns of $\\mathbf{A}$, which motivates the usage of the column-arrival streaming model. In addition, there are two kinds of solutions to $\\ell_p$ regression: (1) $\\textit{cost estimation}$ that approximates the optimal $\\ell_p$ norm; (2) $\\textit{vector-valued}$ that approximates the optimal solution $\\mathbf{x}^*$. For the first kind, this paper achieves space complexity $\\widetilde{O}(\\epsilon^{-2} n^{\\ 1+max\\\\{1, p/2(p-1)\\\\}})$, which is $\\widetilde{O}(\\epsilon^{-2} n^2)$ when $p > 2$, and also gives lower bounds for $p = 0, 1, 2$. For the second kind, this paper also provides lower bounds and upper bounds for different values of $p$.","(1) This paper researches the general $\\ell_p$ regression problem under the column-arrival streaming model, while the existing work only considers the special case, like $p = 2$ or $\\mathbf{A}$ is the vertex-edge incidence matrix.  
(2) For general $p$, this paper establishes both lower and upper bounds on space complexity for the two kinds of solutions: cost estimation and vector-valued problems. These new results strengthen the paper's contributions.  
(3) This paper has an extensive set of references and a comprehensive summary of the related work.","The $p$-norm flow problem defined in (1) (Line 61) is not exact. The correct one should be $\\min_{\\mathbf{Ax} = \\mathbf{b}} ||\\text{diag}(\\mathbf{w}) \\cdot \\mathbf{x}||_p$. To be specific, when $p = 1$ and $\\mathbf{w}$ is the cost vector, then it is a transshipment problem; when $p = 2$ and $\\mathbf{w} = \\mathbf{r}^{1/2}$, where $\\mathbf{r}$ is the resistance vector, then it is reduced to a Laplacian solver; when $p = \\infty$ and $\\mathbf{w} = \\mathbf{c}^{-1}$, where $\\mathbf{c}$ is the capacity vector, then it is the maximum flow problem.      

By this formulation, it is the same as formulation (2). As a special case, when $\\mathbf{A}$ is a vertex-edge incidence matrix in the graph streaming problem, there is some existing work (Line 91-97) on transshipment, electrical flow, maximum flow, etc. Could you compare this work with their results?","(1) In Theorem 1, Line 153, should ""$s$ rows"" be ""$s$ columns""? At Line 164, should the second $\\widetilde{O}(\\epsilon^{-2} n)$ be $\\widetilde{O}(\\epsilon^{-2} n^2)$? Please verify that.     
(2) In Theorem 1, when $p \\to 1$, like $p = 1.1$, $q = p/(p-1)$ would be a large constant. In this case, could you discuss the behavior of the proposed algorithm? If it is indeed a non-trivial issue, could you give a brief analysis or comment in this paper on $p \\to 1$?"
TvwsOrl865,TvwsOrl865,Diffusion Transportation Cost for Domain Adaptation,Reject,h0JUB1r1gk,ICLR.cc/2025/Conference/Submission7570/Reviewer_2EmH,"This paper proposes a novel transportation cost function, termed Diffusion-OT, for the Optimal Transport (OT) problem in the context of domain adaptation. Diffusion-OT leverages concepts from diffusion geometry and manifold learning to account for both intra-domain and inter-domain relationships. The proposed cost function is derived from a composite diffusion operator that consists of three diffusion steps: within the source domain, across domains, and within the target domain. By incorporating source label information into the diffusion process, Diffusion-OT can guide the anisotropic diffusion according to class labels. Experiments on various benchmarks demonstrate that Diffusion-OT outperforms competing methods, achieving state-of-the-art results on non-Euclidean data.","1. The proposed Diffusion-OT cost function is a novel approach that goes beyond the traditional squared Euclidean distance used in OT for domain adaptation. It considers both intra-domain and inter-domain geometries.
2. Experimental results show that Diffusion-OT achieves superior performance compared to baseline and recent OT-based methods across multiple datasets, demonstrating its effectiveness in domain adaptation tasks.","1. The first concern is the complexity and computational cost. The composite diffusion operator involves multiple steps and may lead to higher computational cost compared to simpler cost functions. The computational complexity of the proposed method, especially when dealing with large-scale datasets, is not fully discussed.

2. Theoretical analysis limitations: While the paper provides theoretical analysis, it mainly focuses on the asymptotic behavior of the diffusion operators. A more rigorous analysis of the convergence properties and error bounds of the proposed method would strengthen the theoretical foundations.

3. Incomplete analysis of failure cases: Although the authors admit the limitation of the proposed method is its assumption that both source and target domains reside in the same space. The authors do not provide detailed analysis of failure cases. As a result, we cannot clearly evaluate the negative impact when applying the proposed method in the real-world applications where the domains have substantially different underlying structures. This could limit the generalizability of the proposed method.",See weaknesses.
TvwsOrl865,TvwsOrl865,Diffusion Transportation Cost for Domain Adaptation,Reject,yzAlCeKs9b,ICLR.cc/2025/Conference/Submission7570/Reviewer_e86L,"This paper proposes a domain adaptation method based on optimal transport. A diffusion optimal transport model is leveraged to construct a transport cost function between samples, in which intra-domain local geometry is introduced. The experiments are conducted on simulated and benchmark datasets to evaluate the performance of the proposed method.","1. In general, the paper is well-organized and easy to follow.

2. Both synthetic and real-world datasets are used in the experiments.","1. It is not new to introduce intra-domain geometry for domain adaptation. Intra-domain geometry has been widely considered in optimal transport and domain adaptation. For example, the Gromov-Wasserstein discrepancy considers transport between two metric spaces, and intra-domain geometry is involved in the construction of the metric. This has been introduced for cross-domain applications, as shown in [a][b].

2. Section 1 states that the weights in ETD and RWOT are learned, rather than directly derived from the data as in Diffusion-OT, which limits their applicability to deep learning. This statement is questionable. Intuitively, it is usually a good strategy to learn some properties such as weights or distance from data, which can adaptively extract geometric information involved in data and enhance the performance. Different from the learning strategy, the proposed method adopts a pre-defined approach to obtain a transport cost. The pre-defined paradigm may not obtain good performance if the adopted approach is not appropriate for real-world data.

3. Section 4 states that unsupervised domain adaptation implies that both domains are supported on the same hidden manifold. This assumption is vague. Domain shift could come from different marginal distributions, different conditional distributions, or some other factor. What is the specific assumption adopted in the submission? A detailed discussion should be provided.

4. The compared methods are out-of-the-date. Domain adaptation is an active area with many advances in recent years. It is easy to find state-of-the-art methods published recently as the comparison, such as (but not limited to) [c][d], which are also optimal transport-based methods for domain adaptation.

5. The results on Office-Home are lower than the results shown in [c]. Does the difference come from a different backbone model? If so, it is encouraging to adopt a better backbone to evaluate the performance of the methods.

6. It would be better to evaluate the impacts of the hyper-parameters $\\epsilon$ used in the Gaussian kernel functions.

[a] Gromov-Wasserstein Learning for Graph Matching and Node Embedding, ICML 2019.

[b] Semi-Supervised Optimal Transport for Heterogeneous Domain Adaptation, IJCAI 2018.

[c] Probability-Polarized Optimal Transport for Unsupervised Domain Adaptation, AAAI 2024.

[d] COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport, CVPR 2023.","1. Section 4 states that unsupervised domain adaptation implies that both domains are supported on the same hidden manifold. This assumption is vague. What is the specific assumption adopted in the submission? A detailed discussion should be provided.

2. It would be better to conduct more state-of-the-art methods.

3. The results on Office-Home are lower than the results shown in [a]. Does the difference come from a different backbone model? If so, it is encouraging to adopt a better backbone to evaluate the performance of the methods.

4. It would be better to evaluate the impacts of the hyper-parameters $\\epsilon$ used in the Gaussian kernel functions."
TvwsOrl865,TvwsOrl865,Diffusion Transportation Cost for Domain Adaptation,Reject,ThY32ew8LQ,ICLR.cc/2025/Conference/Submission7570/Reviewer_we8S,"A novel transport cost, Diffusion-OT, is proposed in this paper for OT problem. By utilizing concepts of diffusion geometry, the authors derive an operator to quantify the probability of transporting between source and target samples. The authors give proof that the cost function is defined by an anisotropic diffusion process between the domains. Experiments show the superior performance of the proposed cost.","A new transportation cost, Diffusion-OT, is proposed which enables the learning of the geometries and relationships both between and within the two domains by considering both inter-domain distances and intra-domain structures. 
By incorporating source label information into the cost, the proposed method is compatible with any OT solver and problem formulation.
Experiments demonstrate the effectiveness of the proposed method.","The results in Tab.1 and Tab.2 do not include all baseline methods, for example, results of  RWOT and ETD did not appear in the VisDA experiment.
The improvement on the digits dataset is relatively small.",Does the proposed method has generalization on more general Universal Domain Adaptation tasks？Cause the Universal Domain Adaptation setting is more widely existing in practice.
TvwsOrl865,TvwsOrl865,Diffusion Transportation Cost for Domain Adaptation,Reject,Yk0DCNt0MR,ICLR.cc/2025/Conference/Submission7570/Reviewer_k4Z3,"This paper presents Diffusion-OT, a transport cost designed for the optimal transport (OT) problem with a focus on domain adaptation. Specifically, the authors adopt concepts from manifold learning, i.e., diffusion geometry, to derive an operator that captures intra-domain relationships. This operator quantifies the probability of transporting samples between the source and target domains, forming the foundation of the transportation cost.  Comprehensive theoretical proofs and extensive experiments demonstrate the effectiveness of the proposed method.","1.	This paper presents a graph-based transport cost that accounts for both cross-domain distances and intra-domain structures.
2.	Incorporating theoretical concepts from diffusion processes enhances the depth and rigor of the proposed method.
3.	Empirical results demonstrate the effectiveness of Diffusion-OT across various scenarios, indicating its practical applicability.","1.	The motivation of this paper is unclear. In the Introduction section, the authors state that “one aspect that remains relatively unexplored is the selection of the transportation cost function”, but they fail to provide compelling reasons for the need to investigate this aspect. Moreover, the authors do not analyze the advantages and disadvantages of existing methods, further diminishing the clarity of the research motivation.
2.	The literature review is notably limited, lacking engagement with significant recent advancements in optimal transport and domain adaptation. This paper mainly compares with related works before 2023. It would benefit from a comparison and discussion of additional recent studies, such as [1-5].
3.	Why does the proposed method exhibit inferior performance compared to RWOT on the MNIST-USPS dataset? More discussions are required.
4.	The recent method SPA [2] demonstrates superior performance compared to the proposed methods in the Office-Home (75.3% v.s 72.43%) and VisDA (87.7% v.s 78.56%) datasets. What advantages does the proposed method offer compared to traditional domain adaptation techniques?
5.	The proposed method involves several hyperparameters(e.g., \\lamda, \\epsilon). Conducting ablation studies on these hyperparameters would provide valuable insights into the sensitivity of the proposed method and its performance.

**Reference**

[1] Probability-Polarized Optimal Transport for Unsupervised Domain Adaptation. AAAI 2024

[2] SPA: A Graph Spectral Alignment Perspective for Domain Adaptation. NeurIPS 2023

[3] COT: Unsupervised Domain Adaptation With Clustering and Optimal Transport. CVPR 2023

[4] Towards Unsupervised Domain Adaptation via Domain-Transformer. IJCV 2024

[5] Prototype-Guided Feature Learning for Unsupervised Domain Adaptation. Pattern Recognition 2023",Please see above.
TvwsOrl865,TvwsOrl865,Diffusion Transportation Cost for Domain Adaptation,Reject,rEiqs8a0U6,ICLR.cc/2025/Conference/Submission7570/Reviewer_z6YT,"This paper focuses on the methodological aspect of optimal transport (OT) for domain adaptation. A major motivation is that existing OT works rarely consider the construction and impact of cost functions, which is generally crucial for the property of the OT measure and its induced transport plan. To deal with this limitation, this work proposes a diffusion-based cost formulation, which endows the transport cost with the property of cross-distribution propagation. The theoretical result shows that the proposed cross-domain diffusion operator indeed characterizes the cross-domain discrepancy and intra-domain diversity. Experiments are conducted on standard domain adaptation datasets to evaluate the proposed method.","+ The motivation for improving OT from the perspective of the cost function is technically sounded.

+ Theoretical results that show the implications of constructed diffusion operator is reasonable. 

+ The empirical improvement over other OT-based domain adaptation methods on several benchmarks.","1. The relation between the diffusion process and OT should be further clarified; besides, it seems that there are fundamental issues in the validity of OT with designed cost.

2. The implications of diffusion operator for distribution shift correction could be improved, e.g., the superiority of proposed cost design is not sufficiently explained in the current manuscript.

3. The details for the optimization procedure could be improved; the comparison experiment for method validation should contain more hard transfer tasks.","**Concerns**

Q1. As far as I understand the Diffusion-OT, the key difference between it and existing OT works is that Diffusion-OT introduces the composition of stochastic matrices, i.e., ‘source to source transition’, ‘source to target transition’ and ‘target to target transition’, to construct the cost function. However, it seems that there are no explanations on what special properties are ensured by the Diffusion-OT from OT’s view, e.g., JDOT measures the joint distribution discrepancy, POT/UOT relaxes the strict constraints under severe shift scenarios. Some justifications are appreciated.

Q2. Is the constructed cost function $C=-log(S)$ still a metric? Since the metric property is necessary to ensure the validity of OT. Besides, if it is a metric, which kind of discrepancy does it characterize (e.g., joint/conditional/marginal distribution discrepancy)? 

Q3. Prop. 1 shows that the diffusion operator $S$ can reflect the cross-domain discrepancy and intra-domain diversity from the view of LB operator. However, it seems that there is no guarantee that the proposed method can control the diffusion process, i.e., suppress cross-domain divergence and enlarge the intra-domain divergence. Therefore, it is hard to understand the learning process and the properties of the proposed method. Detailed discussion on the optimization and learning procedure would be helpful to improve the clarity.

Q4. In the diffusion operator $S$, the three stochastic matrices are construed with distance-based kernel function on original space $\\mathcal{X}$, e.g., Eqs. (4)-(5). Should they be formulated in representation space $\\mathcal{Z}$? If so, are the representations considered variables under the optimization process? What are the learning principle and its intuitive goal for the diffusion operator $S$?  An in-depth analysis of the diffusion mechanism in the learning process is high expected.

Q5. Though this work achieves improvements over some existing OT methods, the comparison seems insufficient. On the one hand, some advanced OT methods that have similar goals/ideas to the proposed methods are omitted, e.g., key-point guided OT [a], mask OT [b], and general cost function [c]. Especially, considering the label-guided graph construction in Eq. (11), it indeed has the same idea as the mentioned works. On the other hand, the experiment could be extended to larger and harder datasets, e.g., DomainNet. 

**References**

[a] Xiang Gu, Yucheng Yang, Wei Zeng, Jian Sun, and Zongben Xu. Keypoint-guided optimal transport with applications in heterogeneous domain adaptation. In NeurIPS, 2022.

[b] Jiying Zhang, Xi Xiao, Long-Kai Huang, Yu Rong, and Yatao Bian. Fine-tuning graph neural networks via graph topology induced optimal transport. In IJCAI, 2022

[c] Asadulaev, A., Korotin, A., Egiazarian, V., Mokrov, P., & Burnaev, E. Neural Optimal Transport with General Cost Functionals. In ICLR, 2024."
OJd3ayDDoF,OJd3ayDDoF,OpenHands: An Open Platform for AI Software Developers as Generalist Agents,Accept (Poster),l16Pc5IIq4,ICLR.cc/2025/Conference/Submission7566/Reviewer_ujp2,"This paper introduces OpenHands, is a community-driven platform designed to advance the development of AI agents that interact with the world in terms of code writing, command-line interaction, and web browsing. OpenHands includes an evaluation framework for benchmarking agent performance on tasks like software engineering and web browsing. It supports various LLMs and sandboxed environments for running these AI agents and conducts extensive evaluation of different agents in various benchmarks.","- The motivation to build an open-sourced agent platform is good. 
- The agent interface, with capabilities for code execution and web browsing, is impressive.
- Evaluations conducted across various agents and benchmarks are thorough and extensive.",,"This paper introduces an open platform with the event-driven architecture to advance the AI agent development, especially focusing on coding and web browsing. It also conducts extensive evaluations of heterogeneous agents on various benchmarks. This is a solid contribution to the LLM agent community. I have some questions about the implementation details. 
- The authors mention that event streaming enables human intervention in agent tasks, which I believe is a crucial step for calibrating agent outputs. I wonder whether human intervention is enabled during the evaluation of agents and is there a mechanism to determine whether human intervention is required?
- Regarding the multi-agent delegation feature, I’m unclear on how agents collaborate. Is there an orchestrator agent responsible for task delegation, or is this managed through event streaming? If it’s the latter, how are tasks assigned and dispatched, and where do agents deliver their outputs?"
OJd3ayDDoF,OJd3ayDDoF,OpenHands: An Open Platform for AI Software Developers as Generalist Agents,Accept (Poster),JiWOu2CY4h,ICLR.cc/2025/Conference/Submission7566/Reviewer_4sob,"The paper presents OpenHands, an open-source platform designed to facilitate the development of AI agents that interact with the world through software interfaces such as code execution, command-line operations, and web browsing. A key feature of OpenHands is AgentHub, a platform where users can contribute their own agents (architecture) to the community, fostering collaborative development of agent architectures. The platform provides a simple agent abstraction, making it accessible for users to create and extend agents, and provides a modular architecture for agent interactions and task execution, including an event-driven state management system, multi-agent coordination, and an extensible agent skills library. OpenHands aims to enable agents to even create tools by themselves. The authors describe the architecture, implementation details, and evaluate the platform across multiple benchmarks, including software engineering tasks and web browsing scenarios. It's kind of a combination of AutoGen and BrowserGym.","Originality:
OpenHands uniquely consolidates multiple agent capabilities: coding, command-line interaction, web browsing, and multi-agent collaboration, within a single, open-source platform. The introduction of AgentHub allows users to contribute their own agents, promoting community collaboration and expanding the platform's versatility. This integration distinguishes OpenHands from existing frameworks, although IMO some of the main ones are absent from the comparison (see questions).

Quality: 
The methodological approach is solid, featuring a well-defined architecture that includes an event-driven state management system and a secure, sandboxed runtime environment. The use of an extensible agent skills library enhances flexibility, allowing agents to perform complex tasks and even create tools themselves. The agent abstraction is designed to be simple, enabling users to easily create and extend agents.

Clarity:
The paper is generally well-written and logically structured, making it accessible to both practitioners and researchers. Key components of the platform, such as the agent abstraction, runtime environment, and evaluation framework, are explained clearly. Figures and tables effectively illustrate concepts and results.

Significance:
By providing an open-source, MIT-licensed platform with contributions from a large community, OpenHands has the potential to significantly impact the development and evaluation of AI agents. It introduces a varied range of datasets for each general agent domain, although it could be expanded in the browser domain (see weaknesses). Evaluating multiple close-source LLMs on each task in addition to the average costs of evaluating benchmarks is a valuable addition that offers practical insights.","Limited Novelty in Certain Aspects: While OpenHands integrates various functionalities, much of the work appears to assemble existing components from the domain into one place. Some features, like code execution in a sandbox and web browsing agents, are present in other platforms. The paper could better articulate how OpenHands distinguishes itself from similar frameworks like LangChain, DSPy, or AutoGen.

Basic Implemented Agents: The currently implemented agents are relatively basic. For example, the Browsing Agent is based on WebArena's agent, which is very simple to implement and not competitive with current SOTA agents. There are more advanced multi-agent architectures available that could be implemented in AgentHub to better demonstrate the system's capabilities in facilitating the general agents' development.

Outdated Datasets: The evaluation includes MiniWoB++, which is an outdated dataset. There are newer datasets, such as WorkArena, WorkArena++, and ST-WebAgentBench, that are more relevant. The last test agents in more complex, restricted environments using policy hierarchies. Incorporating these in the framework would strengthen the evaluation.

Evaluation Depth: The evaluation, while broad, could benefit from deeper analysis. Incorporating an automated data collector to track agent success and failure would enhance insight into agent performance and make the framework more complete.","Distinction from Existing Platforms: How does OpenHands fundamentally differ from other agent development platforms like LangGraph, CrewAI and DSPy for multi-agent collaboration or BrowserGym for benchmark evaluation? Could the authors elaborate on the unique features or advantages that OpenHands offers over these frameworks? Including these comparisons in Table 1 would provide a more comprehensive overview.

Agent Performance Analysis: Can the authors provide a more detailed explanation about the data collector in the framework? Specifically, what makes the framework the best solution to develop an agent when one approaches such a task?
Another question is: how will the platform address this in future iterations?

Handling of Observations: How does the platform treat observations and their full composition? Can the authors clarify how observations are managed and utilized by agents? Providing more details on the configuration and usage of the event stream would enhance understanding, especially for developers less familiar with event-driven systems.

Security Considerations: Given that the platform allows the execution of arbitrary code in a sandboxed environment, what security measures are in place to prevent potential exploits or breaches? Has the sandboxing been tested against known vulnerabilities? Discussing security protocols would demonstrate the platform's reliability.

Web Page Understanding Techniques: Regarding the web experiments, it is not clear which page understanding techniques are used to interpret interactable elements and web pages. Does the framework provide screen/page understanding capabilities, or should a new user implement their own methods? Clarifying this would help users understand the platform's capabilities in web interaction tasks."
OJd3ayDDoF,OJd3ayDDoF,OpenHands: An Open Platform for AI Software Developers as Generalist Agents,Accept (Poster),hAjilsg8SY,ICLR.cc/2025/Conference/Submission7566/Reviewer_WpcF,"OpenHands is an open-source platform for the development of general AI agents. This paper details the motivation behind OpenHands, the architecture of the platform, how a user might implement an agent using OpenHands, description of some existing baseline agents, and comparison of some of the OpenHands agents against other open-source agents on software, web, and other assisting tasks. Selected results show that their agents perform reasonably well across a wide range of these tasks (generally underperform the specialized agents).","OpenHands is a valuable contribution to the research community. The OpenAgents platform will surely be used to develop many future agents, and be referred to as a baseline for other agent-developing research projects. Tables 3, 4, and 5 present thorough experimentation of their agent(s) compared to many other baselines on various tasks.","While the value of OpenHands is apparent, the research question of this paper is unclear. If it is that they can develop a general agent that performs better across a range of tasks than any existing agent does, it is my judgement that the results and analysis do not explore this sufficiently to warrant a research contribution. To do so, discussion and ablations should be included for the design decisions of OH Browsing Agent v*, OH CodeAgent v*, OH GPTSwarm v*, etc. To then isolate the effect of those decisions, the choice of backend model should be held consistent to those of the baselines against which the OH agents are compared. Error analysis of results comparing OH to baselines would be another welcome contribution that has been omitted from this paper.","Can we further motivate why we want a generalist agent? If we have specialized agents for different tasks, can we just have a coordinator deploy specialized agents where they perform best?"
OJd3ayDDoF,OJd3ayDDoF,OpenHands: An Open Platform for AI Software Developers as Generalist Agents,Accept (Poster),EPvRALCQYE,ICLR.cc/2025/Conference/Submission7566/Reviewer_nJqs,"This paper describes OpenHands, fka OpenDevin, a platform to develop LLM agents which can take meaningful actions using computers, such as browsing, coding, and interacting with a command line.

Section 2 describes the architecture of the platform, detailing the state and event streams, actions and observations, and an extensible library of tools. Agent delegation is also mentioned.

Section 3 mentions some notable agents implemented using their platform.

Section 4 presents results of many evaluatory benchmarks, showing the OpenHands has a broad and general level of competence across domains including coding and browsing.","OpenHands is an excellent and significant LLM-agent platform, seeing use e.g. as one of the reference agents in OpenAI's MLE-bench https://arxiv.org/abs/2410.07095 , and there is value in having this scaffold's architecture explained and benchmark performance reported in a peer-reviewed form.


- General competence across three different domains (SWE, browsing, and 'misc') is significant

- Solve rate of 26% on SWE-bench is competitive, as are many of the results suggested by the tables (though, see below, I have questions around like-for-like comparison)

- The paper provides a great many benchmark results

- OpenHands has many useful features, such as the API server being inside the docker container, the ability to delegate subtasks to other agents, and an extensible library of tools","The paper is an announcement of a software platform, more than it is a description of a contribution to a research area. Normally I'd expect a paper to look like a research question, with some motivation, experimental design, and results. This paper shows little, if any, scientific exploration, instead the paper gives a high-level overview of the architecture with little motivation, and then uncritically showcases performance on a broad range of existing benchmarks. I'm unsure whether this kind of project-announcement is a fit for the conference.

Some smaller points:

- Why do the (e.g.) GPQA results not compare like-for-like LLMs? I would want to see whether a CodeActAgent powered by a particular LLM was better or worse than the bare LLM, but the table compares (Llama 3 70b chat, GPT 3.5 turbo 16k, GPT 4) with (gpt-4o-mini-2024-07-18 , gpt-4o-2024-05-13 , claude-3-5-sonnet), so it's unclear how much of the difference is due to the scaffold and what is due to the choice of LLM. This is also true for many results in Tables 4 onwards, perhaps an unfortunate downside of comparing to results from the literature, but this limitation precluding rigorous comparison should at least be acknowledged in the text.

- A large fraction of the paper comprises short summaries of many different benchmarks used to evaluate the codebase. Since these summaries are not original work, I question whether so much of the paper should be dedicated to them. (As a minor point, I'd expect this exposition to be in a setup / methodology / problem-setting section, rather than in the results.) Instead, the authors could focus more on the creative decisions made during the creation of their platform.","L305 - I'm uncertain what is meant by ""we compare OpenHands to open-source reproducible baselines that do not perform manual prompt engineering specifically based on the benchmark content""

It seems surprising that OH CodeActAgent w. GPT-3.5 would get 2% on ToolQA, while ReAct (perhaps the most minimal form of agent scaffolding) would get 36%. Do you have any idea what's behind this performance difference?

Also surprising is that CodeActAgent delegating to BrowsingAgent does very slightly worse than straightforwardly using BA directly. Could you discuss this?"
XnX7xRoroC,XnX7xRoroC,Distilling Reinforcement Learning into Single-Batch Datasets,Reject,1jt35rQDow,ICLR.cc/2025/Conference/Submission7558/Reviewer_duG4,This paper proposes a PPO-inspired dataset distillation technique.,Distillation seems like an interesting technique to reduce the data requirement of reinforcement learning.,"I vote to reject primarily because the motivation for the algorithm and it's empirical evaluation is difficult to follow. I had a difficult time understanding the core takeaways of this paper. 

1. Many of the contributions listed can be combined. For instance, contributions 1, 3, 5 and 6 are essentially saying the same thing: this works propose a new distillation technique and demonstrates its effectiveness empirically. 
2. Contribution 2 doesn't seem like a contribution; it's simply a task that was created to demonstrate the distillation method. I suggest omitting. 
3. ""policy gradient methods such as PPO are more sample-efficient, utilizing the entire experience memory rather than randomly sampling from it as in standard DQN learning."" This sentence is unclear to me; PPO does not use experience replay, as it is an on-policy algorithm. Also, off-policy algorithms are often more sample efficient than on-policy algorithms, so this statement seems misleading.
4. First paragraph section 3.1: This paragraph motivates building off of PPO, but I think that's all that needs to be said: you build off PPO because it is a reasonably sample efficient on-policy algorithm and is often the go-to algorithm for RL applications.
4. It's unclear how the experiments extend cartpole to N dimensions. A figure for N=2 would make be informative. 
4. Table 1 is difficult to parse. What exactly are the core takeaways from this table? Why is it informative to consider different model initializations? the experiments would be easier to understand if hypotheses were stated prior to showing results -- what do we expect to see if the method works, and why?
5. If the method works, it would be useful to understand how it distills the dataset -- which samples are ultimately distilled? Can we glean any insights from it?",See weaknesses.
XnX7xRoroC,XnX7xRoroC,Distilling Reinforcement Learning into Single-Batch Datasets,Reject,2i3Zc2wXRt,ICLR.cc/2025/Conference/Submission7558/Reviewer_ucbK,"The paper presents an approach that distills a reinforcement learning environment into a synthetic dataset while allowing agent trained under supervised setting with limited resource to reach a comparable performance vs the direct RL training.
It also presents an generalized algorithm to control the difficulty of distillation for estimating the feasibility of the full distillation.","Overall the paper is well-written. It provides an clearly-defined algorithm with training graph and pseudocode. 
It provides a simple algorithm based on PPO to distill RL environments into a parameterized distiller.
The performance results from an easy task to complex tasks are on par with direct RL training which demonstrates the generalizability and high distillation performance of the algorithm.","* The experiments do not cover the continuous control problems which are also important part of RL environments. Demonstrating distillation on those tasks can greatly benefits to the community as many robot experiments are under continuous action space.
* If I understand correctly, the final baseline RL agent is determined by the time limit and convergence. But I would image using the same amount of training sample as in the distillation's outer loop for a more fair comparison.
* The cost saving part might be better displayed in a graph. Such as the overall time spend/number of parameter updates vs number of agent trained.","* In fig 1: What's $D_{\\theta}$, same in fig 2.
* Line 197: Why is the gradient destroyed? Isn't the bound non-zero?
* Can you also clarify the instance in sec 4.2, it's the sample generated from the distiller?
* In table 2, why would some experiments did not exceed the random performance while the full distillation did?
* In fig 3, what happens to the sudden increase in (e) subplot?"
XnX7xRoroC,XnX7xRoroC,Distilling Reinforcement Learning into Single-Batch Datasets,Reject,7GlivECrSy,ICLR.cc/2025/Conference/Submission7558/Reviewer_AwMk,"This paper introduces a novel distillation framework for distilling online RL environments to a synthetic dataset, which allows performant agents to be trained with one-step gradient descent. It extends PPO to leverage it to produce synthetic examples. Experiments are conducted on the cart pole environment and its n-dimensional extensions. Moreover, a few atari experiments are also performed to demonstrate its generalizability.","* Work is novel considering it is probably the first to introduce dataset distillation for online RL. However, there is a related work on dataset distillation for offline RL that is missing in related work and I believe should be discussed.(https://arxiv.org/abs/2407.20299)

* Results are promising in both Cartpole and atari experiments

* Framework does not introduce a significant overhead to the wall-time of the PPO.","* One of the main issues of this paper is motivation. Dataset distillation is proposed so that a large dataset can be condensed into a synthetic smaller one however I dont think the same analogy is reasonable for RL. Firstly, when you train a supervised model with large datasets, you would get almost the same accuracy which means that this dataset would have a score equivalence given a model whereas this is not accurate for the RL environments because RL environments are not deterministic. So, RL agents' performance varies across different numbers of runs (Agarwal et al., 2021). All in all, the distilling of a single instance of an environment will only represent the data that has been generated by this specific environment, not the environment itself. I would be very surprised if the policy generated by synthetic examples will generalize to the other instances with different seeds. Lastly, the discussion of runtime and the training of 10-20k agents is problematic due to similar reasons. You should get a unique agent if all are initialized the same however RL agent would be different if the environment is not fixed. I believe the randomness of the agents is due to the randomness of initialization.



* Evaluation setup is vague, in particular in Atari. In table 2(a), you present $\\textit{Average End-Episode Reward Achieved at Convergence}$. Is it the average total reward of the last 100 episodes, and also what does the st dev mean for this table, is it the average over multiple runs? I believe the evaluation setup should be clarified to interpret the given results.


* More runs and different environments(could be more atari games, or continuous control env like mujoco) are needed to demonstrate the effectiveness of the proposed method. No need for full distillation for new experiments, l4 is good enough.


* Paper is a bit hard to parse(especially section 3.1), so writing could be improved. I believe Figure 2 is redundant, does not provide any insights, and is not visually appealing. Algorithm 1 is sufficient to get the concept. Lastly, you could import booktabs package for a more used format by the ICLR papers.




 
References
- Agarwal, Rishabh et al. “Deep Reinforcement Learning at the Edge of the Statistical Precipice.” Neural Information Processing Systems (2021).","Q1) How does the policy induced by the synthetic examples preserve the trust region since the policy is not updated by the PPO loss?

Q2) Could you provide visualizations of the learned synthetic examples for 2D cartpole examples?

Q3) Do you have any insights regarding why lower ks are better even in higher dimensional cartpole settings?"
XnX7xRoroC,XnX7xRoroC,Distilling Reinforcement Learning into Single-Batch Datasets,Reject,Y9UJhtlYCB,ICLR.cc/2025/Conference/Submission7558/Reviewer_P1g3,"This paper proposes a new method / framework generalizing dataset distillation approaches to the reinforcement learning setting. A network distills the ""optimal juice"" from an RL environment. During test time, this distilled network can generate a dataset (input-output pairs) such that a supervised learning model trained on this dataset will get optimal rewards. Intuitively, I can think of the distilled network as trying to memorize the state-action stationary distribution of the optimal policy of an environment.","1) I think this paper is tackling a novel problem in a novel way. Its ""freshness"" is definitely a strength.
2) While the results obtained right now are not ground breaking, it does open up avenues for future work.","1) Missing baselines -- As far as I can tell, there are no baselines used in the paper. One fairly obvious (but could be strong) baselines is to train a PPO agent till convergence in the environment and use a diffusion model to learn the state-action distribution of the expert policy. At meta-train time, a newly generated agent can just use the state-actions generated by this model as a training dataset. 

2) Motivation -- The motivation behind why this line of work is important is not clear to me after reading the introduction. Let me elaborate -
    a) Hyper-parameter / architecture tuning - Do you believe that the hyper-parameters tuned using the distilled network will be robust when training on a new environment without distillation. It do not know, nor is there any evidence provided in the paper, that hyper-parameters / architectures that work well using distilled network will work well when training in a new RL environment from scratch.

3) Data anonymity -- I think readers might appreciate of when this is a compelling reason in the RL setting.",See Weaknesses section for points / questions to address.
l2zFn6TIQi,l2zFn6TIQi,Controlling Language and Diffusion Models by Transporting Activations,Accept (Spotlight),pnKeqD72XI,ICLR.cc/2025/Conference/Submission7460/Reviewer_Rxai,"The paper introduces Activation Transport (ACT), a framework based on optimal transport theory to steer model activations and control generative model outputs. ACT is designed to modify activations from a source distribution (e.g., toxic language) to a target distribution (e.g., non-toxic language) while preserving the natural activation distributions within the model. The framework applies to both large language models (LLMs) and text-to-image diffusion models (T2Is), supporting tasks like toxicity mitigation, concept induction, style control, and concept negation. ACT outperforms other methods in robustness and interpretability by allowing fine-grained, interpretable control of generative behaviors with minimal computational overhead.","1. ACT is a simple and efficient transport function approach that seems to perform well on the experimental setups for both LLM and T2I without significant impact on performance.
2. The paper is well written with clear and easy to follow formulation and experimental results, The paper demonstrates ACT’s effectiveness in diverse tasks, including toxicity mitigation, concept induction, style control, and concept negation, showing superior or comparable performance to existing methods. The method’s flexibility and consistent results across both LLM and T2I applications underscore its potential as a general-purpose activation steering tool.
3. By basing the intervention on optimal transport theory, ACT provides a clear, interpretable parameter (λ) that adjusts the strength of control. This parameterisation allows for easy tuning and understanding of model adjustments, enhancing its usability for practitioners.","1. ACT currently relies on linear transport maps, which are computationally efficient but may not capture complex, non-linear relationships within activations, especially in large or multimodal generative models. This assumption could limit its effectiveness in applications requiring nuanced adjustments.

2. The quality of ACT’s transport maps depends on the representativeness of the source and target samples. If the samples do not fully capture the intended distribution (e.g., all aspects of toxic vs. non-toxic language), the intervention may be less accurate, impacting model behavior under real-world conditions with unseen data.

3.  Although ACT shows promising results in mitigating toxicity and inducing specific concepts, the paper provides less evidence of its effectiveness across a broader range of behavioral modifications, especially in challenging or ethically sensitive areas like misinformation suppression or bias control.

4. The performance of ACT can be influenced by which model layers are selected for intervention. While the paper provides some guidance, a more systematic approach to identifying optimal layers would improve robustness and reduce the need for manual tuning.","1. A potential future work could be the focus on data selection to estimate the linear transport. This I would argue might produce better improvements than moving to non-linear estimations

2. although you show good evidence of the relatively minor degradation of model performance, more evidence is needed in a more quantifiable way, for example by showing limited impact on downstream tasks"
l2zFn6TIQi,l2zFn6TIQi,Controlling Language and Diffusion Models by Transporting Activations,Accept (Spotlight),brdLnc5AAu,ICLR.cc/2025/Conference/Submission7460/Reviewer_CvQ7,"This paper studies the problem of activation steering for diffusion models and Transformer language models. The key idea is to view activation steering as optimal transport. Under this umbrella, most existing methods are equivalent to mean transport map, which might not be optimal and do not preserve the activation distribution. Instead, the proposed method, Linear-ACT, can avoid out-of-distribution activations after steering. 

Experiments on Transformer language models (Gemma and Llama) show that the proposed method outperform baselines that use constant vectors for activation steering, on tasks including toxicity mitigation, concept inducing, and truthfulness. Experiments on diffusion models (Stable Diffusion models) show the effectiveness of the proposed method for style control and concept negation.","- This paper studies an interesting problem in activation steering -- out-of-distribution activations. Many existing works require a very large coefficient before the steering vector, which can easily lead to OOD activations. The proposed method instead do not need this extrapolation.
- The experiments are extensive, covering a wide range of control tasks for language models and diffusion models. Many of them are important tasks such as truthfulness and style control.
- The paper proposes a unified framework to understand the connection between different activation steering methods.","- One of the exciting applications of activation steering or representation engineering is safety. It would be interesting to see how well the proposed method perform on safety risk mitigation.
- The baselines are mainly vector addition methods. I wonder how the proposed method compare with vector projection methods such as https://arxiv.org/abs/2303.02536",
l2zFn6TIQi,l2zFn6TIQi,Controlling Language and Diffusion Models by Transporting Activations,Accept (Spotlight),J1rsF2RZnJ,ICLR.cc/2025/Conference/Submission7460/Reviewer_WjWp,"This paper proposes Activation Transport (AcT) as a general approach to steer activations in generative models, to map activations from one source distribution to the target distribution, while minimizing limitations of current inference intervention approaches. The authors show that the optimal transport-based framework proposed here generalizes many of the previous activation steering work. More specifically, AcT accounts for difference in the amount of variance for each activation dimension in its map which previous approaches do not. The representation maps are iteratively derived updated layer by layer, starting from the earlier layers to the last layers. AcT is agnostic to the modalities; as shown in the experiments, AcT can steer generation in the text domain, in applications such as toxicity mitigation, concept induction and truthfulness, and in the image generation, in applications such as style control and concept negation.","The proposed approach is grounded, intuitive and is applicable to several modalities and domains in generative modeling.

The paper is well written and easy to follow.

Extensive experiments are conducted to compare AcT to baselines approaches in text and image generation.","Scope is limited to single modality within a model and linear (not non-linear) mapping.

Experiments and results supporting the claim of AcT better preventing representations from being OOD is lacking.",Choice of pooling operation: Can the authors show data to support the choice of average pooling? Some discussion about why average pooling is ideal versus max pooling or last token would be insightful. Ablation disentangling the choice of pooling layers versus other approaches would better differentiate the contribution of AcT and the pooling operations.
l2zFn6TIQi,l2zFn6TIQi,Controlling Language and Diffusion Models by Transporting Activations,Accept (Spotlight),iibyzTz8ss,ICLR.cc/2025/Conference/Submission7460/Reviewer_eCSW,"The paper presents a novel framework called Activation Transport (ACT) for controlling generative models by steering activations using optimal transport theory. It claims to offer fine-grained control over model behavior with minimal computational overhead. ACT is applied to tasks in both large language models (LLMs) and text-to-image diffusion models (T2Is), demonstrating effectiveness in mitigating toxicity, inducing concepts, and providing style control.","1. The use of optimal transport theory to steer activations is a novel idea that generalizes previous activation-steering methods.

2. ACT is shown to be effective across different types of models (LLMs and T2Is), which suggests broad applicability.

3. The method claims to add negligible computational cost, which is crucial for scalability in large models.

4. The paper provides experimental results demonstrating improvements in toxicity mitigation and style control, among other tasks.","1. The paper mentions that Linear-ACT assumes a linear transmission relationship between activations, which is a simplification made for computational and memory considerations. Is it possible for nonlinear mapping to allow more complex activation relationships and finer control?

2. Since the mapping estimate depends entirely on the samples used, its expressive power is limited by the diversity and coverage of the samples. How to improve the generalization ability of the mapping through more extensive data sampling or enhancement techniques.

3. The paper mainly focuses on the control of a single task, such as toxicity mitigation or style control. How to deal with multiple conflicting objectives or constraints simultaneously under a unified framework?

4. Although Linear-ACT has a certain robustness to the parameter λ, other methods such as ITI-C are very sensitive to the choice of model and layer, which may lead to a lot of parameter tuning in different situations.

5. Although the ACT method claims to have low computational overhead, there is a lack of detailed analysis of the specific computational costs and resource requirements of models of different sizes in practical applications.","1. How does ACT perform in scenarios with highly multimodal distributions, where linear transport might be insufficient?

2. Can the authors provide more details on the computational costs associated with the implementation of ACT?

3. How sensitive is the method to the choice of the transport support $Q_o$ or $Q_∞$?

4. Are there any limitations observed in the diversity or creativity of outputs when using ACT for concept induction in LLMs?

5. How does the method handle potential biases introduced during the transport of activations?"
1XxNbecjXe,1XxNbecjXe,Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions,Reject,TmLuEMbJlB,ICLR.cc/2025/Conference/Submission7440/Reviewer_g6PQ,The paper introduced an attack that enables adversaries to add stealthy “meta-instructions” to images that influence how visual language models respond to queries about these images,"1. Figures clearly illustrate the point of the paper. 
2. The writing is easy to follow
3. Articulate the attack model and assumptions
4. Run transferability test","1. L33, "" but injection attacks in non-text modalities are a new, yet-to-be-explored area of LLM safety research"". This type of attack has been widely explore in [1] and [2]
2. L81, ""users are victims of adversarial third-party content that they ask the model to process"". I'm curious whether the images are generated by the users or not. If the user create the line chart shown in Fig. 1 from their local devices, does it mean the attack studied in the paper doesn't exist anymore?
3. Table 4, why is the transfer rate of llava on negative as low as 0.1?
4. I'm curious what will happen if the system prompt of the VLM contradicts with the meta-instruction in the image?
5. Overall, I think the paper is in a good quality. The major downside is the novelty, as we already know from previous work that optimizing the input image towards a certain attack target is feasible for VLM. Thus, it's not a new vulnerability in VLM. Though the author attempts to differentiate their attack setting from previous jailbreaking and soft prompt attacks, the overall attack surfaces and methods remain largely the same. I would like to the see more insights coming from the paper. 


[1] Are aligned neural networks adversarially aligned?
[2] A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends",See weakness
1XxNbecjXe,1XxNbecjXe,Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions,Reject,yVAm3LKz0e,ICLR.cc/2025/Conference/Submission7440/Reviewer_j1Tw,"The paper introduces a method to create image inputs to Vision Language Models (VLMs) that lead said model  to respond to any user query appended to the image with a certain ""spin"", e.g. responding with a certain sentiment,  or in a certain language. The authors refer to this  as embedding a ""meta-instruction"" in an image. 

Critically, a meta-instruction attack is only successful  if the models response to the users query (and the  attacked image) responds to the query whilst following  the meta-instruction (e.g., if the meta-instruction was  ""talk in French"" and the model responded in French but  did not answer the users query, then this would not  be a successful attack).

To train these meta-instruction attacks, the authors  perform projected gradient descent on an image  to minimize the language modeling loss of the VLM inputted with this image over a dataset of  synthetic question answer pairs with the answers  following some target natural language meta-instruction.

The results of the paper demonstrate that this  method can be used to learn adversarial images  for various different types of meta-instructions. The authors also demonstrate a non-trivial  transfer of meta-instruction images between models.","## Originality

The question of prompt injection vulnerabilities  to large language models is of significant importance.  The authors demonstrate that models are vulnerable to  similar attacks of this nature through their vision input as are  possible through their text input. What's more, they  show the vulnerability is in some cases worse through  the image input.

Whilst the idea of providing meta-instructions through  image inputs its not entirely novel (see weaknesses section), this paper is the most thorough treatment of the subject that I am aware of, and brings to light new and concerning  ways that a model's output can be tampered with using  images.

## Quality and clarity

The paper is well written the method is conveyed clearly. The results section contains a good depth of experiments, most importantly covering a number of popular  open-source VLMs and target meta-instructions.

## Significance

As VLMs are used more frequently for agentic tasks  that will expose them to untrusted data from the internet, prompt injection / meta-instruction attacks will  become more and more concerning. Thus the paper  concerns a timely and interesting threat model  that the adversarial attack community should be  exploring in more detail.","While the critique is long, this is only because I believe the paper has interesting results that could be improved.

## Presentation of previous work

The authors make a number of claims about prior work that I believe are not completely accurate. Editing the language around these claims would help to improve the paper. Here are some examples that I believe need to be addressed:

- Line 32 - ""But injection attacks in non-text modalities are a new, yet-to-be-explored area of LLM safety research."" I do not think this is entirely true. For example, Bailey et al. [1] explore how train an image to convey a certain text prompt, which they demonstrate can be a prompt injection attack. 
- Line 83 - ""By design, jailbreaking and adversarial examples produce contextually incoherent outputs that do not actually answer users’ questions about images."" I think this depends on how you define an image jailbreak. For example, Dong et al. [2] produce adversarially perturbations to harmful images that lead to a model answering coherently about said image --- in particular the model is able to correctly identify what is in the image. While the authors claim here is correct for other image jailbreaking work, such as Qi et al. [3] who learn images unrelated to the harmful request they are trying to elicit a response about from the model, it is not universally correct. For this reason the claim should be softened.
- Line 84 - ""They [jailbreaking and image adv attacks] are not stealthy and cannot be used for indirect attacks because users would notice that the VLM’s outputs are wrong given the conversation context and inputs."" Bailey et al. [1] and Qi et al. [3] both demonstrate methods to create jailbreaking images under epsilon ball constraints, which is the definition of stealthiness the authors use on line 290. 

## Novelty / originality

Following on from some of the comments above,   I believe there is a question of novelty / originality  of this work.  

In particular, the general algorithm presented to  produce meta-instruction attacks essentially involves  creating a dataset of input output pairs, and training  an image by PGD to maximize the likelihood over this  dataset. This method appears to fit into the ""Behavior Matching"" algorithm from Bailey et al. [1] 

Despite this, I believe the work does contain novel  and important contributions. In particular: 
1. The study of the changes in semantic meaning present in images from various different attacks, with meta-instruction attacks preserving meaning.
2. The transfer experiments in Table 4 are very interesting.
3. This is the most thorough treatment of prompt injection image attacks I have seen.

## Summary

Combining the above two points, I believe the paper needs to be
rewritten to more clearly lay out the novelty of the paper 
and more accurately represent the papers contribution. 
My high level suggestions would be:
1. Make it clear that prior works have examined prompt injecting image attacks, however yours is a more complete treatment of the topic.
2. Make it clear that your method to create such attacks is a special instance of what prior works have introduced. 
3. From this, your novelty comes not from the method but rather the results. E.g. line 88 that reads ""We design, implement, and evaluate a method for creating a new type of image perturbations that act as cross-modal soft prompts for a language model while preserving the visual semantics of the image."" needs to be adjusted.
4. Given that I do not think the method is novel, I would suggest running the following additional experiments:
	1. In Table 4, add transfer results to Claude and GPT-4o. These results should feature in the transferability experiment.
	2. More detailed defense experiments. Appendix C shows fairly simple defenses can work to avoid meta-instruction attacks. [1] finds that training perturbations under different constraints (e.g. a moving patch) ends up being more robust to simple defenses. It would be interesting to see if this result is reproducible in your setting.

To reiterate, I think studying prompt-injection images to models is important, and the authors present valuable results. I thank the authors for their hard work! 


[1] - Bailey, Luke, et al. ""Image hijacks: Adversarial images can control generative models at runtime."" arXiv preprint arXiv:2309.00236 (2023).

[2] - Dong, Yinpeng, et al. ""How Robust is Google's Bard to Adversarial Image Attacks?."" arXiv preprint arXiv:2309.11751 (2023).

[3] - Qi, Xiangyu, et al. ""Visual adversarial examples jailbreak aligned large language models."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 19. 2024.","I summarize some of my comments on weaknesses of the paper into questions below:

1) Do the authors agree with my comments about their portrayal of previous works, and if so what steps are the authors taking to address this? Concretely, what sections of the paper have been rewritten.
2) Have the authors been able to run the suggested experiments I have mentioned above, and if so what did they find?"
1XxNbecjXe,1XxNbecjXe,Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions,Reject,MNy3HwV1I7,ICLR.cc/2025/Conference/Submission7440/Reviewer_VLtV,"The paper introduces a new type of attack on visual language models. These attacks, termed meta-instruction attacks, involve subtle image perturbations that act as soft prompts to influence how a model interprets images and responds to queries. The idea is to steer the model’s outputs to satisfy adversary-chosen objectives, such as a specific sentiment, style, or political bias, without the user being aware of the manipulation. The authors demonstrate the effectiveness of this approach across various visual language models, showing that these perturbations often outperform explicit instructions and are transferable across models.","1. The concept of embedding hidden meta-instructions within images offers a new approach to prompt injection for multi-modal models, highlighting a potential vulnerability not extensively covered in existing literature.

2. It is interesting to see how the method reveals hidden capabilities of instruction-tuned models. In some cases, the meta-instructions successfully steer the model's outputs in ways that explicit instructions fail to achieve.

3. The study provides an empirical evaluation on a range of meta-objectives (e.g., sentiment, language, and political bias), demonstrating the effectiveness of the attack method.","1. The paper's reliance on just five images from a single dataset, ImageNet, limits the robustness and generalizability of its evaluation. ImageNet, which is primarily focused on object recognition, may not adequately represent the diversity and complexity of images encountered in real-world scenarios. Incorporating evaluations on datasets with more varied and complex scenes, such as MSCOCO, would provide a more comprehensive assessment of performance.

2. The paper simulates user interaction by generating questions to test meta-instructions, but it provides limited clarity on whether these questions adequately cover a broad range of natural user queries. Limited prompt diversity may affect the robustness of the attack if VLMs encounter different prompts in real-world scenarios.

3. Since the meta-instruction is added as noise to the image, the paper does not demonstrate the effectiveness of meta-instructions against recent inference-time defense methods like DISCO[1], DiffPure[2], and IRAD[3]. This could be valuable for understanding its performance in the context of contemporary robustness strategies.

[1] DISCO: Adversarial Defense with Local Implicit Functions.
[2] Diffusion models for adversarial purification.
[3] IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks.",Please see weaknesses.
1XxNbecjXe,1XxNbecjXe,Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions,Reject,dHR7gQFjNq,ICLR.cc/2025/Conference/Submission7440/Reviewer_YgaV,"This paper proposes a new attack objective in which the output text remains consistent with the input images but adopts an adversary-chosen style, sentiment, or point of view. The adversarial optimization is applied to the input image, ensuring that the modifications are imperceptible to humans. Experiments demonstrate that images containing hidden meta-instructions achieve significantly higher success rates compared to those with explicit instructions. This attack highlights a practical risk, as it enables the dissemination of seemingly coherent but misleading information.","1. The focus on the dissemination of seemingly coherent misinformation is highly practical and addresses a significant real-world concern.

2. The evaluation is thorough, including robustness testing against JPEG compression as a defense (which I suggest moving to the main text, given its practicality in everyday use) and examining the transferability of the attack across different vision-language models (VLMs).","1. A NeurIPS 2024 paper [1] also explores the dissemination of seemingly coherent misinformation in visual language models, but through the lens of data poisoning. While this paper focuses on test-time adversarial attacks, it would be beneficial to discuss the key differences between test-time attacks and training-time poisoning, and in what scenarios each is more practical, given the similarity in objectives between the two papers.

2. The evaluation of image semantics preservation seems suboptimal. In Section 5.3, semantics are defined using cosine similarity between images, but it is unclear why this metric is particularly relevant. A more meaningful evaluation would assess how well the actual text output of the visual language model aligns with the input images, which is the core focus of this paper—consistent outputs with images but in adversary-chosen styles, sentiments, or viewpoints.


Reference:
[1] Xu, Yuancheng, et al. ""Shadowcast: Stealthy data poisoning attacks against vision-language models."", The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024","1. Could you also provide an evaluation when random resizing or cropping is applied? Since this paper addresses practical concerns, it would be valuable to test your method under common “defenses” encountered in everyday scenarios.

2. Are there any failure cases? For example, are there meta-instructions that are particularly difficult to achieve?

3. Why is it necessary to evaluate cosine similarity as done in Section 5.3? Could you clarify the relevance of this metric?

4. Is there an evaluation that checks whether the generated textual outputs remain consistent with the input images?

Overall, I appreciate the practical focus of this paper. I would be happy to raise my evaluation if these concerns are addressed."
pdf6MbXnAS,pdf6MbXnAS,Disentangling Latent Shifts of In-Context Learning Through Self-Training,Reject,cXrUsxzQ7m,ICLR.cc/2025/Conference/Submission7387/Reviewer_dJby,"This paper introduces Self-Training ICL (STICL), a novel approach that stabilizes in-context learning (ICL) through self-training. Specifically, STICL uses a student model (LoRA adapter) trained to mimic the output of prompted teacher models, achieving a disentanglement of query and few-shot examples that goes beyond the approximations of previous studies. The authors demonstrate the superior performance and out-of-distribution (OOD) generalizability of this adapter, highlighting its effectiveness over prior ICL methodologies.","- Motivation and Clarity: The motivation behind this work is well-explained, with clear distinctions from previous studies. The paper is well-organized and easy to follow, making it accessible to readers.

- Empirical Validation: The paper provides strong empirical evidence supporting the effectiveness of STICL, particularly in OOD settings. This solidifies its contributions and highlights its practical relevance.","- While Table 3 discusses OOD generalizability, it would be beneficial to include a cross-tabulation across datasets to show the performance in-domain. Understanding the performance gap between in-domain and OOD settings is a key factor that could add depth to the findings.

- Comparison with MetaICL: This study’s use of adapters for STICL could be interpreted as an adaptation of MetaICL, making it important to explicitly clarify the differences between the two approaches. Unfortunately, this work does not appear to cite MetaICL, which may cause readers to miss relevant context and comparisons.","1. How were the subsets in Table 4 selected? Additional context on this decision would clarify the setup.

2. How are the LoRA parameters configured? Specifically, what level of low-rank approximation is achievable in this setting?

3. Could you clarify the connection between the claim in line 219 and prior studies? A clearer explanation of why this design is impactful would be helpful.

4. In Section 4, the discussion on the Lipschitz constant and weak-to-strong generalization feels somewhat unclear. Could you elaborate on how this contributes to the proposed approach?"
pdf6MbXnAS,pdf6MbXnAS,Disentangling Latent Shifts of In-Context Learning Through Self-Training,Reject,5HyewSnXe0,ICLR.cc/2025/Conference/Submission7387/Reviewer_gSE5,The paper proposes to use unlabeled data to stabilize and improve in-context learning. The paper formalizes a form of decomposition of the information obtained from demonstrations vs query. The particular approach involves utilizing pseudolabels in a teacher-student framework. Empirical results suggest that this form of self-training can effectively leverage labeled data to improve robustness and stability in downstream performance.,"The self-training analysis of the paper is interesting and useful - connecting to ""classical"" ideas of local consistency and coverage expansion. This work brings in the perspective of self-training to LLMs and shows that with just a few demonstrations but large amount of unlabeled data, one can adapt LLMs to have strong and robust performance on various downstream tasks. 

The approach, while adding complexity over vanilla ICL, is fairly intuitive and builds on solid foundations of how pseudolabels can enable weak-to-strong generalization and can be leveraged for better and more robust performance. This is an important question of real-world significance. 

The empirical investigations span a variety of datasets and models.","One main weakness (and source of confusion for me) is the terminology of ""in-context learning"" when the models are being fine-tuned via self-training. The whole point of ICL is to eliminate the need for finetuning models, so it is confusing how the current approach fits into the framework. The current process does some kind of finetuning, so it should be pitched as a finetuning method. The method is much more expensive than n-shot, and in general, the computational aspects need to be considered for fair comparisons. 

Keeping terminology aside, the motivation of the paper - in terms of disentangling latent shifts is also quite confusing and seems unnecessary. It seems like the goal is to just ""distill"" the ICL process into the adapter. I'd recommend the authors to simplify this presentation a bit, because it is currently distracting from the main message. I don't think this disentanglement introduces a fundamentally new perspective - in-context learning can be approximated by (the more expensive) finetuning on related task data. As far as I understand, that is the main connection.","(1) For the pattern-based finetuning baseline - do you finetune on just the {4, 8, 16, 32} demonstrations?

(2) Are the demonstrations used at train and test-time the same or different? How should one think of what the demonstrations are meant 

(3) I'm assuming the ""unlabeled samples"" come from the standard training set, with labels removed? What's the ""oracle"" performance of finetuning on ground truth labels?

(4) Is the following a reasonable story that faithfully captures what's going on? The main idea is to do some form of self-training (via finetuning an adapter), and the labels are generated on this unlabeled data via in-context learning?"
pdf6MbXnAS,pdf6MbXnAS,Disentangling Latent Shifts of In-Context Learning Through Self-Training,Reject,L4JXiwuIIJ,ICLR.cc/2025/Conference/Submission7387/Reviewer_sdep,"The paper collects two issues of ICL: (1) instability: ICL is affected by ordering and selection of demonstrations (2) context length: ICL is limited by context window.
The paper assumes that the demonstration of ICL introduces a latent shift to the language model.
The paper points out a series of works on linear functions have a similar idea that demonstrations shift zero-shot $W_\\text{zs}$ to $W_\\text{zs}+\\Delta W_\\text{ICL}$, where $\\Delta W_\\text{ICL}$ can be regarded as the latent shift coming from demonstrations.
The paper then introduces existing methods on disentangling latent shift and the main idea of the paper, i.e., the proposed method of finetuning an LLM with query aligning its output to an LLM with both demonstrations and query.
Therefore, the latent shift is injected into the LLM via alignment, and when inference, there is no demonstration needed.
Finally, through experiments, the paper shows that the proposed method can improve stability and generalization, and show weak-to-strong generalization.
The paper also mentioned the proposed method is more efficient than vanilla ICL since no demonstration is involved in the input prompt.","(1) The proposed method is straightforward, makes sense, and very interesting -- regarding ICL as an alignment task is very interesting.

(2) The proposed method can handle the ordering issue since in the alignment state, the input of the teacher model can take input with arbitrary order. It's not very clear how to handle the ordering issue in vanilla ICL, but in the proposed method, the ordering issue is handled naturally via training sample construction (STICL-S in Table 2).

(3) The proposed method can reduce the inference cost, since no demonstration is involved when inference.

(4) The proposed method has better scores than baselines for ID generalization (Table 1).

(5) The proposed method has better scores than baselines for OOD generalization (Table 3)","(1) Though the proposed method decreases the inference of cost since no demonstration is involved, it requires training for alignment.

(2) It's unclear how the paper posits Sec. 3.2 since the existence of STICL-R. It's more natural for me to take 16 samples from k$\\times$16 examples to construct 16-shot demonstrations for STICL-R, rather than construct k separate 16-shot sets for k adapters in STICL-S. The author could consider clarifying their reasoning behind the design choices for STICL-R and STICL-S

(3) It's unclear how the paper calculates the Lipschitz constant for each method in Figure 2, due to page limitation. The author could consider providing a more detailed explanation of their Lipschitz constant calculation method in the Appendix.

(4) It's unclear how the paper desgins the pseudo-label correction, due to page limitation. The author could consider providing a step-by-step description of their pseudo-label correction algorithm, possibly with package algorithm.","I like the idea very much, but some things are unclear in the paper, thus I lower my score.

(1) This is an important one. What is the algorithm for the pseudo-label correction? I can only find several descriptions for it. Could the author give a detailed description of it?

(2) This is less important. How is the Lipschitz constant calculated? I still can only find some rough description of it in the appendix on Page 17, but without a detailed description.

(3) It would be interesting to see the scores of 16-shot on the student model. We know the student is good at 0-shot, how about 16-shot?

Others:

(4) The idea of Sec. 3.2 could be applied to joint training of multiple tasks. In Table 3, we see OOD generalization via training on task A and testing on task B. It would be interesting to train multiple tasks and test on the others."
pdf6MbXnAS,pdf6MbXnAS,Disentangling Latent Shifts of In-Context Learning Through Self-Training,Reject,zGMeK7vhvY,ICLR.cc/2025/Conference/Submission7387/Reviewer_eung,"When using in-context learning to solve tasks with LLMs, they can suffer from instability, characterised by biases toward the first and last examples in the demonstrations. Furthermore, using many demonstrations in long contexts extends the length of the LLM’s prefix window, increasing computation. This paper proposes a method to disentangle unlabelled demonstrations in-context from queries using self-training. A teacher model sees the demonstrations along with the query and produces a pseudo-label that a student model is trained on using adapters. These adapters (approximately theoretically) learn the demonstrations, allowing the student to be prompted with only the query and achieve comparable (in fact, better due to a weak-to-strong generalisation effect) performance than n-shot prompting. The method is also shown to outperform other comparable approaches.","As far as I am aware, the method (self-training from unlabelled demonstrations using a teacher to generate pseudo-labels) is simple but novel. However, I note that I am not extensively familiar with the literature in this area. The paper is clearly written and mostly easy to understand. The idea is nicely motivated by the theory used based on linear attention. I think that the work is significant. Long-context models suffer from serious inference-time bottlenecks. This work has not only shown a way to eliminate this bottleneck in certain scenarios where demonstrations may dominate (which on its own I would deem significant), but also improves in terms of quality compared to comparable approaches. The discussion on weak-to-strong generalisation is interesting and valuable.","- There is no section on limitations. I would be interested in seeing some discussion on this with respect to both the chosen method and results. In particular, the computational cost of training vs using demonstrations, when is the method not applicable, and the lack of exploration with very large demonstration sets.
- Some of the discussion on weak-to-strong generalisation in section 2 is unclear. It does not explain why the student is expected to outperform the teacher or about what local-to-global consistency is in the context of in-context learning. I had to refer quite a lot to the cited paper to understand what was being said here. Please clarify.
- The theory is interesting motivation but based on an approximation of quadratic attention. To provide further support, given the limited theory, is there a way you could experimentally determine whether the demonstrations have been disentangled?","- Regarding the evaluation setup, are the queries used in training the student with pseudo-labels disjoint from those used when evaluating the student?
- Is stability an issue beyond transformer-based LLMs? E.g. for state-space/diffusion-based LLMs?
- Do you have any quantification of how much more efficient your method has made inference?"
pjKdWj5NSR,pjKdWj5NSR,ReFOCUS: Recurrent False Object Correction Using guidance Strategies in Object Detection,Reject,6FbVpBEIel,ICLR.cc/2025/Conference/Submission7231/Reviewer_7k3H,"In this paper, two strategies of improving object detectors are proposed to handle the issue caused by erroneous annotations and false positives. It is achieved by guiding the false positives toward true negatives in the latent or the logit space. By further fine-tuning on correct annotations, the proposed methods improve the detection performance in most cases.","1. Two strategies of improving object detectors are proposed.
2. Training details are provided to ensure its reproducibility.","1. The writing is poor. The authors need to carefully revised the paper from aspects of logical structures and grammars.
2. In the first sentence of abstract, “recurrent false positive classification” is required to be further explained. In the first paragraph of introduction, the explanation of “recurrent errors” is also needed. Are these two concepts have the same meaning?
3. The motivation may be questionable. In introduction, the authors claim that “the model consistently detects an object that should not be identified, e.g. people on billboards as instances of real people.” However, the phenomenon that people on billboards are detected may be reasonable in some situations. Classifying these samples to be background by force could increase the risk of model oscillation or overfitting.
4. In introduction, the Motivation paragraph seems to be logically incorrect. It seems that D_{True} is used in all cases. Why not using the f_{True} directly? A detailed explanation of this paragraph is required. Furthermore, the proposed methods only slightly outperform f_{True}. In some cases, the performance is even dropped, which limits the contribution of this paper.
5. The experiments are not convincing. For PASCAL VOC dataset, there are 20 classes of objects. However, the authors only uses a small part of them. The mAP on all classes should be reported.","The motivation of this paper should be re-clarified. The complete experimental results should be reported. The overall contribution is limited. Furthermore, this paper needs a major revision by re-organizing and proofreading its paragraphs."
pjKdWj5NSR,pjKdWj5NSR,ReFOCUS: Recurrent False Object Correction Using guidance Strategies in Object Detection,Reject,d1GyZkxFIO,ICLR.cc/2025/Conference/Submission7231/Reviewer_MM1g,"The work focuses on guiding the false positive object predictions to true negative predictions. The paper shows that models with FP predictions can be efficiently corrected using FP annotations. The paper proposed two correction approaches that guide false positives toward true negatives, i.e., in latent space (LoGF) and in logit space (LaGF). Both two methods required a corrective dataset where all recurrent FPs are additionally annotated. LoGF only modifies the classification logits of FP. LaGF uses an autoencoder architecture to change FP latents (sampled in a trained GMM) to TN latents (decoded by a trained decoder). After the training of the decoder, the original detection model is guided with newly annotated FP labels and  the decoder to bridge the gap between FP and TN in latent space, therefore, avoiding the FP predictions.","The paper explores a new area to show that models with FP predictions can be efficiently corrected using FP annotations with two proposed methods, LoGF and LaGF.","1. Unfair comparison. While these two proposed guidance frameworks both rely on a corrective dataset where all recurrent FPs are additionally annotated, a fair comparison should be a noisy model fine-tuned on the combination of corrective dataset D_c and correct dataset D_{True} instead of fined-tuned only on correct dataset D_{True}.
2. The experiments only explore the situation when only one pair of classes are misannotated, however, in the real world, multiple misannotated pairs are more common.
3. Some mistakes in writing. In Section 4.1, the authors use PASCAL VOC 2007, while in Section 5.1, the authors say to train the model on MS-COCO 2017. And no labels on the caption of tables to point out whether the results are done on the former or the latter dataset.","1. Please refer to the weakness.
2. Why contrastive learning is listed as one subsection of related works?"
pjKdWj5NSR,pjKdWj5NSR,ReFOCUS: Recurrent False Object Correction Using guidance Strategies in Object Detection,Reject,z4iYTaGO2d,ICLR.cc/2025/Conference/Submission7231/Reviewer_MRKq,"This work addresses the issue of recurrent false positive (FP) classification in object detection. The paper proposes two innovative correction frameworks that guide FPs toward TNs in either the latent space or the logit space. The latent guidance framework leverages an autoencoder where a learnable Gaussian mixture model generates the embeddings of appropriate TNs, and a straightforward decoder retrieves the TN embedding given a bounding box. The paper utilizes the properties of the Wasserstein distance to train the Gaussian mixture model through standard backpropagation.","1. The experiments show that the propsoed module improves the performance.
2. The idea of introducing other spaces other than logit space to solve classification errors is valuable.","1. Figure 1 is the network diagram in DETR paper, without any information increment, so this content should not appear in the main text. In addition, although illustrations of motivation may not be necessary in the Introduction section, the description of the motivation section in this article should have an image which includes:
(a) the case visualization and explanation of the problem of false positives in classification errors. 
(b) the comparison with previous practices, as said in 'Motivations' part in Line 68. Tell the reader how the proposed method addresses the problem compared to previous approaches.
(c) the visualization of the noisy dataset D_{Noisy}, and well-annotated dataset D_{True}. And how they are used in the method.

2. Line 062 said the method 'can be generalized across different datasets and detection frameworks.' However, there were no experiments conducted on additional datasets and detectors in the experimental section and supplementary materials. In addition, both the data volume and the number of categories of VOC07 are not representative enough. Can it be validated with a larger dataset  (e.g., COCO, OpenImages)  and extra detectors to  better demonstrate the method's generalizability? Also, could you provide the computational requirements for testing on larger datasets if that is limited in larger dataset experments.

3.  The writing and paper organized (like the structure of method section) should be improved. For example:
(a). Sec 3.3.1 has 'Definition 1' (Line 221), then what? There are no  'Definition 2' in the following part.
(b). The relationship between sections in method. You can provide  an overview of  'LoGF' and 'LaGF' in 'CORE CONCEPT' part and tell us the structure of the following sub-sections.
......
and, there are too many colloquial words in the article, such as ""we"" appearing 107 times.

4. More visual displays are necessary. You don not need to show the results here, but rather make the before-and-after comparisons of specific false positive cases, or visualizations of how the latent space changes with the proposed methods.  In addition, some qualitative experiments on false positives can be added, such as analyzing whether the proportion of false positives decreases after adding methods from multiple dimensions, and so on.",See weakness part.
pjKdWj5NSR,pjKdWj5NSR,ReFOCUS: Recurrent False Object Correction Using guidance Strategies in Object Detection,Reject,5KLlckyDlj,ICLR.cc/2025/Conference/Submission7231/Reviewer_yPBg,"This paper aims to correct false positives in object detection. Starting from erroneous annotations and non-bojects similar to the true samples, the model is corrected in the latent and logit spaces.","1. The paper provides a rich theoretical basis and source of motivation.
2. The paper verifies the effectiveness of the proposed method in both general object detection and few-shot object detection scenarios.","1. The paper mentions corrections in two spaces in both the abstract and contributions, but lacks a description of logit.
2. The paper mentions that ‘we assume that we have access to a corrective dataset DC’. It seems that the paper's method relies on clear FP data with the same data domain as the initial dataset to guide GMM generation and model fine-tuning, but such data is often not easy to obtain. We often cannot access the data after training a network, not to mention the FP information. How to solve this?
3. The paper proposes multiple loss functions in the method stage. It is recommended that the paper mark the different stages and training parameters of each loss in the Figure to facilitate readers' understanding. For example, in the final loss L_decoder stage, the author's network contains two MLP_bbox modules. Which one does ybbox target?
4. In Table 2, it seems that the author's method does not improve significantly compared to directly using f_Ture. Did the author compare the extra cost of this method compared to f_True? For example, a time comparison for such a two-stage process.","Before the Definition 1, the paper set the TN ‘only its position remains to be defined’, but after Definition 1, the paper said the TN ‘shares the same bounding box as the FP.’ How does this information align?"
m3KuuE2ozw,m3KuuE2ozw,CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression,Accept (Poster),68tT8bMqtw,ICLR.cc/2025/Conference/Submission6988/Reviewer_egdp,"The paper introduces a new method called CAT-3DGS for compressing 3D Gaussian Splatting (3DGS) representations. This method aims to optimize the rate-distortion trade-off by using a context-adaptive triplane approach. It captures spatial correlations through multi-scale triplanes and leverages intra correlations within Gaussian primitives for efficient coding. Additionally, it incorporates a view frequency-aware masking mechanism to skip less impactful primitives, achieving state-of-the-art compression performance on real-world datasets.","+ The paper introduces a new compression method for 3DGS that utilizes a triplane-based hyperprior. This method leverages multi-scale triplanes oriented along the principal axes of Gaussian primitives.
+ The paper presents a novel pruning approach in 3DGS using a view frequency-aware masking mechanism. This mechanism assesses the significance of Gaussian primitives based on their impact on rendering quality, allowing less critical ones to be skipped during coding.
+ The paper presents comprehensive ablation studies of various modules in the proposed compression method.","- Entropy coding is a variable-length coding technique, typically decoded sequentially. However, 3DGC, used in rendering, is a highly parallelized method. The paper does not clearly explain how the proposed compression method can manage this parallelization without impacting rendering speed.
- The paper does not address decoding complexity. While CHARM enhances compression performance, it could significantly impact decoding speed, which in turn may affect rendering speed.
- The paper does not clarify whether real entropy coding was used during rendering or if the bit-rate was calculated based on theoretical entropy coding.",* Are there scenarios where CAT-3DGS underperforms? Which modules in CAT-3DGS contribute to the drop in performance?
m3KuuE2ozw,m3KuuE2ozw,CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression,Accept (Poster),SyQZBVg4qY,ICLR.cc/2025/Conference/Submission6988/Reviewer_stVt,"This paper proposed a context-adaptive triplane based hyperprior entropy model to capture the inter correlation among Gaussian primitives in the 3D space  (i.e. spatial correlation) for spatial autoregressive coding in the projected 2D planes. The channel-wise autoregressive coding is performed to leverage the intra correlation within each individual Gaussian primitive. Besides, the view frequency-aware masking mechanism is proposed to actively skip from coding those Gaussian primitives with little impact on the rendering quality. Experimental results show that the proposed CAT-3DGS achieves the state-of-the-art compression performance on the commonly used real-world datasets.","- ***Novelty of Framework:*** The key idea of decomposing the dense 3D hash-graid in HAC with triplane hyperprior entropy model is novel and reasonable.
- ***Technological Innovation:*** This paper proposed the SARM and FARM modules to exploit the inter-correlation and intra-correlation, respectively. Speciflly, the view frequency-aware masking mechanism is designed to to skip less critical Gaussian primitives from coding.
- This paper is well organized and easy to follow.","- Ablation experiment for CARM is not sufficient, the proposed method is only evaluated on one scenario, *e.g.*, bicycle scene.
- The paper is based on the work of HAC. However, the rate parameter setting used by HAC in the comparison experiment is not the same as that used by the proposed method. I doubt the fairness of this setting.","- The FARM proposed by the author is interesting. According to the manuscript, the anchor feature is a latent representation of the anchor point, which does not have a clear mathematical correlation like the spherical correlation coefficient of 3DGS. What is the motivation of the proposed channel-wise autoregressive processing of anchor features?
- It is noted that the rate parameters setting used by the authors on different datasets are not consistent. Why?"
m3KuuE2ozw,m3KuuE2ozw,CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression,Accept (Poster),BcKlow46Vs,ICLR.cc/2025/Conference/Submission6988/Reviewer_tuJM,"The paper presents CAT-3DGS, a context-adaptive triplane approach for compressing 3D Gaussian Splatting (3DGS) data with a focus on optimizing the rate-distortion trade-off. CAT-3DGS introduces multi-scale triplanes aligned with the principal axes of Gaussian primitives in 3D space, leveraging both spatial and channel-wise autoregressive models to capture inter and intra correlations for efficient compression. This approach also includes a view frequency-aware masking mechanism to skip encoding less impactful Gaussian primitives. Experiments demonstrate CAT-3DGS achieves superior rate-distortion performance compared to prior 3DGS compression models like HAC, particularly on commonly used real-world datasets.","1. Innovative Triplane-Based Hyperprior: 

By projecting 3D Gaussian primitives onto triplanes, the paper introduces an efficient hyperprior structure that captures spatial correlation, which improves entropy coding performance.

2. Enhanced Coding Efficiency: 

The combination of spatial and channel-wise autoregressive models effectively utilizes both inter and intra correlations, which is particularly valuable in reducing the bit rate while maintaining high rendering quality.

3. Practical Masking Mechanism: 

The view frequency-aware masking mechanism selectively encodes only those primitives crucial for rendering, further optimizing storage and bandwidth usage.

4. Comprehensive Experimental Evaluation: 

The method is tested across several datasets and compared against notable baselines, showcasing significant rate savings and quality retention.","1. (Minor) Autoregressive Model Use: 

While the channel-wise and pixel-wise autoregressive models are novel in this context, they are a commonly applied strategy in image compression tasks. However, this paper does make a unique contribution by applying it to 3DGS.

2. Lack of Comparative Analysis with ContextGS: 

Although ContextGS is cited as a related work, there is no direct performance comparison with it. Given that ContextGS provides detailed results and its code is available, a comparative evaluation would enhance clarity on CAT-3DGS's improvements over previous models, especially regarding contextual coding efficiency.​",I do not have other questions.
m3KuuE2ozw,m3KuuE2ozw,CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression,Accept (Poster),KrwImI3ngW,ICLR.cc/2025/Conference/Submission6988/Reviewer_GetA,"The paper introduces CAT-3DGS, a context-adaptive compression method for 3D Gaussian splatting (3DGS) that optimizes rate-distortion performance. CAT-3DGS uses multi-scale triplanes to embed spatial correlations of Gaussian primitives and employs autoregressive models for encoding. Additionally, it uses a channel-wise autoregressive model to enhance compression efficiency within each primitive and introduces a view frequency-aware masking mechanism to skip encoding primitives that minimally impact image quality. Experimental results demonstrate the effectiveness of the proposed method.","1. PCA-guided triplanes as the hyperprior
2. Channel-wise Autoregressive Model: Enhances compression by leveraging internal correlations within each voxel.
3. View Frequency-aware Masking Mechanism: Skips encoding for primitives with minimal impact on rendering quality, reducing redundancy and enhancing overall performance.","1. The ablation study of PCA guidance need to be included since there are already serval works that utilizes triplanes to compress the 3DGS/Nerf.
2. The efficiency of the model. The model seems a bit complicated. The coding time greatly increased due to the proposed context model. Besides, how about the training time?
3. Missing quantitative comparison with ContextGS, which is an important baseline method which also uses context model. For many benchmarks, the improvements appear somewhat marginal.
4. Figure 2 may not be entirely accurate; to my knowledge, CompGS does not utilize a context model.",Please see the weakness part.
m3KuuE2ozw,m3KuuE2ozw,CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression,Accept (Poster),Zt3A1VpWTr,ICLR.cc/2025/Conference/Submission6988/Reviewer_VHE1,"In this paper, the authors present a context-adaptive and rate-distortion-optimized 3DGS coding approach named CAT-3DGS. The approach introduces several modules for efficient attribute 3DGS compression, including a triplane-based hyperprior, spatial autoregressive models (SARM), channel-wise autoregressive models (CARM) and view frequency-aware masking. These modules are jointly trained in a rate-distortion-optimized framework. Experimental results demonstrate that CAT-3DGS achieves rate reductions of 78× and 26× compared to 3DGS and Scaffold-GS, respectively, while surpassing existing 3DGS compression schemes in rate-distortion performance.","1.The coding architecture is well-structured and theoretically solid, with four distinct modules specifically designed to address various redundancies.

2.The proposed method achieves exceptional rate-distortion performance, surpassing other 3DGS compression techniques.","1.The effectiveness of view frequency-aware masking is somewhat ambiguous. In Sec. 5.3, the authors report a 16% BD-rate reduction due to view frequency-aware masking. However, it’s unclear whether the anchor configuration (labeled as “w/o VFM” in Fig. 10) excludes masking altogether, or retains the masking operation but removes the weights p_{n,k} only.

2.Spatial redundancies within attributes are not fully exploited. The proposed SARM only addresses inter-redundancies within triplanes (hyperpriors) rather than anchors. Given the fact that spatial redundancies exist between anchors [1], this aspect is not leveraged in the current framework.

[1] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C Kot, and Bihan Wen. Contextgs: Compact 3d gaussian splatting with anchor level context model. arXiv preprint arXiv:2405.20721, 2024b.",Please see weakness
KXiQI6ggFc,KXiQI6ggFc,A Pattern Language for Machine Learning Tasks,Reject,4vSHOCPzXQ,ICLR.cc/2025/Conference/Submission6952/Reviewer_2Po6,"This work introduces a diagram-based language for creating and manipulating ML models with the notion of the task as the fundamental unit of computation. Several types of common models are given in terms of this framework such as autoencoders, classifiers, and in general, manipulators. Experiments are performed on manipulation (attribute selection and editing) for MNIST, a sprites-like dataset of shapes, and CelabA. it seems that the proposed method effectively can perform disentanglement and attribute editing to some degree.

----------------------------------------------------------------------------
I believe the authors provided a good faith effort to address concerns raised here. Therefore I will update my score(s) accordingly.","Originality: I think there is novelty in the category-theoretic foundations of this framework. I haven't seen this diagram notation before. 

Clarity: I think the writing was easy to follow, my concerns below notwithstanding.  

Significance: its clearly very early work, possibly a more developed version of this work could be impactful.","I found this paper extremely difficult to parse for content. It seems like there is something here, but it is hardly even hinted at in the main paper. I did not thoroughly check the proofs in the appendix so possibly I missed some important details there. But in general I fail to see what is actually being proposed here. My concerns are as follows:

* Over half of the main paper is spent restating some form of empirical risk minimization, and reintroducing commonly used notation and modeling paradigms like autoencoders and classifiers. In this exposition I didn't find anything new except different jargon e.g. the operands of the divergence measure called sys and spec. 

* I'm unsure of what was even done for the experiments. Was something implemented? Explaining how this framework gets turned into an algorithm or implemented would be useful. Right now all we have is a small diagram and then results which is impossible to interpret. 

* Much of the nomenclature used here like get, putput, fake, fool, undoability is confusing at best. I don't see how this particular exposition is useful or gets the reader any closer to understanding the framework. 

* I found 3.2 to be a very obscure experiment and dataset. Can we do anything else but attribute editing (manipulation)?","* In what sense are the given CycleGAN generators optimal? Is this optimality with respect to style transfer in general or just CycleGAN?

* What role do these diagrams play? I don't find them in any way intuitive if they are purely illustrative. 

* What is Undoability? I can't parse the diagram well enough to be sure. 

* How does this framework generate new or better components / tasks; are there any examples of improvements over the standard approach?

* Why is attribute editing chosen as the only task for this work? The first sections of the paper belabor the point that all kinds of modeling decisions can be expressed in this framework. So why don't we have autoencoders and classifier examples?"
KXiQI6ggFc,KXiQI6ggFc,A Pattern Language for Machine Learning Tasks,Reject,NMIwVjrOuq,ICLR.cc/2025/Conference/Submission6952/Reviewer_mqHa,"The article describes a formal framework for defining objective functions: these are modeled as a weighted sum of several statistical divergence functions (e.g., log-likelihood, cross-entropy, etc.). Specifically, a diagrammatic language expressing equivalence between diagrams is used to model *atomic tasks*. In contrast, their linear combination (using user-provided scaling coefficients) is referred to as a *compound task* (or objective function).

To showcase the practical advantages of this formal language, the authors propose a style-transfer objective function obtained by borrowing concepts from the Bidirectional Transformation field.","The language provided could be used to succinctly express complex loss function, using a relatively intuitive and succinct grammar. The utilization of a standard notation for referring to minimization objectives could prove useful, as similar concepts have been adopted in computer science.
Furthermore, the formal nature of the grammar lends itself to using formal verification techniques to look for properties of interest in the objective function. How would be done in practice however I am still not sure.","- The article is sometimes difficult to understand and unpolished; this is particularly true for sections 1 and 2. They are difficult to read and the vocabulary is not always defined (e.g. what is a datatype, what does ∶⇒ mean? ). As it stands, the paper can be confusing to read, this might be due to unpolished writing and a foreign vocabulary from the one typically used in the ML community.

- The proposed language could be more intuitive, at least from a ML prospective; this may hinder its usage in the broader ML community.

- I am unsure if the benefits of this pattern language are a strong enough contribution for a publication at this venue. In general, the topic tackled by this work seems to not be a good fit for ICLR. A computer science conference or journal might be more pertinent.

- While I understand the necessity to abstract away the choice of architecture, training schedule, and similar hyper-parameters, more often than not they are crucial for the successful estimation of a ML model. The lack of any type of modeling for these components makes me doubtful of the actual practicality of this pattern language.","1) How would the proposed language formalize size-varying data: For example, how would you represent the autoregressive cross-entropy loss commonly adopted by large language models?

2) What were actually the objective functions used during training for the various experiments? It would be nice to have a mapping from the diagrams shown in Task 3.1 to a conventional mathematically expressed objective function.

3) How were the task weights reported in Appendix D estimated? Were they chosen arbitrarily or a hyperparameter search was performed?  

4) What does the text above the ∶⇒ symbol mean? Does it indicate the parameters optimized by the loss function?

5) What does the ∶⇒ symbol formally mean? Is there a definition in the appendix that I have missed? Is it a bidirectional relation?"
KXiQI6ggFc,KXiQI6ggFc,A Pattern Language for Machine Learning Tasks,Reject,8e4GuN9RU7,ICLR.cc/2025/Conference/Submission6952/Reviewer_zN4b,"This work takes an idealized view of model optimization to formalize ""task"" in terms of a diagram that captures the behavior objective of the model being trained. This formalism is intended to provide better understanding of tasks as well as their relationships to other tasks. Using this formalism, they provide diagrams for several well known algorithms (such as classifiers, gans, and auto encoders), as well as formulate a relationship between auto encoding and energy minimization. Using their specification framework (i.e., ""language""), they construct a possibly novel task called ""manipulation"" and demonstrate its efficacy on toy tasks.","In general, the work is well written and easy to understand. The motivation of introducing the formal language for describing tasks makes sense and the applications are well delivered in a number of common or well-known objectives. Generally this sort of formalism could make it easier to relate different algorithms w.r.t. their target behavior and system specification. The novel training paradigm I believe is novel (manipulation), which is convincingly yielded from the formalism and its efficacy is well established through experiments.","Most of the issues are on clarification, for which I have added questions below. 

Some of the symbols could be better presented, as I continuously had to refresh myself on what the numerous symbols meant while building an understanding of the formalism. Perhaps a table or some other sort of summary that helps the reader reference while looking through diagrams.

One major issue I have is it's unclear how the formalism specifically relates different algorithms w.r.t. the target behavior of the model. Is there a set of rules that allow the user to transform and reduce to a ""unique"" diagram that represents a group of tasks with similar target behaviors? Is uniqueness part of what this formalism brings (which makes finding relations easier), or can be never guarantee a unique representation of the task (e.g., there are equivalences which are not deterministically reducible to the same representation).

The end complaint here is for me the clearest demonstration of usefulness would be more concrete examples on how to relate / compare algorithms using operations on the language. Is this something the language provides? If not, are there reasons why it can at least help relate algorithms?","1) Could you clarify what the role of the compound function, alpha, is? It appears to have something to do with the gradients of the parameters, but either way this should be made explicit or given an example in the work.
2) Could you clarify what the domain and codomain are? From what I can tell, these are the inputs and outputs of the diagrams (left to right), but this could be clearer, either way.
3) is ""system"" and ""specification"" synonymous with ""(trained) behavior"" and ""target"" (behavior)?
4) In Pattern 2.8 should there be a ""y"" in the domain of the specification?
5) why is ""get"" above the arrow for task 3.2 PutPut?
6) Is there a way through schematic manipulation to show equivalence between tasks? Eg between manipulation and strong manipulation it's implied they do similar things, but their diagrams are quite different. How would a practitioner understand that they are doing similar things? (perhaps some of this is in B2,3, but I didn't get a chance to look carefully at the Appendix)"
KXiQI6ggFc,KXiQI6ggFc,A Pattern Language for Machine Learning Tasks,Reject,L51BAHw9Vn,ICLR.cc/2025/Conference/Submission6952/Reviewer_GYtH,"This paper proposes a ""pattern language"", which treats machine learning tasks as constraints that specify desired equivalences.
Some commonly used tasks (e.g. classification, autoencoding, GAN) are called ""patterns"", borrowing terminology from software engineering.

The pattern language abstracts away implementation details (e.g. architecture and training), by treating neural networks are universal approximators that can perfectly solve the task.

The paper uses a task called ""manipulation"" as a proof-of-concept. The paper shows results on manipulating attributes, such as interpolating between shapes and colors of synthetic images, and smile/no-smile on CelebA.","This proposed pattern language with string diagrams provides a type of high-level abstraction of the machine learning pipeline, which can be helpful for conceptual understanding and can help provide intuition for someone who is entering the field.","Although providing a clear mental picture, the benefit of this framework is not clear to me.
- I'm not sure how the string diagrams can help advance theoretical/mathematical understanding; it would be helpful if the paper could provide some concrete examples.
- I don't see how the proposed pattern language improves empirical results either. For example, my understanding is that composing tasks is equivalent to adding regularization terms, which has been commonly used in practice.

I'm also concerned about the audience. I feel that the intended audience of the paper is not machine learning researchers/practitioners, but rather people with a software engineering or programming language background.
- This is partially reflected in the choices of terminology and notation. For example, this paper uses $\\mathbb{P}$ to denote datatypes (e.g. of trainable parameters), which is an uncommon choice since $\\mathbb{P}$ is usually used for probability-related quantities. Please see the clarification questions below for more examples. The concepts such as ""symmetric lenses"" are unfamiliar to most ML audience and would require more explanation.","Regarding the significance of the results, could you explain how the proposed pattern language can help with theoretical understanding or empirical advances?
- For example, it's unclear to me how ""(composite) tasks"" differ from multitask learning or adding regularization terms.

Some clarification questions:
- For specialised tasks, ""structural inductive biases"" refer to parameterization?
- line 188: ""energy minimisation architecture"": is ""architecture"" the same as  ""pattern"", i.e. a particular composition of tasks?
- Prop 3.4: the meaning of ""balanced"" in ""balanced attribute"" has not been explained (until the appendix)."
dRXxFEY8ZE,dRXxFEY8ZE,BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics,Accept (Spotlight),lx94uvhC03,ICLR.cc/2025/Conference/Submission6728/Reviewer_wpw3,"This is an excellent,  comprehensive and carefully curated dataset of bird sounds. The paper also provides an excellent review of the existing literature, and provides baseline code showing use of the data. The benchmark testing tasks are a bit simple, but demonstrate the breadth of the dataset.","Careful curation of the dataset - types of collections, varying types of birdcalls from same species; includes both soundscapes, and individual recordings; comparisons with other datasets.
Inclusion of benchmark code for future researchers to use as a baseline.
Permissive licensing
A large scale project",too many uncommon acronyms which require a reader to keep going back and forth - such shortening of the length was unnecessary. The acronyms are used in the figures as well. Figures and captions are supposed to stand on their own.,Please rewrite with clarity and minimizing acronyms.
dRXxFEY8ZE,dRXxFEY8ZE,BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics,Accept (Spotlight),HFlwhA702T,ICLR.cc/2025/Conference/Submission6728/Reviewer_uitS,"This paper presents BirdSet, a new large-scale benchmark dataset specifically for multi-label audio classification within avian bioacoustics. It significantly extends the scope of existing audio datasets by including approximately 10,000 classes, covering over 6,800 hours of training data, and incorporating 400 hours across eight distinct test datasets. BirdSet addresses several real-world machine learning challenges, such as covariate shift, label noise, and task shift, providing a unique resource for evaluating model robustness in audio classification under diverse conditions. The benchmark includes evaluations of six prominent deep learning models across three different training approaches: training on the full BirdSet, training on a subset containing only classes relevant to the downstream tasks, and training on a small subset for each downstream dataset individually.

Additionally, the authors facilitate accessibility by hosting BirdSet on Hugging Face, where they provide Python code to load the data. They also offer scripts for reproducing the experiments in the paper​.","1. Originality

BirdSet represents a novel contribution to multi-label audio classification. With close to 10,000 classes BirdSet provides a benchmark to develop scalable methods capable of handling extreme class diversity with large imbalance. BirdSet also addresses critical machine learning challenges such as covariate shift, where the testing distribution diverges from the training distribution reflecting real-world environmental shifts in field data.


2. Clarity

The paper is clear and well-organized, with a logical presentation of BirdSet’s structure, design choices, and challenges. The descriptions of covariate and domain shifts, as well as the evaluation protocols and training setups, are concise and well-articulated.


3. Significance

BirdSet can enable the development of new self-supervised learning, active learning, and few-shot learning approaches that are resilient to covariate and domain shifts—capabilities that are increasingly relevant for practical deployment of models in real-world applications. Moreover, its emphasis on multi-label classification reflects real-world scenarios in bioacoustics and similar domains, driving advancements that can translate to other fields beyond audio.


In summary, BirdSet is highly original, well-curated, and impactful dataset that serves as both a resource and a benchmark. It encourages tackling challenges related to robustness to distribution shifts, and multi-label classification, with methods that can be developed that extend across deep learning domains more broadly.",I didn’t find any weaknesses in this paper.,"I see significant potential in BirdSet, and I wonder if it would be feasible to define few-shot tasks within the downstream tasks to address few-shot multi-label classification?"
dRXxFEY8ZE,dRXxFEY8ZE,BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics,Accept (Spotlight),uOzvYCEzB1,ICLR.cc/2025/Conference/Submission6728/Reviewer_5AN6,"The paper proposes BirdSet, which is a dataset, or more specifically, a collection of 11 different sub-datasets. BirdSet attempts to unify 
avian bioacoustic evaluation on focal and soundscape recordings under one roof, providing a large, unified, accessible test bed for testing audio classification approaches.","1. Well written and easy to read.
2. Thoroughly explains the challenges experienced not only in avian bioacoustics, such as covariate shift and mismatch in focal and soundscape recordings, but also in curating, and developing a dataset of such a size and scale. 
3. The pain points addressed in the paper are very real: poor availability and accessibility of AudioSet, lack of a unified benchmark suite for evaluating segment and event-based bioacoustics tasks, and mismatch between train and test time for avian bioacoustics, are all pressing challenges. 
4. A good variety of baseline models were evaluated.","To me, the paper, in several places, tries to pose BirdSet as a replacement for AudioSet and that it should be the exemplary benchmark for evaluating audio classification models, with statements such as ""Avian bioacoustics exemplifies challenges in audio classification..."", and how curated datasets like AudioSet and ESC-50 do not represent real-world complexities. Pain points mentioned w.r.t. AudioSet are all very real, but AudioSet is a much broader dataset than BirdSet. Several people, in industry and academia have found AudioSet useful: a blanket statement saying AudioSet is not useful in real-world scenarios is plain wrong. 
Some statements straight up downplay AudioSet: for instance, line 37-38: where AudioSet poses ""... concerns regarding transferability to real-world environmental domains"". Whereas the paper cited simply shows that AudioSet pretrained models perform ""well enough"": of course models trained on in-domain bioacoustics data will fare better for bioacoustic evaluation!

Also, as per Table 3, AST and EAT models, which are pretrained on AudioSet, seem to perform quite well in the DT setting (cross-domain transfer setting) versus their LT and MT counterparts. For EAT, DT performance matches MT and outperforms LT scenarios. 

BirdSet will be a tremendous contribution to the fields of audio classification and avian bioacoustics, I have no doubt, but I think the current language of the paper comes off as ""posturing"" a tad bit too much.","In the context of how the paper is currently phrased, more direct comparisons, for instance, linear evaluation on cross-domain data between models trained on BirdSet and AudioSet on a variety of downstream tasks, spanning bioacoustics and other audio domains would be needed."
dRXxFEY8ZE,dRXxFEY8ZE,BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics,Accept (Spotlight),No3uxqTJfu,ICLR.cc/2025/Conference/Submission6728/Reviewer_qFwz,"The paper introduces BirdSet, a large-scale audio classification dataset for avian bioacoustics, with around 520,000 recordings from nearly 10,000 bird species. It includes over 6,800 hours of training data and 400 hours of evaluation data from diverse regions. The dataset supports tasks such as multi-label classification and self-supervised learning, with standardized training and evaluation protocols. A comprehensive literature review identifies key challenges in bioacoustics and provides research guidelines. The paper benchmarks multiple deep learning models and offers a codebase for reproducibility.","1. The introduction of *BirdSet* fills a notable gap in audio classification by providing a large-scale, domain-specific dataset for avian bioacoustics.
2. The paper is well-structured and clearly articulates the challenges in avian bioacoustics and audio classification more broadly. 
3. The paper provides a thorough empirical evaluation using six well-known deep learning models, covering various training scenarios, including large-scale training and fine-tuning.","1. The literature review in Section 2 is quite extensive, occupying a significant portion of the main paper. While the analysis is comprehensive, it might be more beneficial to allocate more space to the dataset description and details, rather than using five pages for the related work.
2. The results focus mainly on the multi-label classification benchmark, with limited exploration of other use cases. Given the dataset’s availability of precise event timestamps, it would be valuable to include benchmarks for sound event detection. Additionally, providing results for other mentioned use cases would strengthen the paper.",Please refer to the weaknesses above for the questions.
cjpTu0Op5t,cjpTu0Op5t,VISCON: Identifying and Benchmarking Vision Hallucination for Large Vision-Language Model,Reject,Cg89ofyAkZ,ICLR.cc/2025/Conference/Submission6675/Reviewer_8UyP,"The paper addresses the issue of visual hallucinations in LVLMs by proposing a new evaluation dataset. This dataset spans various image styles and task types. Unlike traditional VQA formats that employ discriminative assessments, this work allows models to generate open-ended responses. To directly evaluate the quality of these generative outputs, the authors propose two new evaluation pipelines and metrics.","1. It explores the impact of different image styles on visual hallucinations.
2. The approach allows models to produce open-ended responses instead of simple yes/no answers. To enable evaluation within these generative outputs, two specialized pipelines and corresponding metrics have been designed.
3. The issue of visual hallucinations is a significant challenge for LVLMs, and research that reflects this problem from multiple perspectives and dimensions is valuable.","1. The motivation for creating a hallucination dataset needs clarification. This dataset does not specifically emphasize hallucination evaluation. While distinguishing between visual hallucinations and errors can be challenging, the paper should at least specify the targeted sources of hallucination rather than broadly adding data and task types. It’s important to clarify why the community needs this dataset.

2. A new hallucination dataset should provide novel insights. The lower performance on attributes and relationships compared to objects is expected and not unique to this dataset. More interesting conclusions, like the impact of domain shift and response length, are only briefly discussed. A deeper analysis of these factors is needed for a dataset paper.

3. It’s also worth noting that if the performance drop is simply attributed to insufficient training (L432-L434), it raises questions about the dataset’s significance. In fact, based on Table 2, the performance differences across image styles seem minimal, suggesting that adding relevant training data could easily address this issue.","1. The link between image style and hallucinations needs further exploration. While a key contribution of this paper is testing hallucination across varied image styles, merely noting that style transformations (e.g., line drawings) cause performance drops is insufficient. Such drops are expected, given the limited training data for alternative styles, which seems more about training gaps than hallucinations. By contrast, POPE’s setup is more convincing, as it attributes hallucinations to textual context coexistence—a persistent issue regardless of training data expansion.

2. The paper lacks analysis on the sources of hallucinations. While POPE links hallucinations to high term co-occurrence and designs its pipeline around this, this paper's dataset mainly supplements existing data with new task types and styles. If the dataset only broadens existing types without addressing specific hallucination issues—especially if more training data could improve performance—its contribution may be limited. 

3. The comparison with related work is insufficient. Some existing work, such as AMBER[1], PhD[2], FaithScore[3], has already expanded hallucination task types and evaluation formats, including attributes, relationships, and open-ended assessments. What is the unique contribution of your work compared to these existing studies?

4. The description of the two metrics designing and pipeline diagram is unclear. I recommend clarifying why both two metrics are needed and how they complement each other. Given the use of LLMs in the pipeline, why not directly use LLMs for end-to-end judgment?

The paper also lacks a clear presentation of the data format—images, prompts, and labels are hard to understand from text alone. Figure 1 devotes much space to visual hallucinations, which is widely understood, and may be unnecessary. Figures 2 and 3 are also hard to follow. For instance, in Figure 2(a), how is the sunflower image derived from transforming the motorcycle’s style? In L262, what is the meaning of the dissimilarity matrix? What is your specific computing method for d_{EDIT}?

Ref:
[1] An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation
[2] PhD: A Prompted Visual Hallucination Evaluation Dataset
[3] FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models"
cjpTu0Op5t,cjpTu0Op5t,VISCON: Identifying and Benchmarking Vision Hallucination for Large Vision-Language Model,Reject,9KkLAC5bVE,ICLR.cc/2025/Conference/Submission6675/Reviewer_QEkM,"They introduce VISCON, a new benchmark framework aimed at evaluating vision hallucinations in Large Vision-Language Models (LVLMs) like GPT-4V and LLaVA. VISCON alleviates limitations of existing metrics by incorporating a diverse dataset with comprehensive visual concept annotations (objects, attributes, relationships) and includes two evaluation pipelines: an Earth Mover's Distance (EMD)-based approach for robust similarity assessment and an ""Evaluate-By-Edit"" pipeline for refining interpretability. Applying VISCON to six leading LVLMs, the study reveals insights into hallucination trends related to image domain shifts and response length, showing VISCON’s enhanced alignment with human judgments over existing methods.","1. VISCON includes a wide range of images across various styles and domains, enhancing the robustness and applicability of the evaluations.
2. Evaluating hallucination from multiple aspects of visual concepts—objects, attributes, and relationships—for a thorough assessment.
3. The paper introduces two metrics (EMD-based and “Evaluate-By-Edit” pipelines) for assessing hallucinations.","1. VISCON seems to directly use annotated scene graphs from VisualGenome and PROCTHOR. However, when the original images are stylized into other styles (e.g., line), the information may also change (e.g., the color of objects), which means the annotations should be adjusted accordingly. It should be clearly stated how the authors handled these changes or if they simply used the same annotations as in the original images.
2. In Table 2, there is too little differentiation in performance between models in terms of Earth Mover’s Distance (EMD). Some models (like GPT-4V) even outperform humans in the real-world original column of Table 2. However, GPT-4V still has many hallucination issues [1], which suggests this metric may not adequately capture the true performance gap between different models.
3. In lines 358-360 of the paper, the authors mention that there are some inconclusive queries in edit distances, though they believe these cases are rare and that edit distance still provides a reliable lower bound for estimating hallucination. This conclusion would benefit from further validation through qualitative or quantitative results to demonstrate the robustness of their metric.


[1] HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models. CVPR 2024",Please kindly refer to weaknesses.
cjpTu0Op5t,cjpTu0Op5t,VISCON: Identifying and Benchmarking Vision Hallucination for Large Vision-Language Model,Reject,7Duku3WBRA,ICLR.cc/2025/Conference/Submission6675/Reviewer_erAn,"VISCON is a benchmark framework designed to evaluate vision hallucinations in Large Vision-Language Models (LVLMs) by assessing their consistency in generating text aligned with images. It includes a diverse dataset and two innovative pipelines: an Earth Mover’s Distance (EMD)-based method for distributional similarity and an ""Evaluate-By-Edit"" approach to gauge response alignment with annotated visual concepts. Extensive experiments on six LVLMs show that VISCON offers insights into hallucination factors, such as domain shifts and visual complexity, while providing results that better align with human evaluation than previous metrics.","1. **Comprehensive Evaluation Framework**: VISCON introduces a structured evaluation framework that includes diverse image styles and evaluates a broad range of visual concepts (objects, attributes, and relationships), enhancing its utility in analyzing vision hallucinations comprehensively.
2. **Innovative Metrics**: The Earth Mover's Distance-based pipeline and ""Evaluate-By-Edit"" method provide novel approaches to measure concept consistency, making VISCON more robust to vocabulary shifts and complex visual relationships, which are limitations in previous hallucination metrics.
3. **Alignment with Human Evaluation**: Extensive experiments reveal that VISCON aligns more closely with human preferences compared to established metrics, demonstrating its relevance and potential for real-world application.","1. **Limited Benchmark Validation**: The effectiveness of VISCON’s metrics has not been validated on a broader set of benchmarks, limiting confidence in its general applicability across various datasets.
2. **Lack of Clarity in Pre-defined Rules**: The framework lacks clarity around the pre-defined rules and dataset selection process, particularly in ensuring attribute consistency after stylization, which could impact the reliability of results.
3. **Insufficient Benchmark Comparison**: While VISCON emphasizes dense annotation, its benchmark is relatively small and its validation relies primarily on comparisons with POPE and MSCOCO, rather than a broader set of dense annotation benchmarks like Reefknot and it previous work, limiting the robustness of its claims.","**Metric Validation**: Have VISCON’s metrics, Evaluate-By-EMD and Evaluate-By-Edit been tested on other dense annotation benchmarks?

**Benchmark Consistency**: What criteria were used to ensure the reliability and consistency of annotations across diverse visual domains? For example, how does VISCON handle potential changes in attributes after stylization, and could clarifying these processes enhance the benchmark’s reliability?

**Insufficient Benchmark Comparison and Size**: VISCON’s validation relies primarily on comparisons with POPE and MSCOCO, and its benchmark dataset is relatively small. Would expanding VISCON's benchmark and comparing it with a broader set of dense annotation benchmarks like Reefknot or previous related datasets enhance the robustness?"
cjpTu0Op5t,cjpTu0Op5t,VISCON: Identifying and Benchmarking Vision Hallucination for Large Vision-Language Model,Reject,uHwdyCuE2s,ICLR.cc/2025/Conference/Submission6675/Reviewer_58J2,"This paper focuses on the evaluation of hallucination in large vision-language models. It proposes an evaluation benchmark and an automatic pipeline to conduct quantitative evaluation. The benchmark comprises of real world images and 3D rendered images, which are further augmented by image stylization. The benchmark includes two modes: an Earth Mover’s Distance and an Edit Distance. The evaluation reveals some interesting phenomenon in LVLMs.","1. The benchmark is curated and annotated comprehensively by considering various image domains, styles, and dense annotations (concepts).
2. The evaluation pipeline provides new perspectives to this venue.
3. The experiments and analysis are sufficient.","1. There are more related works that haven’t been discussed and compared, especially those that are also using Visual Genome and are also considering more concepts beyond object categories, e.g., Table 1 in [1].
2. Visual Genome is a widely used dataset to curate benchmark. In this work, the authors involve 3D rendered images into this benchmark. However, the rendered images are all constrained in indoor room scene. This would inevitably restrict the domain of the data, which actually somehow contradict with the main claim of the paper.
3. Since the visual concepts are come from the scene-graph annotation from Visual Genome, as stated in line 101, how do the authors ensure their generated annotation is more comprehensive than other works that also use visual genome scene-graph? Although there are some comparison for this aspect, it is not clear how does it achieved.
4. I wonder would the evaluation framework, especially the Edit Distance version, prefers short captions? For example, the model can be very lazy, simply describing the main objects to avoid making mistakes. Is the framework robust to such cases? I noticed that there are some discussions regarding caption lengths in Figure 5, but it appears to be hard to read without sufficient explanations. It's better to investigate this issue in detail.
5. The EMD is based on an embedding space of a text encoder. However, as discussed by previous works[1], some text encoders, e.g., CLIP text encoder, has significant bias to object categories, while lacking capabilities on telling attributes and relations. Besides, for most text encoders, distinguishing similar visual concepts with small difference is a very challenging task, while such kind of cases are very typical in visual hallucination. The authors should better cover more implementation details and investigations on this aspect, as it may fundamentally invalidate the assumption of EMD.
    - How does the text encoder perform on attributes and relations compared to objects?
    - How well does it distinguish between similar visual concepts?
    - What investigations have you done to ensure the validity of the EMD assumption given these potential limitations?
6. Similarly, in Edit Distance, how to ensure the refined captions are reliable?
7. In general, the method, including EMD and Edit Distance, seems to be proposed out of thin air without enough motivation. Could you give some motivation about the design? Why do you think it is good?

[1] Hallucination of Multimodal Large Language Models: A Survey. arXiv 2024.
[2] When and why vision-language models behave like bags-of-words, and what to do about it? ICLR 2023.",Refer to weaknesses.
cjpTu0Op5t,cjpTu0Op5t,VISCON: Identifying and Benchmarking Vision Hallucination for Large Vision-Language Model,Reject,L58Htq4eUs,ICLR.cc/2025/Conference/Submission6675/Reviewer_kDMt,"This paper introduces a novel framework, VISCON, which comprises a benchmark dataset and quantitative evaluation pipelines designed to detect hallucinations in the outputs of existing Large Vision-Language Models (LVLMs). Compared to previous benchmarks, such as POPE, VISCON encompasses a wider variety of image styles across multiple visual domains and a broader range of visual concepts. Additionally, it proposes two innovative evaluation pipelines, namely EMD and Evaluate-By-Edit. Extensive experiments validate the efficacy and robustness of VISCON in assessing the hallucination potential of LVLMs.","1. The motivation behind this study is clear, and the method for dataset collection and evaluation is intuitive. 
2. Compared to previous benchmarks, VISCON encompasses a broader range of visual concepts, such as object relations, enabling a more comprehensive evaluation. This is crucial, as prior works often generate numerous false negative samples.
3. The experiments are exhaustive and easy to follow, effectively verifying the robustness of this type of hallucination evaluation.
4. Overall, the paper is well-written and easy to read.","1. The evaluation pipeline is quite complex and tedious. For instance, in the case of EMD, the process begins with extracting visual concepts from the detailed caption using GPT-4. Following this, it is necessary to obtain the text embedding for each visual concept before the final evaluation. I am concerned that such a long may hinder its adoption within the community.
2. This type of evaluation can be easily manipulated. For example, if a model is trained to generate short captions, even when given the prompt, ""describe the image in detail,"" it may hallucinate less than a model that generates longer captions. Consequently, the performance of such a model might be overestimated.
3. I am unable to find detailed statistics about the evaluation dataset constructed by this paper. Additionally, there is a lack of information regarding the detailed procedure for constructing the evaluation dataset, including aspects like annotation and sanity checks.",See weakness
bb2Cm6Xn6d,bb2Cm6Xn6d,Intriguing Properties of Large Language and Vision Models,Reject,Hvk2TmJp3b,ICLR.cc/2025/Conference/Submission5588/Reviewer_LdVk,"This paper looks at how Large Language and Vision Models (LLVMs) like LLaVA handle different tasks. It finds that LLVMs don’t rely much on image order, sometimes solve math without exact details, and lose some visual skills when fine-tuned for language tasks. The study also notes that the first layers process visuals best. The authors suggest building tougher benchmarks to better test and improve these models.","This paper is useful because it digs into how large language and vision models (LLVMs) really work with visual information. It shows that LLVMs are flexible, able to handle scrambled image pieces and solve math problems without all the visual details. It also highlights that when LLVMs are tuned for complex reasoning, they lose some basic visual skills. These findings can help make LLVMs better by balancing complex reasoning with simpler perception tasks, potentially guiding the creation of new, stronger models.","The paper would benefit from more precise and carefully scoped conclusions. While the experiments provide interesting observations, the claims drawn from them are often overly broad. For example: 1) The claim about permutation invariance is based on VQA tasks, but this alone cannot support a general conclusion about LLVMs' visual processing capabilities.
2) The benchmarks used don't adequately test basic visual skills to support such sweeping statements about visual understanding

Specificity Needed:
Each conclusion needs clear boundaries about when it applies and when it doesn't.
The paper makes approximately 10 major claims, but lacks sufficient context about their limitations and specific application scenarios.
More precise scoping of these findings would make them more actionable for future research.
Currently, the broad generalizations make it difficult for other researchers to build upon these results effectively.

Minor points:
Figure 5's text is too small to read effectively, hampering understanding of key results.","Besides the weakness, to improve, I suggest the authors:

1. Clearly specify the conditions under which each conclusion holds
2. Acknowledge the limitations of their experimental setup
3. Provide more nuanced interpretations of their results
4. Consider additional experiments to support broader claims"
bb2Cm6Xn6d,bb2Cm6Xn6d,Intriguing Properties of Large Language and Vision Models,Reject,YTnFMGXilp,ICLR.cc/2025/Conference/Submission5588/Reviewer_v3XH,"The paper analyzes some properties of LLVMs with respect to permutation invariance, mathematical reasoning, robustness to perturbations, and other aspects. The problem tackled is interesting and relevant to the VLM community. The authors analyze a wide variety of VLMs properties by focusing on the LLAVA family and a diverse set of classification/instruction following benchmarks.","* The paper addresses an important and timely problem that is of interest to the VLM community.
* Some observations made by the authors are insightful and have the potential to contribute to the understanding of VLMs.
* The set of analysis is diverse and has the potential to inform the  development of a new set of architectures/benchmark.
* Detailed set of benchmark datasets in the Appendix.","* Many conclusions are not clearly backed up by evidence or rigorous analysis, which undermines their validity (I've given some examples in the questions section)
* Some observations appear to be cherry-picked (by dataset or sample?), which raises concerns about the representativeness and generalizability of the results. For instance, the authors want to evaluate the LLaVA family of models but some results are given on a single model and data sample while the conclusion is general (Figure 1 and Section 3.7 for instance, i've given more specific questions in the next section)
* The choice of datasets, metrics, and evaluation methods for different parts of the analysis is often unclear or unjustified, making it difficult to assess the generalizability of the findings.","* The article focus on the LLaVA family. Could the authors  explain their rationale for focusing on the LLaVA family and discuss potential limitations in generalizing these results to other LVLM architectures?
* In Section 3.3, how is the grouping done to quantify local information in patches? How are neighbours defined, and what is the impact of this definition on the results?
* The explanation of backpropagation and loss signal coming from the assistant in line 234 is unclear. Can the authors provide more details or clarify the average performance drop, which seems to be benchmark-dependent?
* In Table 2, what setting is used to measure the frequency of outputting 1 (synthetic or original)? Is there a difference between these cases? Can the authors also report the frequency of outputting ""no"" and precision/recall for yes/no questions?
* Evaluating the loss of  cross-modal alignment is interesting. However the evaluation method used in Table 3 is unclear and may not be fair compared to a contrastive approach. Could the authors give more details about their evaluation setup (prompt used?) as it is known that prompt optimization impacts even zero-shot CLIP models classification performance.  Can the authors consider alternative approaches, such as log-likelihood comparison of description prompts (e.g., ""Describe the image: This is an image of a [class label]"")? As it is done in captioning models evaluation for instance?
* How is the 1k subset obtained in Table 3?
* The section on shared world concepts is hard to follow. What metric is used to measure alignment, and why was the DOCCI dataset chosen? How is alignment measured with long captions given CLIP's 77-tokens context?
* Are the local scores in Figure 8 averaged over the whole dataset or just one example? Same question for Figure 1 ? 
* Can the authors comment on the choices of datasets for different parts of the analysis (e.g., MMVet for localization of information, MMStar for layers importance)? Are these results model-, dataset-, or sample-specific, and how do they impact the generalizability of the findings? I know this question is quite general but the justification is not clear to me and the drawn conclusions seem to be a general so I would except such visualizations to be performed on a subset of the different datasets.
* The conclusion drawn from Figure 9 lacks a clear explanation of how the importance score is obtained. Can the authors provide more details on the method used (e.g., noised input or weights, which proportion)?

* In general it would help if the authors could : 
1. Provide explicit justifications for their dataset choices in each analysis section
2. Discuss potential limitations in generalizing results from specific datasets to broader conclusions
3. Consider extending their analysis to multiple datasets where feasible, or explain why this might not be possible or necessary"
bb2Cm6Xn6d,bb2Cm6Xn6d,Intriguing Properties of Large Language and Vision Models,Reject,XhwfeXHv50,ICLR.cc/2025/Conference/Submission5588/Reviewer_ePc1,"This paper presents an empirical study of large vision language models with a focus on the llava model families using perception and reasoning tasks across various benchmarks. The authors revealed several findings of LVMs including image patch permutation invariance, overfitted cross-modal alignment, importance of lower block layers, etc..",The paper comprehensively evaluated several VLMs across various perception and reasoning benchmark to better understand their behavior and performance. These empirical findings could be valuable providing insights for model developments.,"1. While the paper shows some good empirical studies of the LVM performance, they are loosely connected to the suggested future directions. This makes the contribution of the paper less clear. For example, the authors suggested ""deeply consider innovative model architectures"" in Sec 4 for enhancing cross modal alignment, yet it's not clear how this related to the empirical findings discussed in the previous sections and what necessary enhancements are entailed from their analysis.

2. The authors should consider toning down the rhetoric of the study from general LVMs to llava model families. Reported results do not show whether these findings would generalize to other LVMs as they are not evaluated yet.

3. Some of the findings are more or less expected, e.g., some degree of robustness to occlusion due to the robustness from image encoder. The conclusion from these findings are not clear. I suggest the authors append more focused discussions on a few key findings and tie them to any future directions or proposed improvements.",Please see weaknesses above.
bb2Cm6Xn6d,bb2Cm6Xn6d,Intriguing Properties of Large Language and Vision Models,Reject,QxvpvhEuzP,ICLR.cc/2025/Conference/Submission5588/Reviewer_Tcxa,"This paper presents a detailed investigation of how vision is perceived in large language and vision models. Comprehensive experiments reveal the properties of LLVMs from multiple aspects including permutation invariance, robustness, math reasoning, alignment preserving and importance.",This paper elaborates on a critical problem in multimodal LLMs that how vision is perceived. It is a profound question which could benefit the future development of MLLMs.,The examples to be investigated in this paper are mainly from the LLaVA family whose vision representation is continuous. An analysis on discrete vision representation would be interesting.,Please see weakness.
My9MBsO41H,My9MBsO41H,Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching,Accept (Poster),wWV5i9CGOu,ICLR.cc/2025/Conference/Submission5295/Reviewer_5tK1,"This paper addresses a challenge in image-text matching referred to as clone negatives, which are negative image-text pairs that are semantically consistent with the positive pairs, leading to ambiguous and suboptimal matching results. To tackle this issue, the authors propose Adaptive Contrastive Learning (AdaCL), which employs two margin parameters with a modulating anchor to dynamically enhance the compactness between positives while mitigating the influence of clone negatives. The modulating anchor is selected based on the distribution of negative samples without explicit training, facilitating progressive tuning and improved in-batch supervision. Experiments on two benchmark datasets underscore the effectiveness of AdaCL.","1. The paper introduces a novel approach to address the issue of clone negatives in image-text matching, which is a pertinent problem in the domain.
2. The proposed Adaptive Contrastive Learning (AdaCL) framework dynamically adjusts the compactness between positives, showing potential for enhanced performance in image-text matching tasks.","1. The experimental results are not convincing. The main contribution of the paper is the improvement of Contrastive Learning, with CLIP being the most representative of this field. To prove the effectiveness of the paper, a comparison within the OpenCLIP framework using larger datasets (e.g., LAION) should have been conducted, rather than experiments on the relatively small-scale COCO dataset.
2. The proposed method appears to be somewhat complex, which goes against the inherently simple and scalable nature of contrastive learning. Additionally, the paper does not provide an analysis of the introduced hyperparameters or the potential increase in overall complexity.
3. The authors do not clearly explain why clone negatives pose a problem within the paradigm of contrastive learning. Clone negatives refer to texts that are aligned with the image but are not as accurate as the ground-truth text. Hence, the logits of clone negatives should naturally be lower than those of the ground-truth, which aligns with the basic optimization framework of contrastive learning. The authors should provide more empirical evidence or theoretical analysis demonstrating why standard contrastive learning fails to handle clone negatives adequately.","The method figure (Figure 2) is unclear, and neither the figure caption nor the text adequately explains what the ""reference clone negatives"" refer to."
My9MBsO41H,My9MBsO41H,Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching,Accept (Poster),WaWk6VSLdy,ICLR.cc/2025/Conference/Submission5295/Reviewer_PFrK,"This paper focused on solving the task of Image-Text Mathing (ITM) especially the key challenge of clone negatives: the captions that have consistent semantics with the GT sentences, but with less coarse-grained information. To solve this, the Adaptive Contrasvite Learning (AdaCL) method is proposed, which introduces supervision of the clone negatives, and can dynamically penalize the clone negatives. This will enlarge the distance between the positives and the clone negatives.","1. The authors focus on a critical issue of clone negatives in the field of Contrastive Learning under the background of Vision-Language Models and Image-Text Matching Tasks.   
2. Experiments are well performed, which show effectiveness of AdaCL.","1. More pre-trained models and more down-stream tasks could be test to further demonstrate the effectiveness of the proposed method.

2. Spelling error(s): 
p2 077 andand
p2 Fig 1(b) T2 -> T3
p1 034 action-level -> global level
p6 274 double curly braces “}” -> just one “}”, delete the last one

3. Sequential adverbs: 
p4 205 There are “Two conditions”. I can see the “First”, but where is the “Second”?
It should be before p4 214 ”(Second, )to achieve this”. 

4. Meaning of subscript: 
p4 208 What does the subscript “u” mean? What is its abbreviation? Is it “unique(==specific)”?
And what about p4 210 the “u-” in k∈M_u-? Is it “not unique”? I can’t sure. 

5. Inconsistencie(s): 
p4 Sec 3.2 one anchor + one pos + M neg ==> Why not all M neg? Because Sec 3.2 and Sec 3.3 are connected. 
p5 Sec 3.3 one anchor + one pos + (M-1) neg
p6 Algorithm 1 a mini-batch of N pos + M neg??? ==> Why not one pos + M neg?
  May be the authors want to say “a mini-batch of N, in which 1 pos and M neg”?
  So, N==1+M?

6. p5 254 what is the superscript of j in Eq. (8)? Is it the same as Eq. (7) or Eq. (3)?

7. p5 263 Why Eq. (9) is that? Can you give us a concrete deduction? And what about p(C=0|s)? Can you give a specific formulation?  

8. p10 509 How to correctly understand the heat map, since a sentence contain ALL WORDS, and every mentioned instance in the image SHOULD be highlited. So, what does these Attention Map mean indeed? What Information can I get from them? Or what should we expect from them?",Please refer to weakness above.
My9MBsO41H,My9MBsO41H,Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching,Accept (Poster),S0qB56LLPI,ICLR.cc/2025/Conference/Submission5295/Reviewer_vmnW,"This paper recognises the clone negative issue in image-text matching, where negative image-text pairs are semantically consistent with the positives. To address this issue, the authors modulate the softmax function of original contrastive losses with anchors that are selected from each in-batch similarity score based on Gaussian discriminant analysis. The proposed method, called Adaptive Contrastive Learning, demonstrates superiority in both supervised and weakly-supervised image-text matching.","1. The motivation of this work is clear. The issue of clone negatives is well illustrated and intriguing, which is common and significant in matching images and texts.

2. The proposed method is extensively verified under both supervised and weakly supervised settings, exhibiting superior performance under several benchmarks.

3. The authors provide comprehensive ablation studies and analyses, with both quantitative and qualitative verification.","1. The method is not applied to or compared with the recent CLIP-style models, which I believe would strengthen the significance and soundness of this work.","1. From what I understand, this method is matching images with texts. Then why is FasterRCNN, an object detection model, used in the experiments?"
My9MBsO41H,My9MBsO41H,Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching,Accept (Poster),2gGlzpvCfY,ICLR.cc/2025/Conference/Submission5295/Reviewer_8azX,"The authors of this paper propose a new method to improve cross-modal learning by adding a conditioning factor to the loss function to alleviate what the authors call ""clone negatives"". The proposed method is effective from the experimental results. However, the concept of clone negatives is more like the False-negative problem mentioned in some existing studies. For this reason, the concept of clone negatives still needs further discussion.","1. The author's idea of ​solving the false-negative problem is clear.
2. The experiments are rich and the proposed method is effective.","1. The authors proposed a new concept, namely clone negative examples. However, this is actually the false-negative problem, which is not a new topic. For example, there has been similar research in [1,2].
2. The authors should enrich the caption of Figure 2 to make the core idea clear and concise.
3. Line325: “Comparisons of Image-text matching” should be “Comparisons of image-text matching”
4. The 2024 baselines are missing.
5. Line 954,955: Missing citations.
6. There are usually three mainstream benchmarks for text-based person search tasks. I suggest the author add the results of RSTPReid. Also, does the authors‘ R@ refer to the Recall accuracy? However, in the text-based person search task, the Rank accuracy is usually used as the evaluation indicator.

Reference:

[1] Li H, Bin Y, Liao J, et al. Your negative may not be true negative: Boosting image-text matching with false negative elimination[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 924-934.

[2] Li Z, Guo C, Feng Z, et al. Integrating language guidance into image-text matching for correcting false negatives[J]. IEEE Transactions on Multimedia, 2023, 26: 103-116.",See weakness.
bqoHdVMIbt,bqoHdVMIbt,Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap,Accept (Poster),2vDfnbBZLq,ICLR.cc/2025/Conference/Submission5217/Reviewer_qfP3,"This paper introduces Multimodal Unsupervised Domain Generalization (MUDG), a novel framework that leverages large-scale unlabeled datasets (like LAION-2B) to improve model generalization on target domains. The paper makes three main contributions: (1) proposing paired k-means algorithm to improve cross-modal retrieval accuracy by updating centroids in query space rather than image space; (2) designing an unsupervised text augmentation scheme that adaptively selects appropriate descriptors for target labels; and (3) introducing a clustering-based sample selection strategy and diversity-preserving loss function to construct more representative training datasets. The method demonstrates significant improvements over existing zero-shot learning and source-free domain generalization baselines across 20 diverse datasets.","1. The paper proposes a novel MUDG framework that effectively leverages large-scale unlabeled datasets for domain generalization through cross-modal retrieval and adaptive text augmentation.
2. The paired k-means algorithm effectively addresses the modality gap in cross-modal retrieval, supported by rigorous theoretical analysis and empirical validation.
3. Extensive experiments across 20 diverse datasets demonstrate significant improvements over state-of-the-art baselines in zero-shot learning and domain generalization, showing strong practical value.","1. While the paper introduces MUDG as a new paradigm beyond UDG, it would benefit from a visual comparison/illustration showing the differences between DG, UDG, and MUDG. Moreover, the necessity and practical benefits of this new task definition are not sufficiently justified - authors should more explicitly demonstrate why existing frameworks like UDG are inadequate and how MUDG addresses real-world challenges.
2. In this paper, authors explore a way to retrieval task-agonistic source dataset to assist in model learning in the target domain. What is the intuition behind this operation? Is the assumption reasonable?
3. The writing of the Introduction Section is confusing: authors present a group of operations and six contributions in the section of introduction. However, what is the core challenge and the corresponding solution method proposed in this paper?","For detailed questions and suggestions, please refer to the Weaknesses section"
bqoHdVMIbt,bqoHdVMIbt,Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap,Accept (Poster),SsvvLM4DEI,ICLR.cc/2025/Conference/Submission5217/Reviewer_TM1x,"This paper presents a novel approach to multimodal unsupervised domain generalization (MUDA), where the source data is both unlabeled and task-agnostic. First, it theoretically examines the issues with existing cross-modal approximate nearest neighbor search methods. Next, it proposes a paired k-means algorithm to enhance neighbor search efficiency. Finally, we introduce a text augmentation scheme aimed at improving zero-shot accuracy.","1.The paper introduces the MUDG setting and presents a novel paired k-means algorithm to address the cross-modal nearest neighbor problem.

2.The paper provides a thorough theoretical analysis demonstrating the limitations of existing cross-modal nearest neighbor methods, supported by extensive experimental validation

3.Figures are effectively used to illustrate empirical observations, the improvements brought by paired k-means, and the visualization of Voronoi cells, enhancing the understanding of the proposed method.","1.The paper compares MUDG setting to zero-shot (ZS) and source-free domain generalization (SFDG) setting in Tables 1. ZS and SFDG impose more stringent conditions, while MUDG relies on task-agnostic source data, making it less practical in some contexts. Therefore, comparing MUDG methods with SFDG or ZS methods may not be entirely fair.

2.The paper lacks a comparison of the computational resources and time required by its proposed method in relation to other SFDG and ZS methods. Since this setting involves large-scale datasets during training, it may significantly increase the computational resources and time needed

3.In Table 2, the proposed method shows only a slight increase of 0.6% compared to the second-best method.

4.In the ablation experiments (Table 5), when adaptive augmentation is used alone, there is a drop of about 1%, with some datasets not even reaching that. This indicates that the paired k-means method, which is the main contribution of this work, is not effective.","In Table 3, there are no MUDG methods included. Could you combine the PromptStyler method with the proposed methods for a fair comparison?"
bqoHdVMIbt,bqoHdVMIbt,Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap,Accept (Poster),CaWstQAXIl,ICLR.cc/2025/Conference/Submission5217/Reviewer_gdYC,"This paper presents a novel approach to multimodal unsupervised domain generalization that leverages a large unlabeled source dataset for finetuning, without requiring a direct relationship with the target task. Key contributions include (1) a theoretically derived paired k-means algorithm that enhances nearest neighbor recall, (2) an adaptive text augmentation scheme to improve zero-shot accuracy, and (3) two additional components that boost target accuracy. Experimental results demonstrate consistent accuracy improvements across 20 diverse datasets.","This paper is well-motivated and appears to be reproducible based on the author's code. The author discusses the challenges of cross-modal retrieval from a theoretical perspective and proposes methods to address the issue of low recall. Additionally, the author proposes an adaptive text augmentation scheme to tackle unsupervised domain generalization in multimodal contexts. Comprehensive analyses, comparative experiments, and insightful visualizations highlight the appropriateness of the method's genuine effectiveness.","1. While the methodology appears to be fundamentally solid, my primary concern regarding this paper is the clarity of expression and how it affects overall readability. Several figures in the paper, such as Figure 1, present ambiguous meanings, and others, like Figures 4 and 5, lack sufficient quality. Additionally, there are issues with the details in the descriptions, including an overuse of abbreviations and numerous typos in the introduction. These factors contribute to a challenging reading experience for the audience.
2. I believe a more detailed introduction to the task setting and motivation is necessary. Although the author mentions that MUDG is more practical, it seems to be primarily a combination of text-image representation and UDG. Could the author elaborate on specific application scenarios?
3. In terms of retrieval, I’m curious why only the first-level k-means is utilized instead of employing a fine quantization scheme at the second level. What are the implications of this choice, and might it impact the recall rate?
4. In Table 2, the proposed method does not seem to achieve superior performance across many datasets. Given that MUDG is being addressed so specifically, what could explain the lack of a more significant effect? What might be the underlying reasons for this?
5. Lastly, regarding the key hyper-parameters listed in Table 9, how are these parameters determined? Are they sensitive to variations, and how do they influence the performance of the method? This aspect warrants further discussion.","See 'weakness' for details. A more thorough introduction to the motivation and clarification of the methods are needed. If the author can improve the presentation of the paper in the next version, I would be happy to raise my score!"
bqoHdVMIbt,bqoHdVMIbt,Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap,Accept (Poster),JaAM7i0TUk,ICLR.cc/2025/Conference/Submission5217/Reviewer_6mKA,"This paper studies multimodal unsupervised domain generalization (MUDG), where only task-agnostic source data and target label names are required. Accurate yet diverse retrieving image samples that match the specific text description for a certain class name becomes the crux. Towards the former, the authors propose paired k-means, maximizing the probability of a text query and its closest image sample belonging to the same Voronoi cell by updating the centroids to be in the query distribution. As for the latter, the authors develop a heuristic augmentation strategy. Experiments under various evaluation protocols demonstrate the effectiveness of the proposed method.","- This paper is well-written overall, and the motivation is quite reasonable.
- Each proposed component is supported by sufficient theoretical and empirical analysis.
- Experiments are sufficient.","- Table 1 should be placed before Figure 1 as Table 1 was mentioned before Figure 1.
- The bottom right subfigure of Figure 1 was an illustration. Could you provide a quantitative comparison between good augmentation and bad ones (such as using t-SNE visualization)?
- Ablations in Tables 4 and 5 can not reflect the effectiveness of each component, as the improvements are marginal. Could you report the results of statistical testing to see if the improvements are significant or not?","I have no further questions. Please refer to the ""Weaknesses"" section."
d5HUnyByAI,d5HUnyByAI,CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale,Accept (Poster),O3fcGMULxf,ICLR.cc/2025/Conference/Submission5177/Reviewer_Ujh5,"The authors present a paper on a CLIP-based approach combining images, DNA barcodes and textual taxonomic description for biological classification (specifically of insects) in an open-set setting. Experimental results show that the method is effective and outperforms dual-modality contrastive learning approaches, as well as other approaches from the literature on this specific task.","1) The paper is well-written and and organized. The proposed approach and the experiments are clearly described.

2) The proposed approach is effective in tackling the problem at hand, while being simpler than common alternatives in the field.","1) The paper presents no methodological novelty, and mostly applies existing techniques in a standard way to a particular use case.","1) In Tab. 1, what is the point of comparing non-aligned embeddings? It seems intuitive that there should be no correspondence in the learned representations.

2) The attention visualization is not discussed in detail. Also, as mentioned above, what information comes from checking classification before alignment? Wouldn’t correct predictions be random in that case?"
d5HUnyByAI,d5HUnyByAI,CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale,Accept (Poster),x5DuxqDmdn,ICLR.cc/2025/Conference/Submission5177/Reviewer_s416,"The approach proposes multimodal contrastive learning between image, DNA and text (taxonomic labels) as opposed to Image+TaxonomicLabel only approach followed by previous work (BioCLIP). The usage of DNA is claimed to be better because, (1) classifying unseen species would be difficult with a taxonomic-label-only model because the species name would not have been seen during training, (2) DNA is easier to obtain compared to taxonomic label which requires careful examination by human experts.","1.	Incorporation of DNA as a modality to align the image embedding against instead of text is well motivated.
2.	Extensive experiments and ablations are provided.",1.	The accuracy when doing image to DNA on unseen species is not quite significant although it is better than BioCLIP’s approach of doing image to text. This indicates the image encoder is still not strong enough to generate a good DNA aligned embedding just from the image. Perhaps this can improve with more data.,"Some suggestions and questions,

1.	Comparison with BioCLIP at different taxa levels: Going by the works claim, incorporation of DNA embeddings could help with classifying unseen species up to species level, but I’m guessing at higher taxonomic levels the BioCLIP performance should be comparable to CLIBD. It would be interesting to see a comparison.
2.	From Table 1, Image-to-DNA performance on seen species reduces going from (I+D) to (I+D+T), why is this happening? I would have expected the performance to improve.
3.	I’m also curios to know why BIOSCAN-5M was not used, considering that foundational models such as these can greatly benefit from more data.
4.	I personally felt Figure 7 was more informative to understand the data partitioning than Figure 2 (Just to consider in the future revisions)."
d5HUnyByAI,d5HUnyByAI,CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale,Accept (Poster),vTMo4toETV,ICLR.cc/2025/Conference/Submission5177/Reviewer_CfWv,"This paper proposes a CLIP-based method to jointly embed images, DNA barcodes, and taxonomic strings for different insect species. The method is evaluated on cross-modal matching and species classification, with qualitative and quantitative results provided. The paper compares against one external method (BIOCLIP) in one set of experiments and one external method (BZSL) in a second set of experiments. The key claim of the paper is that jointly learning from the modalities of images, DNA, and taxonomic information leads to stronger representations for downstream use.

Note: Score increased after rebuttal.","* The idea of jointly embedding DNA barcodes with images and taxonomic information is interesting.
* The experiments in the paper are extensive - there is a lot of technical content, and it's clear that a lot of effort went in to this work. There are many quantitative results, in addition to interesting qualitative results (e.g. Fig. 3, Fig. 5).
* The paper is very well-written. 
* The hyperparameters and training procedures are clearly spelled out.","My major issue with the paper is missing baselines:
* The paper compares their multimodal representation learning approach against the unimodal pretrained models they start with. They show that their method is better, and conclude that multimodality is important. However, this claim is not justified - couldn't the benefit be from the additional training each modality received? It seems to me that the fair comparison would be to take the unimodal models and run unimodal CLIP-style training (with a similar computation / # steps budget) for each. If the multimodal model beats these CLIP-fine-tuned unimodal models, then that provides stronger evidence of the benefit of multimodal learning. 
* For the image encoder, wouldn't a model pretrained on BIOSCAN-1M be a more appropriate starting point / baseline than an ImageNet-pretrained model? 

The paper should address a few items that were not discussed:
* Are there confounders we need to worry about in this data, e.g. the facility the data was collected by? 
* The DNA to DNA matching results are very high, but shouldn't we expect this? Would simple homology methods based on string matching do a very good job at this task? What is the benefit of using deep embeddings to solve this problem? 
* Doesn't adding a modality increase the number of steps of training the model receives? Couldn't this be partly responsible for the differences between 1, 2, and 3 modalities in Table 1? 
* Image-DNA and Image-text matching don't seem very good based on Fig. 4, with the average being pulled up by a few good cases. What are those cases, and why are they different? Is there any insight to be gained there? 

A few claims made in the paper were not clear to me:
* The paper claims that ""BIOCLIP... requires taxonomic labels to be available in order to obtain text descriptions. These labels can be expensive and time-consuming to obtain"" and that ""DNA barcodes can be obtained at scale more readily than taxonomic labels"". These claims are not intuitive to me. How much does DNA barcoding cost per individual, compared to the cost of having an expert inspect an image to identify the species? 
* The training data for BarcodeBERT is claimed to be ""different from, but highly similar to"" the data used in this paper  - can you expand on what this means, and the implications for the results presented?","Please see weaknesses for primary questions. 

Minor comments / questions (no need to respond):
* This work focuses on the cases where aligned data is available: images and DNA barcodes from the same individuals. It might be useful to extend the method to also take advantage of abundant unpaired data: images and DNA barcodes that are not paired. 
* Figure 2 could be clearer. Why are there different shades of blue and orange? Why don't the numbers in the boxes sum up to 36729? Generally, this figure did not aid my understanding (though many of my confusions were clarified in later text and figures). 
* It would be nice to have a chance-level baseline in Table 1. 
* Down the road, it might be interesting to try to integrate additional modalities, e.g. geospatial location (https://arxiv.org/abs/1906.05272)."
d5HUnyByAI,d5HUnyByAI,CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale,Accept (Poster),aVDeW6Tm7W,ICLR.cc/2025/Conference/Submission5177/Reviewer_JVaW,"The paper proposes CLIPBD, a tri-modal embedding space consisting of image, text and DNA of insect specimens. CLIPBD is trained using a three-way contrastive learning objective between image, text and DNA on the BIOSCAN-1M dataset. Using a reference database of images and DNA, CLIPBD is able to classify images of seen and unseen species. Results reported in the paper show superior performance as compared to BioCLIP and other state-of-the-art DNA encoder models.","- The motivation and problem formulation is sound and interesting.
- The proposed model fuses images, text and DNA into a contrastive embedding space enabling zero-shot image classification of unseen species.
- The flow of paper and writing is good in general.","- The abstract claims the paper is the first to use contrastive learning to fuse DNA and image. However, there are existing works [1, 2, 3] which have done this for other applications and they should be discussed in the related works.
- The claim that *DNA is a better target than taxonomic labels* (**Line 316**) is highly questionable. This claim is reiterated in **Lines 359-362**. The paper clearly mentions that majority of the BIOSCAN-1M dataset does not have taxonomic labels. In fact only 3.36% pretraining data has labels upto the species level (**Line 321**). It is clearly seen from **Table-1** that aligning with taxonomic labels outperforms aligning with DNA at the order level. I believe if the authors used an **unbiased dataset** containing the same proportions of DNA labels and taxonomic labels, the results would have been similar if not worse. This is more of a problem of the dataset and not the modality itself. 
- Following the previous point, why does Image-to-DNA retrieval performance improve at the species level when aligning all three modalities (**Table 1**)?
- For inference to work on unseen species during training, the framework assumes that their DNA and/or images are available in the lookup database. This is an unrealistic assumption. If the images and DNA are already available for unseen species, they might as well should have been used for training.
- Limited technical novelty considering no new representation learning technique has been proposed. The paper uses an existing dataset containing 1M insect specimens and unbalanced DNA and taxonomic labels, raising questions on the effectiveness of the method on other real-world datasets. Majority of the experiments and evaluations are only shown for a single dataset.
- The authors correctly pointed that BioCLIP was trained on diverse set of species including natural images. However, one easy way to utilize the BioCLIP embedding space would be to align the DNA modality with frozen BioCLIP vision and text encoders. Have the authors compared their method with this ImageBind-style training?


References

[1] Taleb, Aiham, et al. ""Contig: Self-supervised multimodal contrastive learning for medical imaging with genetics."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[2] Xie, Ronald, et al. ""Spatially Resolved Gene Expression Prediction from Histology Images via Bi-modal Contrastive Learning."" Advances in Neural Information Processing Systems 36 (2023).

[3] Min, Wenwen, et al. ""Multimodal contrastive learning for spatial gene expression prediction using histology images."" arXiv preprint arXiv:2407.08216 (2024).",Details about the reference database is missing. How many images and DNA barcodes are present in the reference database during inference?
rPkCVSsoM4,rPkCVSsoM4,A Causal Lens for Learning Long-term Fair Policies,Accept (Poster),CSiVaW2xPM,ICLR.cc/2025/Conference/Submission5143/Reviewer_YG39,"This paper studies long-term fairness through a causal decomposition framework. In particular, the authors first show that the total causal effect can be decomposed into a direct impact, delayed impact, and spurious impact. Moreover, a connection between benefit fairness and the causal effect of switching from a baseline policy to a virtual policy is established. In order to promote long-term fairness, the authors propose to update the PPO objective by introducing a fairness regularizer. Then the experiment section compares various methods on a simulated environment and the proposed method attains improved benefit fairness compared to the other methods.","1. The decomposition of long-term fairness into direct and delayed impact is interesting, although it is not surprising. The connection between benefit fairness and the proposed measure of fairness (through policy intervention in an MDP) is quite interesting.

2. The experimental section shows that the proposed modification of the PPO objective works, and in particular it can achieve approximate benefit fairness.","1. The authors claim that they introduce a general framework to study long-term fairness. However, the proposed framework is based on MDP which has been extensively studied in dynamic fairness.

2. The causal decomposition result is very similar to existing decomposition results of causal fairness for static settings. In fact, the proof is very similar and the only difference seems to be that the indirect effect is replaced with the delayed impact.

3. Finally, the main drawback of the work is that there is no provable guarantee that the proposed method achieves long-term fairness. Since the result is not evaluated across a large class of benchmarks/simulation studies it is difficult to tell whether the method attains long-term fairness in different types of MDPS.","1. Is there a typo in Proposition 1 (main result)? The inner sum already marginalizes the feature $x'$.

2. You have expressed DPE in terms of benefit fairness. Can the same be done for the delayed impact effect?"
rPkCVSsoM4,rPkCVSsoM4,A Causal Lens for Learning Long-term Fair Policies,Accept (Poster),OW61oYdoH2,ICLR.cc/2025/Conference/Submission5143/Reviewer_hawr,"The paper studies long-term fairness in causal reinforcement learning. The paper proposes the difference between the average qualification gain achieved by each group as their notion of long-term fairness and uses a PPO-type optimization objective to enforce fairness. Through a casual decomposition, the authors break down the qualification gain difference into 3 terms: direct impact, delayed impact, and spurious effects. Empirically, the paper studies how to lower the disparity in average qualification gains across groups and study the effect of each of the three terms.","-- The paper studies long-term fairness which is an important, yet understudied problem.

-- The paper proposes a novel fairness metric for long-term fairness in causal reinforcement learning.","-- There are no technical algorithmic novelties in the paper as the optimization problem is inspired by PPO. This is not a major weakness as the paper introduces novel elements like the qualification gain function and proposes a causal decomposition of the qualification gain.

-- The experimental setup does not clearly show the advantage of the approach over prior baselines. For example, the difference between the worst and best approach is a mere 0.2% in all experiments in Figure 2. Are there other settings where this difference is more significant? The qualification gain disparity in Figure 3, does not go to 0 for each of the proposed metrics and seems to increase with increasing horizon. Can the authors provide why this is the case? Are there other settings where the qualification gain disparity goes to 0?","-- Can the authors specify how I should interpret the results of Figures 2 and 3 in light of the weaknesses mentioned above?

-- What is the relationship of this notion of fairness, with the ones aiming to equalize rewards across all groups? See https://proceedings.mlr.press/v130/wen21a.html and references within."
rPkCVSsoM4,rPkCVSsoM4,A Causal Lens for Learning Long-term Fair Policies,Accept (Poster),ArAAR0eZ3B,ICLR.cc/2025/Conference/Submission5143/Reviewer_KhgC,This paper uses the causal perspective to study the long-term fairness problem where decisions can,"1. The long-term fairness under causal inference perspective seems novel
2. The fairness penalty decomposition is useful in practice","1. Theoretical guarantee not sufficient:
(1). Convergence of the algorithm
(2). How the penalty coefficients \\beta^{KL} and \\beta^{\\Lambda} quantitatively impact fairness and model performance (e.g., precision/recall per round)

2. Despite fairness metrics improvement, it is difficult to tell if the model performance improved and if the fairness-aware optimization can result in a Pareto superior result for every group or every individual, there should be more theoretical and numerical evidence on the utilities instead of just utility gaps

3. References: 
(1). The long-term fairness problems are studied in a previous line of work related to repeated distributionally robust optimizations, e.g., ""Fairness Without Demographics in Repeated Loss Minimization""
(2). The performative predictions and the strategic classification/regression problems are other research areas closely related to model decision having causal effect on data distribution, please compare with them

4. Similarities and differences between the new metrics and the conventional fairness metrics:
It will be more helpful to build connections between the fairness terms in the long-term causal formulation with metrics like demographic parity, equal opportunity, equalized odds in the conventional settings like single shot classification and regression problems, which I think equalized odds (both TPR and FPR being the same) as well as loss disparities do not have counterparts in the causal formulation, and I encourage authors to provide explanations on why not.",1. Why fairness penalty instead of re-weighting in the optimization problem?
rPkCVSsoM4,rPkCVSsoM4,A Causal Lens for Learning Long-term Fair Policies,Accept (Poster),j45xc8Pd27,ICLR.cc/2025/Conference/Submission5143/Reviewer_FkQm,"The paper proposed a general causal framework to characterize long-term fairness in reinforcement learning (RL) settings, where the unfairness is represented by ""qualification gain disparity"" between the advantaged group and the disadvantaged group ($C_{\\pi}$). The authors decomposed the unfairness into three different parts (DPE, IPE, SPE) and used Proximal Policy Optimization (PPO) to optimize the decision policy. Particularly, DPE is closely related to benefit fairness (i.e., individuals from different groups who gain similar qualification improvement should have similar opportunities to get the treatment), and if the benefits are independent of the group attribute, then benefit fairness means with 0 DPE, demonstrating some kind of alignment between the long-term fairness proposed by the authors and benefit fairness. Motivated by this, the authors also added fairness as a separate term to the fair objective function. Thus, there are 2 fairness mechanisms ($PPO-C$, $PPO-Cb$) proposed, and the authors did simulation studies on real datasets to verify their findings.","1. Studying Long-term fairness as qualification gain disparity is well-motivated and of social importance.
2. I like the idea of the causal decomposition using $\\pi_{PS}$.
3. The experiments demonstrate the effectiveness of the fairness mechanisms.","Overall, I think the paper is interesting, but so far it does not situate well in previous literature. I am willing to increase my score if the authors clarify their contribution.

1. Though I appreciate the efforts the authors made to compare their work to previous literature, I still feel the novelty of the paper is not clear enough. Firstly, just as the authors stated, [Hu et al., 2023, Hu & Zhang, 2022, Yu et al., 2022] considered long-term fairness in a causal framework. Especially for the framework proposed by [Hu & Zhang, 2022], the main difference seemed to be: (i) they considered the disparity of qualification rate (not the gain of qualification) as the fairness notion; (ii) they did not consider RL settings. Could the authors try making the comparison clearer?

2. There are other recent papers on strategic classification and performative prediction considering long-term fairness in terms of qualification rate improvement (also known as social welfare) when population distribution can be shaped by the decision policy (e.g., [Jia et al., 2024, Jin et al., 2024]). Moreover, there are at least two fairness notions related to qualification gain: (i) equal improvement [Guldogan et al., 2022]; (ii) bounded effort [Heidari et al., 2019]. It is necessary to review these papers.

3. The fairness mechanisms just stack all terms together and I am not sure about the novelty. Plus, in Figure 5, it seems that IPE and SPE are rarely influenced by the proposed fairness mechanisms. Could you explain why?

4. Why not provide error bars for Figure 4?

> References

Jia, Zhuangzhuang, et al. ""Distributionally Robust Performative Optimization."" arXiv preprint arXiv:2407.01344 (2024).

Jin, Kun, et al. ""Addressing Polarization and Unfairness in Performative Prediction."" arXiv preprint arXiv:2406.16756 (2024).

Guldogan, Ozgur, et al. ""Equal improvability: A new fairness notion considering the long-term impact."" arXiv preprint arXiv:2210.06732 (2022).

Heidari, Hoda, Vedant Nanda, and Krishna P. Gummadi. ""On the long-term impact of algorithmic decision policies: Effort unfairness and feature segregation through social learning."" arXiv preprint arXiv:1903.01209 (2019).","1. Could you make your contribution clearer and review previous work more comprehensively? (weakness 1, 2)

2. Why are IPE and SPE not influenced by the fairness mechanisms? (weakness 3)

3. Could you justify why it helps by directly adding benefit fairness as another regularization term? (weakness 3)

4. Could you provide error bars for Figure 4, or am I missing something? (weakness 4)"
auZZ2gN0ZN,auZZ2gN0ZN,Dense Video Object Captioning from Disjoint Supervision,Accept (Spotlight),bKZrRPRWDf,ICLR.cc/2025/Conference/Submission4872/Reviewer_szRA,"This paper introduces a novel task towards the field of video understanding—dense video object captioning—with the aim of advancing the comprehension of temporal and spatial information within videos. In the absence of training data for this task, the authors innovatively proposes an end-to-end approach that integrates various datasets to train different modules. The superiority of the proposed methodology is demonstrated through extensive zero-shot and full fine-tune experiments.","1. **Clear and Precise Definition of a New Benchmark**: The paper introduces a novel task aimed at achieving more comprehensive video understanding, thereby setting higher standards for a single model's capability to interpret videos. It also establishes well-defined evaluation metrics that are appropriate for this new benchmark.

2. **Design of a Concise and Effective Training Framework**: In response to the proposed task, the paper presents a streamlined, end-to-end trainable framework that effectively integrates multiple tasks such as detection, tracking, and description generation. The authors ensure seamless end-to-end training by efficiently constructing ground truth for the tracking task.

3. **Innovative Data Organization Method**: To Address the lack of a dedicated dataset for the proposed task, the authors decompose the task and combine or generate data from various sources to train different modules effectively.

4.  **Comprehensive Experimental Evaluation**: The authors conducted extensive experiments, demonstrating the advantages of their approach over simply concatenating state-of-the-art solutions for individual tasks. They highlight the effectiveness of combining multiple datasets, the superior generalization ability of the trained model, and its outstanding performance on relevant datasets. The open source of the code further strengthens the credibility of these findings.","1. **Clarification of Task Significance**: Although the task imposes higher demands on deep models for video understanding, the paper does not clearly articulate the associated benefits. It would be advantageous to illustrate the task's relevance in more challenging or representative application scenarios, such as sports commentary or intelligent animal monitoring. This would better underscore its significance and research value.

2. **Examination of Method Generalizability**: The authors successfully modularize the training process by breaking down the overall task into distinct sub-tasks and aligning them with suitable training datasets, as demonstrated by the experimental results. However, a more in-depth discussion on the domain differences between datasets and the rationale for task decomposition could provide a stronger basis for the broader applicability of this approach. Such an analysis would significantly enhance the paper's contribution to the research community.

3. **Correction of Typos and Formatting Inconsistencies**: The paper contains several typos and formatting inconsistencies that require attention. For example, in Section 3.5, the symbols in Equation (1) do not correspond with the preceding descriptive content. Additionally, there are inconsistencies in citation formats in the paper, for example, the format of citations in Table 2.","Beyond the questions in the weaknesses, I have an additional curiosity regarding the approach. In the Spoken Moments in Time (SMiT) dataset, captions are provided at the video level. The approach of using entire frames as detection proposal and generating captions contrasts significantly with the method of using detectors on the Visual Genome dataset to identify objects and then generating captions, resulting in a noticeable domain discrepancy. I am uncertain whether this discrepancy should be addressed, as the examples provided suggest that the generated results for single targets also include interaction information with other objects, such as ""A brown dog chasing an adult on the grass."" Given that the input to the text decoder is supposed to be a single-track feature and the model does not explicitly model interactions between targets, could this intriguing phenomenon be attributed to the way the SMiT dataset is utilized in training?"
auZZ2gN0ZN,auZZ2gN0ZN,Dense Video Object Captioning from Disjoint Supervision,Accept (Spotlight),O2rCo3wV17,ICLR.cc/2025/Conference/Submission4872/Reviewer_o3Kj,"This work introduces a new task and model for dense video object captioning that unifies spatial and temporal localization by detecting, tracking, and captioning object trajectories in videos. The proposed end-to-end model combines visual understanding and language generation, outperforming multi-stage pipelines in accuracy and temporal coherence. Leveraging a novel training strategy that uses disjoint, large-scale datasets for pretraining, the model achieves impressive zero-shot capability and serves as a robust base for fine-tuning. Additionally, new metrics for task assessment and successful adaptation of video grounding datasets further demonstrate the model's effectiveness in video spatial grounding tasks.","This paper introduces a novel task—dense video object captioning with unified spatial and temporal localization—bridging video understanding and natural language description in a unique way.

The end-to-end model presented is robust, outperforming multi-stage pipelines by integrating detection, tracking, and captioning into a cohesive approach that achieves notable zero-shot capabilities.

The authors clearly outline the model architecture, training strategy, and metrics, making complex components understandable and well-motivated.

By developing metrics and reusing datasets for this new task, the work contributes valuable tools and benchmarks to the video understanding community, advancing video spatial grounding tasks and surpassing previous state-of-the-art results.","The paper primarily focuses on a few datasets, which may not fully represent the diversity of real-world scenarios. Expanding evaluations across a broader range of benchmarks could strengthen the validity of the results.

While quantitative metrics are important, including qualitative evaluations—such as human assessments of captioning quality—could enrich the understanding of model performance and highlight potential areas for improvement.

The paper does not provide an in-depth analysis of failure cases, which limits understanding of specific scenarios where the model struggles, such as occlusions, fast motion, or complex interactions between objects.","Can you provide a detailed breakdown of common failure cases observed during the evaluation? Understanding these could clarify the model's limitations and areas for improvement.

How does your model perform with unseen objects? Providing results or insights on this aspect could strengthen the claims of generalizability.

Could you elaborate on the rationale behind the chosen evaluation metrics? A more comprehensive justification would help clarify their relevance and effectiveness in measuring performance."
auZZ2gN0ZN,auZZ2gN0ZN,Dense Video Object Captioning from Disjoint Supervision,Accept (Spotlight),6F4pIyVh52,ICLR.cc/2025/Conference/Submission4872/Reviewer_zAgu,"This paper proposes the new task of detecting, tracking and captioning objects in a video. The authors propose evaluation metrics for this task, a baseline method and a training strategy involving disjoint supervision.",This work tackles an important problem that was missing from the literature. The paper is well-written and easy to follow. Extensive experiments have been performed.,"My only concern is that the end-to-end tracking algorithm (listed as a contribution) seems to be naive and not novel enough. There are other methods that perform identity association within the model (like MinVIS[1], CAROQ [2], trackformer [3]). The authors have explored some other ways of integrating temporal information/tracking in Table 2, but how about different ways of association within the model (e.g., query vector propagation in trackformer [3])?


[1] MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training, Huang et al., NeurIPS 2022.

[2] CAROQ: Context-Aware Relative Object Queries to Unify Video Instancd and Panoptic Segmentation, Choudhuri et al., CVPR 2023.

[3] TrackFormer: Multi-Object Tracking with Transformers, Meinhardt et al., CVPR 2022",Please see Weaknesses.
auZZ2gN0ZN,auZZ2gN0ZN,Dense Video Object Captioning from Disjoint Supervision,Accept (Spotlight),0kqK5dXt7b,ICLR.cc/2025/Conference/Submission4872/Reviewer_58ev,"The paper proposes a new task called Dense Video Object Captioning (Dense VOC), which involves detecting, tracking, and generating captions for objects in a video. The model aims to unify spatial and temporal localization while providing a natural language description of object trajectories. The approach combines detection, tracking, and captioning into a single end-to-end model, which is shown to be more accurate and temporally consistent compared to traditional multi-stage pipelines. The authors also propose a novel training strategy using disjoint supervision from different datasets, enabling zero-shot generalization and strong performance after fine-tuning. New metrics are introduced to evaluate the combined tasks, and the model achieves state-of-the-art results on tasks like video grounding without explicit training for those tasks.","1. The proposed approach integrates detection, tracking, and captioning, which ensures more temporally consistent results and holistic video understanding compared to multi-stage pipelines.
2. The use of disjoint tasks for training allows the model to generalize well even without specific full annotations for the task, showcasing impressive zero-shot capabilities.
3. The model outperforms strong baselines and even specialized models on tasks like video grounding and multi-object tracking, demonstrating its effectiveness across various video-related tasks.
4. The introduction of new evaluation metrics (e.g., CHOTA) tailored for Dense VOC ensures a more comprehensive assessment of model performance, capturing detection, tracking, and captioning jointly.
5. The model is successfully applied to related tasks such as video grounding and person tracking, proving its versatility.","1.	The framework demands significant computational resources, requiring more than 32 GPUs, and the specific GPU model was not provided. While separate training offers the potential for lightweight product applications, the experiments lack consideration of this aspect.
2.	The experimental section lacks an analysis and comparison of inference efficiency across different stages of the model as well as with existing backbones.
3.	The model heavily relies on datasets that do not fully cover Dense VOC, which could limit the effectiveness of pre-training. Although the authors address this by using multiple disjoint tasks, the absence of specialized, annotated data for Dense VOC remains a challenge.
4.	The term “Dense” is somewhat confusing and seems ambiguous in the context of the task, which unifies spatial and temporal localization in video with fine-grained visual understanding through natural language. Perhaps a more precise term could better capture the essence of this process.
5.	The caption tracking process is event-oriented. Is there an analysis of different event types in this task?

If the authors can address my concerns, I am inclined to raise my rating.","1. The use of disjoint datasets and diverse forms of supervision makes the training process complex and resource-intensive, posing challenges for reproduction and practical deployment. Is it possible to unify this process within a robust code infrastructure to facilitate easier incremental implementation and application?"
JtGPIZpOrz,JtGPIZpOrz,Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains,Accept (Poster),EO5Bbe6IEs,ICLR.cc/2025/Conference/Submission4829/Reviewer_Tw8m,"This paper contributes to the field of self-improvement fine-tuning for LLMs by proposing a multi-agent cooperation approach. It replicates an LLM into multiple generation agents and corresponding critic agents. A multi-agent debate architecture is utilized to generate label responses for generation agents and critic agents. Each agent is then fine-tuned using its unique generated dataset. During the inference phase, the final result is generated following the multi-agent debate. The method demonstrates superiority over the baselines in a series of math-related language reasoning tasks.","1. The paper is easy to follow and the content is well-organized.
2. The paper proposes a method for agent self-improvement fine-tuning based on multi-agent collaboration, allowing for multiple rounds of self-improvement fine-tuning, which could be a promising approach.","My primary concerns with this paper are centered around the experimental section.

(Major)The first concern is regarding the selection of experimental datasets. The paper exclusively uses mathematical language reasoning tasks, and each task is not particularly challenging. Arithmetic is limited to arithmetic operations, GSM corresponds only to Grade School level difficulty, and MATH selects only the first three levels. If the tasks are not challenging enough, it may lead to questioning the need for incorporating multiple LLMs when a single LLM might be adequate. What’s more, conducting generalization experiments solely within mathematical datasets may not demonstrate the unique advantages in terms of generalizability, given the highly similar nature of the tasks. Including a graduate-level mathematical reasoning set or challenging datasets from other fields would make the experiments more convincing.

(Major)The second concern is that the performance improvement on datasets of modest difficulty is not significant, especially considering the introduction of multiple large models collaborating. It may be worthwhile to quantify other metrics beyond answer accuracy, for example, the KL divergence of each LLM from the original LLM distribution? Introducing multiple large models might allow agents to achieve better results without significant parameter changes.

*Overall, I believe the idea of the paper is commendable. However, as a practice-oriented paper lacking in theoretical explanation, its experimental design is insufficient. Without addressing these major concerns in the experiments, I'm afraid this is the highest score I can give.*

（Minor）The diversity of models: When reading the sentence "" each model to capture parts of a task of interest."", I am very excited. However, the authors do not take any measures during the implementation to activate the heterogeneous characteristics among the generative agents. In other words, diversity is only brought about by the quantity of agents. The diversity cannot be theoretically guaranteed, nor can it be conceptualized and defined for specific agents after it appears. If the authors could enable different agents to collaboratively complete a complex task and truly capture the sub-tasks they are interested in or excel at, it might greatly enhance the contribution of this paper.","In addition to my primary concerns, there are also the following questions: 
1. In this paper, is each individual agent fine-tuned using the SFT (a combination of query and label response data) approach? If yes, can this method be integrated with DPO (a combination of query and human feedback preferences data) and PPO (a combination of query and reward signal data) approaches? Please provide a detailed discussion. 
2. How is the Combination of datasets implemented in line 25 of Algorithm 1 pseudocode? What does the hyperparameter denote? The authors need to clearly explain how the two types of data are used respectively in the training of the critic agent and what the role of the weight hyperparameter is.
3. The abstract mentions “multiagent society of language models”, yet the experiments only utilized 3-5 agents (and the improvement with 5 agents does not seem significant). How do the authors view the relationship between the number of agents and performance improvement, as well as the issue of number versus resource consumption?
4. The selection of baselines. From the setup of the baselines, this work seems more like an “A+B+C” approach. However, would it be fair to compare “B+C” with B alone, even if the A module is ablated?"
JtGPIZpOrz,JtGPIZpOrz,Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains,Accept (Poster),Ofy0dUVglQ,ICLR.cc/2025/Conference/Submission4829/Reviewer_TeFj,"This paper proposes a method to address a key limitation of LLMs: their dependency on static datasets, which limits their ability to self-improve. The authors propose a multiagent approach for LLM finetuning that enables iterative self-improvement through interactions with other models. By using distinct Actor (generation) and Critic (evaluation) agents, their approach improves feedback quality and response diversity, which results in better responses over iterations of finetuning. The authors empirically demonstrate their approach's effectiveness across 3 reasoning benchmarks (Arithmetic, MATH, GSM), showing performance gains of approximately 1-15% across 4 baselines. Additionally, the finetuned agents may demonstrate zero-shot generalization from MATH to the other two benchmarks.","- Significance: This paper presents a promising approach to LLM self-improvement and could offer a valuable contribution.
- Clarity: Most of the Figures in the paper are clear and the paper is generally well-written.","There are several comments I would like the authors to address to make some details clearer and the paper more complete.

**Major comments**
1. Role Specialization: The paper introduces distinct roles for models (generation agents and critic agents). However, it would be helpful to clarify the specific objectives each role optimizes. Additionally, I suggest emphasizing that only two roles are used in this paper (generation and critic) to avoid confusion. 
2. Zero-shot Generalization: In Section 4.3 The authors claim zero-shot generalization on held-out benchmarks. I find the claims made in Section 4.3 not very convincing for three reasons:
- (a) The authors only evaluate on 100 random samples. Could the authors elaborate on why this specific sample size was chosen, and why not more?  If it is computationally possible to evaluate more samples (e.g. 1000), this would make the evidence for zero-shot transfer more convincing.
- (b) In Figure 5, the standard errors for performance (accuracy) are absent; could you please report these like was done in the Table 2
- (c) The choice of finetuning on MATH and evaluating on the GSM dataset is unclear; could you explain the rationale? Why not report all cross-dataset generalizations, such as MATH → GSM or Arithmetic → GSM? If the results are consistent across all cross-evaluations, this would strengthen the findings.
3. Computational Trade-offs: The limitations section mentions increased computational demands but does not quantify these trade-offs. Details on the additional costs or time requirements would help to evaluate the compute/performance balance.
4. Limitations Discussion: The current discussion of limitations is very short. Future researchers building on this work would benefit from more detailed insights into any bottlenecks or constraints of the approach. Based on these more detailed suggestions for future research directions would be valuable.
5. Response diversity: Looking at the slope, Figure 3 seems to indicate that all the benefits could be coming from the initial improvement in diversity. It seems important to mention the initial diversity for each ablation. 
6. Standard error: How exactly is the standard error in Table 2 computed?

**Minor comments** (that did not affect my score)
- Abstract: The sentence“ A set of language models are initialized from the same base model and then are specialized by independently updating each model using data generated by the model under multiagent interaction with other models” is somewhat lengthy and could be simplified for readability.
- Citation Formatting: Throughout the paper, `\\cite{}` is used in places where `\\citep{}` might be more appropriate (e.g., lines 137, 244, 448).
- Figure: In Figure 4, what are the multiple dots in each box for each method? I’m assuming that their order is also the order of performance across iteration but it’s not obvious.","See my questions in the ""Major comments"""
JtGPIZpOrz,JtGPIZpOrz,Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains,Accept (Poster),RfgBlrwMH6,ICLR.cc/2025/Conference/Submission4829/Reviewer_85F3,"This paper explores the fine-tuning of multiple LLM agents within the framework of multi-agent debate. The authors propose a method where multiple LLMs are fine-tuned and organized into generation and critic agents to facilitate multi-agent debate. Experiment results indicate that the proposed method outperforms baseline methods, and the authors further demonstrate that fine-tuning multiple LLMs helps preserve diversity compared to fine-tuning a single LLM.","1.	Jointly optimizing the LLM in the roles of generators and critics appears to be a robust method for enhancing the reasoning ability of LLMs.
2.	The work shows that finetuning multiple LLMs on independent datasets derived from multi-agent debate can preserve diversity, which is a critical challenge for LLM finetuning.
3.	The evaluation results show the strength of the proposed method.","1.	The title “Multiagent Finetuning of Language Models” may imply a broader scope than the paper addresses. Multi-agent applications of language models can indicate a much broader range of settings besides reasoning tasks and multi-agent debate, such as gaming and social simulation; however, this work focuses solely on multi-agent debate.
2.	The terms  “Single Agent” and “Multi Agent” is vague and unclear in this paper. For example, Sec 2.2 “Fine-tuning Single Agent”discusses scenarios involving multiple agents rather than a true single-agent setting.
3.	This work might not obey the standard training and evaluation procedure on GSM and MATH dataset, as only 500 examples are selected for training.","1.	Line 77: How does the proposed approach “promotes diversification within the society of models”? 
2.	What precisely does ""Single-Agent"" in ""Fine-tuning Single Agent"" refer to? Is it intended to indicate one generation agent or a single LLM? If the latter, a more fitting term might be “Fine-tuning Single LLM.”
3.	How is a fair comparison with baseline methods established?
4.	How does varying the number of agents affect the performance of the proposed method?
5.	It is pretty costly to train multiple LLMs, especially considering the inference-time compute and resources required to serve N LLMs. A straightforward possible strategy to avoid training multiple LLMs while also maintaining diversity is to include an unique identifier (e.g. an ID) or a special token in the input for each agent. How does this strategy compare to finetuning multiple LLMs?"
0ctvBgKFgc,0ctvBgKFgc,ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids,Accept (Oral),CHOXKi41RG,ICLR.cc/2025/Conference/Submission4802/Reviewer_7wor,"This paper proposes a framework to generate protein structures by conditioning on layouts specified through 3D ellipsoids. The conditions include location, size, orientation and secondary structure. These conditions are injected to flow-base protein generative models via proposed cross-attention and update modules. It shows greatly improved controllability and designability over baselines.","This paper is well motivated, formulated, written, and evaluated. 

1. The injection of ellipsoid information is achieved through cross attention. This allows the ellipsoids to be unordered set. i.e. user doesn’t have to specify the order or ellipsoids; model also decides the order of ellipsoids. 
2. The formulation of ellipsoid token is effective and is easy to be extended to different conditions other than secondary structure. (such as hydrophobicity)
3. Thorough analysis of ellipsoid consistency, including both geometric and probabilistic metrics.
4. Fig4: The comparison on designability/diversity with baseline methods are done as comparing Pareto frontiers with varying sampling temperatures/guidances. This clearly shows the tradeoff between the methods and the performance improvement from baselines.  I found this analysis insightful and believe other papers arguing increased performance could benefit from a similar evaluation scheme.
5. The practical use-case of the method is shown in Section4.3 flexible conditioning.
6. The controllability is greatly improved from Chroma (Table1). Accuracy and coverage is very impressive.","How is sequence design performance? What I understand is that the designability is solely based on the generated structure (i.e. generated sequence is discarded). Can you also present co-design designability value as in MultiFlow paper?

Other than that, I did not find any major weaknesses from the paper. However, the ablation study can be improved. Most of the model ablations are based on guidance strength, but I am also curious about ablation study on (i) ellipsoid segementation cutoff (5A currently), (ii) allow residue token to update ellipsoid token or not. For (ii), explaining the reasoning behind the design choice may suffice.",Is it possible to set the order of ellipsoids? Or how complicated would it be to extend this framework to allow user to set the order of ellipsoids?
0ctvBgKFgc,0ctvBgKFgc,ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids,Accept (Oral),aFl2mFLnKW,ICLR.cc/2025/Conference/Submission4802/Reviewer_679L,"This work extends Multiflow to accept spatial conditioning of secondary structures via 3D ellipsoids, aiming to improve control in protein generation and reduce the overrepresentation of alpha-helices in current generative models. Building on Multiflow’s architecture, the authors address two main challenges:

1. Integrating and updating ellipsoid conditioning with structure embeddings with minimal modifications: they introduced an *Invariant Cross Attention* module to update residue embeddings while preserving local SE3 invariance.
2. Implementing an effective conditioning approach for flow-matching models: they used classifier-free guidance to interpolate flow vectors across translation, rotation, and amino acid spaces, and employ self-conditioning to refine predicted structures
    
Extensive experiments show that the proposed model can faithfully follow spatial conditioning, resulting in greater diversity, novelty, and improved secondary structure composition. This improves Multiflow by generating proteins with secondary structures more similar to natural proteins. 
Overall, this work presents a straightforward approach to control protein generation, enhancing Multiflow's diversity, novelty, and secondary structure accuracy.","**[Clarity & Quality]**
- The manuscript is well-written, with a thorough introduction and background information on protein structure generation, spatial conditioning, and flow matching for data generation. Overall, it provides a smooth reading experience.
- The paper is of good quality, presenting clear mathematical foundations grounded in current techniques for protein modeling, diffusion-based generation, and guided sampling.
- The problem is well-defined, and the authors designed several experiments to evaluate model performance in 1) following conditioning, 2) improving general performance, and 3) demonstrating practical use in flexible conditioning, with both quantitative and qualitative comparisons.

**[Significance]**
- The spatial conditioning approach using ellipsoids is intuitive for practical applications and has potential implications for the utility of protein generation models.
- The proposed methods appear generalizable to various spatial conditioning scenarios. (*However, this paper focuses solely on secondary-structure conditioning.*)

**[Originality]**
- The authors introduce a novel conditioning modality using spatial ellipsoids for protein generation, along with a new layer, ""invariant cross-attention,"" to integrate this information.","**[Clarity]**
- Some design choices and model details lack clear explanations or in-depth examination (see Q1, 2, and 4).

**[Soundness]**
- Performance on Natural Proteins: While the authors demonstrate high designability at fixed helicity levels on synthetic data, it isn't as clear if these benefits hold for natural proteins (see Q3).

**[Significance]**
- Scope: The current methods are examined only on Multiflow and for secondary structure guidance. Their practical impact on other protein generation models and types of spatial conditioning (e.g., domains, hydrophobic cores) is not extensively explored.","1. **Ellipsoid Representation**

    a. Choosing the Number of Ellipsoids (k):
    - *Training:* Is k determined by the structure of the training protein? If so, what is the distribution of k in natural proteins?
    - *Evaluation based on the statistical model:* The authors appear to have used a fixed k=5 in the experiments. How was this number chosen? Have the authors tested other k values?

    b. Number of Residues per Ellipsoid:
    - The current representation specifies the number of residues in each ellipsoid, but the authors show that this number directly depends on ellipsoid volume. Could specifying the number of residues be redundant, and might removing this constraint provide the model more flexibility in generation? Have the authors examined the impact of residue count on amino acid (AA) prediction?

2. **Invariance Cross-Attention (ICA) Layer Design**

    a. The authors separately model the SE3 features (E_k) and scalar features (e_k) of ellipsoids using the proposed ICA and transformer to achieve SE3 invariance in the local frame. Has the team considered alternative approaches, such as modeling ellipsoids as “pseudo frames” with SE3 and scalar features and simply using IPA to update ellipsoid and residue features together?

    b. In Algorithm 1:
    - Could the authors clarify the *PosEmbed* used in line 223? Does it include distance, angles, or local coordinates?
    - Could they also explain why the query uses un-updated $s$, while the key and value use $a$, which incorporates current ellipsoid information?

3. **Results in Table 2 (natural proteins):** The model, even with the strongest guidance, tends to overestimate helices in proteins. Additionally, the authors did not present designability results, which, based on Figure 16, may be compromised with strong guidance. Could the authors elaborate on the model's performance in addressing the “overrepresented helix problem,” the trade-offs with other metrics, and its overall comparison to models like *RFDiffusion*?

4. In self-conditioning (line 290), they propose supplying interpolated conditions to both conditional and unconditional models, suggesting this improves “designability and ellipsoid adherence for all $\\lambda$ values.” However, no ablation studies were provided to verify this claim."
0ctvBgKFgc,0ctvBgKFgc,ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids,Accept (Oral),q9SPqgwx6i,ICLR.cc/2025/Conference/Submission4802/Reviewer_D7Wa,"This paper presents ProtComposer, which is a fine-tuned model from MultiFlow to support conditional generation based on 3D ellipsoids, showing success at controllable design and achieving SOTA on the Pareto frontier of designability and diversity.","The paper is well written with clean visualizations demonstrating methods and results. The concept of utilizing ellipsoids as conditions for protein generation is interesting and novel, providing a bridge between protein-level conditioning and atom-level conditioning. In addition, the authors propose an effective Invariant Cross Attention module for integrating ellipsoid conditioning and demonstrate success at achieving SOTA performance on the Pareto frontier of designability and diversity.","I have no major concerns about this paper. However, it would be helpful if the authors could elaborate on the applicability of the ellipsoid-based conditioning approach on practical protein design tasks. How would it help with or facilitate the protein design process?","- Line 355: How is the length between ellipsoids determined/sampled? Also, consider an ellipsoid with beta strand annotation, how is the length between each stranded segment determined (particularly if a strand ellipsoid is formed by segments that are distant in sequence but close in structure)? 
- It would be helpful if the authors could provide ablation study results on the effect of self-conditioning, particularly the two self-conditioning schemes described in line 290.
- Figure 9: What is the linear fit and statistical significance in both cases?
- During training, is the ellipsoid conditioning information always provided, or only provided for a percentage of time?
- Line 367: Why is a structured residue considered as “covered” if it is inside at least one ellipsoid instead of inside the ellipsoid it is assigned? 
- Table 1: Could the authors provide some intuition on why over-guidance (\\lambda > 1) performs better than the conditional model itself (\\lambda = 1)?
- Table 2: Does “PDB proteins” correspond to the validation dataset or does it include the training dataset?"
0ctvBgKFgc,0ctvBgKFgc,ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids,Accept (Oral),A1jrejawF4,ICLR.cc/2025/Conference/Submission4802/Reviewer_eiq5,"Introduces ProtComposer, a generative model for proteins. ProtComposer seeks better control over the shape of generated proteins, as well as allow for greater novelty in the generation of proteins. Notably, the user has to choose between control or novelty, ProtComposer does not achieve both simultaneously. These tasks are accomplished by the introduction of 3D ellipsoid frames to guide the generation of proteins by the pre-existing Multiflow algorithm. 

Without a pre-existing metric the authors feel sufficiently quantifies compositionality, they introduce their own metric. It is shown that ProtComposer outperforms existing methods in this metric.","It addresses an important and relevant area. The results are overall quite impressive as well. Additionally, showing the ability to work with handcrafted ellipsoid frames or the more easily scalable ML generated frames shows the practicality of ProtComposer.

I really like the point about using simple ML models for ellipsoid sampling as compared to NNs. I would like to see some stronger analytical or experimental justification of the claim though.","Since a custom metric is introduced in this paper and then used as justification for the performance of the model, a section (either in the main paper or the appendix) justifying this metric would be nice. Showing comparisons to other pre-existing metrics, performance of many other algorithms under this metric, or stronger domain justification would strengthen the meaning of the results. I did not reject over this since the metric intuitively looks good, but further empirical or analytical support would be nice.

The formatting of the extra results in the appendices results in very odd page layouts, where some pages are blank, others have odd whitespace gaps, etc. Adjustment to make these pages more presentable would be nice.","I am assuming that the user can redefine K during each trial as they see fit, though studies on guidelines in choosing K would be helpful. In situations where the user has only a vague idea of what they are looking for (an unfortunately common occurrence), having a guide on where to start would be beneficial."
uSg854MOWu,uSg854MOWu,Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,Reject,nXMWgSHfIx,ICLR.cc/2025/Conference/Submission4598/Reviewer_kzDK,"The paper investigates the phenomenon of clean generalization and robust overfitting (CGRO) within the context of adversarial training. Initially, the authors demonstrate that, from the standpoint of model expressive power, achieving a CGRO classifier is significantly easier compared to robust generalization, which, in the worst-case scenario, necessitates exponentially large model capacity. Subsequently, the paper examines the training dynamics of adversarial training by considering a patch data distribution, employing a three-stage analysis technique to explore the feature learning process involved in adversarial training. Finally, numerical experiments are conducted to validate the theoretical insights presented in the study.","1. The paper is well-structured and easy to follow.

2. The three-stage analysis of the training dynamics of adversarial training, based on the assumption of a patch data distribution, is both intriguing and innovative.

3. The theoretical findings are well-supported by the numerical experiments, demonstrating a strong correlation between the two.","1. The paper falls short in clarifying how its results contribute to understanding why CGRO occurs during adversarial training. The paper raises this broad question in the introduction but lacks a detailed explanation of how its findings can offer insights into addressing it.

2. From the perspective of representation complexity to illustrate the CGRO phenomenon, the main theorem relies on Assumption 4.3, which posits the existence of a clean ReLU network classifier of polynomial size. However, this assertion pertains to a factual claim and cannot be directly treated as an assumption. Instead, it would be more appropriate to formulate it as a theorem accompanied by a rigorous mathematical proof. This would strengthen the foundation of the theoretical results presented in the paper.

3. From the perspective of training dynamics to elucidate the CGRO phenomenon, the analysis depends on the assumption that the perturbation radius 𝛿 is only marginally smaller than the signal norm 𝛼. The theoretical results are derived specifically under this condition for the perturbation radius. However, in practical scenarios, the perturbation radius can be chosen arbitrarily. The paper does not adequately address whether the same theoretical results can be achieved in this more generalized context, which leaves an important gap in understanding the robustness of the findings. Further exploration of this aspect would enhance the comprehensiveness of the study.",See the weakness.
uSg854MOWu,uSg854MOWu,Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,Reject,YXXgrYrbdm,ICLR.cc/2025/Conference/Submission4598/Reviewer_iwSa,"The paper studies the robust overfitting phenomenon in adversarial training, where despite the model's generalization on clean data the robust accuracy differs significantly between training and test samples. The paper aims to show the existence of classifiers with clean generalization and robust overfitting (CGRO) and section 4 includes Theorem 4.4 with a polynomial upper-bound for CGRO classifiers and Theorem 4.7 on the exponentially growing required number of ReLU network parameters to achieve robust generalization. Next, the paper studies a structured-data setting and analyzes the training dynamics of a convolutional network, where the network is shown to converge to robust memorization. A few numerical results on MNIST and CIFAR10 are presented in Section 6.",1- The paper studies a relevant research problem on robust overfitting in adversarial training. The paper presents several theoretical results on the phenomenon and the possibility of robust overfitting and clean generalization in adversarial learning.,"1- The paper's presentation of the theoretical results can be significantly improved. One confusion I have about the theory discussion in the paper is the order-related notations and their meaning in the results. Beginning in Definition 3.4, the authors clarify in Remark 3.5 that the notations are with respect to data dimension $D$. However, I could not understand how the authors treat the data dimension $D$ as a parameter that they can change and perform asymptotic analysis for. Isn't it the case that the data dimension $D$ is fixed and the asymptotic analysis should focus on the model complexity (number of parameters) or the sample size for asymptotic analysis? 

Continuing about my confusion with the asymptotic analysis on the data dimension, let me refer to Theorem 4.4 where the authors make Assumptions 4.1 -4.3. Looking at these assumptions, they seem quite dependent on the dimension of $X$. For example, if one changes the input dimension, should not the constants $R$ (Assumption 4.1) and $\\delta$ (Assumption 4.2) be updated as the dimension $D$ changes? Also, in the statement of Theorem 4.4, how can the dimension $D$ change while the theorem sims to fix the input space $mathcal{X}$? If so, then the function $f_{CGRO}$ will change with dimension $D$ and it is not a fixed function to perform CGRP analysis for.

As I discussed above, the authors' asymptotic analysis in terms of the input dimension seems quite confusing, because it changes the input $X$, the classifier architecture $f$, and also affects the assumptions altogether. I may have missed some details about the authors' analysis and will look forward to discussing the analysis in the discussion period.

2- The numerical results seem rather insufficient. I understand that this is a theory work, but the results in Figure 2 and Table 1 does not support the authors' claim. First of all, the input dimension does not change when the authors vary the model size, therefore the experiments do not analyze the asymptotic scenario when $D$ grows. Also, the improvement in robust test accuracy seems to follow from the improvement in clean test accuracy as the model keeps growing in capacity and hence can achieve higher test accuracy on both clean and perturbed data.

3- Minor comment: In my opinion, copying a figure from another paper (Figure 1 in the introduction) is inappropriate in a top-level machine learning paper. It would have been better if the authors' replicated the results on a different dataset or architecture and while giving credit to the cited paper, they presented their own computed version of the figure in the submission.","1- I explained my confusions with the authors' asymptotic analysis in data dimension $D$. Can the authors clarify on the questions I raised on the analysis?

2- Can the authors provide more numerical results where the clean test accuracy remains stable while adversarial test accuracy improves where the model complexity grows?"
uSg854MOWu,uSg854MOWu,Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,Reject,4KYWAYyaMR,ICLR.cc/2025/Conference/Submission4598/Reviewer_bQGX,"This paper studies the Clean Generalization and Robust Overfitting (CGRO) phenomenon in adversarial training from two theoretical perspectives: representation complexity and training dynamics. Using ReLU networks, they show that CGRO classifiers require only poly(D)+ND parameters while robust classifiers need exponential complexity. They also analyze training dynamics using a two-layer CNN on structured data, showing how the network converges to a state of partial true feature learning and exact spurious feature memorization.","The analysis is particularly noteworthy for its comprehensive approach - it not only establishes complexity bounds showing that CGRO classifiers require only poly(D)+ND parameters while robust classifiers need exponential complexity, but also provides insights into how this phenomenon emerges during training through a clear three-stage analysis of the training dynamics. The theoretical framework is well-supported by experimental validation on both real and synthetic datasets, demonstrating practical relevance. Most importantly, the paper's findings help explain a commonly observed phenomenon in adversarial training, offering valuable insights into why models often achieve CGRO in practice. The technical depth combined with practical significance makes this work a meaningful contribution to understanding adversarial robustness.","I have two major concern regarding the representation complexity and training dynamics, respectively.

**Representation Complexity:**

The central result at Line 255 states:

Clean Classifier (poly(D)) ≲ CGRO Classifier(poly(D)+ND) ≪ Robust Classifier (Ω(exp(D)))

However, the poly(D) complexity of clean classifiers is simply assumed in Assumption 4.3 rather than proven. This is problematic because if clean classification also requires exp(D) parameters, the entire complexity hierarchy becomes meaningless. The authors need to either:

1) Prove that clean classifiers can indeed be approximated by neural networks with poly(D) parameters, even though it is trivial.

2) Cite existing results that establish this fact, even though this is a well-known classical result.

I assume this is only a **clarity** issue, because the authors think this is too trivial or too classical and not worth discussing? 

**Training Dynamics:**

While the analysis clearly shows that training leads to partial true feature learning and exact spurious feature memorization, there is a gap between this result and CGRO that needs to be addressed. Specifically, the paper should analyze:

1) For an unseen samples, what is the probability that the model will correctly classify it? Need analysis of the condition $α^q U + V ≥ 0$ for a new sample.

2) For adversarial examples generated from this unseen sample, what is the probability of misclassification? Need analysis of the condition $α^q U + V < 0$ for adversarial example.

These probability analyses would help complete the connection between the training dynamics results and the CGRO phenomenon.",See weakness.
uSg854MOWu,uSg854MOWu,Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,Reject,vAF6XM7qMu,ICLR.cc/2025/Conference/Submission4598/Reviewer_hyTi,"This paper analyses the phenomenon of clean generalization and robust overfitting (CGRO) from two perspectives: representation complexity and training dynamics. They first show that achieving a robust classifier on test distribution requires many more parameters than achieving a classifier with CGRO. Then, they further investigate the optimization dynamics of adversarial training and show that the weights of the classifier will finally move to the CGRO regime. Their theory about training dynamics is based on a specific data structure: Patch Data Distribution. Some experimental results are provided to validate the CGRO phenomenon and the three-stage dynamics.","This paper provides two theoretical perspectives to understand the CGRO phenomenon. The theory is solid in general although it needs some assumptions of the data structure. The writing is clear and they provide detailed descriptions of the notions, setup, and related background.","* The model representation perspective shows that the models need more parameters to achieve robust generalization than CGRO. But how do we validate that the number of parameters of the deep networks such as ResNet is in this CGRO range?
* The data structure is not very realistic. I think a more realistic assumption would be the data lies in a low-dimensional subspace or manifold. The assumption that only a pixel-space patch has useful signals would be too tough in my opinion.
* Although the authors validate the three stages of training dynamics in the synthetic structure data, they don't validate it on real data. Could you provide some experimental results to show it?",Please refer to the Weaknesses part.
uSg854MOWu,uSg854MOWu,Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,Reject,9EViRzUqsO,ICLR.cc/2025/Conference/Submission4598/Reviewer_Be1H,"The paper investigates the CGRO phenomenon observed in adversarial training (AT)—specifically, that a model trained by AT achieves good clean generalization (CG) while simultaneously encountering robust overfitting (RO). The paper offers a theoretical explanation for this phenomenon from two perspectives: the model’s representation complexity and the training dynamics of AT.

From the representation complexity perspective, the paper considers a binary classification problem where a classifier that achieves only CG exists. A CGRO classifier can then be constructed based on this ""CG-only"" classifier, requiring slightly more parameters, which implies that achieving CGRO requires a mildly higher representation complexity.

Regarding AT’s training dynamics, the paper considers a structured data setting where data consists of one patch with a true feature and other patches with spurious features. Under additional assumptions, the model trained by AT is shown to partially learn true features while memorizing spurious features, suggesting that such a model is a CGRO classifier.","The paper explores an interesting phenomenon arising in AT and offers two theoretical explanations. The paper is well-written, with clear descriptions of the problem setup, definitions, assumptions, and proof sketches that effectively convey the intuition behind the theory.","There are several technical issues that I would like to discuss:

- **Lemma 4.5**:  the lemma constructs a classifer $f _ {\\cal S}$ based on $f_{\\rm clean}$ and claim that $f_{\\cal S}$ is a CGRO classifier. However,  although by construction $f_{\\cal S}$ memorizes the training data ${\\cal S}$, this does not necessary imply that $f_{\\cal S}$ will exhibit robust overfitting. The generalization performance of $f_{\\cal S}$ depends on the choice of ${\\cal S}$. If ${\\cal S}$ nicely covers the support of the data distribution ${\\cal D}$, that is given ${\\cal S}$ we have $X\\in \\cup_ {i=1} ^{N}{\\mathbb B} _ {p}(X_{i}, \\delta)$ with high probability, then $f_{\\cal S}$,  despite memorizing ${\\cal S}$, is still able to achieve a good robust testing accuracy. In this case,  $f_{\\cal S}$ is not CGRO.  Could you specify conditions on ${\\cal D}$ under which $f _ {\\cal S}$ would exhibit robust overfitting with high probability over the randomness of drawing ${\\cal S}$ from ${\\cal D}$? Could you also provide theoretical analysis towards how the sample size $N$ as well as $\\delta$ and $p$ affect ${\\cal L} _ {\\cal D}^{p,\\delta}(f _ {\\cal S})$ ?
- **Section 5.1, line 325**:  the definition of ${\\mathbb A} _ {p}(X, \\delta)$ is unclear. It seems that for any  $X'\\in{\\mathbb A} _ {p}(X, \\delta)$ other than $X$, we have $X'[{\\rm signal}(X)]=q w^{*}$ for some $q\\in{\\mathbb R}$ and $X'[j]=0$ for $j\\ne {\\rm signal}(X)$ . Is this correct? The construction of ${\\mathbb A} _ {p}(X, \\delta)$ is important for understanding the results in Theorem 5.9. I hope the authors could clarify the definition of ${\\mathbb A} _ {p}(X, \\delta)$ . 
-  **Theorem 5.9** : Theorem 5.9 attributes CGRO to the model trained by AT simultaneously learning the true feature and memorizing the spurious feature. Under the settings in Section 5, is it possible to show that a model trained via standard (non-adversarial) training would only learn the true feature and ignore the spurious feature? Establishing this distinction is important, as it would suggest that CGRO arises from the nature of AT itself rather than from the artificial construction of the data model.



I recommend the authors address these concerns. I'm willing to engage in a discussion with the authors and am flexible about adjusting my score.",See weaknesses.
5187wrocJq,5187wrocJq,Dice-GAN: Generative Adversarial Network  with Diversity Injection and Consistency Enhancement,Reject,dU2boUbe9U,ICLR.cc/2025/Conference/Submission4454/Reviewer_r6db,"This work proposes DICE-GAN, a single-stage text-to-image GAN to produce high-quality and high-diversity images with improved semantic consistency with text condition. The paper proposes two modules: The Diversity Injection (DI) module, which adds learnable noise to the image features for increasing diversity in generated images, and the Consistency Enhancement (CE) module, which allows the model to dynamically adjust the weights of different image features according to input text conditions for improved semantic consistency and fidelity.

--
The authors have provided the ablation study for the DI module on the CUB dataset with a small improvement on the IS metric. However, it is unclear if these gains will be present when scaling to larger datasets like COCO or Imagenet.
The presented argument for the novelty of the DI module is not new: ""injects noise several times during the image generation process, fuses the noise with the textual information, and incorporates a self-attention mechanism to help the generator maintain global structural consistency."" Further, the reported IS score is low compared to AttnGAN and DM-GAN on the MS-COCO dataset and is missing a full-scale comparison with Imagenet.","1. The idea of adding learnable noise in different training phases and correction with self-attention to improve generation diversity is novel and interesting.

2. The authors demonstrate improved performance on the IS and FID metrics on the CUB dataset and on the FID metric on the MS-COCO dataset.

3. The authors provide an ablation study demonstrating improvements in results by adding Diversity Injection (DI) and Consistency Enhancement (CE) modules.","1. The novelty of the work is limited. The idea of feature fusion in Eq 1 in the DI module is not novel and has been explored before[1,2,3] in the context of image generation. Further, the idea of masking features in a condition-dependant manner has limited novelty. 2. Lack of clarity in Sec 3.2 writing and Fig 4. The idea behind Conditional Channel Attention mask($M_c$) and Spatial Attention attention($M_s$) is unclear. The motivation behind generating masks from both average and max channels is also unclear. Further, quantities including $G^{c}_{max}$ and $G^{c}_{avg}$ are missing in Fig 4, making it difficult to understand figure pipeline. 3. The authors claim that Dice-GAN utilizes a single-stage model structure for improved performance but are missing comparisons with multi-stage methods, including StackGAN++[4]. 4. Missing ablation studies: - Why are two feature fusion layers are needed in the DI module? How was this hyperparameter determined? - How does learnable noise $\\sigma$ vary when going from lower to higher layers in the trained model? - Missing ablation on design choices in CE module on use of average and max features and conditional channel attention and spatial attention submodule. 5. The proposed method achieves a lower IS score on the MS-COCO dataset, and the authors argue that this is due to the Inception model used in IS computation being pre-trained on the ImageNet dataset. The authors should provide results on Imagenet or Imagenet subset to back their claims.

[1] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. 2, 5
[2] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. 5
[3] Peebles, William, and Saining Xie. ""Scalable diffusion models with transformers."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
[4] Zhang, Han, et al. ""Stackgan++: Realistic image synthesis with stacked generative adversarial networks."" IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1947-1962.",Please see weaknesses
5187wrocJq,5187wrocJq,Dice-GAN: Generative Adversarial Network  with Diversity Injection and Consistency Enhancement,Reject,9koizuwtds,ICLR.cc/2025/Conference/Submission4454/Reviewer_TDGk,"In this work, they propose the diversity injection and consistency enhancement module for text-to-image generation. This method contribute to produce high-quality images with increased diversity and enhanced semantic consistency based on text descriptions.","1. Enhanced Diversity: The Diversity Injection module injects noise and text vectors multiple times, ensuring a broad range of image outputs without sacrificing structure.

2. Improved Consistency: The Consistency Enhancement module dynamically adjusts focus on image regions, aligning visuals closely with text descriptions.","1. A comparison with recently proposed text-to-image generation models is needed. Not only should there be an analysis of issues with GANs, but also recent Diffusion models, along with performance comparisons. Is there a specific reason you only compared with ShiftDDPMs in the case of Diffusion models? Please provide a detailed response.

2. Please provide a detailed explanation of the table and figure captions.

3. Performance comparisons on diverse datasets are required. Additionally, besides IS and FID, comparisons with other performance metrics are requested (e.g., CLIP score).

4. The examples of qualitative results are too limited.

5. There is a lack of experimental analysis demonstrating the effectiveness of the proposed model structure.","Please, see the weakness."
5187wrocJq,5187wrocJq,Dice-GAN: Generative Adversarial Network  with Diversity Injection and Consistency Enhancement,Reject,CIPmMs0OpY,ICLR.cc/2025/Conference/Submission4454/Reviewer_66gE,"The manuscript introduces Dice-GAN which incorporates Diversity Injection and Consistency Enhancement modules to address critical challenges in generating high-quality, diverse images while maintaining semantic alignment with textual descriptions. Experimental results demonstrate that Dice-GAN outperforms state-of-the-art models on the CUB and MS-COCO datasets, underscoring its efficacy in enhancing visual quality and fidelity.","1. The introduction of the DI and CE modules marks a significant advancement in text-to-image synthesis. The DI module, which injects noise at multiple stages of generation, and the CE module, which integrates word vectors with hybrid attention, effectively improve both image diversity and semantic consistency.

2. This method achieves SOTA performance.","1. The manuscript lacks a detailed examination of the model's performance across varying levels of text complexity. Given that text descriptions can range from simple to highly nuanced, an analysis based on text complexity would provide stronger evidence of the model's robustness and its ability to handle diverse linguistic inputs.

2. The reviewer wants to see the experiment about computational efficiency.

3. The study does not thoroughly investigate the model's capacity to handle various textual attributes, such as color, size, and object positioning. A more focused evaluation of these specific attributes could offer deeper insights into the model's capability to accurately reflect detailed descriptive features and further demonstrate its adaptability.","1. How does Dice-GAN perform under different levels of input noise? Given the pivotal role of the DI module, understanding the model's sensitivity to noise levels could provide valuable insights into balancing image diversity and visual quality effectively.

2. What measures were implemented to ensure that the DI module does not excessively degrade visual quality due to noise injection? A detailed discussion on the strategies used to balance noise injection and maintain visual quality would be beneficial.

3. Does the CE module exhibit limitations in maintaining semantic consistency for longer, more detailed text descriptions? An analysis of the CE module's performance with nuanced and complex descriptions would provide a clearer understanding of its efficacy in handling diverse linguistic inputs."
5187wrocJq,5187wrocJq,Dice-GAN: Generative Adversarial Network  with Diversity Injection and Consistency Enhancement,Reject,oTjA9VcFZs,ICLR.cc/2025/Conference/Submission4454/Reviewer_zzQP,"he paper proposes Dice-GAN, an efficient attention-based text-to-image synthesis model. To enhance image diversity, a diversity injection module is introduced, incorporating noise and a self-attention mechanism. A consistency enhancement module, combining word vectors and a hybrid attention mechanism, improves semantic consistency. Experimental results on CUB and COCO datasets demonstrate Dice-GAN's superiority in image fidelity and diversity compared to existing approaches.","- Clear and well-organized presentation.
- Superior performance to other GAN-based methods.","- Limited novelty: While the diversity injection module is a contribution, the core idea of adding noise is not entirely novel.
- Lack of comparison to diffusion models: Given the current dominance of diffusion models in text-to-image generation, a more comprehensive comparison to state-of-the-art diffusion-based methods is essential to establish Dice-GAN's significance.
- Insufficient discussion of other generative models: The paper could benefit from a more in-depth discussion of how other generative models, such as flow-based models and StyleGAN, could be adapted or combined with Dice-GAN to further enhance diversity and quality.",Please see weakness.
X9OfMNNepI,X9OfMNNepI,MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses,Accept (Poster),6wqTKd3LtF,ICLR.cc/2025/Conference/Submission4321/Reviewer_zDBz,"This paper investigates the potential of LLMs to automatically discover novel and valid hypotheses in the field of chemistry. The study introduces a framework called MOOSE-Chem, which decompose the main research question into three smaller questions, respectively focusing on (1) the identification of inspiration papers; (2) the inference of unknown knowledge that is highly likely to be valid; and (3) the identification and ranking of hypotheses generated by the LLMs.","Originality:
Firstly, while LLMs have been utilized for scientific discovery in social science and NLP, this paper is the first to investigate their potential in chemistry. 
Besides, The MOOSE-CHEM framework employs a three-step approach to retrieve inspiration papers, inference valid knowledge, identify hypotheses and rank them, which hasn’t been used in previous research. 
Moreover, the use of the evolutionary algorithm to foster a broader diversity in hypothesis generation is also an innovation point.

Quality:
The paper includes extensive comparative experiments and ablation studies, providing a thorough evaluation of LLMs' performance. It also demonstrates the consistency between expert evaluation and automated evaluation of MS, enhancing the reliability of the results.

Clarity: 
The methodology is well-structured, and the three steps based on smaller questions are well-defined. Besides, detailed evaluation methods and corresponding formulas are provided for the three research questions, Q1, Q2 and Q3.  The practical implementation process is also relatively clear.

Significance:
The ability of LLMs to generate hypotheses can significantly accelerate the pace of scientific discovery by reducing the time and effort required for hypothesis generation. It can also open researchers’ minds for the application of LLMs in scientific discovery.","Firstly, using the same large language model to evaluate its own generated results may introduce bias. It is recommended to try using different LLMs to evaluate the results so as to guarantee the reliability of the results. For example, consider using models like LLaMa[1], Claude[2], Gemini[3], or other recent LLMs to compare outputs. If using the same LLM is necessary, you could collect hypotheses generated by humans and also have both experts and GPT-4 evaluate them. Then, compare their Hard/Soft Consistency Scores as well as distribution of MS with those of the hypotheses generated by the LLM.

[1] Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.
[2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024.
[3] Team G, Anil R, Borgeaud S, et al. Gemini: a family of highly capable multimodal models[J]. arXiv preprint arXiv:2312.11805, 2023.

Besides, during the evaluation process (as demonstrated in section 5.1), in the constructed dataset, other papers besides the ground truth inspiration papers may also be helpful for the current background. This step may also require further evaluation to obtain more reliable results. You might select several high-ranking papers that are not included in set I, particularly those that repeatedly appear in experiments with varying corpus sizes or screen window sizes, and conduct case studies on them.

Additionally, this paper only employs the titles and abstracts for retrieval, which might overlook inspirations that could arise from the detailed content of the articles. Sections like Conclusion or Discussion often contain insights into the paper's limitations and suggestions for future work. These elements are usually crucial for generating inspiration.

Moreover, the benchmarks mentioned in the paper contain only 51 references, which may not be sufficient to fully assess the effectiveness of the proposed method. You are advised to increase the number of samples, especially selecting literature from different chemical fields. In addition to traditional categories such as Analytical Chemistry, Organic Chemistry, and Inorganic Chemistry, broader research topics like Environmental Chemistry, Medicinal Chemistry, and Nuclear Chemistry can also be considered. Furthermore, Nature Chemistry includes interdisciplinary areas such as Bioinorganic Chemistry, Bioorganic Chemistry, and Organometallic Chemistry, which could be investigated as well. Of course, this may involve considerations related to the resource consumption of LLMs.

Finally, although the paper used chemistry PhD students for the evaluation, there is still subjectivity that may affect the reliability of the evaluation results. You might consider inviting more experts to participate in the evaluation and analyzing the consistency of the assessments from different human individuals.","1. There is a lack of experimental exploration for mutation settings.   I am curious about the impact of mutations on the final results. 
     For example, according to the ablation studies, what proportion of high-quality hypotheses can be obtained directly without mutations?

2. How can we explain that a smaller window size leads to better performance in the inspiration retrieval phase?

3. Did the authors replace GPT-4o for all comparison methods for generation? Please specify.

4. How helpful is it to add new dimensions of significance?"
X9OfMNNepI,X9OfMNNepI,MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses,Accept (Poster),GJxAuqP0OV,ICLR.cc/2025/Conference/Submission4321/Reviewer_rMit,"This work investigates whether large language models (LLMs) can autonomously generate novel and valid hypotheses in chemistry based solely on a research background. Specifically, the study explores if LLMs, when provided with a research question or background survey, can independently identify inspirations, synthesize hypotheses, and evaluate their quality. Building on discussions with chemistry experts, the authors hypothesize that most chemistry hypotheses can be derived from combining research background with relevant inspirations. They break down the central question into three key tasks: retrieving valuable inspirations, generating hypotheses, and ranking the quality of these hypotheses. To test this approach, the authors create a benchmark of 51 high-impact chemistry papers published in 2024. LLMs, trained only on data up to 2023, attempt to rediscover these hypotheses from the background and an extensive chemistry literature corpus. The results, achieved through a multi-agent LLM framework, show promising success, with many rediscovered hypotheses closely matching the originals and capturing core innovations.","1.	Generating research hypotheses is a complicated task, and the authors heuristically decomposed hypothesis generation into two steps: 

(1). inspiration retrieval, and 

(2). hypothesis refinement. In the hypothesis refinement step, the authors propose a novel “mutate and recombine” trick to help generate good hypotheses.

2.	The experiments to verify each of the research questions are well-designed with good quality.","1.	The introduction section could be written better and more clear. (a) It would be great if the authors could provide a summary of the major contributions of this work at the end of the introduction section. What are really the contribution to the field? (b) It would be great if the authors could briefly discuss why the decomposition of the major question is necessary, what’s the difference or connection between the proposed inspiration identification (the first step of the three) and Retrieval Augmented Generation (RAG).

2.	As the end goal is to rediscover the chemistry scientific hypotheses, the upper bound of “rediscovery” is the exact match of the original hypothesis, intuitively if we rank the original hypothesis with the generated hypothesis, the original hypothesis may be ranked at the top for most of the time. Experiment on this and analysis about the cases where the original hypothesis is not ranked at the top would be interesting to better reflect LLM’s ability to perform R(h).

3. Although the proposed approach constructed a benchmark as the basis to evaluate the performance, it is unclear to what degree the generated hypotheses can be trusted or reliable, not made up from baseless hallucination.",See the weaknesses listed above.
X9OfMNNepI,X9OfMNNepI,MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses,Accept (Poster),pojsHiILGK,ICLR.cc/2025/Conference/Submission4321/Reviewer_TMW5,"This paper introduces a framework for scientific hypothesis discovery
and generation using large language models (LLMs). The authors gathered
a carefully curated dataset of scientific papers where questions and hypotheses were extracted for evaluation. The authors propose ""MOOSE-CHEM"", a
method based on previous work ""MOOSE"" but with modifications
to adapt it for chemistry literature and that improve Overall
performance (refinement, mutation). The framework is also formally motivated.
The proposed approach is evaluated in each of the main three tasks
that compose it: P (i|b), P (h|b, i), and R(h). Overall the results show great
performance in the tasks with GPT-4o, with evaluations that include both 
experts and GPT-4o, and where various hyperparameters are tested (e.g. corpus size).","The paper is generally well-written, and in good English. The text is clear and
the authors did a good job guiding the reader through the motivation,
the derivation of the method and motivating each of the proposed steps
and experiments. The topic of the paper is very relevant and the results
are positive. Related work is well covered, and experiments are included
that compare the proposed method with previous work. Every claim made 
on the performance of the method is generally backed up with experiments.
I think the paper is generally a great and novel contribution to the field.","My main concerns with the paper are regarding the reproducibility,
clarity and discussion of the approach:

- Reproducibility: we note that the authors introduce a scientific benchmark,
along with a novel framework for hypothesis generation. However,
the authors do not provide access to the novel-introduced benchmark,
which hampers the ability to really discriminate the difficulty of the tasks
at hand. Additionally, this impedes the ability to reproduce the results or
for future work to compare the performance with the proposed method.
I would suggest the authors to make the benchmark data available if possible.
Similarly, no access to the source code is provided. While available source code is not
a requirement, for a method that relies heavily on LLMs prompting, it would
be very beneficial to at least have access to the prompts used in the
work, just for the sake of reproducibility.

- (Related to the previous issue): There are parts of the paper that lack
a formal definition, which again, hampers the 
ability to reproduce the work. For instance, the authors mention that they
introduce a novel ""Evolutionary Algorithm (EA)"" that mutates, refines and
recombines hypotheses, however, it is unclear how the mechanisms of
mutation, refinement and recombination are implemented. I would suggest 
the authors to provide, when possible more technical details on the
proposed method.

- While the work includes a fair comparison with previous work, the work
doesn't include any discussion on the limitations of the proposed method,
nor any future work. It would be very much appreciated if the authors
could provide a discussion on the limitations of the proposed method
to guide potential future work in the field.","This includes a list of minor issues, questions and suggestions:

- Through the paper, it is mentioned that the inspiration shouldn't be related
to the given background. Is this enforced in the method, or is this verified
somehow?

- The proposed method of Evolutionary Algorithm (EA) sounds to be related
to other methods in LLMs that try to improve the consistency/quality of
the answers, e.g. 
  - Self-Consistency (SC) (Wang et al., ""Self-Consistency Improves Chain of Thought Reasoning in Language Models"")
  - Universal SC (Chen et al. ""Universal Self-Consistency for Large Language Model Generation"")
  - Multiple Chain of Thought (MCT), Self reflection and/or self-correction approaches, etc. I thus believe that some related work could be included regarding this algorithm in the paper.


- On line 351, claim that ""is possible to be true"" that LLMs are able to create
novel knowledge. While the authors are correct here since it is not clear
whether this is true or not, could the authors provide other explanations
on why the LLMs are able to correctly find inspiration papers?


- Please if possible provide some description of how P (i|b), P (h|b, i), and R(h)
inputs and outputs look like. From the example provided, I can guess that
hypothesis is one paragraph long, but it would be great for the reader 
to have this stated clearly.

- I honestly don't fully understand the experimental setting reported in section 5.1.
What are the ""ground truth inspiration papers"" and how are these related to 
the 51 papers from the benchmark? In Table 3, does this mean that when a corpus size
of 150 is used, 120 papers are considered ground truth? 

- From Table 3, it almost seems like corpus size doesn't have much effect on the Hit ratio,
is this correct? If so, could you elaborate more in this regard?

- If I got the idea right, in the results of Table 1, if a screen window size is used, the corpus is 300 and 3 papers are selected for each window, then round 1 has (300/10)*3 = 90, round 2 has
(90/10)*3 = 27, and so on, is this correct? It would be more clarifying to include the amount of papers in each round in the table or in a separate table. It would be also clarifying 
to include a baseline comparison of the performance based on pure random screening (this is, select papers randomly).

- From the results reported in Table 7, it seems that the background survey (almost) doesn't have any 
effect on MS, however, this is somehow counterintuitive. Shouldn't the background survey improve performance as it happens in Q1?

- I would suggest the authors format the rounds in Table 4 to go along
with the direction of the text, this is, from left to right.

- Could you please clarify why there are NAs in Tables 3 and 4?

- On line 134: ""contrusted by multiple chemistry PhD students"".
For clarity, it would be great to know how many students.

- While it is not mandatory, given the stochasticity and prone-to-hallucinations nature of LLMs,
it is a good practice to use countermeasures to reduce hallucinations, e.g. sampling multiple times with majority voting,
or using different models for comparison. This is not an issue, but it would be
interesting to see how much variability the results reported in the experiments
are or if the method is robust to hallucinations."
X9OfMNNepI,X9OfMNNepI,MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses,Accept (Poster),hwdijx9eT7,ICLR.cc/2025/Conference/Submission4321/Reviewer_km2P,"This paper explores the potential of Large Language Models (LLMs) to generate novel, valid hypotheses in the chemistry domain by employing a multi-agent framework. It targets the rediscovery of scientific hypotheses using LLMs based on recent high-impact chemistry publications (since 2024), using a framework that separates tasks into three stages: (1) finding ""inspiration"" papers from a predefined literature corpus, (2) generating hypotheses by associating these inspirations with a background question, and (3) ranking hypotheses based on quality. To solve this, the authors proposed a multi-agent framework existing framework with an evolutionary algorithm to iteratively mutate hypotheses based on reward feedback (validness, novelty, clarity, and significance).","1. The use of LLMs for scientific hypothesis generation is a growing area, and extending this work to complex fields like chemistry holds practical importance if achieved meaningfully.
2. The use of beam search and evolutionary algorithms for hypothesis refinement seems simple and powerful, as it systematically explores multiple possibilities.
3. The framework does not require expensive training or fine-tuning.","1. There is a key fundamental assumption that the authors did not discuss here: reliance on general-purpose LLMs containing implicit domain knowledge. While this may be reasonable for powerful, large-scale models like GPT-4 or domains like social sciences, it is overly strong and may not apply to smaller or open-source models in this case. A deeper analysis (maybe more experiments) of how this assumption impacts performance across other LLMs (apart from GPT-4 or closed-source ones) is needed.
2. The assumption that LLMs have sufficient knowledge to connect novel and unrecognized scientific relations without any domain-specific tuning seems too bold. The model's effectiveness may degrade in more niche areas or specific research questions.
3. Much of this framework — multi-agent, hypothesis scoring, and the background-inspiration format closely resembles existing work in this area. Given the limited scale of benchmarking and experiments in this paper, the technical contribution seems limited for this venue.
3. The selection of inspiration papers seems unclear and might not align with typical challenges or open research questions in chemistry. A more detailed explanation of the selection process of those 51 papers and corresponding inspiration papers, including how relevance, significance, and novelty are ensured, is needed.
3. The paper presentation could be improved, particularly in the introduction and methodology. The technical details regarding probabilistic inference seem unnecessary.","1. Can the authors clarify how/whether MOOSE-Chem substantially differs from previous work in multi-agent scientific discovery systems? 
2. How would the authors envision MOOSE-Chem supporting a chemist in hypothesis generation beyond rediscovering existing knowledge? Have any of the generated hypotheses been practically tested, or do they align with ongoing chemistry research needs?"
Pdh1yMqwev,Pdh1yMqwev,Adaptive Priors from Learning Trajectories for Function-Space Bayesian Neural Networks,Reject,5ZngnCsuAD,ICLR.cc/2025/Conference/Submission4147/Reviewer_Z8Gp,"This manuscript presents a function-space prior that can be integrated into common deep neural network architectures, treating DNNs as Bayesian last-layer models. The mean and variance functions for the prior parameters are set using weight and feature statistics from the learning trajectory.",The manuscript focuses on the important aspect of using informative priors and applying them to efficient Bayesian inference.,"The important weakness of the work is limited novelty. The method combines the two approaches SWAG and T-FSVI. The extensions proposed in this work are limited in novelty. There are already some works (eg. [1]) in the literature that capture the weight priors from the phase-I of training the neural network models which provide better accuracy and uncertainty quantification, for larger models.
From the results, the benefits of phase-I are limited. 

[1] Krishnan, Ranganath, et.al. ""Specifying weight priors in Bayesian deep neural networks with empirical bayes."" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.",Please refer to comments in weakness section.
Pdh1yMqwev,Pdh1yMqwev,Adaptive Priors from Learning Trajectories for Function-Space Bayesian Neural Networks,Reject,Q6ryNCWd4P,ICLR.cc/2025/Conference/Submission4147/Reviewer_rfj3,"The paper introduces a new prior designed for functional variational inference in Bayesian neural networks. To achieve this, the authors propose an explicit function-space prior by modeling deep neural networks (DNNs) as Bayesian last-layer models. Initially, the method constructs a prior in weight space similar to the approach used in SWAG (stochastic weight averaging Gaussian). From this weight-space prior, the corresponding functional prior is derived. The effectiveness of the proposed method is demonstrated through validation on tasks including image classification, transfer learning, and UCI regression.","- The paper aims to tackle an important problem which is choosing sensible priors for Bayesian neural networks.
- The paper is well-written and provide good illustrative figures that enhance the clarity of method.","-	The contribution is marginal, as functional variational inference is a well-established area in the literature. Additionally, the approach of constructing priors using SWAG has already been explored by Shwartz-Ziv et al. (2022). 
-	The proposed prior is not valid from a Bayesian perspective, as it resembles a posterior. It is constructed from the training data and multiple trained models, as illustrated in Figures 2 and 5.
-	The idea presented in this paper appears to be very similar to the work found at https://openreview.net/pdf?id=AZVmYg3LvS. If this paper is a revised version, a comparison with Table 2 in that paper raises a concern: why are all the results for the baseline methods in Table 1 significantly worse, particularly for the NLL, ECE, and AUROC metrics?
-	From lines 47 to 54, the paper discusses the work of Rudner et al. (2022), highlighting that it restricts the randomness to the last layer due to GPU memory limitations, which can reduce the flexibility of Bayesian neural networks (BNNs). However, the paper ultimately adopts a similar approach, effectively presenting a simple extension of that work. This could be misleading to readers, as the paper suggests a novel contribution while largely replicating the limitations discussed.",Please see the third bullet in the Weaknesses
Pdh1yMqwev,Pdh1yMqwev,Adaptive Priors from Learning Trajectories for Function-Space Bayesian Neural Networks,Reject,1z3De23gWy,ICLR.cc/2025/Conference/Submission4147/Reviewer_i5t7,"This paper proposes a novel method for specifying function-space priors in Bayesian neural networks that can be integrated to widely used neural network architectures. The parameters of these explicit functions are determined using the weight statistics over the learning trajectory by leveraging stochastic weight averaging Gaussian (SWAG) method. The work is motivated to address the scalability challenges of function-space priors such as Gaussian process prior in large neural network architectures with many parameters and high dimensional datasets. The proposed approach utilize context feature to compute the function-space KL divergence without relying on external datasets. Empirical evaluation is performed with experiments on image classification and regression, demonstrating the proposed method is effective in improving uncertainty estimation.","* The paper addresses an important problem of specifying meaningful function-space prior in Bayesian neural networks based on the learning trajectories through data.

* The use of adversarial context feature to compute the functional-space KL-divergence without relying on external dataset is an interesting approach, which could make the model to be more robust.

*  Empirical results supports the proposed method. However, the empirical evaluation could be strengthened by including experiments where all layers of the model are Bayesian. Please see the comments in the next section.","* While the paper is motivated to address the scalability issues of traditional Gaussian process priors with high-dimensional datasets and large neural network architectures with many parameters, the experimental evaluation is limited to a model where only the final layer is Bayesian. This raises concerns about the scalability of the proposed approach. Additionally, prior works [Ovadia et al. 2019, Xiao et al. 2022] has shown that the quality of uncertainty estimation in models where only the last layer is Bayesian is inferior to other comparable methods, such as Deep Ensembles and Monte Carlo Dropout. I encourage the authors to evaluate with a model (e.g. ResNet-18), where all layers are Bayesian in their experiments. This analysis would address the scalability concerns of the proposed function-space priors.

* The paper provides literature review of related works, but do not compare with any of them in their experiments. For example, how does the proposed method compare with other methods that define priors through Empirical Bayes [Krishnan et al. 2020, Shwartz-Ziv et al. 2022]

[Ovadia et al. 2019] Ovadia, Yaniv, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. ""Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift."" Advances in neural information processing systems 32 (2019).

[Xiao et al. 2022] Xiao, Yuxin, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency. ""Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis."" In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 7273-7284. 2022.

[Krishnan et al. 2020] Krishnan, Ranganath, Mahesh Subedar, and Omesh Tickoo. ""Specifying weight priors in bayesian deep neural networks with empirical bayes."" In Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 04, pp. 4477-4484. 2020.

[Shwartz-Ziv et al. 2022] Shwartz-Ziv, Ravid, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and Andrew G. Wilson. ""Pre-train your loss: Easy bayesian transfer learning with informative priors."" Advances in Neural Information Processing Systems 35 (2022): 27706-27715.","* what is the motivation behind the adversarial context feature, specifically why FGSM attack method? Any other perturbations/corruptions to the features will serve the purpose of not relying on the external dataset?"
Pdh1yMqwev,Pdh1yMqwev,Adaptive Priors from Learning Trajectories for Function-Space Bayesian Neural Networks,Reject,Cypap5pC5x,ICLR.cc/2025/Conference/Submission4147/Reviewer_Lynm,"The authors propose an explicit function-space prior for DNNs that can be applied to common model architectures and overcomes limitations of other function-space BNN approaches. They utilize DNNs as bayesian last-layer models and leverage the learning trajectory of the SGD from a set of checkpoints. They demonstrates the effectiveness across several (image) datasets and models, offering a practical method for function-space BNNs.","- **Clear structure**: From a macro perspective, the paper is well-organized, with a logical sequence of topics for good readability. Each section transitions smoothly, making it easy to follow the progression of ideas.
- **Contribution**: The authors seem to present an interesting and promising approach to function-space BNNs, addressing limitations that have previously hindered practical applications. The results demonstrate improvements over existing methods, showcasing the effectiveness of the proposed approach. However, since I am not very familiar with this field, I must trust the authors in this case. 
- **Comprehensive introduction, limitations and related work**: The introduction effectively outlines the challenges of BNNs, particularly in the function-space domain, and establishes the motivation for this work. Section 3 is particularly well-structured, providing clear explanations of the limitations of BNNs and framing them within specific perspectives. The related work section makes clear distinctions between previous approaches and the authors' work.
- **Technical explanations**: The technical explanations seem well-organized, making the complex concepts easier to follow. 
- **Diverse experimental perspectives**: The experiments cover three distinct perspectives, providing a quite comprehensive evaluation of the proposed approach and supporting its versatility.","**Grammar and Presentation**

Overall, the paper appears rushed and has numerous grammatical/wording issues. While they are minor individually, these errors accumulate and do not achieve the standard expected for an ICLR paper. Below is a list of specific issues I observed during my review (I eventually stopped listing them all):

- L.43, 46, 47-48, L.54: Improper usage or missing articles (""a,"" ""the,"" pluralization issues).
- Abbreviations are repeatedly reintroduced (e.g., GP is introduced both in the intro and again at L.139).
- Abbreviation clarity: Some abbreviations have consistent capitalization (e.g., SGD), while ELBO only capitalizes ""evidence"".
- L.45: Should be ""scalability issues"" not ""scalable issues"".
- L.38: ""NNs"" is not introduced properly.
- L.39: ""facilities"" should be corrected to ""facilitates"".
- T-FVI is introduced for ""function-space variational inference"" but is inconsistently referenced (e.g., missing in L.54).
- L.409-410: ""However, unlike theses work"" is wrong. 
- Inconsistent capitalization (e.g., L.432 has ""Averaging"" capitalized).
- References: VIT transformer paper is referenced twice; some arXiv papers are cited instead of their final venues.
In related work: many citations are not properly bracketed (using \\citep) (e.g., L.401, 406, 417).


**Clarity**

Another major issue is the overall clarity, which makes the paper difficult to follow in detail:

- The contributions are not clearly stated, particularly with respect to differences in “…, and adaptively introduce different levels of uncertainty based on the function’s inputs” and “adaptively incorporate higher uncertainties for each function’s input”, which are used frequently. To me, it is particular unclear what “higher uncertainties” means and why/how it is useful. This terminology is used throughout the paper without sufficient explanation.

- ""Higher-dimensional dataset"" from the introduction needs clarification. What does this mean exactly, and why is it relevant to the discussion? This term is introduced for motivation but not addressed in the results. Are the datasets used in the results actually high-dimensional?

- The introduction would benefit from a brief explanation of why BNNs are advantageous in the first place.

- The phrase ""widely-used DNN architectures"" could be more precise. Does the method apply to all CNNs, transformers, or specific models? Also, clarify whether it applies to image classification models exclusively or to other classification tasks as well (only mentioned shortly in the abstract). Are the methods also applicable to other domains? I do not suggest do to more experiments but rather be more precise about the domain and maybe mention limitations/possibilities in others. The data types should be more clearly stated in the introduction. 


**Result presentation**

The results section is short and difficult to follow due to the extensive usage of the appendix. It may be more effective to focus on two of the perspectives or to create more space for a comprehensive discussion.

- Table 1: The abbreviations for methods are not introduced. Although explained in the appendix, the table itself is hard to interpret without this.
- Tables and figures in general not well-placed throughout the text (e.g., Figure 5 is referenced before Figure 4, and Figure 4 appears two pages before it is discussed). This disrupts the flow and makes it challenging for the reader.
- While the design decisions regarding datasets and models may be understandable for image domain experts, a brief motivation for these choices would be beneficial.
- It’s unclear why only a subset of methods was selected for Table 2 (transfer approach); why? 
- Table 2: Metrics like auroc-c and auroc-s are not properly introduced in the text.
- The sections on transfer learning and regression are very brief, which is especially notable as the transfer approach is a highlighted contribution. Metrics and abbreviations for the regression task are not well-introduced, causing further confusion..

**Short Conclusion**
- The conclusion is very short and lacks a discussion of future work. 
- While it’s good that limitations are noted, the impact on practical adoption should be discussed. Additionally, suggestions on how this might be improved in the future would add value.","I encourage the authors to address the weaknesses highlighted above, as well as to provide answers to the questions."
Pdh1yMqwev,Pdh1yMqwev,Adaptive Priors from Learning Trajectories for Function-Space Bayesian Neural Networks,Reject,1JjmpuoyqQ,ICLR.cc/2025/Conference/Submission4147/Reviewer_7Ju7,"In function-space VI there are two challenges: (1) how to specify meaning prior. (2) how to calculate the KL term in ELBO. This paper proposes to tackle the first challenge by first using SWAG to get a learning trajectory, and then constructing a tractable ""prior"" form based on it. Specifically, the authors only treat the last layer Bayesian and treat the feature extractor as fixed. The weighted sum of trajectory history is used to construct the feature extractor and the ""prior"". For the second challenge, the authors propose to use the adversarial context feature which removes the need of an external dataset for the context set. Experiment results show the proposed method has good uncertainty estimation in image classification and UCI regression.","- The adversarial context feature removes the need for an extra dataset as a context set.
- The constructed tractable ""prior"" is based on a posterior, and hence is capable of assign large variance for data differ from training set
- The paper is well-written and easy to follow.","- The proposed method is only for last-layer Bayesian neural network
- The construction of ""prior"" is computationally expensive and might be sensitive to the initialization","- I put a quote sign on prior because this is more like a posterior. By definition prior is your belief in the problem before you see the data, but here the ""prior"" is constructed using SWAG and is loosely connected with the posterior obtained by SWAG. I think there needs to be at least a few sentences discussing this.

- The construction of the feature extractor seems overly complicated, is it really necessary to do so? What happens if you just use the MAP?

- The prior variance for T-FVI (line 472) seems pretty large (5, 10, 50), as I'm not very familiar with function space VI, I'm curious is this very common for FVI?

- As the adversarial context feature is combined with the tractable prior, it's hard to see the effect of it. How much does it affect the results when compared with using extra dataset as context set?"
hDBrQ4DApF,hDBrQ4DApF,Real-Time Video Generation with Pyramid Attention Broadcast,Accept (Poster),6TtRQySjli,ICLR.cc/2025/Conference/Submission4140/Reviewer_R3bt,"The paper propose one technique, namely pyramid attention broadcast (PAB), which is a training-free for speed-up DiT-based video generation.
The idea is simple, which broadcast the attention results for the spatial, temporal, cross attention, which consumes more computation in video DiTs than that in CNN approaches.
The proposed PAB works well in OpenSora, Open-Sora-Plan, and Latte, demonstrating the effectiveness of the proposed method.","- The motivation is clear, which relies on one training-free method to speedup the video generation (inference).
- The attention redundancy in video DiTs are extensively studied, such as the attention cost, the attention patterns, the attention similarity and diversity. 
- With the extensive studies on attention redundancy, the authors proposed the pyramid attention broadcast. Moreover, different broadcast ranges for each attention are tailored based on the rate of change and the stability of each attention type.
- Experiments and ablation studies are conducted to demonstrate the effectiveness and efficiency of the proposed PAB metric.","- The proposed PAB is simple and effective. However, the optimal solution for PAB, such as how to determine the optimal broadcast ranges are not specified. According to the Figure. 8, the larger broadcast range, the low latency and low video generation quality. Such studies seed to be obvious to the authors. For the specific video generation model, such as open-sora, open-sora-plan, latte, how can we set the broadcast range. Please provide more discussions.
- The authors claimed that the broadcast the attention outputs other than attention scores. In my opinion, the effect seems to be the same or very similar for broadcasting the attention scores outputs. Please provide more explanation.
- One important questions is that the authors reveal that the attention outputs (spatial, temporal, cross attention) are redundancy. As such, they can broadcast the attention outputs with one training-free to speedup the video generation. In my opinion, such studies also means that the existing video generation model, such as open-sora, open-sora-plan, latte, learns a lot of redundancy information, which results in the redundancy attention outputs. Therefore, can PAB helps to learn effectively of the video generation model. Please make more discussions.",Please check the detailed comments in the weaknesses part.
hDBrQ4DApF,hDBrQ4DApF,Real-Time Video Generation with Pyramid Attention Broadcast,Accept (Poster),pVitDPevzL,ICLR.cc/2025/Conference/Submission4140/Reviewer_PKxB,"This paper proposes Pyramid Attention Broadcast (PAB), a method for DiT-based video generation that leverages a pyramid-shaped broadcast design. This design is inspired by two key observations: (1) the attention output differences are minimal in the middle 70% of the diffusion steps, and (2) spatial, temporal, and cross-attention differences decrease in a hierarchical, pyramid-like manner. Based on these insights, PAB sets different broadcast ranges for each attention type. The method is further extended to distributed settings, supporting multi-GPU parallelism (e.g., 8-GPU, 16-GPU) to further reduce generation latency. Experiments on VBench and WebVid using three types of DiT models demonstrate that PAB effectively reduces latency and accelerates video generation.","The method’s effectiveness is demonstrated on multi-GPU setups, achieving real-time video generation (e.g., within 2 seconds) on an 8-card H100 configuration. This efficiency is also validated across multiple open-source Video DiT models.","1. In Figure 4, the quantitative analysis of attention differences uses 30 inference steps based on Open-So ra, but the visualized attention differences cover 50 steps, creating inconsistency. Is this visualization based on Latte? Additionally, it’s unclear whether the attention feature difference plot is derived from an entire dataset analysis or a single video generation process.

2. Some comparisons appear to be unfair. For example, in Figure 7, the original generation using a single device is compared to an 8-device setup, leading to a claimed 10.5x speedup. This comparison may be misleading and should be clarified.

3. Key implementation details should be presented in the main paper. For instance, the most crucial setting, **PAB_246**, which defines the spatial, temporal, and cross-attention broadcast ranges, should be clearly explained rather than shown in supplementary materials.

4. Broadcasting attention is not entirely new. For instance, TGATE, an image generation acceleration approach, also uses iterative caching and reusing of self-attention (SA) for acceleration. This reduces the novelty of the proposed approach.

5. The qualitative results focus primarily on natural, static scenes. In real-world applications, generating videos with complex, dynamic actions, such as those involving people or animals, is more impactful. The qualitative evaluation scope is therefore too limited to showcase the method’s full potential in diverse scenarios.","The broadcast sequence parallelism in this work relies on DSP, so it’s unclear how much of the latency reduction is directly attributable to PAB alone. In Table 4 or in the main text, it would be helpful to further clarify PAB’s independent contribution to latency improvements."
hDBrQ4DApF,hDBrQ4DApF,Real-Time Video Generation with Pyramid Attention Broadcast,Accept (Poster),r8cQ0Iu80h,ICLR.cc/2025/Conference/Submission4140/Reviewer_6TRe,"This paper proposes Pyramid Attention Broadcast (PAB), which is a real-time, high-quality, and training-free approach for DiT-based video generation. Motivated by the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. This paper broadcast attention outputs to subsequent steps in a pyramid style. PAB demonstrates up to 10.5x speedup across three models compared with baselines, achieving real-time generation for 720p vdeos.","1. Figure 2 vividly illustrates the motivation, and the results appear highly plausible, with the spatial attention difference being the most significant, followed by the temporal attention difference, and then the cross attention difference.

2. The proposed broadcasting method appears to be straightforward, potent, and adaptable.

3. Broadcasting the entire sequence in parallel significantly enhances inference speed. It seems that sequence parallelism is exceptionally effective for accelerating the process.","* Why opt for the Mean Squared Error (MSE) metric to assess redundancy, and does the MSE metric accurately reflect the nuances of redundancy?

* It appears that utilizing broadcasting may lead to an increase in communication time. How can we quantify this additional overhead?

* Is it feasible to apply the PAB (Parallel Attention Broadcasting) technique to T2I (Text-to-Image) models, such as FLUX? Given that current T2I models are becoming increasingly large, there is a pressing need to accelerate models that exceed 5 billion parameters.","Overall, I find this to be a well-crafted paper. Nevertheless, I am curious if there might be a more suitable metric for assessing redundancy. Additionally, I wonder if it's possible to dynamically select layer-wise broadcasting in conjunction with distillation-based methods to facilitate real-time video generation."
hDBrQ4DApF,hDBrQ4DApF,Real-Time Video Generation with Pyramid Attention Broadcast,Accept (Poster),jJIvMyi6Od,ICLR.cc/2025/Conference/Submission4140/Reviewer_4GeG,This paper proposes an accelerated algorithm for video generation that is training-free. They observe the redundancy in attention calculations and adopts different acceleration strategies for different attention mechanisms. This paper gives detailed quantitative and qualitative results.,"1)This paper is easy to follow and the code is open source.
2)he acceleration effect is obvious from the qualitative and quantitative experimental results, 
3)The ablation experiment is thorough","1)This is essentially a cache technology, and the idea has been studied before, such as T-GATE and DeepCache.
2)The generalization of this method is questionable in scenarios with long video duration, complex textures, and drastic dynamic changes. For more details, please refer to Question 2 below.","1)In Table 1, the PAB range is different for different models. Besides, the diffusion timesteps in the Appendix are different, such as Table 6 in the Appendix? Does this mean that these hyperparameters need to be determined experimentally? Will the hyperparameters used for the same model be very different when trained with different data?
 2)When generating long (greater than 1 minute), complex texture, high dynamic range videos, will acceleration lead to more quality loss. Can you give some quantitative results on how video quality metrics change as video length and complexity increase
 3)In Figure 11, why does the attention-related operation take up such a large proportion of the time? Are there other operations besides normalization and projection? Can you give a more detailed breakdown of the attention-related operations？"
VsxbWTDHjh,VsxbWTDHjh,Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics,Accept (Poster),lLqYdK6tkn,ICLR.cc/2025/Conference/Submission3880/Reviewer_gPtN,"The paper introduces Fengbo, a novel computational pipeline designed to solve 3D partial differential equations (PDEs) specifically for applications in computational fluid dynamics (CFD). Utilizing Clifford Algebra, Fengbo employs an architecture that consists solely of 3D convolutional and Fourier Neural Operator (FNO) layers, effectively modeling the PDE solution process as a clear mapping from geometric representations to the underlying physics of the problem. Despite its relatively simple architecture with only 42 million trainable parameters, Fengbo demonstrates competitive accuracy, outperforming five out of six models previously proposed in the literature for the same dataset. The architecture achieves this with reduced computational complexity compared to graph-based methods while estimating both pressure and velocity fields. A notable feature of Fengbo is its transparency; the output of each layer can be visualized as objects and physical quantities in 3D space, thereby classifying it as a ""whitebox"" model. By integrating geometry with physics, Fengbo offers an efficient, interpretable, and physics-aware solution for addressing complex PDEs in CFD applications.","The paper presents a novel approach to solving 3D partial differential equations (PDEs) in computational fluid dynamics (CFD) through the use of Clifford Algebra. The use of a pipeline that incorporates only 3D convolutional and Fourier Neural Operator (FNO) layers within a Clifford Algebra framework is a creative combination of mathematical structures and neural network architectures. This uniqueness not only offers a fresh perspective but also has the potential to influence future research in both machine learning and PDE solving. The ability to visualize outputs as physical quantities in 3D space transforms the model into a ""whitebox"" system. This enhances the interpretability of complex models, which is increasingly important in scientific computing, as it allows for better understanding and trust in the results produced by neural networks.","1. Lack of Comprehensive Benchmarking
2. Absence of Generalization and Scalability Discussion
3. Potential Overlook of Limitations","1. Please compare with more recent developed methods
2. Please conducts more experiments on different datasets or PDEs."
VsxbWTDHjh,VsxbWTDHjh,Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics,Accept (Poster),SRVuuEwkhL,ICLR.cc/2025/Conference/Submission3880/Reviewer_TsLb,"This work is essentially the combination of ""Clifford Neural Layers for PDE Modeling"":

https://arxiv.org/abs/2209.04934

and FNO:

https://arxiv.org/abs/2010.08895

with the extension to computational fluid dynamics (CFD). 3D test cases are considered in this work, with the goal of prediction of the pressure and velocity fields. The complexity of the algorithm, error analysis, and visual comparison between the ground truth and prediction were conducted in this work.",High quality of writing and figures. Details explanations. A successful extension of Clifford Neural Layers for PDE Modeling to the Navier-Stokes equations for molding fluid dynamics.,"--> The novelty is limited. This is simply just another application of the Clifford Neural Layers for PDE Modeling paper.

--> In the literature review, the classes of PointNet and PointNet++ for deep learning of CFD have been missed. I suggest that the authors take a look and search on Google Scholar to find those articles and perhaps discuss them. Note that PointNet is suitable for unstructured grids and much lighter than graph neural networks since there is no connectivity between nodes.

--> I disagree with the claim of this manuscript saying that their proposed method is appropriate for irregular grids compared to graph neural networks or PointNet because they still convert irregular grids to Cartesian grids and this definitely introduced errors no matter how much you ""carefully"" convert these data.

--> Following my previous comment, I believe the information listed in Table 4 is misleading. FNO can be used for irregular geometries if one uses geometric transfer. See the following paper:

https://www.jmlr.org/papers/v24/23-0064.html

On the other hand, the proposed method is not inherently designed for irregular geometries, similar to CNNs and FNOs.

--> As a minor comment, it is better to write L2 as the $L^2$ norm","--> In Table 2, for FNO, the test error is lower than the train error, how is this possible?

--> In Eqs. 20 and 21, the loss function is a combination of the relative $L^2$ norm with the absolute $L^1$ norm. Mathematically, it does not seem reasonable. How do you justify that?

--> I had some concerns, listed in Weakness. Please address them. Thanks."
VsxbWTDHjh,VsxbWTDHjh,Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics,Accept (Poster),g5Qm5yECEj,ICLR.cc/2025/Conference/Submission3880/Reviewer_E7Y6,"The paper presents a new deep learning pipeline to predict the velocity and pressure fields in 3D CFD simulations. The algorithm relies on the use of Clifford algebra layers, a mathematical construction which enables the processing of n-dimensional multivector fields. First, the input fields are upsampled and mixed to translate local to global features. Then, the global information is processed with a FNO-like frequency learning algorithm in a regular voxelized domain. Last, the processed information is decoded to the desired outputs: pressure and velocity fields. This pipeline is tested in the ShapeNet Car and Ahmed Body benchmark datasets and compared with other state-of-the-art architectures.","* The Clifford algebra architecture is an interesting inductive bias and is generalizable to more complex multidimensional fields. 
* The method has less trainable parameters compared to other techniques.
* The last blocks of the algorithm can be interpreted as velocity and pressure fields, so one can have a visual intuition of how the network learns the final  prediction stage.","* The method is limited for steady-state simulations, and only tested for low density/viscosity fluids.
* The performance of the method is very similar in error compared to existing techniques.
* The voxelization of the space might be very inefficient with more complex geometries.","* Line 245: Why is the bivector component left blank in the case of the velocity? The surface information is specially important for the velocity field as it determines the boundary layer dynamics, crucial for drag/lift analysis in aerodynamics. 
* The method's claim of being a white-box model may be overstated. While the final layers may provide some intuitive information during the decoding stage, the true learning of the underlying physics primarily occurs in the 3D Clifford FNO block, which still operates within a latent space. Furthermore, how practical is the interpretability of the final layers beyond offering intuition about the predictions? Have the authors observed any relevant phenomena, such as different energetic modes of the pressure and velocity field across each decoding stage? Without any analysis of this regard, the interpretability claim remains largely qualitative rather than quantitative and has little practical use.
* Table 2: Why MeshGraphNets are not tested for the ShapeNet car dataset? The original paper predicts both the pressure and the momentum field of the fluid.
* Line 388: ""[...] being the only architecture reported able to do so while jointly estimating the scalar pressure field and the 3D velocity field"". Any of the reported architecture can be trivially modified to include an additional output (the velocity field), so I don’t see this as a specific advantage of the proposed methodology.

Final comment: The presented paper proposes almost the same methodology as GINO (""Geometry-Informed Neural Operator for Large-Scale 3D PDEs"" paper) but changing the GNO layers with Clifford algebra layers. From an accuracy and novelty perspective, the results offer only incremental improvements. While the interpretability of the final layers might provide some vague intuition, the paper fails to extract any substantial insights from an engineering or mathematical perspective. The only real novelty of the paper is the reduction in number of parameters, which in my view is insufficient for the standards of this venue. For these reasons, I'd rate this paper as marginally below the acceptance threshold."
VsxbWTDHjh,VsxbWTDHjh,Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics,Accept (Poster),u6CsGPm3lT,ICLR.cc/2025/Conference/Submission3880/Reviewer_sKmQ,"The paper introduces Fengbo, a neural operator pipeline that uses Clifford Algebra to solve 3D PDEs in computational fluid dynamics. Fengbo leverages 3D convolutional and Fourier Neural Operator layers within a Clifford Algebra framework to map 3D geometries to physical fields, such as pressure and velocity. It demonstrates competitive accuracy on CFD datasets with fewer parameters and lower computational complexity, while offering interpretability.","* The paper introduces a novel approach by embedding the entire architecture in Clifford Algebra, which allows for a unified treatment of geometric and physical data, enhancing model interpretability and preserving geometric relationships.

* This model provides white-box interpretability by representing intermediate outputs as multivectors, which correlate with physical quantities in 3D space.","* No comparison with the most advanced deep learning-based methods (e.g. transolver, etc.).

* The main results in the paper show better results with fewer parameters, but not the best performance, and it would be better if the performance could be compared with the same parameter Settings. 

* The paper was validated on a limited dataset, and it is hoped that it can be validated on more diverse datasets and tasks (e.g., point cloud, structured mesh, regular grid, etc.), which can be referred to transolver's experimental design.

Minor comments:
* L257""... were the range of the summation of l,m,n isspecified by the kernel size and cin,cout are the ..."" has some grammar and typo issues in line 127.

* The definitions of metrics in lines 346 to 363 (formulas (11),(12)) are inconsistent (groud truth and estimated results are used in the denominators respectively).

I would consider revising my rating if the author is able to address my questions and effectively improve on the areas I identified as weaknesses.",Please refer to weaknesses section for questions.
VsxbWTDHjh,VsxbWTDHjh,Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics,Accept (Poster),AJuGn1tKm9,ICLR.cc/2025/Conference/Submission3880/Reviewer_FKFg,"This submission targets the learning of 3D flow fields together with pressure distributions by using Clifford algebra. This approach has been proposed in previous work, and the submission at hand extends its implementation, and add geometry and physics blocks that seem to primarily aim for up- and down-sampling.

As the paper is largely extending previous work, it does not include and evaluate simpler cases, but directly focuses on 3D flows. Results are shown for flows around obstacle geometries from ShapeNet cars, and the Ahmet body. 

The results are mixed: in some cases the proposed architecture seems to perform well, but is outperformed by previous work in others. Especially the classic Unet still seems to do a fairly good job, and probably has a much lower computational workload (and simpler implementation).

Overall, I like the direction of the paper: 3D flows are definitely a challenging topic, and important for practical applications. At the same time, the proposed method does not seem overly convincing to me: it is very tailored towards 3D flows, and seems somewhat incremental given the previous work on Clifford based GNNs. With the results in the paper, I would be hesitant to try this approach, and correspondingly, I also find it difficult to really argue for accepting this paper to ICLR. I think with the current, somewhat narrow scope on 3D pressure (+velocity) the submission would be better suited for a more specialized conference or journal.","I think the paper has the following strong points:
- it targets non-trivial flow scenarios in three dimensions
- the ShapeNet cars and especially the Ahmet body are interesting use cases
- a nice range of baselines models is compared to
- the underlying theory is complex","At the same time, the submission has weak spots:
- the approach seems to be specialized to 3D flows, and I don't see how it would naturally extend to other problems
- the gains in terms of accuracy seem to be mild, which is a pity given the complexity of the approach
- the Clifford algebra comes from previous work, and I have to admit that I don't find it intuitive to work with
- the properties of the baselines are not fully clear (e.g., parameter counts are missing)","What is the bottleneck that caps the model size at 42M parameters? This does not seem overly large for 3D problems.

Minor, but why are the sizes of the other models in table 2 not listed? How many parameters did they have?

(Very minor recommendation, it's a good idea to give intuition with figures like fig. 2 about the Clifford algebra setup, but figure 3, for example, did not add much information.)"
WRLj18zwz6,WRLj18zwz6,A Manifold Perspective on the Statistical Generalization of Graph Neural Networks,Reject,e4fbmynPKS,ICLR.cc/2025/Conference/Submission3844/Reviewer_mBN7,"This paper discusses the generalization of GNNs from a manifold perspective. In this setup, the graph is considered as sampled from an underlying manifold equipped with a Hausdorff probability measure. 

The authors aim to investigate the generalization gap between the following: A **GNN** trained on a sampled instance of the graph, and an **MNN** (Manifold Neural Network) that has the same structure and parameters as the GNN mentioned above.
Here, GNN and MNN specifically refer to low-pass filters.

The core of the paper lies in two theorems, which provide generalization gaps for both node-level and graph-level tasks. The main findings of the theorems are:
- The GNN trained on the sampled graph can generalize to the MNN on the original manifold.
- The trained GNN can generalize to other graphs sampled from the same manifold.
- The generalization error of the GNN is primarily influenced by several **key factors**: the number of nodes, the manifold dimension, and the spectral continuity constants $C_{L}$. Practically, this implies:
  - As the manifold dimension increases, more nodes are required to obtain a model with good generalization.
  - (**Important**) A larger $C_{L}$ represents stronger discriminative power but results in weaker generalization. This is a critical trade-off to consider when choosing a model.

The paper uses experiments to demonstrate the impact of these key factors.","1. The paper is well-structured, and the experiments respond well to each part of the theorem.

2. The paper presents two theorems, which contribute to the study of GNN generalization.

3. The two theorems provide practical guidance for the selection of filters.","1. The relationship between the experimental and theoretical settings is not clearly explained. In the appendix, the authors mention that the experiments use a graph convolutional layer (GCN). **Does GCN fall under Definition 4** as required by the theorems? Moreover, does the input used in the experiments meet the requirements of **Definition 1**? Also, GCN does not seem to match the filter structure that is analyzed in the paper.

2. The explanation of MNNs is insufficient for me. Are MNNs implemented by sampling graphs and running GNNs on them?

3. If the answer to (2) is yes, then shouldn't the paper provide a generalization bound between two separate runs of MNNs (effectively GNNs on two different sampled graphs)?","Please refer to the Weaknesses section. 

If my questions can be addressed, I will consider increasing the score :)"
WRLj18zwz6,WRLj18zwz6,A Manifold Perspective on the Statistical Generalization of Graph Neural Networks,Reject,54o2HmLtJb,ICLR.cc/2025/Conference/Submission3844/Reviewer_wcCm,"This work studies the generalization bound of GNN by comparing the bound of the corresponding manifold neural networks. The results show that the generalization gap decreases as the number of nodes increases or the spectral continuity constant decreases. The experiments justify the theory, especially including the discussion about the regularize of the training.","1. The paper is clear and well-written.

2. The theoretical analysis is solid and correct based on my understanding.","1. Some definitions are not clear enough. For example, it is better to relate Eqn 16 to the expectation of Eqn 15 so that the generalization gap definition in Eqn 17 is aligned with the classical definition. Please let me know if my understanding is incorrect.

2. The contribution of the trade-off between discriminability and generalization gap is weak. I find that better discriminability is interpreted as a smaller regularizer, which is indicated by a larger continuity constant. This explanation is quite indirect. A stronger analysis of discriminability could be a discussion based on some quantity to measure the discriminability.

3. The theoretical contribution and the novelty of this work are not very clear. It is better to provide a table or a list to show the comparison between this work and existing theoretical works.","In Eqn 7 and 8, there are $K_\\epsilon(||x_i-x_j||^2/\\epsilon)$ in both first steps. Is it a typo, otherwise isn't it a notation abuse of two different kernel functions?"
WRLj18zwz6,WRLj18zwz6,A Manifold Perspective on the Statistical Generalization of Graph Neural Networks,Reject,Zawjecx47s,ICLR.cc/2025/Conference/Submission3844/Reviewer_fFcq,"This paper considers the generalization performance of GNNs when the data lies in a manifold. Consider the problem of predicting target value $g$ from a one-dimensional graph signal $f$ on a compact Riemannian manifold $\\mathcal{M}$. Two types of models are considered: the Manifold Neural Networks (MNN) induced from the Laplace-Beltrami operator on $\\mathcal{M}$, and the GNN constructed on a finite number of i.i.d. sampled data points using a graph kernel. 
This paper evaluates the generalization performance of GNNs when the similarity graph is a Gaussian kernel-based graph and a $\\epsilon$-graph, respectively. Here, the generalization performance is defined as the difference between the statistical risk in the sense of the average risk on $\\mathcal{M}$ when applying the MNN and the empirical risk using the GNN on a finite number of points $\\{(x_i, y_i)\\}$. In particular, the rate of generalization gap with respect to the number of sampled data points and its dependence on the dimension of the underlying manifold $\\mathcal{M}$ are derived. Furthermore, the generalization gap for node classification tasks is derived when we assume that data points with different labels are on different manifolds.
In numerical experiments, GNNs are applied to node prediction tasks on synthesis and real networks, and node classification tasks on point clouds. The dependence of generalization on data points and spectral continuity constants is also evaluated.","- A single methodology can be applied to the analyses of both regression and classification tasks. Furthermore, for classification, the analysis can be extended to the case where data points with different labels are on different manifolds.
- This paper provides detailed background knowledge on spectral theory on compact Riemannian manifolds, in particular, the Laplace-Bertlami operator, and MNN based on it. It is intended for those who are not familiar with the field, making it accessible to them.","- I have a question about the significance of Theorem 1, especially about the dependence of the upper bound on the spectral continuity constant.
- If I understand correctly, the definition of generalization performance in theoretical analysis differs from that in the usual statistical learning theory.
- I have a question about whether the generalization gap in numerical experiments appropriately corresponds to that in the theoretical analyses.","- L.261: Is it correct to assume that the feature vector (input to the first layer of GNN or MNN) at each point $x$ is 1-dimensional because the input signal $f$ belongs to $L^2(\\mathcal{M})$.
- l.293: I think $GA$ is different from the generalization gap in the usual statistical machine learning theory. Usually, the generalization gap is the difference between the empirical and statistical risks of the same model (i.e., hypothesis). In contrast, this paper uses GNNs for the empirical risk and MNNs for the statistical risk: when a new data point $x$ is sampled, the prediction value for $g(x)$ is computed using MNN $\\Phi(\\mathbf{H}, \\mathcal{L}_\\rho, \\cdot)$ not GNN $\\Phi_G(\\mathbf{H}, \\mathbf{L}_N, \\cdot)$. This setting may be justified if we interpret $\\mathcal{H}$ as a hypothesis space. However, I do not think it is a very popular setting.
- l.308: *Theorem 1 shows that the generalization gap decreases approximately linearly with the number of nodes $N$ in the logarithmic scale.*: I want to clarify what this means mathematically. Does it mean that the main term of $\\log (GN)$ is $\\tilde{O}(-\\log N)$, where $\\tilde{O}$ is the O-notation that ignores logarithmic orders.
- l.311: *More importantly, we note the bound increases linearly with the spectral continuity constant $C_L$ [...].*: I have a question about this claim. In Theorem 1, the left-hand side $GA$ is uniform with respect to the hypothesis space $\\mathcal{H}$, that is, independent of the specific hypothesis $h\\in \\mathcal{H}$. On the other hand, the upper bound on the right-hand side depends on the specific hypothesis $h$ (or specific MNN/GNN) through spectral continuity constant $C_L$. In particular, let $C_\\mathcal{H}$ be the infimum of the spectral continuity constant over $\\mathcal{H}$: $C_{\\mathcal{H}}:= \\inf_{h\\in \\mathcal{H}} C_L$. The inequality replacing $C_L$ with $C_\\mathcal{H}$ on the right-hand side must also hold and is a tighter uniform bound independent of specific hypothesis $h$.
- l.378: In Figure 4, the linear fitting is applied only on a region with a training accuracy of over 95%. However, I wonder whether this is justifiable. In fact, since the previous argument for l.311 provides a tighter uniform bound on the hypothesis space $\\mathcal{H}$, the inequality holds regardless of the training accuracy.
- l.378: Related to the previous, as the theoretical analysis applies to untrained GNNs, I suggest conducting numerical experiments using untrained GNNs to see if the implication from the theory is valid numerically for them.
- l.474: *In all cases, we vary the number of nodes in the training set by partitioning it in $\\\\{1, 2, [...], 1024 \\\\}$ partitions when possible.*: Does this mean that when the whole dataset is divided into $k$ partitions, the training dataset is one of those partitions? If so, how is the test dataset chosen if the number of partitions is one? I also want to check whether this experiment is a transductive problem, i.e., the test dataset is available during training, and the training data is available during the test.
- l.478: In the theoretical analysis, the generalization gap is defined as the difference in risk between MNN and GNN. However, interpreting the generalization gap in numerical experiments in that way is questionable for the following two reasons: first, numerical experiments use GNN for both training and testing. Second, assuming my understanding of the previous question about l.474 is correct, it is a transductive problem, which is different from the assumption in the theory. I want to ask the authors why it is justifiable to interpret the generalization gap in the numerical experiments as $GA$ in the theoretical analysis.
- l.497: If we want to make a statistical claim that there is a linear correlation, it is more appropriate to apply hypothetical testing such as t-test and ANOVA rather than using an arbitrary threshold of 0.95.
- l.506: In the experiment in Figure 7, it is claimed that the relationship between the spectral continuity constant $C_L$ and the generalization gap is verified. However, the experiment does not measure the value of $C_L$ is not directly and assumes that increasing the regularizer reduces $C_L$, which shoulde be verified.
- l.506: Even if we do not take the tight uniform bound as we discuss in the question about l.311, I still have a question about the validity of Theorem 1 in numerical experiments. In Theorem 1, the dominant term in the upper bound, when $N$ is sufficiently large, is the last term $F^LC_3(\\log N/N)^{1/d}$. However, this term is independent of the value of $C_L$. It implies that the generalization gap should be almost independent of $C_L$ when the number of nodes is large; this is not the case in Figure 7.

**Minor Comments**
- l.161: $P$ is undefined.
- l.165: $M$ is undefined.
- l.184: I think it is not appropriate for the main text to refer to Equation (23), which appears only in the Appendix.
- l.187: The fact that the Laplacian eigenvalue is discrete and, at most, countable should be proved. We can derive it from the fact that $\\mathcal{M}$ is compact.
- l.329: Since $\\rho$ indicates the density of the manifold $\\mathcal{M}$ in the previous sections, it is not recommended to use $\\rho$ as a general symbol for the Laplace operator such as $\\mathcal{L}\\_{\\rho, k}$. The same applies to the graph Laplacian $\\mathbf{L}\\_{N, k}$ in l.333.
- l.334: Figure 3 is not referenced from the main text.
- l.359: $\\epsilon$ is undefined in this section. Also, the dependence of $\\epsilon$ on $k$ should be made explicit."
WRLj18zwz6,WRLj18zwz6,A Manifold Perspective on the Statistical Generalization of Graph Neural Networks,Reject,0og60f6ygs,ICLR.cc/2025/Conference/Submission3844/Reviewer_Juiw,"This paper investigates the statistical generalization capabilities of Graph Neural Networks (GNNs) from a manifold perspective. The authors propose a theoretical framework proving that when graphs are sampled from manifolds, the GNN's generalization error bound decreases logarithmically with the number of nodes and increases linearly with the spectral continuity constants of filter functions. The theory applies to both node-level and graph-level tasks, providing guidance for practical GNN design. The theoretical results are validated through eight real-world datasets.","1.First to analyze GNN generalization from a manifold perspective, providing a new theoretical viewpoint that is more relevant to practical applications compared to previous work.
2. Not only presents theoretical framework and generalization bounds, but also validates theoretical results through extensive experiments and provides practical guidelines for GNN design.","- The paper requires input signals to be bandlimited (Definition 1) and filters to be low-pass (Definition 4). Please clarify that these assumptions are not difficult to satisfy in practical applications.

- The proposed Gaussian kernel-based graph construction method (Definition 2) requires computing the full adjacency matrix, which is computationally expensive for large-scale graphs with O(N^2) time complexity.

- The theoretical results heavily depend on the choice of spectral continuity constant C_L, but the paper lacks concrete guidance on how to select appropriate C_L values in practice.",Please refer to weaknesses.
WRLj18zwz6,WRLj18zwz6,A Manifold Perspective on the Statistical Generalization of Graph Neural Networks,Reject,2xhKkWtxR5,ICLR.cc/2025/Conference/Submission3844/Reviewer_RtDJ,"This paper derives generalization bounds for spectral graph neural networks, based on the assumption that the graphs are obtained from a sampling procedure from an underlying manifold. The obtain generalization bound decreases with the number of nodes, and increases with the number of features and network depth. Experimental results corroborate the dependence of the generalization gap on these factors.","The paper is overall well written, and as far as I have checked the technical details seem to be correct. The experimental evidence that the generalization gap decreases as the number of nodes increases is interesting.","I don't like the assumptions made in this paper so much. It feels to me like they make the proof rather straightforward, but are rather far from reality. In particular, how common are the GNNs defined in equation (2)? How related are the GNNs in equation (2) to GCN, for which the experiments were carried out in practice? How likely is the assumption that the graphs are sampled from a manifold, for applications of interest (e.g., for the Cora, citeSeer datasets), and why is this assumption more reasonable than the assumption that the graphs were sampled from graphons as in previous work?

Overall, in comparison with the related work of Levie and Maskey cited in the paper,  I don't feel there is a significant upside. My impression that Maskey and Levie also showed generalization gaps which decrease with the number of node, and increase with model complexity (which can be equated with model complexity and  depth in this paper). True, these results were obtained for different GNNs and with a different limit model, but not clear to me why the manifold model, and spectral GNN, are  any ""better""?","Main issue is a more detailed discussion of Maskey and Levie- if you would like to try to convince me what your contribution is over those papers, I will be happy to listen.

In addition, here are some minor points which you don't have to discuss in the rebuttal, but could address in your next version of the paper:

* Line 155 instead of ""with"" should be ""from""

* In definition 4, Is the low-pass filter defined with the same d as the manifold? If so why? 

* Line 280 you say assumption I makes sense, you can also explain why assumption II makes sense (e..g I believe it is satisfied by norm-based losses like MAE and RMSE)

* Line 370: not sure I understood your point about a single graph. As far as I recall, an alternative where several graphs were sampled from each manifold was not discussed.

* Mention in main text which GNN you use. I understand from the appendix that it is GCN."
fGdF8Bq1FV,fGdF8Bq1FV,Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors,Accept (Spotlight),l9iyc7REB6,ICLR.cc/2025/Conference/Submission3622/Reviewer_c35E,"This paper studies the setting of representation learning, where the task is to jointly learn a good representation and a good predictor. PAC-like generalization bounds are provided, which crucially depend on the complexity of the latent representation. Based on these theoretical insights, the authors then suggest a new regularizer term, which shows promising results in some applications.","- Novel generalization bounds for representation learning: The paper derives generalization guarantees for representation learning. To my knowledge, this setting has not been studied in depth previously. I found the remarks about these bounds interesting, particularly the fact that they do not depend on the encoder component.
- Novel regularization term based on the theoretical analysis: As is often the case with generalization bounds, they can directly inspire regularization terms or other modifications to the learning pipeline. The authors propose a new regularization term for representation learning. In experiments with image classification datasets, the introduced method is shown to compete favorably with other recently introduced methods for this task.","In general, I found the presentation of some results in the paper to be a bit lacking, while the discussion of related work seems somewhat insufficient. The authors discuss prior work in the introduction, but the discussion is high-level and mainly directed at 'experts' in the field. An additional (perhaps small) section covering related work would have served this paper well. Additionally, the main results of the paper (Theorem 1 and Theorem 2) are presented without much (if any) discussion on the techniques used in their proofs. Another example is in line 305, where the authors mention and emphasize the 'geometrical compression phenomenon' but do not describe it further. In various places, the manuscript would benefit from additional polishing.","I have some concrete questions:
- What is the ""joint"" (line 1188, Appendix C.3) learning procedure that you followed in the experiments? Can you provide more details on the loss used, etc?
- How do the proof techniques compare to the ones used by SZK23?"
fGdF8Bq1FV,fGdF8Bq1FV,Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors,Accept (Spotlight),kKdc83UNLg,ICLR.cc/2025/Conference/Submission3622/Reviewer_6GSR,"This paper provides generalization guarantees for representation learning algorithms by deriving in-expectation and tail bounds on generalization error. The authors develop these bounds based on the MDL principle, using a data-dependent Gaussian mixture prior as a regularizer. By establishing bounds related to the relative entropy between representations from training and test datasets, the method aims to leverage the simplicity and structure of the encoder for improved generalization.","1. The paper provides non-vacuous generalization bounds for representation learning.

2. The proposed regularizer has been validated in many image datasets.

3. Theoretical results have been rigorously presented and supported.","1. The Gaussian mixture prior introduces additional computational overhead, especially during training, which may make the approach less practical for very large datasets.

2. Although the emergence of a weighted attention mechanism is highlighted, the paper could benefit from a more detailed analysis of this component and its role in generalization.

3. The empirical validation is limited to standard classification datasets. Testing on real-world tasks beyond classification (e.g., transfer learning, semi-supervised learning) would strengthen the claims of generalization benefits.","1. What might be the potential limitations of the current method? What could be the future work?

2. Are there any constraints on the applicability in real-world scenarios introduced by relying on a Gaussian mixture prior?"
fGdF8Bq1FV,fGdF8Bq1FV,Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors,Accept (Spotlight),BfjDmbjyNR,ICLR.cc/2025/Conference/Submission3622/Reviewer_ce5p,"The paper studies the generalization error of representation learning, in which we have the encoder-decoder model. The authors establish a new generalization bound based on the minimum description length (MDL) of a symmetric prior and the induced representation distribution of the encoder. The paper explains how to choose the prior using a mixture of Gaussians, and connects the generalization bound to the regularizer of the optimization problem, in which the prior and the representation distribution are learned jointly. The authors examine their method in simple datasets and model architectures, and in both lossy and lossless scenarios.",The paper is well-written and the motivation is clear. The authors improved the generalization bound of the encoder-decoder representation learning from $\\sqrt{\\text{MDL}(Q)/n}$ to $\\text{MDL}(Q)/n$. The proposed symmetric prior using mixtures of Gaussians is relatively practical and easy to implement. The idea of adding the generalization bound as a regularizer and learning the prior and the induced distribution of the encoder together is interesting.,"While the authors have improved on the previous generalization bound, the technical work and the idea of regularization are heavily based on [1]. However, I think the mixture of Gussians is a nice addition.

The lossy generalization bound in Section 3.3 seems a bit incomplete to me. While the authors explain what lossy means, I could not follow how it results in Eq 14 and 15. I also checked the appendix, but could not find anything related to this section. I would appreciate it if the authors could elaborate on this section more, or add a theorem and the proof either in the main text or the appendix. 

I also think there are some parts that the author can improve the writing or the intuition of their work (see questions).

[1] Minimum Description Length and Generalization Guarantees for Representation Learning, Sefidgaran et al. 2023","1. In section 3.3, the bound in Eq 14 seems to be similar to [1] with $\\sqrt{\\text{MDL}(Q)/n}$ rather than $\\text{MDL}(Q)/n$. Do the authors have any intuition/proof on why this change happens in the lossy setting?

2. Is there any intuition behind $h_C$ in Eq 6?

3. In section 4.1, M denotes the number of mixtures. Do the authors suggest any systematic way of choosing M? Or is it a hyperparameter that needs to be tuned?

Some typos also can be addressed:
- Line 342, an addition | in KL
- Eq. 17, the index of X should be batch size and not beta"
fGdF8Bq1FV,fGdF8Bq1FV,Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors,Accept (Spotlight),C78GoWolpx,ICLR.cc/2025/Conference/Submission3622/Reviewer_w8tc,"In this paper, the generalization performance of representation learning algorithms (where the models consist of an encoder and a decoder) is considered. Specifically, bounds based on the compressibility of the latent variables of the models are derived. On the basis of these bounds, regularizers using priors with a Gaussian mixture model are devised. These regularizers are numerically shown to provide improved performance compared to prior work for some tasks.","The general problem under consideration is interesting, and the twin goals of explaining generalization and finding ways to improve it are valuable. Interesting connections are drawn between, e.g., the presented bounds and classical compressibility, and between the form of the prior updates and attention. The way in which the regularizer is constructed is, as far as I am aware, new and yields promising results. The numerical experiments compare to several prior alternatives from the literature.","The presentation is not always clear. For instance, one of the key concerns about the validity of the data-dependent prior, as far as I understood, is to guarantee that it is symmetric with respect to the training data and a ghost sample. However, the procedure seems to only depend on the training data, and it was unclear to me whether symmetry was preserved. Noise was added at some stages to “partially” preserve symmetry, and allusions were made to prior work which allows for conditions to be only partially fulfilled (e.g. differential privacy), but the results were discussed for exact symmetry.

The relation between prior literature and the presented generalization bounds and data-dependent priors could be discussed to a greater extent.

Minor:

— “date-dependent” in abstract

— Mix of numbering and words in some lists (“(1) First” in line 218, Lines 350-356)","1. Can you clarify the data-dependence of the prior, and how this relates to the assumptions under which Theorem 1 is derived?

2. For Theorem 1, the prior to be symmetric (equivalent to the exchangeable priors of Audibert (“A better variance control for PAC-Bayesian classification”, 2004) and Catoni (“PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning”, 2007). In their work, it is often sufficient to consider “almost exchangeable priors”, where permutations are restricted to only swap the $i$th element of the training sample with the $i$th sample in the ghost sample. Would a similar weaker requirement work for your results?

3. Line 266: “when the empirical risk is set to 0.05” What does this mean exactly? Does it mean that you train until the empirical risk reaches 0.05 or below and then stops?

4. In Theorem 1, the bound is provided in terms of a scaled Jensen-Shannon divergence between two Bernoullis. This is reminiscent of the Maurer-Langford-Seeger (MLS) bound in PAC-Bayes, where the corresponding KL divergence is used in the LHS. There are results (Foong et al, “How Tight Can PAC-Bayes be in the Small Data Regime?”, Thm. 4, 2021) indicating that the MLS bound is the tightest possible bound (in some sense) up to the log term. What is the relation between Thm. 1 in the present paper and such preceding bounds? Is it possible to instead use the binary KL in the LHS? (Such bounds also have a fast-rate behavior for zero training loss). I understand that the present paper considers a slightly different setup."
fGdF8Bq1FV,fGdF8Bq1FV,Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors,Accept (Spotlight),fafpQylcEI,ICLR.cc/2025/Conference/Submission3622/Reviewer_9bKp,"This paper provides a new generalization bound for representation learning in multi-class classification tasks. The error bound incorporates the recent notion of minimum description length (MDL), which empirically has proven to be more useful than mutual information-based bounds. In particular, the in-expectation bound and tail bound only depend on the encoder part. This has implications that in order to achieve better generalization bound, one can propose a regularizer based on the MDL. This is further studied by approximations using a Gaussian mixture whose mean and variance are data-dependent, and an update scheme for the prior is provided. Several numerical experiments verify the theory.","1. The paper is clearly written and well-organized.

2. The derived bound is tighter than the best bound with MDL.

3. This work provides a quantitative explanation that the encoder plays the role of generalization, as reflected in the bounds in Theorem 1 and 2.

4. Based on the regularization using MDL, a practical and explicit optimization scheme for the prior is provided using Gaussian mixture models.

5. The theory is verified on a few datasets.",See questions.,"1. Typo in line 22 of the abstract, ""date-dependent"" should be ""data-dependent"".

2. In line 49, the author should make it clear what ""MI"" stands for before writing ""MI-based"".

3. In line 86, the author should cite earlier references about the universal approximation with Gaussian mixture, for example, some articles in JRSSB.

4. How do you determine the number of modes in the Gaussian mixture prior?"
BpfsxFqhGa,BpfsxFqhGa,Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity,Accept (Poster),1N6yJYR8E9,ICLR.cc/2025/Conference/Submission3520/Reviewer_1xQ5,"In this work, the authors present a novel method for video reconstruction from fMRI recordings. There are two core components to this novelty, (1) the method is interpretable and learns separate semantic, structural, and motion features from the fMRI - later used to generate video frames, and (2) the motion component of the generated video is solely based on the motion predicted from the fMRI because the video generator is an inflated image diffusion model. The videos are generated in two stages, fMRI-to-feature which has trainable components, and feature-to-video which is completely frozen. At training time of stage 1, a Semantic decoder using frozen CLIP encoders, a Structure decoder using a frozen VQ-VAE, and a custom transformer-based Consistency Motion Generator with a masked causal frame prediction task are trained to learn the semantic, structural, and motion features respectively. The performance of the reconstruction model is evaluated on 3 public fMRI datasets with 8 different (semantic, pixel, and spatiotemporal) evaluation metrics, compared against previous work, analyzed with ablation studies, and assessed under two different interpretability analyses. The model is found to exceed state-of-the-art both in quantitative and qualitative metrics, yield sensible component contributions in the ablation, and offer interpretability insights that include neurobiological plausibility.","This paper has good quality, with sound methodology and extensive experiments. It is an original, systematic, and rational approach that addresses important issues in previous work; namely the entanglement of different types of features that are decoded from fMRI (semantic, structural, motion), and the entanglement of motion information learned by external training of the video generator with the motion information derived from fMRI. The improvements brought forward by this approach are clearly indicated in the quantitative and qualitative results and consist significant contributions in the research aiming to recover, as well as understand, dynamic visual information from brain recordings.","**Presentation** could be more clear at many points throughout the paper. There are technical parts that require more explanation or are currently creating some confusion, which are outlined as follows in order from more major to more minor issues.
* In section 3.2, it is not mentioned at all how the ground truth text condition $\\bf{c}_i$ is obtained - are these from the training dataset of the (frozen) inflated image generator or are these the same as the video captions $\\bf{t}$? It would help to have an intuitive explanation of this in figure 3 as well ($\\bf{c}$ is not shown at all in the training pipeline).
* In section 5.1, it is not clear why the videos are external in the “External Videos (EV)” case; This is described as “we further fine-tuned the image diffusion model using videos from the CC2017 dataset (with the training set)” which is still videos from within the video-fMRI dataset, already seen during training in earlier stages of the pipeline.
* In section 3.2, authors are not explicit enough in the description of the trainable modules Semantic & Structure decoder; From the notation it can be assumed that the Semantic decoder is just one trainable vector $\\bf{f}$ initialized by the fMRI vector, and the Structure decoder is an MLP $D_{Structure}$ (of unknown number of layers) but it needs to be outlined explicitly.
* In the same section, it is also not explicit how the frames are aggregated to create $\\bf{v}$ - is it the average of the CLIP visual embedding across all frames or another aggregation?
* In section 4.2, the description of End-Point-Error(EPE) is missing - the reader has no clue what it is, if it is something existing in the literature (missing citation) or introduced by the authors (missing formula). There are also missing citations for some of the other metrics (Hue-pcc, CLIP-pcc).
* In section 3.3, a mention to the VQ-VAE decoder depicted in the figure is missing - the reader is left to wonder about this. Additionally, this section would benefit from more information on the inflation process (e.g. at least an in-line equation).
* In section 5.2, there is no mention of which dataset the ablation study is on.
* In section 3.2, figure 4 is missing $\\bf{E_{pos}}$ and would benefit from a more informative caption. Also, the abbreviation LDM used in the text is probably not familiar to all readers (either remove or expand it).
* In the rightmost box of the training pipeline in figure 3 (CMG), the bracket of $\\bf{L_{consistency}}$ is placed in a misleading way - a suggestion would be to move the mask matrix to the bottom and have the input and output frames on the right, with $\\bf{L_{consistency}}$ connecting the original (masked) future frames with the predicted future frames.
* In figure 2c the arrow for the noise would be more accurate if it pointed to the Motion instead of the Structure box.
* Prior work is sometimes too generically described e.g. “Subsequently, Han et al. (2019), Wen et al. (2018) and Wang et al. (2022) map brain responses to the feature spaces of deep neural network (DNN) to reconstruct video stimuli.” - what DNN?
* In section 6.2, where authors describe “visual cortices”, probably a more adept term is to say “visual cortical areas” or “areas of the visual cortex”.

**Soundness** related issues exist in parts of the methodology, or sometimes its interpretation.
* The reader is not yet completely convinced that motion information is from the fMRI and not from the training videos. It would help to see an ablation with the CMG trained without the fMRI entirely (output from temporal module is then the Q, K, and V of spatial module), and see if the next frames can be predicted at inference from the structural latent fMRI embedding of the first frame. If the CMG module is still good, then the motion information is not from the fMRI but from the videos in the training set.
* The reader finds several issues with the analysis in  6.1. First the y-axes should end at 1.0 as the maximum value of $\\sum _i \\delta _i$ is 100. Additionally the y-axes are different across the 3 panels which is also misleading. Second, it seems that although p-values are lower with the CMG, they are still very high and much higher than 0.05, meaning that the order of the generated frames does not matter significantly for these metrics. This is not commented on at all by the authors. Here, the reader’s suggestion for a better baseline to compare against (instead of the standard threshold of 0.05) is the p-value of the shuffle test with the ground-truth video, as it is not certain that even this would be below 0.05. Third, it is not clear why the authors are examining the structural metrics for the shuffle test instead of solely the spatio-temporal metrics, and why for the latter only CLIP-pcc is shown and not EPE. In the view of the reader, only CLIP-pcc and EPE are relevant and should be shown. Finally, the results are vastly different across the 3 subjects which is also not (and should be) commented on in the text. 
* In section 5.2, table 5, it is observed that the structure metric Hue-pcc is increasing significantly when the structure module is removed. This seems like an important inconsistency in the results, yet the authors do not comment on it. It is expected to provide some sort of explanation, perhaps based on how this specific metric works and on how the w/o Structure videos look, since the other pixel-level metrics seem to decrease.
* In the analysis in 6.2, it is noticeable that the weight proportion of V1 in the motion information is almost double that of the next highest-weighted areas (e.g. TPOJ, MT, V2, V3), which seems very significant and is not (and should be) commented on - this effect is hidden when the whole of LVC weights are added up.","The reviewer would like the authors to address the points outlined in the “Weaknesses” section, in each case either by making the suggested change or another change that to the authors’ opinion fixes the issue better, or lastly by giving a sufficient (and convincing) explanation of why no change is needed. This way the reviewer’s opinion of the paper would be improved, to a rating of 6 or above.

**Update after rebuttal:** The score has been updated from an initial 5 to an 8, since all of this reviewer's points were addressed in a very careful, timely, and complete manner. More specifically, the crucial points outlined above in ""Soundness"" were fixed and the additional ablation experiments strengthen the paper and alleviate this reviewers concerns, which on its own brings the paper rating above acceptance threshold. On top of this, the paper's presentation was substantially improved, making it a consistent and clear read, and overall a good paper (8)."
BpfsxFqhGa,BpfsxFqhGa,Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity,Accept (Poster),MA5o1kICWS,ICLR.cc/2025/Conference/Submission3520/Reviewer_GXPV,"Paper presents an approach the reconstruct video clips for fMRI brain recordings.
The reconstruction is broken to 3 streams: structure, semantic and motion.
First the fMRI signal is transformed to align with image embeddings, the fMRI embedding is used with pretrained image diffusion models to generate reconstructions.
The results are compared against multiple previous works on a variety of metrics.","Competitive results on multi evaluation metrics, the provided reconstruction look good visually.","- Full reconstruction of the video clips not provided, this would make the work more transparent, and would allow future works to easily compare on new metrics.
- No comparison to other methods is provided for retrieval metric (Which I think is one of the most objective/relevant metrics)
- Authors don't show results for sequences with actual motion, give that one of the focuses of the work is motion, it would make sense to show the ability to reconstruct motion. (for example the clip with the soldier)","- Figure 7 color scheme is not consistent across plots(relevant to all similar plots) 
- I think it makes sense to put retrieval results in a more centric place in the paper. 
- The work "" Kupershmidt22"" provides a retrieval metric for itself and ""Wen18"", you should consider adding this results.
- It would be helpful to add standard error/ statistical tests to the comparisons between the models."
BpfsxFqhGa,BpfsxFqhGa,Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity,Accept (Poster),CiYkB4Mhbs,ICLR.cc/2025/Conference/Submission3520/Reviewer_5KvP,"This paper addresses the challenge of video reconstruction from fMRI data, identifying three key components for an effective decoder: semantics, visual structure, and motion patterns. Building on prior works that successfully capture semantic content using CLIP and Stable Diffusion, the authors aim to improve fidelity to the visual structure and motion dynamics present in the original videos. They propose a decoupled, multi-step reconstruction approach, utilizing distinct decoders and specialized reconstruction criteria. Their method combines tri-modal CLIP (fMRI, image, text), single-frame Stable Diffusion (T2I), and a uniquely learned internal motion prior that avoids biases from external video datasets. Applied to three publicly available movie-fMRI datasets, the approach demonstrates moderate gains in some metrics across different configurations, with notable improvements in preserving visual structure.","The authors’ effort to achieve simultaneous reconstruction of visual structure, semantics, and motion patterns represents a significant and timely contribution. Many prior works prioritize visually appealing reconstructions or focus on semantic fidelity, often overlooking finer alignment with the ground truth video structure. Advancing fidelity in this regard is important; without it, decoders risk becoming general natural video generators with limited adherence to fMRI cues.

The primary improvement over previous methods appears in the preservation of visual structure, evidenced both qualitatively and quantitatively, though this improvement is moderate. The authors’ thorough analysis across three datasets, coupled with an extensive benchmarking on various metrics assessing semantics and visual structure, adds robustness to the work. Additionally, the website offers a helpful visual supplement to this complex work, making it more accessible and digestible.","The results, both qualitatively and quantitatively, lack compelling evidence of substantial improvement over prior work, particularly when compared to Chen et al. (2024). In fact, it appears that with proper statistical analysis (e.g., Table 2-4), there may be no significant gains. Methodologically, the advancement over Chen et al. (2024) seems minimal, with certain versions of the authors' approach even reintroducing external videos—the very bias this paper aims to avoid. Furthermore, the claim of improved motion pattern reconstruction is insufficiently supported, as the authors compare only against shuffled frames. A meaningful comparison should have been made with prior works, similar to the comparisons on semantic and visual structure fidelity.

Additional comments:
- [36-38] Statement is unclear; consider rephrasing.
- [54-59] The text appears to confuse BOLD integration duration with fMRI sampling rate. The BOLD signal integrates neural activity over a period greater than 10 seconds (~300 video frames), which is a major limitation in recovering any motion patterns potentially encoded in neural activity. The fMRI sampling rate is limited for other technical reasons but even if was sampling at a higher rate this would likely not resolve this fundamental issue.
- [100-101] The claim about enabling video reconstruction may be overstated, as prior methods have also achieved this to an extent.
- [102-104] Similar to above, there is a mix-up regarding temporal aspects. Clarification of this contribution would help.
- [104-106] The visualizations mentioned are not novel to this work; they have been used in prior studies and are reintroduced here rather than proposed anew.
- [107-110]  It remains unclear what metric is used to evaluate successful recovery of motion patterns. Detailing this metric is necessary for interpreting the results.","1) Consider adding robust statistical analyses and uncertainty estimates for the primary results. This would clarify the significance of the observed gains, if any.

2) For the claim regarding improved recovery of motion patterns, stronger evidence is needed. As presented, the motion generator appears modest/limited in comparison to the tools used for recovering structure and semantics. Since improvement in motion fidelity is a key goal, it would be impactful to demonstrate and emphasize substantial gains in this area. If stronger evidence cannot be shown, it may be best to reconsider this claim.

3) The authors note that MT, an area associated with motion processing, shows significant activation for semantics, typically associated with the ventral stream. If this outcome supports the motion recovery, it could benefit from additional context. Some clarification on MT’s role in semantics would be helpful for the reader’s interpretation of these findings.

4) For Figure 8, consider enhancing its visual accessibility to make it easier to interpret. Improvements in layout or clarity could aid the reader in understanding the figure’s contribution."
BpfsxFqhGa,BpfsxFqhGa,Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity,Accept (Poster),NLhwkJ3VKg,ICLR.cc/2025/Conference/Submission3520/Reviewer_fFuu,"This work addresses the problem of reconstructing high-quality video from fMRI data. The authors suggest that the key to achieving high-quality video reconstruction lies in decoupling and modeling semantic, structural, and motion information, also carefully handling the frequency discrepancy between fMRI data and videos. To this end, the authors develop a tri-modal contrastive learning scheme along with a next-frame prediction task. Lastly, to ensure that the generated videos are derived purely from the fMRI data, the input is fed into an untuned inflated Stable Diffusion model. Empirical evaluations show promising results and strong interpretability performance.","[Quality, Significance]:

The structure of this work is very easy to understand and follow, with sufficient context and supported citations. The model notations and figure presentations are excellent. 

The authors provides comprehensive empirical metric evaluations and compared their results with many other state-of-the-art approaches, and provide comprehensive ablation study and interoperability results.

[Novelty]: One particular point that I find this work novel is that the authors do not fine-tune the stable diffusion, ensuring us that the videos that we see are not coming from the overfitting.",I do not find this work particularly having major weaknesses.,1. How did the author chose $\\lambda_1$ and $\\lambda_2$ in equation 4? Can the author provide the ablation study/ discussion on how these choices affect the downstream performance?
s5epFPdIW6,s5epFPdIW6,MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models,Accept (Poster),wBlLwlVrGy,ICLR.cc/2025/Conference/Submission3401/Reviewer_3Jou,"This work proposes MMed-RAG, a RAG system that corrects for misalignment issues for context introduced by RAG for multimodal large vision models (LVLMs). MMed-RAG creates preference pairs for cross-modality alignment and finetunes their LVLM (LLaVA-Med-1.5 7B) via DPO. This is described as RAG-based preference fine-tuning (RAG-PT). Other components in MMed-RAG include heuristics for domain-aware retrieval across different sources. MMed-RAG is evaluated across several medical imaging domains (radiology, ophthalmology, pathology), and compared with recent LVLMs (Med-Flamingo, RadFM, miniGPT-Med) and other RAG systems (MedDr, FactMM-RAG, RULE).","- RAG-PT (RAG-based preference fine-tuning) is a nice idea in improving the alignment performance when introducing RAG context to current LVLMs.
- Comprehensive evaluation against various medical LVLMs and their strategies for multimodal RAG and mitigating hallucination.","- The contribution of domain-specific retrievers for RAG context in MMed-RAG is not clear to me. Practically, it is simpler to build domain-specific LVLMs that solves a targeted clinical problem well than generalist biomedical LVLMs. If we are evaluating an off-the-shelf for VQA in radiology, why would we also include pathology and ophthalmology as possible documents for RAG? We To evaluate this contribution, we would need to compare performance of MMed-RAG against domain-specific LVLMs (radiology-, pathology-, ophthalmology). All comparisons in this experiment here should ideally also adopt RAG-PT.
- Though the idea of RAG-PT is novel in the application to medical imaging, there exist several related works in exploring DPO for RAG finetuning in LLMs. The authors should discuss these works in the related work section and how MMed-RAG adds to the literature [1,2].
- It is unclear if RAG-PT is specific to only the medical imaging domains, or if it can be applied broadly to other domains. I think it would be valuable to assess the contribution in RAG-PT in problems outside of medical imaging, especially where image-text prompts are likely to come from heterogenous domains.
- Is the problem of misalignment in introducing RAG context only in LLAVA-Med 1.5 7B, or does this exist in other open-source (etc., Med-Flamingo, miniGPT-Med) and commercial LVLMs (like GPT-4O)?


Refs
1. Dong, G., Zhu, Y., Zhang, C., Wang, Z., Dou, Z. and Wen, J.R., 2024. Understand what llm needs: Dual preference alignment for retrieval-augmented generation. arXiv preprint arXiv:2406.18676.
2. Li, X., Mei, S., Liu, Z., Yan, Y., Wang, S., Yu, S., Zeng, Z., Chen, H., Yu, G., Liu, Z. and Sun, M., 2024. RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards. arXiv preprint arXiv:2410.13509.",
s5epFPdIW6,s5epFPdIW6,MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models,Accept (Poster),bs2OGhd4Gi,ICLR.cc/2025/Conference/Submission3401/Reviewer_tmxy,"The authors proposed MMed-RAG, a Multimodal Medical Retrieval-Augmented Generation framework. The authors introduce a domain-aware retrieval mechanism, using a domain identification module to adaptively select the appropriate retrieval model for medical images. MMed-RAG employs adaptive calibration to determine the number of retrieved contexts. RAG-based preference fine-tuning is used to enhance cross-modality and overall alignment, ensuring the model effectively utilizes input medical images and relevant retrieved information.","The authors address the limitations of the original retrieval-augmented generation (RAG) method by tackling two key challenges: the reliance on retrieved contextual knowledge without consideration of the input image, and interference from incorrect retrievals, which results in misalignment with the ground truth. To solve these challenges, the authors proposed MMed-RAG. First, it aims to enhance cross-modality alignment by ensuring the model utilizes input medical images before generating responses. Second, it aims to improve overall alignment by prompting the model to rely on retrieved contexts when uncertain, while minimizing interference from irrelevant information.
MMRAG, as an innovative multimodal retrieval-augmented generation framework, significantly enhances the performance of medical image-text tasks. The manuscript is well-written and easy to follow.","(1) The manuscript lacks a comparison with other general methods, such as GPT-4, Gemini, and QwenVLM. An intuitive comparison to these general-purpose models would provide valuable context for understanding the advantages and limitations of MMed-RAG.
(2) A major concern is the use of preference datasets for fine-tuning, which may cause the model to focus excessively on hard samples, potentially leading to challenges such as catastrophic forgetting and overfitting. It would be beneficial to use an external validation dataset from the same domain to assess the model's generalizability.",The same as weakness.
s5epFPdIW6,s5epFPdIW6,MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models,Accept (Poster),Sgi8mGObYa,ICLR.cc/2025/Conference/Submission3401/Reviewer_WFqW,"This paper presents MMed-RAG, a retrieval-augmented generation (RAG) framework to improve factual accuracy in medical vision-language models (Med-LVLMs), addressing issues of factual inaccuracies in current models. MMed-RAG incorporates a domain-aware retrieval mechanism, adaptive context selection, and preference-based fine-tuning to enhance alignment with medical knowledge. Experimental results across five datasets show improved accuracy compared to existing methods. While primarily refining existing RAG techniques for medical applications, the approach is well-executed and clearly presented. MMed-RAG offers a focused improvement for Med-LVLMs, potentially increasing their reliability in clinical use.","1. MMed-RAG introduces a domain-specific retrieval mechanism, adapting RAG techniques to address the nuances of different medical fields. By implementing adaptive context selection and preference-based fine-tuning, the paper offers a creative and impactful advancement over traditional RAG methods, effectively enhancing cross-modal and ground-truth alignment.
2. The paper is methodologically sound, thoroughly evaluating MMed-RAG on five diverse medical datasets. Empirical results show significant improvements in factual accuracy, with gains up to 43.8%, underscoring the effectiveness and robustness of the approach.
3. The paper is well-structured, making complex ideas accessible. Each component of the framework—domain-aware retrieval, adaptive context selection, and fine-tuning—is clearly explained, with helpful visual aids to support understanding.
4. Addressing factual reliability in medical diagnostics, this work has substantial real-world implications. By reducing hallucinations in Med-LVLMs, MMed-RAG contributes to safer, more reliable AI diagnostics and could be adapted to other high-stakes domains where factual accuracy is critica","1. In the analysis of RAG-PT, the authors evaluate the effectiveness of each individual RAG-PT component (1, 2, and 3). To better understand how RAG-PT mitigates misalignment and improves performance, it would be helpful to include experiments with different combinations, such as RAG-PT 1+2, RAG-PT 1+3, and RAG-PT 2+3.
2. In Table 2, it is unclear which specific BLEU score is being reported—is it BLEU-1, BLEU-2, BLEU-3, BLEU-4, or an average of these?
3. In the ""Comparison with Other Med-LVLMs"" section, it’s not clear which metrics are averaged for the medical VQA and report generation tasks. Additional clarification on this point would help interpret the results.
4. In the section on Cross-Modality Alignment, more intuitive explanations are needed to clarify how constructing preference pairs (by introducing noisy images) addresses the issue of cross-modality alignment.",please refer to Weaknesses
s5epFPdIW6,s5epFPdIW6,MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models,Accept (Poster),47p4xSlVPV,ICLR.cc/2025/Conference/Submission3401/Reviewer_mHiX,"The authors propose a method, MMed-RAG, to address alignment issues in medical applications, which is essential for enhancing factual accuracy. MMed-RAG is a generalizable approach for medical RAG applications, with key contributions as follows: (1) a domain-aware retriever, (2) adaptive context selection, and (3) a direct preference-optimized algorithm based on a newly developed preference dataset curation process. Additionally, a theoretical justification is provided.

MMed-RAG is evaluated on five datasets across three different medical image types—Radiology, Ophthalmology, and Pathology—on tasks such as VQA and report generation. The authors also conducted a detailed ablation study and analysis of the misalignment issues addressed by MMed-RAG. Overall, MMed-RAG achieves better performance across all tasks and demonstrates the ability to reduce misalignment.","- Originality:  The perspective of decomposing RAG error into cross-modal and overall misalignment is novel, which bring about a new algorithm to collect preference data
- Quality: The quality is solid with a theoretical proof
- Clarity: The authors illustrate their motivation and proposed method well with good example and figures, which is easy to follow
- Significance: The authors are working on improving the safety of VLLM in high-stake field (i..e, medicine). Therefore, I appreciate the the idea and improvement from MMed-RAG.","I think the method is overall well-motivated and well-written. However, I have some concerns which make me doubt the evidences demonstrated in the experiments and would like hear author's thought 

- A bit over-claimed novelty: The author claimed MMed-RAG is a general approach to address images from different sources. While it is technically true, the way author implement is merely put 3 independent piece together without certain level creativity and claimed it is general (i.e. still relying on training ""individual"" retriever model on each image source). It is more like engineering trick than methodological novelty.  
- Lack of potential baseline and need more clarification in the evaluation :
     - When comparing to Med-Flamingo, which setting did you use? (6-shots or zero-shot?)
     -  I would suggest including LLaVA-Med 1.5 few shot as baseline. Based on the github record [1], LLaVA-Med 1,5 was published on 05/13/2024. while Med-Flamingo, RadFM,MedVInT and miniGPT-Med were published on 2023 based on the reference. So, I suppose LLaVA-Med 1.5 is a more powerful VLM than those. I think it is important to see how far this new more powerful model can go with simple trick (i.e., Few shot in context learning) like what Med-Flamingo author did (they have 0-shot and few-shot results)
- Concerns about the alignment analysis:
    - First of all, what data did you use to evaluate in Figure 3?
    - Copy reference rate: If I understand correctly, this refer to line 8 y_{l,o1} in Algorithm 1 , which model overly rely on retrieved text. I find the construction of this idea confusing in practice. Since X_{r} is based on context retrieved by original image then you pair this context with the noise image. However, in practice, you would never get this clean X_{r} from ground truth. What you will have is noisy image + noisy context, then the model take this as input. I am bit unsure what is the point of this evaluation
    - Over reliance rate:  If I understand correctly, this refer to line 16 y_{l,03} in Algorithm 1m which model is interfered by noisy retrieved context. My question is? Does it just mean the retriever is bad or not well-trained so that the retrieved context is nosy and  hurt the model alignment? Referring to Figure 3 OR, it shows over 0.4. With such number, I wonder how well the retriever is trained in the paper, do authors has some results or any observation?
   -  Following the logic above, I feel like a better evaluation scheme is to (1) Use original image (2) Feed it to retriever (3) Selecting data with bad retrieved context (i.e., retriever fail). Then see how MMMed-RAG can rescue these examples.  Any comment for this?




[1] https://github.com/microsoft/LLaVA-Med","My main questions are
(1) Do authors have any rational judgement on not adding more basic baseline so that we can have a better reference method in the paper? 
(2) I am willing to hear any thought or clarification about my concern of alignment analysis."
M9iky9Ruhx,M9iky9Ruhx,Grounding Multimodal Large Language Model in GUI World,Accept (Poster),XZBzaLMiEz,ICLR.cc/2025/Conference/Submission3062/Reviewer_9768,"In this work the authors propose a framework for improving the GUI grounding of multimodal LLMs, i.e., the localization of relevant elements in a UI from a textual description of the elements. 

The framework presented includes an automated way of collecting large scale data across a variety of websites along with automatic annotation of the locations, properties and descriptions of the various UI elements. The authors then clean this data programmatically to remove data with low-quality annotations (mislabeling, missing annotations) to arrive at a clean dataset. 

This data is used to train a novel lightweight grounding model to identify the location of a UI element given a textual query. This grounding model achieves SoTA (or close to SoTA) performance on grounding tasks like Screenspot. The authors then present how to integrate their grounding model with a multimodal LLM for completing automation tasks on datasets like MiniWob, Mind2Web and Android in the Wild. They fine-tune an MLLM to work with their grounding model and demonstrate strong performance compared to other image based approaches. Finally, the authors perform ablations to measure the impact of their dataset and model on the performance of their overall framework.","1. SoTA image only performance: This work achieves SoTA performance on image based grounding across Screenspot and Automation tasks like MiniWob, Android In the Wild, Mind2Web. Given the recent interest in developing MLLM based GUI agents, and also given that grounding of actions in UIs is identified as a bottleneck for current approaches, this work can be very impactful for the research community.

2. Light-weight model for grounding: Propose a light-weight model for grounding compared to other approaches like Fuyu, SeeClick although this is a single-task model specifically fine-tune for grounding. The authors also presented detailed ablations for the modeling choices which is very informative.

3. Extensible dataset collection approach: The authors present an approach for large scale data collection and annotation which will be of interest to the research community. The dataset seems to result in SoTA performance on grounding tasks.","1. The paper achieves state of the art results for image based models on grounding/automation tasks. However, I have some questions about the novelty of this work. The dataset collected has many similarities to the dataset from SeeClick with the main difference being the source of the annotations: UI Automation framework vs attributes from the DOM. The model architecture is quite similar to LISA and SAM with detection heads (and loss) replacing segmentation ones.

2. The process for cleaning the annotations is missing some details. In particular, to identify elements with erroneous element names, restructuring the hierarchy of UI elements it is not clear the source of the “correct information” to identify errors and to do restructuring.

3. Error analysis: In lines 484-487 the authors note that image only approaches fall short of text-based approaches. It will be very useful if the authors can analyze the errors compared to a text-based approach and try to identify patterns (if any) which can point to future directions of improvement.","* ""Next, we clean the element data by removing entries with erroneous or missing element names."" [203-205] 
How are erroneous element names identified? What is the groundtruth to which they are compared to identify a name as erroneous?

* ""We also address cases where the element hierarchy is ambiguous or misrepresented by restructuring it to accurately reflect the parent-child relationships between GUI elements."" [206-208] 
Can you explain how you identify ambiguous or misrepresented hierarchies? Also how do you identify the parent-child relationships between GUI elements?

* ""First, we eliminate websites that are either empty or inaccessible as many URLs may lead to pages with no meaningful content, broken links, or require authentication that cannot be handled automatically."" [201-203]
Can you specify how many of unique websites are present in the filtered dataset? Also, did this result in any of the topics listed in Table 10 being completely removed?

* Did you conduct any ablations for the filtering and cleaning of the pre-training data on Screenspot or other evals? Any numbers here will be useful for differentiating the pre-training dataset from other existing datasets.

* For the agent/UI automation evals, are the models separately fine-tuned for each of Miniwob, AiTW and Mind2Web? Do you have a comparison to one model trained on all 3 datasets? It will be useful to assess the robustness of the grounding model across different UI types.

* For mind2web, did you run any evals that can be directly compared to a text-based multiple choice or Set of Marks setting (in terms of using the same LLM output for planning)? I think it will be useful to see what is the gap between different grounding methods since you have SoTA image based grounding.

* Can you specify what is the metric/dataset used for Tables 8 and 9? 

* For mind2web, can you verify if some of the test-set domains are present in your grounding training set? I don't think it is a big problem but it will be useful to know for the test-website and test-domain splits.

* Do you have any experiments on the scaling of grounding performance with more automatically generated training data? This could indicate if there is any headroom with scaling of pre-training data."
M9iky9Ruhx,M9iky9Ruhx,Grounding Multimodal Large Language Model in GUI World,Accept (Poster),p7iRjyO7cQ,ICLR.cc/2025/Conference/Submission3062/Reviewer_3qqW,"This paper seeks to enhance the grounding capabilities of MLLM-based GUI agents by introducing a new grounding dataset. The authors propose a lightweight grounding plugin trained on this dataset, demonstrating its effectiveness. Additionally, they fine-tune an MLLM-based GUI agent using Qwen-VL, showing promising results on various GUI grounding and navigation benchmarks.","- Comprehensive GUI Grounding Dataset: The paper presents an automated data collection engine that generates large-scale, diverse, and annotated GUI screenshots. This helps mitigate the shortage of publicly available GUI datasets, significantly contributing to the field.
- Improved Grounding Capabilities: The proposed GUI grounding module demonstrates enhanced task accuracy and adaptability, yielding impressive performance across various GUI benchmarks.
- Robust Technical Contribution: The paper includes extensive experiments that effectively demonstrate the efficacy of their model for GUI grounding and agent tasks.","- Over-claimed Contributions and Misused Terminology: While the ablation studies are appreciated, the authors overstate their contributions. The claim of strong performance in general GUI grounding is unsupported, as they evaluate their model only in web-based tasks (Android-AITW and Mind2Web). Additional experiments on software- or OS-level benchmarks, such as OS-World [1], WindowsAgentArena [2], or VisualAgentBench [3], are necessary to substantiate their claims. Moreover, the term ""general GUI grounding"" is misleading, as their dataset is collected from webpage. It would be more accurate to call it a ""web grounding dataset"", although I admit that GUI elements may transfer across different platforms.

- Writing Suggestions: The Experiment section lacks clarity, particularly in its structure. It is recommended to introduce details about benchmarks and evaluation metrics in the Experiment Setup section, rather than in the Results section. For instance, information about the sample size and metrics for AITW is provided in line 432 of the Results section but should be included in the setup. A clearer organization would be to separate experiments by benchmarks, as each uses different baselines and metrics.

- Failure Cases: Including failure cases of the proposed grounding model and agent would offer a more comprehensive evaluation of the approach and provide insights into its limitations.

- Dataset Analysis: While the paper discusses the dataset size, a more detailed analysis is essential. Providing statistics on different levels of data and characteristics of the dataset would offer more insight. Additionally, the authors should address whether the dataset contains NSFW content, which would be crucial for future research and deployment. Moreover, given that your dataset is crawl from the internet, copyright problems should be discussed in this paper.

- Comparison with Stronger Baselines: The paper lacks comparison with recently released baselines, such as SeeClick-V. Including such comparisons, especially highlighting cases where their model succeeds while others fail, would further emphasize the model's strengths. Additionally, comparing against Set-of-Marks, a common baseline for non-GUI-specific models, would provide a broader perspective on the effectiveness of their GUI-specific model.

[1] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments

[2] WindowsAgentArena: Evaluating Multi-Modal OS Agents at Scale

[3] VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents","- Conduct experiments on additional benchmarks to solidify claims about the model's general GUI grounding abilities.
- Include a more thorough analysis of the dataset and address potential concerns regarding NSFW content and copyright.
- Include failure case analysis.
- Compare with stronger baselines, such as SeeClick-V and Set-of-Marks, to underscore the model's performance relative to current approaches."
M9iky9Ruhx,M9iky9Ruhx,Grounding Multimodal Large Language Model in GUI World,Accept (Poster),KIAlxCl44w,ICLR.cc/2025/Conference/Submission3062/Reviewer_ivbR,"This presents an automated data synthesis for web screenshots and a lightweight model for precise UI element localization, which can be further integrated with MLLMs. The approach enhances MLLM accuracy and adaptability across GUI grounding and agent benchmarks.","The paper introduces a lightweight model architecture based on SAM and BERT that achieves strong grounding performance (on ScreenSpot), demonstrating impressive results despite being much smaller (0.4B) than contemporary 7B models.

The writing is clear and cohesive, and the experimental results, including ablation studies, reasonably support both the model design and the data’s effectiveness.","The web-based data collection pipeline is not novel or technical.

As shown in Table 2, the model struggles with grounding icon elements, which are both essential and challenging in GUI tasks. In contrast, text elements are relatively easier to ground through OCR. Maybe the authors should add more discussions about the discrepancy and the ways to address the challenges.

The model's results do not significantly outperform SeeClick on benchmarks like ScreenSpot, AITW, and MiniWob, despite using a larger and higher-quality training dataset. The significant improvement on Mind2Web is likely due to overlap witt websites and domains (e.g., shopping, travel) rather than a generalized performance boost, whereas SeeClick’s data is more randomly sampled from Common Crawl.","Overlap Issue in Mind2Web Evaluation: Is the model's high performance on Mind2Web genuinely indicative of its capabilities, or is it influenced by an overlap with similar website domains (e.g., shopping, travel) in the training data?

For the e2e agent setting, despite the ablation study, finally we probably can solve the problem with only an end2end MLLM, without AGG (for example, using Qwen 2 VL, which is known for being capable of grounding). Can you discuss more about the advantage of this work compared to a better finetuned MLLM?

A minor issue: In Fig. 3, does recording ""Click at (coordinates)"" as part of action history make sense for web agents? (When there is no screenshot recorded for understanding clicking at a point)"
M9iky9Ruhx,M9iky9Ruhx,Grounding Multimodal Large Language Model in GUI World,Accept (Poster),3p1Jpf0jLJ,ICLR.cc/2025/Conference/Submission3062/Reviewer_UKHa,"This paper seeks to enhance the grounding capabilities of MLLM-based GUI agents by introducing a new grounding dataset. The authors propose a lightweight grounding plugin trained on this dataset, demonstrating its effectiveness. Additionally, they fine-tune an MLLM-based GUI agent using Qwen-VL, showing promising results on various GUI grounding and navigation benchmarks.","- **Comprehensive GUI Grounding Dataset:** The paper presents an automated data collection engine that generates large-scale, diverse, and annotated GUI screenshots. This helps mitigate the shortage of publicly available GUI datasets, significantly contributing to the field.
- **Improved Grounding Capabilities:** The proposed GUI grounding module demonstrates enhanced task accuracy and adaptability, yielding impressive performance across various GUI benchmarks.
- **Robust Technical Contribution:** The paper includes extensive experiments that effectively demonstrate the efficacy of their model for GUI grounding and agent tasks.","- **Presentation Issues:** The paper appears rushed, with room for improvement in formatting and clarity. For instance, the overview figure on the first page should be placed above the Abstract section instead of between the section title and content of the Introduction. Additionally, there are several missing spaces before `\\cite`, and incorrect usage of `\\citep` and `\\citet`.
- **Overclaimed Contributions and Misused Terminology:** While the ablation studies are appreciated, the authors overstate their contributions. The claim of strong performance in general GUI grounding is unsupported, as they evaluate their model only in web-based tasks (Android-AITW and Mind2Web). Additional experiments on software- or OS-level benchmarks, such as OS-World [1], WindowsAgentArena [2], or VisualAgentBench [3], are necessary to substantiate their claims. Moreover, the term ""general GUI grounding"" is misleading, as their dataset is collected from webpage. It would be more accurate to call it a ""web grounding dataset,"" although I admit that GUI elements may transfer across different platforms.
- **Writing Suggestions:** The Experiment section lacks clarity, particularly in its structure. It is recommended to introduce details about benchmarks and evaluation metrics in the Experiment Setup section, rather than in the Results section. For instance, information about the sample size and metrics for AITW is provided in line 432 of the Results section but should be included in the setup. A clearer organization would be to separate experiments by benchmarks, as each uses different baselines and metrics.
- **Failure Cases:** Including failure cases of the proposed grounding model and agent would offer a more comprehensive evaluation of the approach and provide insights into its limitations.
- **Dataset Analysis:** While the paper discusses the dataset size, a more detailed analysis is essential. Providing statistics on different levels of data and characteristics of the dataset would offer more insight. Additionally, the authors should address whether the dataset contains NSFW content, which would be crucial for future research and deployment. Moreover, given that your dataset is crawl from the internet, copyright problems should be discussed in this paper.
- **Comparison with Stronger Baselines:** The paper lacks comparison with recently released baselines, such as SeeClick-V. Including such comparisons, especially highlighting cases where their model succeeds while others fail, would further emphasize the model's strengths. Additionally, comparing against Set-of-Marks, a common baseline for non-GUI-specific models, would provide a broader perspective on the effectiveness of their GUI-specific model.

[1] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments

[2] WindowsAgentArena: Evaluating Multi-Modal OS Agents at Scale

[3] VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents","- Improve the paper's presentation by addressing formatting issues and clarifying the organization of sections.
- Conduct experiments on additional benchmarks to solidify claims about the model's general GUI grounding abilities.
- Include a more thorough analysis of the dataset and address potential concerns regarding NSFW content and copyright.
- Include failure case analysis.
- Compare with stronger baselines, such as SeeClick-V and Set-of-Marks, to underscore the model's performance relative to current approaches.

**I will consider raising the overall score if author address my concerns, especially the writing and presentation.**"
rN7Ewo2lV4,rN7Ewo2lV4,Generating Synthetic Genotypes using Diffusion Models,Reject,Nm4OjvpEc8,ICLR.cc/2025/Conference/Submission2943/Reviewer_Dfcy,"This article presents a method for generating full synthetic human genomes using diffusion. Four diffusion architectures are compared, and the quality of the generated genomes is analyzed. The article demonstrates that this method can generate useful synthetic data for geneticists.","This article is very exciting. It is incredibly well-written with a clear and enjoyable prose. It tackles an important issue, human genome analysis, with state of the art methods, diffusion models. It presents an interesting comparative study for deep learning in a data regime not often considered, especially in the diffusion literature, but of great import and scientific interest. The insights from the architecture comparison study are useful for a machine learning readership beyond the biomedical application; an understanding of diffusion methods applied to different data types, and how generated data can be measured and made useful, is relevant to ICLR. The demonstration of generating full human genomes is both an impressive showcase of the capabilities of diffusion and an important step in the study of the human genome.","It would have been helpful to contextualize the results in comparison to other methods, like the work of Szatkownik et al, or HyenaDNA. It is unclear to me how feasible the application of these methods to the full human genome is, but this could have been discussed in the results. However, it should be possible to generate smaller sequences with the proposed diffusion method in order to compare to previous works. The experimental results, as they are, demonstrate the viability of diffusion models on the full human genome (and provide a useful architecture comparison). However, they do not give an indication of the relative quality of the generated data over the data generated by other works.

Other than that, I identified no major weaknesses. Rather, here are some minor quibbles.

+ the plots are not vectors and have small fonts, making them blurry in standard sized readers. pdf plots with larger fonts would help with clarity.
+ line 451: ""We draw two conclusions from this:"" this is an incomplete sentence, or the next paragraph shouldn't be presented as a new paragraph.
+ eq 2 could be defined inline
+ lines 453 and lines 483 begin paragraphs that say roughly the same thing. Those two paragraphs could be merged or each trimmed.
+ there's an unneeded comma in line 492","The MLP architecture seems very shallow, especially compared to the CNN, and the first downscale layer, from 147k to 1024, seems very abrupt. What motivated these choices? Were deeper architectures tried?

What is the explanation for the MLP loss not moving? line 363: ""even though only small drops in loss during training can be observed for
the MLP, the reconstruction error keeps improving."" Is the MLP learning the data distribution? Is it acting as more than a regularization term in the MLP+CNN architecture?

It seems odd that a CNN, albeit with 1D kernels, would outperform a transformer on such long sequences. What are the intuitions behind the transformer architecture not working as well, especially on recovery rates of data coming from the transformer? This seems odd. I agree with the comments on lines 760/761, but am curious if there was more analysis here.

A greater exploration of DDIM could have been nice. Was DDIM tried, and if so, at what timestep reduction? If DDIM is used just for inference but not for training, is there a large decrease in quality?"
rN7Ewo2lV4,rN7Ewo2lV4,Generating Synthetic Genotypes using Diffusion Models,Reject,oAQDURRNqn,ICLR.cc/2025/Conference/Submission2943/Reviewer_Q7FW,The authors present a novel approach to generating full-length synthetic human genotypes with diffusion models. They empirically show that the synthetic genotype closely resembles the actual data. They show that the performance of disease and population classifiers was maintained when synthetically generated data was used for training those classifiers. The study also demonstrates that augmenting the training data with synthetically generated data improves the performance of the classifiers. This approach can significantly improve data availability and access and preserve data privacy.,"1. The paper is well-written and presents a technically sound and well-structured approach.
2. Using diffusion models to generate full-length genotypes is a novel approach. The rationale and motivation for this approach are clearly stated. 
 2. The paper presents a significant contribution to addressing the challenges with data access restrictions and privacy in genome data. 
 3. The paper presents empirical evidence for comparable performance between the models trained with synthetic and real data.","1. Authors should discuss potential limitations of their current evaluation approach in more detail.
2. Could authors discuss any limitations in generalizing their approach to other genomic datasets?","1. Can the authors explain why they opted for variable PCA component usage and did not use a consistent number, such as 8, for all genes?
2. Given its relevance for assessing distributional similarity, why was the Frechet Distance not used for holdout evaluation?
3. Could authors clarify their data splitting procedure and discuss any potential implications for the validity of their results?"
rN7Ewo2lV4,rN7Ewo2lV4,Generating Synthetic Genotypes using Diffusion Models,Reject,Ohg0kgCHNw,ICLR.cc/2025/Conference/Submission2943/Reviewer_nELF,"The authors develop diffusion models to generate realistic human genotypes. Their claimed contribution is the first models to produce full-length human genotypes. They develop CNN-, CNN+MLP- and Transformer- based models optimized for reconstruction loss. They evaluate on ALS and 1000 Genomes data. They evaluate the ability of the models to augment classifier training and also test the privacy loss associated with generating genotypes with the models.","The paper is relatively clear although the language and grammar could use polishing. The claimed contribution is clear and the authors discuss prior work clearly. For the most part the paper is easy to follow. The problem of synthetic data generation is a significant one and the development of synthetic human genomes is surely of interest to some in the research community, although I do not work in this area.","Although the authors discuss prior work, they do not seem to compare to modeling approaches used in it. It is not clear if this is because of technical limitations or an oversight. The motivation for generating synthetic genotypes to improve ML models is clear, but the authors do not spend enough time discussing the possible negative ethical implications of generating human genotypes. I appreciated the evaluation of privacy loss but the implications were not fleshed out. 

Specific comments:

- All of the figure texts are too small to read.

- change "" to `` for front quotes

- "" that do not only"" -> ""that not only""","I would like a more thorough ethical discussion including, but not limited to, privacy concerns. What would a bad actor do with this technology?"
rN7Ewo2lV4,rN7Ewo2lV4,Generating Synthetic Genotypes using Diffusion Models,Reject,cKDckgWZe9,ICLR.cc/2025/Conference/Submission2943/Reviewer_iupA,"In this work, the authors propose a technique for generating complete human genomes. Motivated by the challenges of obtaining individual genomes due to sequencing costs and data privacy concerns, they argue that generating synthetic genomes will improve the performance of predictive models. To address the long-context problem faced by machine learning models processing full genomes, they employ a previously introduced technique to embed genomes in a low-dimensional latent space. A diffusion model is then trained within this space. The authors train their model using classifier guidance on two datasets: one for generating genomes with or without ALS, and another for generating genomes representing 26 populations from the 1000 Genomes Project. To validate their diffusion model, they try to demonstrate (1) that classifiers trained on the generated data achieve comparable accuracy to those trained on real data, and (2) that the generated data distribution is both diverse and sufficiently distinct from the real data distribution.","Most current deep learning models in genomics operate at the sequence level, preventing them from processing complete genomes due to their immense length. Additionally, most models rely on the reference genome, hindering their ability to accurately capture intra-individual variants. These limitations, well-documented in the literature, are likely central to the challenges faced in improving these models. Therefore, I appreciate this paper's focus on working at the complete genome level to represent full individuals. Employing an embedding technique to represent full genomes in a low-dimensional space and utilizing latent diffusion within this space is a promising approach, and I am happy to see research in this area. 

Furthermore, the choice of datasets for training the model is well-justified.","However, the paper presents several limitations:

- The most compelling aspect, in my view, is the embedding of complete genomes into a low-dimensional space and subsequent model training. However, the employed technique is not a novel contribution and lacks sufficient discussion. The authors also fail to adequately address the extent to which complete genomes can be reconstructed from this latent space and the impact on the final sequence. While reconstruction is briefly mentioned, a detailed explanation of the process and precise reconstruction metrics at the sequence level are missing.

- The literature review is incomplete. The authors should include references to D3 [1] and DNADiffusion [2]. While these works have slightly different motivations and are not directly comparable, they warrant mention. Given the discussion of long-range limitations in current models, referencing Enformer [3] and Borzoi [4] would also be beneficial as they represent significant advancements in addressing this issue.

- The paper lacks crucial technical details, hindering a comprehensive understanding of the proposed method. Certain sections require rewriting for improved clarity. Specifically, a dedicated section should detail genome processing and embedding, and another should explain the diffusion model training process, including duration. The model architecture section is difficult to follow, and the ablations lack proper introductions. The section introducing the classifier accuracy recovery metric is overly long and imprecise; it could be condensed into a few concise sentences. Section 3.4.2, while critical, is poorly detailed. The introduced metrics lack explanations, particularly the notation 'd' (presumably representing distance), which is not defined, nor is its measurement space specified. Additionally, the mathematical notations in the equations are incorrect. Measuring the quality and diversity of generated data is crucial in generative modeling and has been a key research area in computer vision. I strongly encourage the authors to refine this aspect, provide thorough introductions to their metrics, and better reference existing literature.

- Employing a convolution-based architecture might be suitable, but the authors should reference existing architectures, use them as a starting point, and discuss necessary modifications for their specific context. Inspiration can be drawn from architectures used in genomics, such as those in D3, Borzoi, BPNet, and Enformer. The 1D U-NET architecture from SegmentNT [5] could also be relevant. Additionally, a vast body of literature exists on 1D convolution and 1D U-NET in signal processing.

- The figures are of poor quality. The cartoons are unclear and lack information, and the font sizes are too small. The evolution plots are difficult to read, and using screenshots from run monitoring software is unacceptable. The authors should regenerate these plots using a plotting library like matplotlib.

- The paper's structure is unconventional. The authors underutilize the allotted space and employ numerous small sub-paragraphs. A denser presentation is expected for a conference like ICLR. The authors should utilize the available space to provide a detailed description of the method and experimental setup.

- The experimental section is also unconvincing. The experimental setup lacks a proper introduction; for instance, the classifier's input and architecture are not specified. As previously mentioned, the privacy metrics are inadequately introduced, making it difficult to assess the reported performance. Lastly, the experiment investigating the use of synthetic data for improving predictors in data scarcity regimes is flawed. When training the classifier on k% of the dataset, the diffusion model should also be trained on k% to accurately reflect the intended use case. In reality, with only k% of the data available, training a diffusion model on 100% is not feasible.

Overall, this paper falls short of the expected quality and contribution standards for ICLR, and I confidently recommend strong rejection.

[1] https://www.biorxiv.org/content/10.1101/2024.05.23.595630v1.abstract

[2] https://www.biorxiv.org/content/10.1101/2024.02.01.578352v1

[3] https://www.nature.com/articles/s41592-021-01252-x

[4] https://www.biorxiv.org/content/10.1101/2023.08.30.555582v1

[5] https://www.biorxiv.org/content/10.1101/2024.03.14.584712v1.full.pdf",See my points above.
rN7Ewo2lV4,rN7Ewo2lV4,Generating Synthetic Genotypes using Diffusion Models,Reject,ijcgWWhsbw,ICLR.cc/2025/Conference/Submission2943/Reviewer_ydeS,"In this paper, the authors use diffusion models to synthesize genomic data, allowing researchers to access it at a low cost. They expanded the synthesis scale to encompass the entire human genome. To handle long sequences, the authors employed a method similar to Latent-Space Diffusion Model: first splitting the genome into parts. For each part, they used PCA as an encoder/decoder to reduce the dimensionality to 8. The authors applied various methods to test the model's generation accuracy, generalization performance, and efficiency in downstream tasks. Notably, models trained on synthetic data performed almost identically to those trained on real data.","As the authors claim, they propose the first model that augments human genomic data at full scale, which is larger compared to previous works. Their experiments demonstrate that their generated data can be used as effectively as real data to train downstream models.","The paper's most significant weakness is its clarity and writing quality. In the section on previous works, there is only a table without main text, making it difficult to understand how the authors' approach compares to prior research. What are the limitations of previous works beyond their smaller scale? How does the authors' approach relate to existing methods? 

The authors' claim of ""using diffusion model"" as a novelty is questionable. Diffusion models aren't fundamentally different from other generative models. While using a diffusion model might be justified based on performance, it doesn't constitute a novel contribution in itself.

Lines 92 to 94 are confusing: they state that DDIM is beneficial, but DDPM is also necessary, without propose any solutions. In fact, DDPM is a special case of DDIM with specific alpha and sigma values.

On line 95, the authors mention using classifier-free guidance, claiming it can improve quality. However, classifier-free guidance was developed for conditional generation, not specifically for quality improvement. The authors should clarify their reasoning for this claim.

The experiment section lacks details about the data generation process. Although the model was trained with classifier-free guidance, it's unclear how the data is generated with conditions. The authors should clarify how they generate the data and how they select the conditions for generation.

Several other details also require improvement. The UMAP experiment is distracting and should be removed, as it's highly subjective and doesn't provide useful information. In Figure 2, the horizontal lines are not truly horizontal, and the markers are misplaced. The notations in this figure are unclear, making it difficult to determine which parts they refer to. Figure 3 appears to be a screenshot from Weights & Biases, which is unprofessional. Figure 5 is challenging to read due to its poor layout and formatting.

Regarding the model design, the use of CNNs is questionable due to the lack of translational symmetry. The same vector can represent entirely different information in the 18,279 or 26,624 distinct latent spaces created by PCA.","My biggest question concerns how exactly you augment the data. There are no details about how you utilize the conditional generating capabilities. Given a small amount of data, how do you select the conditions? What is the relationship between your ""seed"" training data and the generated data?"
lA6du4Q0Zc,lA6du4Q0Zc,Neural-Symbolic Message Passing with Dynamic Pruning,Reject,9QXfKsK8uy,ICLR.cc/2025/Conference/Submission2632/Reviewer_ThD9,"The research proposes a novel method for answering Complex Queries within knowledge graphs ($\\text{EFO}_1$) through the use of Neura-symbolic  Message Passing with neural link prediction and fuzzy set theory for aggregation. The method is claimed to be computationally efficient as is does not require training on complex queries and can simply utilise a pre-trained link prediction on a given Knowledge graph. The method also allows dynamic pruning that filters the noise from the initial, un-updated layers during the message-passing process. The method shows competitive results on a set of Benchmarks from BetaE and FIT and provides ablations showcasing the need for dynamic pruning and impact of the hyperparameters.",The method suggests a novel method for answering Complex Queries($\\text{EFO}_1$) over knowledge graphs that uses pretrained link predictors with Neura-symbolic message passing and fuzzy set theory for aggregation. The method allows the encoding of both local and global information along with both neural and symbolic representations during the message-passing process. This is coupled with an interesting dynamic pruning technique that filters out the impact/noise from the initial layers of un-updated variable nodes. The method is rather competitive on a set of benchmarks from BetaE and FIT and allows for more interpretable reasoning by analysing the fuzzy intermediate representations.,"1. While the method is ripe with novel ideas I feel that the benefits it brings have already been explored in prior research. Methods like and stemming from CQD and CQD-A train only a single link predictor, thus circumventing the need to train on complex queries. They, along with GNN-QE and others, offer interpretable query answers by exposing the intermediate answers (top-k). The use of fuzzy logic is also explored within these papers. We see that for example, in NELL (both BetaE and FIT versions), other methods are only marginally different (CQD-A) or are on average, better (FIT) than the suggested framework. Can you please elaborate on the novel additions in NSMP and what do they exactly contribute?  Can you elaborate on the comparison in the amount of parameters w.r.t. other benchmarks are they comparable?

2. The novel idea of dynamic pruning is rather interesting; however, the ablation shows a sizable average increase only for queries sampled in FIT. Can you explain the reason for this? How much efficiency does dynamic pruning add to the overall network in terms of inference?

2.5. As direct link prediction is chosen along a product T-norm for fuzzy aggregation, the method is bound to have intermediate answers that do not interact together as outlined in CQD-A. This means that because the probability ranges of different intermediate answers are not homogenous aggregating with a product t-norm can cause massive discrepancies during message propagation. Has any analysis of intermediate answers been completed? 

3. The choice not to include ""pni"" is not justified, as Yin et al. (2024) includes that scheme with an $\\text{EFO}_1$ definition (Example 10 and Property 12 in the paper) and just resamples w.r.t. their definition. The original version of ""pni"" proposed in BetaE is simply the universal quantifier version that is resampled in FIT w.r.t. their definition. Can you elaborate on the reasoning to not include ""pni"" ? Has it been tested on ?

4. The paper cited as the reason to not include FB15K (Toutanova & Chen, 2015)) does not claim test leakage but rather proposes a reconstruction of FB15 with the removal of ""near-duplicate"" or ""inverse-duplicate"" relations. While FB 15L does have some discrepancies bot BetaE  and FIT still include it in dataset construction and most of the CQA methods are benchmarked on it. Can you please elaborate on the choice of not using FB15K and if there is a stronger argument against the use of FB15K?","**Question set Q1: For context, see Point 1 in weaknesses:**

*Can you please elaborate on the novel additions in NSMP and what do they exactly contribute?*  

*Can you elaborate on the comparison in the amount of parameters w.r.t. other benchmarks are they comparable?*

**Question set Q2: For context, see Point 2 in weaknesses:**

*Can you explain the reason for this?* 

*How much efficiency does dynamic pruning add to the overall network in terms of inference?*

**Question set Q2.5: For context, see Point 2.5 in weaknesses:**

*Has any analysis of intermediate answers been completed?* 

**Question set Q3: For context, see Point 3 in weaknesses:**

*Can you elaborate on the reasoning to not include ""pni"" ?* 

*Has it (pni) been tested on ?*

**Question set Q4: For context, see Point 4 in weaknesses:**

*Can you please elaborate on the choice and if there is a stronger argument against the use of FB15K ?*"
lA6du4Q0Zc,lA6du4Q0Zc,Neural-Symbolic Message Passing with Dynamic Pruning,Reject,jeiSpo3T2f,ICLR.cc/2025/Conference/Submission2632/Reviewer_tbFB,"This paper discusses the neural symbolic approach to address the complex query answering over knowledge graphs by leveraging pretrained link predictor in a neuro-symbolic way. The proposed neuro-symbolic framework follows the message passing GNN framework. The key technique is to formulate the message passing and answer ranking module into the ensembles of both neural update (following previous logical message passing) and symbolic updates (following the fuzzy logic). The performance in many datasets for various query types demonstrated the effectiveness of the models. Notably, the framework is shown to be very efficient than previous fuzzy logic based methods.","1. The presentation is easy to follow.
2. The overall framework is somewhat novel and empirically significant.
3. the experiments is comprehensive on both different underlying knowledge graph and different query types.","My particular concern for this paper is whether it can achieves the real efficiency over the fuzzy logic inference approach, such as QTO and FIT. Because the updates on the internal states also includes an adjacency matrix $M_r$, which is $O(n^2)$ space and the fuzzy vector of $O(n)$ space, where $n=|V|$ is the cardinality of the entity set. The calculation of the neuro-symbolic message is of the same cost as (sparse) matrix multiplication, which is already at least quadratic. In that sense, I cannot see the reason in Figure 3, that the speed up of NSMP over FIT, which suggested at least 10 times speed up in NELL.","1. Could you please explain the complexity of FIT and your algorithm on both acyclic and cyclic query? in terms of the size of graph and the size of query.
2. Please address my weakness.
3. For the dynamic pruning of the message, I would like to know whether the total message to compute and total states to update can be significantly better than the original LMPNN without pruning, say $O(L m) = O(m^2)$, where $L$ is the total number of layers of message and $O(m)$ is the number of predicates in the query graph."
lA6du4Q0Zc,lA6du4Q0Zc,Neural-Symbolic Message Passing with Dynamic Pruning,Reject,hnzJZFIQ1u,ICLR.cc/2025/Conference/Submission2632/Reviewer_fVNG,"This paper proposes Neural-Symbolic Message Passing (NSMP) to integrates neural and symbolic reasoning for answering complex queries on knowledge graphs. It adopts pre-trained neural link predictors and fuzzy logic operations to estimate the embeddings and the fuzzy set for each variable in a given query respectively. NSMP doesn’t require to be trained on complex query datasets, and is claimed to provide interpretable answers. The authors further introduce a dynamic pruning strategy to filter out noisy messages in the message passing process. Empirically, NSMP achieves competitive performance on both BetaE and FIT datasets, even though most baselines are trained on complex queries.","- NSMP achieves competitive performance against state-of-the-art methods on complex queries, even though NSMP doesn’t require to be trained on complex queries. The results should be deemed as solid.
- The writing of this paper is generally good except for the abstract and the intro. It’s easy to comprehend most technical details of this paper.","- The contributions of this paper are not clear. The authors claimed three challenges in the intro: bad performance on negative queries, noisy messages and interpretability, but there is no clear correspondence between the model components and the challenges.
- Most components of NSMP have limited novelty regarding the literature of complex queries. One-hop inference and message passing (Section 3.5 & 5) is almost identical to LMPNN[1]. Combining neural and symbolic representations (Section 4 & 5) is very close to EmQL[2], which the authors didn’t even cite in the paper. The idea of using non-parameteric fuzzy logic to avoid training shares many spirits with CQD[3], though the actual design is different.
- The usage of fuzzy logic in this paper is not standard compared to [3, 4, 5]. In [3, 4, 5], every entity has a probability to be bound to a variable, independent of other entities. This enables fuzzy logic operations to form a closure and satisfy logic laws (e.g. De Morgan’s laws). In this paper, all entities form a distribution to be bound to a variable and have to be normalized after every operation (Equation 7-15, 17). As a result, NSMP doesn’t satisfy De Morgan’s laws and has to resort to DNF to handle disjunctions.
- The authors claimed interpretability as an advantage of NSMP, but there are no supporting qualitative experiments. I would expect some experiments to visualize the intermediate variables in complex queries similar to [3, 4].

[1] Logical Message Passing Networks with One-hop Inference on Atomic Formulas. Wang et al. ICLR 2023.

[2] Faithful Embeddings for Knowledge Base Queries. Sun et al. NeurIPS 2020.

[3] Complex Query Answering with Neural Link Predictors. Arakelyan, Daza and Minervini et al. ICLR 2021.

[4] Neural-Symbolic Models for Logical Queries on Knowledge Graphs. Zhu et al. ICML 2022.

[5] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors. Yin et al. ICLR 2024.","- In my opinion, dynamic pruning refers to pruning strategies that depend on the learned representations. Here the pruning strategy only relies on the query graph and the layer index, which should be regarded as a static pruning strategy. A better alternative is to call it Bellman-Ford iteration or breath-first search. You may refer to [6, 7] for more insights.
- Line 473-475: It looks like that the hyperparameters of NSMP have to be tuned for each dataset. Could you provide an analysis on hyperparameter sensitivity? Do there exist some robust default hyperparameters?
- Line 20-23: The logic of this sentence is weird. How does “re-using a simple pre-trained neural link predictors” serve as an approach to “generalizes to complex queries based on fuzzy logic theory”?
- Line 132-133: What do you mean by “the more complex existential first order logic formulas”? Is it a disadvantage?
- Line 182: Is DNF really scalable for handling disjunction operators? I feel the opposite.
- Equation 9 & 10: What if we get negative values after subtraction?
- Typos:
    - Line 30: graph representation → graph representations
    - Line 34: A more straightforward → A straightforward
    - Line 261: symbolic representation → symbolic representations

[6] Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction. Zhu et al. NeurIPS 2021.

[7] Distance-Based Propagation for Efficient Knowledge Graph Reasoning. Shomer et al. EMNLP 2023."
lA6du4Q0Zc,lA6du4Q0Zc,Neural-Symbolic Message Passing with Dynamic Pruning,Reject,OqZLJhzrLy,ICLR.cc/2025/Conference/Submission2632/Reviewer_d38M,"The paper proposes a Neural-Symbolic Message Passing framework (NSMP) for complex logical query answering. NSMP uses the same components as many other baselines in the literature: a pre-trained link predictor (ComplEx-N3) as in CQD and FIT and the same message passing mechanism as in LMPNN. The only difference between NSMP and previous works is “dynamical pruning” which is a manual stop on message propagation from intermediate variables whose node states have not yet been updated. Experimentally, NSMP underperforms to GNN-QE and CQQ-A on BetaE datasets and underperforms to FIT on EFO-1 dataset while being somewhat faster than FIT.",,"The paper is either a direct plagiarism of [1] or a poor attempt to sell a very marginal change using the same template as [1] with a few changed words and butchered phrases.

Below are some examples of the significant text overlap with several replaced words:

* **Section 1: Introduction**

[1] `Knowledge graphs (KGs) store factual knowledge in the form of triples that can be utilized to support a variety of downstream tasks`

[this work] `Knowledge graphs (KGs) store factual knowledge in the form of graph representation, which can be applied to various intelligent application scenarios`

-----

[1] `However, given that modern KGs are usually auto-generated [48] or built through crowd-sourcing [52], so real-world KGs [9, 11, 45] are often considered noisy and incomplete, which is also known as the Open World Assumption [24, 28].`

[this work] `However, given that modern KGs are usually auto-generated () or built through crowd-sourcing (), real-world KGs () often suffer from incompleteness, which is also known as the Open World Assumption (OWA) ().` - note that even citations are the same.

-----

* **Section 2: Related Work** (almost verbatim content copy of the same sections and paragraphs), some particular examples:

[1] `Reasoning over KGs with missing knowledge is one of the fundamental problems in Artificial Intelligence and has been widely studied`

[this work] `Reasoning over KGs with missing knowledge is one of the long-standing topics in machine learning and has been widely explored`

-----

[1] `Other methods for link prediction include rule learning [42, 69], text representation learning [43, 53, 55], and GNNs [50, 70, 72].`

[this work] `... other research lines for one-hop KG reasoning include rule learning (), text representation learning (), reinforcement learning (), and graph neural networks ().` - again, references are the same

-----


[1] `… they embed entities and relations into continuous vector spaces and predict unseen triples by scoring triples with a well-defined scoring function. Such latent feature models can effectively answer one-hop atomic queries over incomplete KGs.`

[this work] `… neural link predictors () have been proposed to answer one-hop atomic queries on incomplete KGs. These latent feature models learn a low-dimensional vector for each entity and relation. By employing a well-defined scoring function to assess link confidence, they can effectively predict unseen links.`

-----

* **Section 2.2** is a rephrased version of the same paragraph from [1]

* **Section 3.1** (Knowledge Graphs) copies the content from Section 3 (Model-Theoretic Concepts for Knowledge Graphs) from [1]

* **Section 3.2** (Existential First Order Queries With a Single Free Variable) is almost a verbatim copy of Section 2.2 (EFO-1 Queries and Answers) from the FIT paper [2]

* **Section 3.3** (Query Graph) re-phrases the same section in [1]:

[1] `Since disjunctive queries can be solved in a scalable manner by transforming queries into the disjunctive normal form, it is only necessary to define query graphs for conjunctive queries`

[this work] `As mentioned above, since a DNF query can be addressed by solving all its sub-conjunctive queries, it is only necessary to define query graphs for conjunctive formulas`

-----

And copies the definitions 1-6  from [2]

* **Section 3.4** (Neural Link Predictors) copies the same from [1] and just replaces the model name CLMPT with NSMP
* **Section 3.5** (Neural One-Hop Inference on Atomic Formulas) re-uses the same-named subsection from [1]:

[1] `On each edge, when a node is at the head position, its neighbor is at the tail position, and vice versa.`

[this work] `On each edge, when a node is at the head position, its neighbor is at the tail position, and vice versa`

-----

[1] *Specifically, a logical message encoding function $\\rho$ is proposed to perform one-hop inference.*

[this work] *Specifically, a logical message encoding function $\\rho$ is proposed to define this neural one-hop inference on edges.*

Then, Equations 3-6 are exactly the same as Equations 6-9 from [1]

-----


* **Section 4** (Neural-Symbolic One-Hop Inference on Atomic Formulas) is largely based on ENeSy [3] (Section 4.2, page 8) without citing it: including the derivation of negation as $\\frac{\\alpha}{|\\mathcal{V}|} - p_tM_r^{\\top}$ and defining the encoding function as a softmax after concat where concat *is a function mapping the similarity between all entities $e \\in \\mathcal V$ and $\\mathbf{v}$ to a vector* (ENeSy) and *is a function that maps the similarities between all entities and the intermediate embedding inferred by $\\rho$ to a vector* (this work).

* **Section 5.1** (Message Passing with Dynamic Pruning) almost rephrases Section 4.1 (Conditional Logical Message Passing) from [1], they even start with the same sentence:

[1] `A query graph contains two types of nodes: constant entity nodes and variable nodes. `

[this work] `A query graph contains two types of nodes: constant and variable nodes`

-----

Then, 

[1] `We decide whether to pass logical messages to a node based on its type.`

[this work] `we decide whether a variable node should pass messages to its neighboring variable nodes based on whether its state has been updated`


* **Section 5.2** (Node State Update Scheme) uses the same text and notation as Section 4.2 (Node Embedding Conditional Update Scheme) from [1] with small additions, eg,

[1] *Next, we discuss how to calculate the $z_e^a$ nd $z_v^l$ from the input layer $𝑙 = 0$ to latent layers $𝑙 > 0$*

[this work] *Next, we discuss how to compute these representations from the input layer $l = 0$ to latent layers $l > 0$*

-----

[1] For each neighbor node $n \\in N(v)$, one can obtain information about the edge (i.e., the atomic formula) between $n$ and $v$, which contains the neighbor embedding $z_n^{(l −1)} \\in D$, the relation $r_{nv} \\in R$, the direction $D_{nv} \\in \\\\{ℎ \\rightarrow 𝑡, 𝑡 \\rightarrow ℎ\\\\}$, and the negation indicator $Neg_{nv} \\in \\\\{0, 1 \\\\}$

[4, LMPNN paper, Section 6.2] For each neighbor node $v \\in N(n)$, one can obtain its embedding $z_v^{(l−1)} \\in D$, the relation $r_{v \\rightarrow n} \\in R$, the direction $D_{v \\rightarrow n} \\in \\\\{h2t, t2h\\\\}$, and the negation indicator $Neg_{v \\rightarrow n} \\in \\\\{0, 1\\\\}$

[this work] For each neighboring node $n \\in N_{DP} (v)$, one can obtain information about the edge between $n$ and $v$, which contains the neighbor mebdding $z_n^{(l-1)}$, the neighbor symbolic vector $s_n^{(l-1)}$, the relation embedding $r_{nv}$ , the relational adjacency matrix $M_{r_{nv}}$ , the direction $D_{nv} \\in \\\\{h \\rightarrow t, t \\rightarrow h\\\\}$, and the negation indicator $Neg_{nv} \\in \\\\{0, 1\\\\}$

It is obvious that [this work] just re-hashes the same content from [1] and [4] with a slightly different notation.

-----

Experimentally, the proposed NSMP underperforms well-established models on all benchmarks: worse than CQD-A on BetaE queries and worse than FIT on EFO-1 queries.

-----

Based on the above arguments, I do not see any scientific contribution in this manuscript, the issue has to be elevated to ACs, SACs, and PCs. It is sad that in this submission we have to measure the degree of plagiarism and deception instead of novel scientific contributions.


[1] Zhang et al. Conditional Logical Message Passing Transformer for Complex Query Answering. KDD 2024.   
[2] Yin et al. Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors. ICLR 2024.  
[3] Xu et al. Neural-Symbolic Entangled Framework for Complex Query Answering. NeurIPS 2022.  
[4] Wang et al. Logical Message Passing Networks with One-hop Inference on Atomic Formulas. ICLR 2023.",
ST6i7VMyYn,ST6i7VMyYn,Unlearning Virus Knowledge Toward Safe and Responsible Mutation Effect Predictions,Reject,GpifdU622f,ICLR.cc/2025/Conference/Submission2472/Reviewer_z71T,"This paper presents a method called PROEDIT for “unlearning” virus-related knowledge in pre-trained protein language models (PLMs) to reduce ethical and biosafety risks in protein engineering tasks. This approach appears to be adapted from text LLM unlearning techniques. The authors choose a three-scope training scheme, including an unlearning scope, a retention scope, and a corruption scope, to selectively remove virus-specific data while retaining general capabilities on non-virus proteins.","1. The approach focuses on unlearning knowledge of the protein model, a specific verticle domain, which is novel.

2. The discussion of baselines in the experimental section appears sound.","1. I believe that prior work on unlearning knowledge in text-based LLMs, with similar designs, should be properly credited in the methods section. 
The terms designed in the method section feel **similar** [1, 2] and (maybe thus) the design of the algorithm appears to lack explanation. 
Unfortunately, I could not find any explicit citations or discussions on how you design (or analogy, or choose) the algorithm used, and I hope I missed something, as it would otherwise be**inappropriate**.

2. Compared to unlearning in text, the task of unlearning in the protein context feels somewhat artificial, with relatively limited significance.

3. Regarding the algorithm, is there a more principled way to select $D^{\\text{sim}}$?

4. A minor issue: please keep the notation for  $x_M$  and $x_{/M}$  consistent.


[1] Chen J, Yang D. Unlearn what you want to forget: Efficient unlearning for llms[J]. arXiv preprint arXiv:2310.20150, 2023.

[2] Yao Y, Xu X, Liu Y. Large language model unlearning[J]. arXiv preprint arXiv:2310.10683, 2023.","In the domain of safety alignment in text generation, there are methods to teach models to abstain. Is there any feasibility for this approach in protein modeling?"
ST6i7VMyYn,ST6i7VMyYn,Unlearning Virus Knowledge Toward Safe and Responsible Mutation Effect Predictions,Reject,iQ6eyzbhhw,ICLR.cc/2025/Conference/Submission2472/Reviewer_gH9B,"This paper introduces PROEDIT, a framework for knowledge unlearning that enables pre-trained protein language models (PLMs) to selectively forget virus-related information while preserving predictive accuracy for non-viral proteins. Empirical validation across benchmark datasets shows that PROEDIT successfully reduces predictive capacity for viral mutations without compromising performance on beneficial proteins. However, the paper lacks clarity on the real-world necessity of selective virus knowledge unlearning, and the practical implications of this setting remain underexplored. Additionally, the evaluation pipeline does not sufficiently substantiate PROEDIT’s claims regarding safety and efficacy, as it lacks rigorous alignment with the framework's stated goals. For these reasons, I do not believe this paper meets the standards for acceptance.","* **Precise Knowledge Unlearning Framework**: PROEDIT’s method incorporates a carefully designed approach to model parameter adjustment by defining unlearning, retention, and corruption scopes, allowing the model to selectively forget virus protein-related knowledge. This layered optimization helps to some extent in reducing the risk of affecting non-virus protein knowledge while also providing a degree of stability and adaptability to the model.

* **Comparison of Different Edit Methods**: The paper systematically examines various edit methods, including No Edit, Gradient Ascent, Joint Gradient Ascent and Descent, Random Labels, Gradient Ascent with KL Constraint, and the proposed PROEDIT. These comparisons provide clearer insights into the impact of each method on model performance and virus knowledge unlearning.","* **Unclear Justification of the Setting**: The paper fails to clearly explain why it is necessary to remove virus-related knowledge in protein language models, especially as this task is uniquely proposed by the authors themselves. It states, ""This approach reduces ethical risks, specifically the ability of deep learning models to enhance the properties of viruses, while maintaining the model’s overall performance in designing normal, non-harmful proteins."" However, the paper confuses the concept of ""proteins present in viruses"" with ""harmful proteins."" Not all ""proteins present in viruses"" are harmful. On the contrary, some viral proteins are beneficial for protein engineering; for example, the capsid protein in adeno-associated virus (AAV, https://en.wikipedia.org/wiki/Adeno-associated_virus) is widely used in gene therapy and vector design due to its low pathogenicity and efficient gene delivery capability. Such viral proteins play an important role in certain biomedical applications, so removing virus-related protein knowledge across the board may be inappropriate. Furthermore, in the paragraph starting with ""The same concern has been raised in mutation effect prediction tasks,"" the paper provides no references to support why removing “virus protein” knowledge is necessary. Overall, the justification for this setting lacks persuasiveness.

* **Methodological Limitations**: The entire approach has fundamental limitations. According to the paper’s inference, protein engineering should aim to “design normal, non-harmful proteins.” However, using an unlearning approach cannot effectively achieve this goal. A more straightforward method would be to determine if a generated protein sequence is harmful (or, as the paper suggests, a “virus protein”). If a “virus protein” is designed, it could simply be rejected. In bioinformatics, this task is relatively simple. For instance, one can use BLAST or other homology search tools to compare the protein sequence with known “virus protein” databases, such as “virus protein” sequences in UniProt. High sequence similarity often suggests functional or origin-related associations. Alternatively, protein function databases like Pfam or InterPro can be used to identify functional domains. If the protein contains domains commonly found in “virus proteins,” such as reverse transcriptase or capsid proteins, it may be identified as a “virus protein.”

   Even if we insist on a machine learning approach, a binary classifier could be trained using “virus protein” data to predict whether a given protein is a “virus protein.” Using an unlearning framework forces the model to forget information about “virus proteins.” However, if the model does generate a “virus protein” (even with a lower probability), it will not recognize it as such. Thus, an additional predictor would still be needed to identify potentially harmful proteins, as outlined in the simpler approaches above.

* **Question Regarding Results on the AAV Dataset**: The PROEDIT results on the AAV dataset, shown in Table 2 and Figure 4a, appear questionable due to the negative Spearman correlation, which remains consistently negative throughout training—a common mistake when fine-tuning ESM models on supervised mutation effect datasets. Since Spearman correlation measures rank-order agreement, flipping the predictions' signs would make the correlation positive. In this context, considering the absolute value of Spearman correlation would be more meaningful, as it better reflects whether the model still retains “virus protein” information. The results suggest that PROEDIT, particularly at the 150M scale, may still retain virus-related knowledge and even outperform the pre-unlearning model, thereby casting doubt on the true effectiveness of the unlearning process.",See **Question Regarding Results on the AAV Dataset** in weakness.
ST6i7VMyYn,ST6i7VMyYn,Unlearning Virus Knowledge Toward Safe and Responsible Mutation Effect Predictions,Reject,ROYYjiXeyu,ICLR.cc/2025/Conference/Submission2472/Reviewer_Myjj,"In this paper, the authors introduce protEdit, a protein language model fine-tuned with a specialized loss function aimed at ""unlearning"" certain virus sequences. They utilized the pre-trained ESM2 model, fine-tuning it on a dataset consisting of 65 million non-virus proteins and 560,000 virus proteins. The model was aligned with original non-virus-like protein sequences from the dataset. Several experiments were conducted to demonstrate the capabilities of the tuned model.","The paper is well-structured and easy-to-follow. 

The concept of addressing safety concerns within AI applications in biology is innovative and timely. While the paper’s exploration into protein fitness prediction has debatable relevance, the attempt to integrate ethical considerations into AI model training is commendable and adds a layer of originality to the work.","The fundamental premise of what constitutes 'harmful' protein engineering is not convincingly established in the paper. While the intent to prevent harmful applications is clear, the practical implications and ethical guidelines are not thoroughly explored. For instance, engineering on adeno-associated viruses (AAVs) can be beneficial for gene therapies, contradicting the notion of universal harm. (https://www.science.org/doi/10.1126/science.adm8386) Moreover, experiments on engineering antibiotic resistance genes can help us better understand its mechanisms and lead to the discovery of novel antibiotics (https://www.cell.com/fulltext/S0092-8674(15)00078-1). The experiment design, that simply define ""engineering virus is unsafe"" is not valid and fundamentally flawed from this point of view. 

The authors should provide a more detailed discussion of how they define ""harmful"" protein engineering, acknowledging potential beneficial applications of viral protein engineering, and explaining how their approach could be refined to distinguish between beneficial and harmful applications.

I noticed that the authors conducted both supervised and unsupervised learning experiments on the virus-like proteins. For the supervised learning task, without LLM, other machine learning methods, even the one-hot encoding of the sequence, may work quite good – as we have the labels and any model can learn from the dataset (https://pubmed.ncbi.nlm.nih.gov/35039677/). From Table 2 it’s surprising for me to see protEDIT does not work at all for supervised tasks, it suggests that the model embedding for these sequences are simply useless. Moreover, it’s confusing that the authors tried their best to learn an embedding that is probably worse than one-hot. If so, why do we need your model? 

For the unsupervised variant effect prediction, I have to say that it's the MSA that matters. The EVE (https://www.nature.com/articles/s41586-021-04043-8), DeepSequence (https://www.nature.com/articles/s41592-018-0138-4) can performs better than language model. Even the BLOSUM62 can carry some information for the mutation effect prediction. SO I did not see what the hint was in poisoning the language model. The entire experiment design does not sound right to me. 

For me the idea of this paper is like a straw man proposal. The authors try to do something that seems to be useful but totally miss the key points and lead to something useless.",See weakness
ST6i7VMyYn,ST6i7VMyYn,Unlearning Virus Knowledge Toward Safe and Responsible Mutation Effect Predictions,Reject,UOKYYPF2x6,ICLR.cc/2025/Conference/Submission2472/Reviewer_9h4t,"This paper addresses mutation effect prediction. The authors proposes PROEDIT, a knowledge unlearning-based approach, which refines pre-trained models by distinguishing among three sets of training data and unlearning specific targets. The method is validated on open benchmarks, showing it can reduce the model's ability to enhance virus mutants without sacrificing performance on non-virus proteins. 

*I am not familiar with the research problem, so I am eager to see the feedback from the authors and the opinions of the other reviewers.*","- This paper investigates a critical problem: the safety issue in pre-trained PLM models. 
- The authors demonstrate the effectiveness of the method through experiments on various benchmarks.","- Insufficient baseline comparisons; it is advised to add more PLM models mentioned in the related work.
- As shown in Table 3, it seems that PROEDIT only achieves the best performance in ProteinGym (Virus) on the Perplexity metric, which diminishes the contribution of PROEDIT.
- The efficiency section should include more comparisons with other methods.
- The ablation study for the different scopes (Unlearn, Retention and Corruption) is not provided.

Minor:
- It lacks a discussion of the relationship of this work in the related work section, which would help readers better understand this study.",See weaknesses.
foQ4AeEGG7,foQ4AeEGG7,Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference,Accept (Poster),aL9gjPeuHF,ICLR.cc/2025/Conference/Submission2331/Reviewer_EAHD,"The paper addresses causal inference in situations involving interference, where both the interference graph and the summary function are unknown (note that the interference graph is distinct from a social network). The authors assume that interference occurs within an 
L-order neighbor network, where peer information can be aggregated using a graph transformer. They utilize a cross-attention mechanism to capture complex sequential interference representations. Additionally, they apply techniques such as regularization to balance the distribution of confounders across control and treatment groups, and they leverage moment conditions to account for confounding and mitigate model misspecification.","The paper has a well-defined research focus on causal inference under interference with unknown interference structures.

The experiments conducted are engaging and effectively demonstrate the proposed methods.

 Overall, the paper is well-organized and presented.

A few sections lack clarity, as noted in my specific questions.","The paper’s primary contribution—clearly distinguishing the interference graph from the social network—is highly valuable. However, if the approach is limited to neighboring nodes only, it risks undermining this motivation. On the other hand, if this limitation is not imposed, the computational complexity becomes quadratic in the number of nodes when using transformers, which poses feasibility issues for large graphs.

Additionally, a category of related work on non-iid data, particularly doubly robust estimators, IPW, and representation-based methods, appears to be missing from the review. Including this literature [1, 2, 3] could provide a more comprehensive view of the field.

Since doubly robust estimators represent the state-of-the-art for causal parameter inference and have advantages over IPW-based methods, adding at least one of these approaches as a baseline would further strengthen the comparisons [1, 2, 3].


refs:
[1] Ogburn, Elizabeth L., et al. “Causal inference for social network data.” Journal of the American Statistical Association 119.545 (2024): 597-611.
[2] Khatami, Seyedeh Baharan, et al. “Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects.” arXiv preprint arXiv:2403.11332 (2024)
[3] Leung, Michael P., and Pantelis Loupos. “Unconfoundedness with network interference.” arXiv preprint arXiv:2211.07823 6 (2022).","Could you provide some additional justification or intuition for how the bridge function makes the residual independent of x?
	Line 377: You mention, “If the model is correctly specified and properly optimized…” Could you elaborate on how you ensure the model is both correctly specified and optimized?

Also, why is it necessary for the residual to be independent of t?	Section 4.3 could benefit from a bit more clarity in its presentation.

Many of the baselines, as far as I’m aware, assume a fixed and known exposure mapping. In your experiments where the interference graph is unknown, or the graph is known but the summary function is not, could you clarify how comparisons are made with these baselines that assume the exposure map is already known?

line 414 and 415: In the presence of unmeasured confounders, we will collect negative control variables and embed them into the Eqs. (11) and (12)

Could you elaborate on this aspect of your technique? line 481: We conduct 10 replications to report (mean±std) results. Given that this is a relatively small number of trials, could you provide some reasoning behind this choice?"
foQ4AeEGG7,foQ4AeEGG7,Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference,Accept (Poster),rvb0qc2GAQ,ICLR.cc/2025/Conference/Submission2331/Reviewer_uH9y,"CauGramer leverages a Graph Transformer architecture with cross-attention to aggregate peer information within an L-order neighborhood.  This approach addresses the limited receptive field of traditional GCNs.  The model learns interference representations for both features and treatments, using cross-attention to capture complex interactions between units.  To mitigate confounding bias, the model incorporates confounder balancing and minimax moment constraints, further enhancing the robustness of the treatment effect estimation.  The authors extend the model to handle unmeasured confounders by incorporating negative control variables.","Addresses a significant limitation: The paper directly tackles the crucial challenge of estimating treatment effects in networked data with unknown interference structures and aggregation functions, a problem neglected by many existing methods.

Novel architecture: CauGramer's architecture is innovative, combining the strengths of graph neural networks and transformers to capture complex interference patterns. The use of cross-attention is particularly insightful for broadening the receptive field.

Robustness: The incorporation of confounder balancing and minimax moment constraints significantly improves the robustness of the model. The extension to handle unmeasured confounders further enhances its practical applicability.

Comprehensive evaluation: The empirical evaluation is thorough, covering various scenarios and comparing against multiple strong baselines.","NA, see the questions","Can you elaborate on the computational complexity of CauGramer, especially compared to other graph neural network and transformer-based methods? Are there any specific strategies employed to mitigate the computational burden, such as approximation algorithms or distributed training?

How sensitive are the results to the choice of the L-order neighborhood parameter in CauGramer? What is the impact of choosing too small or too large an L value? Is there an optimal way to select this parameter?

The paper employs the Wasserstein distance (IPM) to measure the discrepancy between treatment and control group representations. Have you considered other divergence measures (e.g., Jensen-Shannon divergence, Kullback-Leibler divergence)? How would the results change with different choices of divergence measures?

You address the issue of unmeasured confounders by using negative controls. What is the impact on the performance of CauGramer if the negative control variables are not perfectly valid or are missing? How robust is the model to misspecification in the negative control variables?

What are the current study's limitations, and what are your plans for future work? Specifically, are there any plans to explore alternative architectures, different attention mechanisms, or more sophisticated methods for handling unmeasured confounders?

In some sense, I think this paper's theoretical is somehow a bit thin. I am curious whether we could give your method some theoretical guarantee."
foQ4AeEGG7,foQ4AeEGG7,Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference,Accept (Poster),DJXhn3GiMI,ICLR.cc/2025/Conference/Submission2331/Reviewer_qEMS,"The paper ""Causal Graph Transformer for Treatment Effect Estimation under Unknown Interference"" introduces an innovative Interference-Agnostic Causal Graph Transformer (CauGramer) framework for causal effect estimation under unknown network interference. This approach captures complex interference information across nodes through an L-order graph attention mechanism combined with cross-attention, without relying on a known interference graph. The model incorporates confounder balancing and minimax moment constraints to achieve robust causal effect estimation.","1.The study introduces a novel approach by addressing causal inference from the perspective of an unknown interference graph. The combination of L-order graph attention with cross-attention for modeling interference information.
2.The paper is well-structured.
3.Causal inference under unknown interference graphs is a significant challenge in network data analysis, and the outcomes of this study provide a reference for future research.","1.In Table 1, which includes i.i.d. data, the title ""Causal Networked Data"" may not be fully accurate. Perhaps ""Observational Data"" would be more accurate.
2.On line 129, ""Representation"" could be more accurate formatted in lowercase as ""representation.""
3.The method appears to rely on an unconfoundedness assumption, yet latent confounders are mentioned later, violating this assumption.
4.In Section 4.1, Ma & Tresp’s work is not necessarily limited to first-order interference.
5.There are multiple instances of quotation mark formatting errors in lines 375, 449, 487, and 516, among others.
6.How did you address latent confounders in their approach?
7.When network interference is unknown, how does the proposed method differ from existing work (e.g., Rf1-Rf3)?
8.The work lacks a description of the number of layers (L) and attention heads (M) used.
9.While the paper compares CauGramer with several mainstream methods, it lacks a detailed discussion of specific methods for unknown interference graphs (e.g., Rf1-Rf3).
Rf1: Sävje, Fredrik, Peter Aronow, and Michael Hudgens. ""Average treatment effects in the presence of unknown interference."" Annals of Statistics 49.2 (2021): 673.
Rf2: Hoshino, Tadao, and Takahide Yanagi. ""Causal inference with noncompliance and unknown interference."" Journal of the American Statistical Association (2023): 1-12.
Rf3: Lin, Xiaofeng, et al. ""Treatment Effect Estimation Under Unknown Interference."" Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer Nature Singapore, 2024.","1.In the COVID case study, where transmission primarily occurs through contact, how can interference occur in the absence of a direct link? Could you clarify potential mechanisms of transmission under such conditions?
2.Could you clarify why the final output \\( r_x^{(L)} \\) is used as the input to the first layer of the peer-treatment representation network, specifically \\( r_t^{(1)} = r_x^{(L)} \\)?
3. Please explain the statement: ""we simulate unmeasured confounding by removing features.""
4. The paper states that the relationship between peer-treatment \\( t_{Pi} \\) and outcome \\( y_i \\) remains confounded by the common causes \\( r_i \\), leading to biased estimates of peer and total effects. Moreover, the peer-treatment \\( t_{Pi} \\) is unknown. In the main text, it is mentioned that \\( r = r_x \\oplus r_t \\). Could you clarify why \\( r_i \\) acts as a confounder for both \\( t_{Pi} \\) and \\( y_i \\)?"
foQ4AeEGG7,foQ4AeEGG7,Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference,Accept (Poster),vfnSiXtw4E,ICLR.cc/2025/Conference/Submission2331/Reviewer_ii3V,"This study aims to estimate treatment effects in the presence of unknown interference (including unknown network and summary mechanism). To achieve this goal, it proposes a method named CauGramer, which uses the transformer to address unknown interference.
However, some important studies and baseline methods for unknown interference are ignored.","1. The presentation is clear.
2. A new method is proposed.
3. Identifiability of total effect is given, which provides insight for studying unknown interference.
4. Experiments are conducted to verify the proposed method.","# Weakness
1. Some important studies are ignored. Unknown interference has been addressed in many existing works [R1-R4]. These works have excellent contributions for unknown interference, but they are ignored by authors. Authors should introduce the difference between their work and these works. Specifically, the authors add a paragraph in the related work section comparing their approach to [R1-R4], highlighting key differences in methodology, assumptions, and capabilities.

2. As introduced in Weaknesses 1, unknown interference is not a new challenge in causal inference, because there are some existing methods [R1~R4] that focus on this issue. 
The novelty of this study might be limited. It might be helpful if the authors could introduce the limitations of these existing methods and how this study addresses them. Specifically, what limitations of prior methods does CauGramer address? What new capabilities does it enable?

3. In addition, existing methods [R1-R4] have a more relaxed assumption for unknown interference. They do not require any network information, whereas the proposed method requires network information, i.e., $A$.

4. For experiments, authors need to compare with the existing methods for unknown interference. In the current version, authors did not compare with [R1-R4]. The authors can add [R1], [R3] and [R4] as baselines in their experiments, following the implementation details.  [R1] can create a network (you can only take the edges among different nodes) when the input graph is a complete graph or an initial graph, i.e., $A$ in the authors' setting. Then, you can apply G-HSIC on the network created by [R1] as a baseline. [R3] and [R4] can directly work when you give or not give an input graph. [R2] can be ignored, as it is not a learning method. 

5. In definitions 1 and 3, as well as equations (1) and (2), should the $t$ in the right of '=' be 1, as the treatment is a binary value, i.e., 1 or 0.





# Reference
[R1] Bhattacharya, et al.  Causal inference under interference and network uncertainty.  UAI 2019.

[R2] Sävje, et al. Average treatment effects in the presence of unknown interference. The Annals of Statistics 2021.

[R3] Mayleen, et al. Staggered rollout designs enable causal inference under interference without network knowledge. NeurIPS 2022.

[R4] Lin, et al. Treatment effect estimation under unknown interference. PAKDD 2024.","# Questions:
1. Why do you not introduce the existing works [R1-R4], which have excellent contributions for unknown interference? Can you introduce the difference between your works and these works?  What limitations of prior methods does CauGramer address? What new capabilities does your method enable?

2. [R1-R4] can work when the network information is totally unknown. Can the proposed method work when the network information is totally unknown? How does the performance change in this case?

3. In definitions 1 and 3, as well as equations (1) and (2), should the $t$ in the right of '=' be 1 instead of $t$?

4. For more details, please see Weakness.



# Reference
[R1] Bhattacharya, et al.  Causal inference under interference and network uncertainty.  UAI 2019

[R2] Sävje, et al. Average treatment effects in the presence of unknown interference. The Annals of Statistics 2021

[R3] Mayleen, et al. Staggered rollout designs enable causal inference under interference without network knowledge. NeurIPS 2022.

[R4] Lin, et al. Treatment effect estimation under unknown interference. PAKDD 2024."
aJnKjvTtPq,aJnKjvTtPq,Low Rank Quantization Adaptation for Large Language Model,Reject,wBvZXHgkl9,ICLR.cc/2025/Conference/Submission1723/Reviewer_FBJK,"This paper extends the QA-LoRA method by introducing Low-Rank Quantization Adaptation (LoQA) to enhance the fine-tuning of large language models (LLMs) within a quantized framework. LoQA addresses the integration challenges between quantization and Low-Rank Adaptation (LoRA), proposing Holistic Quantization Low-Rank Adaptation (HQ-LoRA), a quantization-compatible adaptation technique that allows finetuning of all quantization parameters (including scale and zero points) resulting in improved model performance. Additionally, the paper introduces Quantized Bit-aware scaling (QBAS), which adjusts LoRA scaling to account for the impact of integer weights at varying bit-widths. Overall, LoQA extends the QA-LoRA paper by modifying not only the quantization zero points (as in QA-LoRA) but also the quantization scale.","Strengths:

-Considering the quantization scaling as well during finetuning gives much more representation power compared to earlier approaches like QA-LoRA, which focus only on quantization zero points. Also, as demonstrated by empirical results, it usually results in better generalization across diverse set of models and bit budgets.

-LoQA especially outperforms previous methods on low-bit budgets, which would make it useful for ultra resource-efficient settings.

-In the 4bit regime, LoQA seems to perform competitively to algorithms like IR-QLoRA that use a mix of 4/16 in their inference deployment

-Given that it doesn’t carry 16bit LoRA parameters to the inference phase (as opposed to methods like QLoRA), it would bring considerable inference efficiency compared to ""4+16"" type of methods.","In general, this method is interesting - however, in terms of novelty,  this work is indeed an incremental work on top of the QA-LoRA work. Moreover, this paper could benefit from improvements in the presentation and description of the experiments.

Weaknesses:

-The paper presentation could be improved, especially in the experiment section. Implementation details are underspecified in many parts. Given that this research effort is based on empirical results, this lack of experimental setting would make it hard to reproduce the results for the community.

-For experiments comparing LoQA with QA-LoRA (as well as other similar methods), the corresponding ranks of the methods are not reported in most cases, which makes the reader question if the improvement could be due to using more parameters by LoQA or not.

-There’s lot of focus on MMLU evaluation after finetuning on alpaca/Flan datasets, which might not be representative of how this algorithm would actually work for other type of tasks/datasets. The only exception, Table 5, focuses on commonsense reasoning, which the performance is very close to QA-LoRA, while it uses half of your training complexity by having a single set of LoRA parameters for zero points.

-Table captions in many cases are not ""self contained"" and often have a very general caption, which make it hard to go through different tables and understand the results.",-Regarding QBAS: the scaling that you proposed is then being multiplied by a hyperparameter. Are you using the same $alpha$ across different bits?
aJnKjvTtPq,aJnKjvTtPq,Low Rank Quantization Adaptation for Large Language Model,Reject,MHCCD6spDH,ICLR.cc/2025/Conference/Submission1723/Reviewer_45yq,"The paper introduces Low-Rank Quantization Adaptation, called LoQA, a new approach for fine-tuning large language models (LLMs) while preserving quantization. LoQA consists of two techniques: (1) Holistic Quantization Low-Rank Adaptation (HQ-LoRA): This approach fine-tunes all quantized parameters, enabling broader optimization without losing the quantized model structure. (2) Quantized Bit-Aware Scaling (QBAS): This technique dynamically adjusts scaling factors based on bit-widths, enhancing performance stability across various quantization levels. LoQA is tested against state-of-the-art quantization methods and is effective across different model sizes and bit configurations. It yields significant improvements, particularly under ultra-low bit-width scenarios.","+ LoQA introduces a novel quantization-aware fine-tuning method that can support quantized LLMs to effectively balance model compression with performance.
+ Experiments show that LoQA consistently outperforms previous methods in accuracy and efficiency, even in ultra-low bit-width configurations.
+ The method supports multiple bit-widths and integrates smoothly with existing post-training quantization methods, making it adaptable to a wide range of applications.
+ LoQA is evaluated using the MMLU and common sense QA tasks, with consistent results.","- LoQA requires more training time and memory compared to some baseline methods, which may hinder its practicality in resource-constrained environments (e.g., compared with QA-LoRA)
- The study did not use the latest datasets or the newest training paradigms, which might limit the generalizability of the results to other, more current LLMs (as also mentioned in Appendix).",Would you please describe how LoQA could be implemented in production?
aJnKjvTtPq,aJnKjvTtPq,Low Rank Quantization Adaptation for Large Language Model,Reject,o3e2y5skYd,ICLR.cc/2025/Conference/Submission1723/Reviewer_n8i8,"This paper aims to enable efficient LLM fine-tuning by combining quantization with LoRA. Rather than keeping LoRA inference in FP/BF16 during the forward pass, it proposes a holistic quantized LoRA approach where LoRA is fused into the quantization set to enable fully quantized inference. Additionally, the paper introduces a method to determine the LoRA scaling factor based on the target bit precision.

The proposed method achieves higher fine-tuning performance and training efficiency compared to QLoRA and QA-LoRA at 4/3/2-bit precision.","- The related work and motivation are clearly presented, making the problem the paper addresses easy to understand.
- The holistic quantized LoRA approach follows a similar direction to QA-LoRA, aiming to achieve fully quantized inference by fusing LoRA. The paper introduces a way to learn both the scaling and zero-point parameters, which adds flexibility to the method.","- The holistic quantization method formulation is somewhat difficult to follow. Figure 2 is overly simplified, and would benefit from revision to visually clarify the equations in Section 3.2.
- Experiments are limited to the LLaMA family, and there is a lack of results on LLaMA-3. If dataset is an issue, it may be preferable to use more recent open-source datasets (e.g., SlimPajama) instead of the relatively old and small sample Alpaca dataset. Alternatively, comparing performance on domain-specific fine-tuning tasks (e.g., GSM8K, OASST1) as done in LoftQ/LQ-LoRA could provide additional insights.
- The main contribution of this paper appears to be preserving the quantization set in inference. However, the proposed method lacks significant innovation compared to QA-LoRA, and there is no report on the actual efficiency gains achieved during fully quantized inference. Section 4.3 only briefly mentions the potential for kernel optimizations, which seems insufficient given that fullly quantized inference is the main focus of this paper.

In summary, while the proposed method consistently outperforms QA-LoRA and addresses an important problem, the limited experimental results and unclear explanations of key methods indicate room for improvement.","- In Section 3.2, the proposed method is still not entirely clear. On Line 249, what exactly do you mean by “two LoRA variants”?"
aJnKjvTtPq,aJnKjvTtPq,Low Rank Quantization Adaptation for Large Language Model,Reject,HxP7yhtwW2,ICLR.cc/2025/Conference/Submission1723/Reviewer_jKth,"This paper proposes a novel quantization-aware, parameter-efficient tuning method for large language models (LLMs) based on QA-LoRA. The approach reparameterizes both scaling and zero factors to mitigate quantization errors. Experimental results demonstrate its effectiveness.","1. Deployment is a crucial consideration for large language models (LLMs), with quantization-aware training being one of the key challenges in their deployment.

2. The proposed method is simple yet promising, as indicated by the experimental results.","1. While I understand the core idea the author aims to convey, I believe the writing could be improved by providing a more general analysis of approaches to quantization-aware (QA) training or reparameterization of quantization parameters. This would be more important to the community and the contributions will be more important.
2. The contributions appear to be incremental.
3. Figure 2 could be enhanced: it currently labels the LoRA parameters 𝐴 and 𝐵 for QLoRA, but lacks annotations for QA-LoRA and the proposed method. Additionally, the concept of Holistic LoRA is not clearly conveyed.
4. This method seems have limitations which only support group-wise quantization.","1. Any inference time results?
2. other questions please refer to the weaknesses."
aJnKjvTtPq,aJnKjvTtPq,Low Rank Quantization Adaptation for Large Language Model,Reject,pH2sP2V64j,ICLR.cc/2025/Conference/Submission1723/Reviewer_2qNk,"This paper introduces Low Rank Quantization Adaptation (LoQA) for LLMs, a novel parameter-efficient fine-tuning method for quantized models. LoQA aims to reduce memory consumption across model weights, gradients, and optimizer parameters. It includes two core modules: **HQ-LoRA** (Holistic Quantization Low-Rank Adaptation) and **QBAS** (Quantized Bit-Aware Scaling), aligning quantization parameter granularity with LoRA parameters in group quantization.","- The proposed method is well-motivated and sounded.
- This approach is straightforward yet effective, especially in ultra-low bit-width scenarios.
- Code is provided for reproducibility.","- **Limited novelty**: The method’s innovation is incremental, building on QA-LoRA and closely resembling group quantization applied to LoRA parameters. In Sec 3.2, LoQA uses two LoRA components, whereas QA-LoRA requires only one, which may lead to a higher parameter count.
- **Experimental limitations**:
  - LoQA does not demonstrate clear improvements on LLaMA 2, and for LLaMA 3, only training loss rather than performance results is presented. Providing promising results on more advanced models would strengthen the paper’s contribution.
  - Important baselines, such as **IR-QLoRA** and **QLoRA**, are absent in Tables 5,7,9, affecting comparison fairness.
  - While the authors claim broad applicability of LoQA across post-training quantization techniques, only **GPTQ** settings are evaluated.
- **Poor presentation and writing**:
  - Figure 2 lacks clarity in distinguishing between QA-LoRA and LoQA.
  - In Sec 3.3, the rationale for setting $maxq$ to $2^{N-1}$ is not sufficiently analyzed.
  - In some tables (e.g., Tables 4, 7, 8), entries are missing bold formatting to indicate the best results.
  - The LoRA rank used in the compared methods is not clearly specified.
  - The ""4+16 bit"" setting is not formally defined.
  - ""Llama-7b"" and ""Llama-7B"" are inconsistently referenced throughout the paper.
  - ""Post-fine-tuning"" in line 269 is incomplete.",Please refer to the Weaknesses above for questions.
WBUVagRgsd,WBUVagRgsd,Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification,Accept (Poster),xpLn6snkn7,ICLR.cc/2025/Conference/Submission1596/Reviewer_d1Q4,"The authors highlight the shortcomings of existing methods for existing Shapley-based explainers and propose a method called Salvage, which effectively learns and samples based on the Shapley value distribution.","1) The issues identified with existing methods appear valid and relevant.
2) The authors demonstrate an improvement in explanation accuracy compared to existing methods across various datasets.","1) The explainer model is essentially an estimation model for interpreting the behavior of the target classification model; however, this paper does not clearly define what the target model is. Furthermore, there is insufficient evidence to show that the method operates effectively across different target models.
2) Some results seem incomplete, as suggested by Figure 1.
3) The ablation study is lacking. While the proposed method focuses on effectively learning the distribution and sampling, there is no analysis of which aspect is more critical to the overall success of the method.
4) The problems identified with existing methods are described conceptually but lack empirical validation.","1) What exactly does Figure 3 illustrate?
2) Since the goal is to derive an explainer model for a specific classification model, I am curious not only about the classification performance but also about how well the predictions align with those of the existing classification model (as seen in Table 2)."
WBUVagRgsd,WBUVagRgsd,Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification,Accept (Poster),ttVgxEfAXe,ICLR.cc/2025/Conference/Submission1596/Reviewer_sz6D,"This paper introduces a novel explainability method for image classification known as Salvage. The paper employs a removal-based technique coupled with the concept of Shapley-distributions. These techniques are used to train an explainer model that learns the prediction distribution of the classifier on masked images. The authors address the imbalance between important and unimportant features by devising an informed sampling strategy. This strategy facilitates better approximation of the classifier’s distribution and helps the estimation of underrepresented features. The effectiveness of Salvage is validated through experiments on the ImageNette, MURA, and Pet datasets. The study illustrates that Salvage outdoes various baseline explainability methods and can additionally be used as a fully explainable classifier without a considerable fall in classification performance. The paper concludes by pointing out future optimizations and improvement possibilities for Salvage.","The paper presents ""Salvage,"" a novel removal-based explainability method for image classification that tackles unbalanced important features via an informed sampling strategy. The invention of Shapley-distributions for a more accurate approximation of classification probability distributions is impressive. The paper's comprehensive and clear presentation, alongside robust experimental evaluation, underlines the method's potential for explainable classification, with comparable accuracy to standard classifiers.","1. Although the paper demonstrates a performance comparison with a couple of explainability methods like ViT-Shapley and RISE, more extensive comparison with a wider array of contemporary removal-based explainability methods could provide a more robust evaluation of the Salvage algorithm's efficacy.
2. It would be beneficial to see Salvage's performance with other types of data or in other domains, beyond the ones mentioned in the paper (ImageNette, MURA, and Pet datasets). This would help in evaluating the broad applicability and versatility of the approach.
3. The impacts of temperature parameter changes in the softmax or sigmoid functions during the approximation of the classifier’s distribution could have been explored in more depth. More extensive experimental study in this aspect could enhance the robustness of Salvage.","1. Could you please elaborate more on why the informative sampling's improvement in performance is less noticeable in the Pet dataset compared to the other datasets?
2. How would different neural architecture designs for the explainer model impact the performance of Salvage?
3. Given that the model has been tested on a limited number of datasets, have you considered testing Salvage on a wider array of datasets, particularly more complex or diverse ones, to evaluate its broad applicability?"
WBUVagRgsd,WBUVagRgsd,Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification,Accept (Poster),dBFaIgADjm,ICLR.cc/2025/Conference/Submission1596/Reviewer_7jFJ,"This paper introduces Salvage,a removal-based explainability method for image classification. It includes a concept of Shapley-distributions,which offers a more accurate approximation of classification probability distributions and an informed sampling strategy that leverages approximated feature importance scores to reduce imbalance and facilitate the estimation of underrepresented features.","1. A  new concept of Shapley-distributions,which offers a
 more accurate approximation of classification probability distributions, is introduced.
2. The comparison methods, datasets, and metrics are quite comprehensive. On some metrics, it has a clear advantage.","**The presentation should be improved a lot.**

1. The explanation of symbols in the formulas is not clear enough, causing difficulty in understanding, such as Eq.2.
2. In Table 1, there are several obvious typographical errors of the experimental results, for example “68,56”. It should be “68.56”.
3. The bottom line of the Table 2 is not drawn!

**Soundness**
1. There is no theoretical  proof or experimental results can demonstrate that the INFORMATIVE SAMPLING has improved efficiency.
2. In Table 2, on PET, in terms of RRA, Salvage underperforms about 3 point compared with SOTA. There should be appropriate analysis and discussion regarding this.
3. Only make ablation study on INFORMATIVE SAMPLING.","1.How is the experimental performance when using SHAPLEY DISTRIBUTION ESTIMATION alone?

2. Is this method applicable to other types of tasks (such as object detection or segmentation)?"
WBUVagRgsd,WBUVagRgsd,Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification,Accept (Poster),Oh8z9vj9dw,ICLR.cc/2025/Conference/Submission1596/Reviewer_Hzto,"This article proposes a new explainability method for image classification. Most of the explainability methods target CNNs; instead, Salvage, the proposed method is architecture agnostic. Salvage is a removal-based approach based on ViT-Shapely with further improvements. The method surpasses the current SOTA.","The paper is clear and well-explained. The methodology is adequate. The method is well evaluated. The method, despite not being completely novel, builds on SOTA methods (ViT Shapely) and improves their shortcomings.",The method is incremental with respect to ViT-Shapely but better. So no much concern.,I find the paper good as is.
ZyCuQxyPJK,ZyCuQxyPJK,NeuroLifting: Neural Inference on Markov Random Fields at Scale,Reject,WWigiY45Oz,ICLR.cc/2025/Conference/Submission1486/Reviewer_jC41,"The authors propose a GNN based approach to approximate MAP inference in MRFs.  The contribution definitely follows recent trends in trying to use NNs to approximately solve hard problems, it falls a bit short in its treatment of historical work and references to modern approaches.  Overall, the presentation needs quite a bit of improvement.","- The proposed approach is a novel combination of optimization ideas with modern NN approaches.

- The authors provide a wide array of experimental results across different domains and both synthetic and real data.","- Some of the text looks like it was fed into an MRF for ""improvement,"" but it reads a bit awkwardly and uses imprecise statements.  Even if this isn't the case, I would suggest tidying it up a bit so as to make it read more like a traditional technical paper.

- The descriptions of MRFs and related work is poor.  Some parts of the discussion make it seems like some of the ideas being presented are novel to the community when they aren't really.  For example, the authors define MRFs factorizing over the cliques of a graphs and then claim novel insights that these cliques can be subdivided into edges when representing the graph.  This is, of course, what it means to be a clique in a graph.  Equation (4) isn't the standard loopy BP representation where inference is no longer exact (it isn't even clear why you need this formula anyway).  Saying that message passing looks like GNNs when it is in fact the other way around by design. Among many others.

- Some of the decisions made in the setup, e.g., the padding seem like that could lead to poor results in practice.  There is a comment about this in the paper, but the chosen method seems significantly suboptimal to me especially when the range of values that each potential function can take is small.","- Figure 1 is really difficult to understand.  

- I don't love calling this approach NeuroLifting because the are already ""lifted"" methods in relational MRFs.  I'm not sure what else I would call this, but given the long history in that domain optimization lifting strategies are not the first things that come to my mind when I hear the name.  Equation (6) is quite similar to traditional MAP relaxations (some of which are actually cited).  Saying

- Can you further explain the issues that arise with padding?  It seems clear that padding essentially with infinities is the correct theoretical thing to do -- what is the practical issue here?

- Lots of references should probably be added.  Here's one that is somewhat related to help get you started:

Arya, S., Rahman, T. &amp; Gogate, V.. (2024). Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models. <i>Proceedings of The 27th International Conference on Artificial Intelligence and Statistics</i>, in <i>Proceedings of Machine Learning Research</i> 238:2791-2799 Available from https://proceedings.mlr.press/v238/arya24b.html."
ZyCuQxyPJK,ZyCuQxyPJK,NeuroLifting: Neural Inference on Markov Random Fields at Scale,Reject,jwPXE5VDP0,ICLR.cc/2025/Conference/Submission1486/Reviewer_fwQZ,"The paper introduces NEUROLIFTING, a novel inference method that leverages Graph Neural Networks (GNNs) to optimize decision variables in large-scale MRFs. in contrast traditional methods, such as belief propagation and Toulbar2, which struggles to balance efficiency and solution quality as MRFs scale, NEUROLIFTING reparameterizes MRF variables with GNNs which enables efficient optimization using gradient descent while taking advantage of the smooth neural network landscape. This approach shows competitive results against exact solvers on smaller problems and superior performance on larger ones, with linear computational complexity growth.","* NEUROLIFTING introduces a new way to approach MRF inference by incorporating GNNs, enhancing scalability and parallelism compared to traditional methods.

*  Empirical results indicate that NEUROLIFTING consistently outperforms approximate inference methods in terms of solution quality and closely matches exact solvers on smaller instances.

*  The model shows linear growth in computational complexity, making it well-suited for large-scale MRFs.

* The method effectively transforms high-order graphs into a GNN-compatible format, expanding its applicability across various MRF-based problems.","* The use of GNNs, especially for large graphs, may still introduce significant computational costs and memory requirements, potentially limiting applicability in extremely large or resource-constrained environments.

* The effectiveness of NEUROLIFTING could be sensitive to the choice of GNN architecture, number of layers, and feature dimensions, which might require extensive tuning.

* Although NEUROLIFTING outperforms existing methods on large problems, there may be cases where it still falls short of the accuracy provided by exact solvers, especially on smaller and well-structured graphs.","1. How sensitive is NEUROLIFTING's performance to different GNN architectures and hyperparameters? Can it be generalized across various MRF problems without extensive tuning?

2. How does the model perform on MRF instances with unique or less common graph structures that differ significantly from those used in the empirical tests?

3. Are there alternative ways to optimize the energy function or refine the padding strategy to further enhance efficiency or solution quality?

4. Could the NEUROLIFTING framework be extended to handle dynamic graphs or incorporate temporal information for applications in time-varying MRFs?"
ZyCuQxyPJK,ZyCuQxyPJK,NeuroLifting: Neural Inference on Markov Random Fields at Scale,Reject,useoTGqzFR,ICLR.cc/2025/Conference/Submission1486/Reviewer_HyCv,"This paper focuses on the inference in large-scale Markov Random Fields (MRF), where conventional methods often suffer from either efficiency or solution accuracy. To address this issue, this paper advocates mapping the inference of MRF to a Graph Neural Networks (GNNs), which enables smooth loss landscape and scalable inference. Numerical experiments on several datasets showcases that the proposed NeuroLifting presents comparable performance to the non-scalable exact solver Toulbar2.","1. The organization of this paper is clear, and the writing is easy to understand. 

2. The figures and the example provided in figure 1 are helpful for understanding the paper.","Several parts of the paper are confusing, as listed one-by-one below. 

1. When introducing Lifting in Section 2, it is stated that ""Lifting reformulates an optimization problem into a more tractable form by introducing auxiliary variables or constraints, making the optimal solutions more accessible."" However in Section 3.3, the high-dimensional feature vectors are learnable and randomly initialized, without any guarantee on the equivalence between the original problem and the alternative GNN optimization problem. It would be beneficial if some theoretical analysis can be provided to ensure the GNN is actually/approximately solving the original problem, or an equivalent problem. In the current version, there is not even an objective function for the alternative optimization problem, making it hard to connect this paper with ""Lifting"". 

2. This paper claims that NeuroLifting is a ""non-parametric"" method. Nevertheless, line 249 argued that ""we optimize both the GNN parameters and those of the encoder,"" which conflicts with the proceeding claim.  

3. Line 61 says that NeuroLifting ""outperforms all existing approximate inference strategies in terms of solution quality without sacrificing computational efficiency."" In the experiments, only the time limit for the exact solver Toulbar2 is reported, while it is unclear how NeuroLifting compares to conventional approaches in terms of both time and space complexities. 

4. No codes or supplementary files are provided along with the submission. As a result, the reviewers cannot verify the results reported in the manuscript. 

Since I'm not quite familiar with Lifting and MRFs, I will keep a low confidence score.",See Weaknesses.
ZyCuQxyPJK,ZyCuQxyPJK,NeuroLifting: Neural Inference on Markov Random Fields at Scale,Reject,7URRxeYjx3,ICLR.cc/2025/Conference/Submission1486/Reviewer_qPjP,"The paper investigates approximating Markov random fields (MRF) by optimizing a graph neural network (GNN) to maximize a surrogate to its joint density. 
It suggests using learnable padded embeddings of the MRF decision variables as GNN features, allowing gradient-based optimization. 
The paper claims that:
 1. It is the first to introduce neural network-based lifting to MRF inference.
 2. The approach, called NeuroLifting, simplifies the optimization process for large MRFs.
 3. Loss computation with NeuroLifing scales linearly with the MRF nodes and enables parallelization and GPUs accelerated computation.
 4. It empirically demonstrates that NeuraLifing finds near-optimal solutions on moderate-size MRFs and solutions superior to baselines on large MRFs.","- The central idea of embedding decision variables in an MRF to use a GNN is original.
- The selection of GraphSAGE for convolution is well justified and supported by empirical results (Figure 3).","## Reasons for score
- The abstract states that NeuroLifing ""delivers superior solution quality against all baselines"" on large-scale MRFs. However, Table 1 does not show this, and two baselines (LBP and TRBP) need to be included in Table 2. Please consider modifying the abstract to reflect your results better. For example, you could restrict your claim to Toulbar2, i.e., exact methods.
- The experimental results lack standard error estimation, which makes method comparison difficult because it's hard to judge the significance of the difference in energy. Considering your method is probabilistic, you could use multiple initializations to estimate the standard error.
- A convergence criterion needs to be clearly stated for NeuroLifting UAI22 dataset. 
- Related work is delegated to the appendix.
- It needs to be clarified whether NeuroLifing is guaranteed to produce a valid solution. In particular,
  - Please explain how the padding scheme suggested produces only valid solutions, but padding with the largest of all energies can produce infeasible solutions.
  - Please explain with evidence (either theoretical or empirical) how the rounding scheme for $p_i(\\theta)$ avoids multi-assignment issues.","1. Does a GNN with jumping knowledge preserve the Markov assumption?
  2. Why does Table 2 not include LBP and TRBP?
  3. Was a hard time constraint of 1200 seconds enforced for LBP, TRBP and NeuroLifing?
  4. Is NeuroLifing sensitive to the initialization of the embeddings and network?
  5. Why does Toulbar2 fail on H_Instances_2? 
  6. Table 1 reports ""the final loss given by the loss function,"" whereas the other tables report the final energy. It needs to be clarified that the distinction between loss and energy exists for the baseline methods. If so, what is the gap between loss and energy?
  7. What is the gap between loss and energy for NeuroLifting on synthetic, UAI and PCI? 
  8. The interpretation of the linear complexity needs to be clarified. By mentioning the linear complexity in node size, is the paper claiming an asymptotic improvement over alternatives? 
  9. How does the suggested padding scheme produce only valid solutions, whereas padding with the largest of all energies can produce infeasible solutions?
  10. How does the rouding scheme for $p_i(\\theta)$ avoid multi-assignment issues?
  11. What does ""showing remarkable scalability and efficiency"" mean in (lines 71-72)? What is the concrete evidence that shows this claim?
  12. What stopping criteria are used for NeuroLifting on UAI?

##  Minor comments (did not affect score)
  1. Tables 1, 2, 3 and 4 would be more legible and convey the same results if reported as whole numbers instead.
  2. Capitalize the titles of the references.
  3. Use only one of: `DOI:`, `ISBN` or neither for book references. 
  4. The caption for Figure 1 should describe the stages in the diagram. Figure 2 does this well. 
  5. In Figure 1 $\\mathcal{P}$ and $H$ are not defined. 
  6. Cross entropy is not defined.
  7. In eq. 5, $v_i(\\theta)$ is a mapping, but above $v_i$ is defined as a bit string (not a mapping).
  8. Cliques should be capitalized in Tables 1, 2, 4 and 5.
  9. $\\mathcal{X}$ is not clearly defined. From the padding paragraph and section 3.4, it looks like it should be $x_i \\in \\mathcal{X}_i$ and $\\mathcal{X} = \\cup_i \\mathcal{X}_i$.
  10. Table 5 should use scientific notation.
  11. Is the learning rate really $1e^{-4} \\approx 0.018$ in 4.1?"
B5VEi5d3p2,B5VEi5d3p2,SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination,Accept (Poster),BMqGsmb7L0,ICLR.cc/2025/Conference/Submission626/Reviewer_Dg5B,"This paper presents SleepSMC, a sleep stage classification framework that integrates uncertainty-based feature reweighting with modality-level contrastive learning to handle multimodal physiological data (EEG, EOG, EMG) effectively. The reweighting mechanism assigns weights to auxiliary modalities based on their uncertainty. Then, they use contrastive learning to align representations of the same sleep stage across different modalities. SleepSMC achieves modality-invariant embeddings that allow for robust performance even when only a single primary modality is available at test time. SleepSMC is evaluated on three public datasets, consistently outperforming SOTA baselines in both multimodal and unimodal settings.  They present ablation studies demonstrating that reweighting and contrastive learning are both effective individually and moreso when combined. Visualization analyses further demonstrate the model’s interpretability, showing clear, well-separated embeddings for each sleep stage.","This paper presents SleepSMC, a sleep stage classification framework that integrates uncertainty-based feature reweighting with modality-level contrastive learning to handle multimodal physiological data (EEG, EOG, EMG) effectively. The reweighting mechanism assigns weights to auxiliary modalities based on their uncertainty. Then, they use contrastive learning to align representations of the same sleep stage across different modalities. SleepSMC achieves modality-invariant embeddings that allow for robust performance even when only a single primary modality is available at test time. SleepSMC is evaluated on three public datasets, consistently outperforming SOTA baselines in both multimodal and unimodal settings.  They present ablation studies demonstrating that reweighting and contrastive learning are both effective individually and moreso when combined. Visualization analyses further demonstrate the model’s interpretability, showing clear, well-separated embeddings for each sleep stage. 

Originality
The uncertainty reweighting technique is clever and novel.  Most importantly, it is both empirically and theoretically effective. The contrastive learning component, combined with reweighting, enables the model to create modality-invariant embeddings, allowing SleepSMC to generalize effectively across different primary modalities.


Quality
The quality of the work is high. The evaluation is thorough, providing empirical, theoretical, and qualitative support for their approach. Reweighting consistently outperformed non-reweighted setups in both multimodal and unimodal testing, with clear gains in accuracy, Macro F1, and Kappa scores, particularly under noisy conditions. Combining reweighting with contrastive learning further boosted performance compared to contrastive learning alone, showing that these methods work well together, especially with unreliable auxiliary modalities. SleepSMC also generalized effectively across target modalities, performing robustly whether EEG, EOG, or EMG was used as the primary modality, demonstrating flexibility across configurations.
In both multimodal and unimodal testing, SleepSMC maintained high accuracy, with unimodal performance benefiting notably from reweighting. t-SNE visualizations showed clear, well-separated clusters for each sleep stage, supporting the model’s modality-invariant embedding space and interpretability. These results confirm that SleepSMC’s reweighting and contrastive learning mechanisms enhance robustness and adaptability, making it a practical solution for real-world sleep staging.


Clarity
The work is mostly clearly presented. The methodology and results are well-structured, and the theoretical analysis is solid, providing strong support for the approach. However, the introduction of mathematical notation in Section 3 could be organized more effectively; starting with a table or list of symbols and definitions before diving into the equations would help clarify the notation and reduce cognitive load for the reader. The paper is otherwise transparent in discussing its limitations, and the overall clarity of the empirical findings is strong.


Significance
The work makes a meaningful contribution, adding to the literature on multimodal integration, particularly in scenarios where only unimodal data is available at test time—an approach that aligns well with many real-world applications.","1. The introduction of mathematical notation in Section 3 is somewhat disorganized and could be clarified better. Starting this section with a concise table or list of symbols and definitions would provide readers with a quick reference point, making it easier to follow the subsequent equations. This adjustment would enhance readability, especially for readers less familiar with the specific notation conventions used.

2. The work makes a solid contribution to the field, with gains in the range of 2-3%, which are statistically significant but don’t drastically improve performance over baseline methods. While these improvements are valuable they're somewhat incremental.  Perhaps the gains are more significant in other multimodal learning problems.","An important aspect of multimodal classification models is cross-subject generalization. Given that real-world applications often involve new subjects with varying characteristics, an evaluation on unseen subjects would provide valuable insight into the model’s robustness. If cross-subject generalization was analyzed, could the authors include these results? Otherwise, adding this analysis would strengthen the paper’s practical relevance.

Additionally, while the reported improvements are statistically significant, they appear relatively modest, with gains in the range of 2-3%. Could the authors elaborate on why these incremental gains are meaningful in the context of sleep staging? Providing additional context on the impact of these gains for real-world applications would clarify the value of these results.

Although the reweighting technique is effective within sleep staging, it would be useful to understand its potential applicability beyond this domain. Do the authors see possible applications of uncertainty-based reweighting in other multimodal learning tasks? Expanding on this would highlight the versatility and broader impact of the proposed method.

In terms of practical applications, it would be helpful to know how this model performs in real-world scenarios where data quality and availability are less controlled. Could the model handle dynamic shifts in modality reliability, and do the authors envision practical deployments for health monitoring or similar applications?"
B5VEi5d3p2,B5VEi5d3p2,SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination,Accept (Poster),5WsaqlQSHJ,ICLR.cc/2025/Conference/Submission626/Reviewer_iRq4,"This paper introduced multimodal collaboration in sleep staging, leveraging multiple auxiliary modalities to improve the performance of primary modality-based sleep staging in an end-to-end manner. The authors utilized supervised modality-level instance contrastive coordination to capture category-related consistency and complementarity across intra-modality and inter-modality.",Experiments are performed using 3 different datasets. The organization of the paper is good.,"The limitations are as follows:

The motivation is not clear.
How this contribution is different from the following contributions? a)CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities,"" in IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 32, pp. 840-849, 2024, doi: 10.1109/TNSRE.2024.3354388. b) Multi-Modal Sleep Stage Classification With Two-Stream Encoder-Decoder,"" in IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 32, pp. 2096-2105, 2024, doi: 10.1109/TNSRE.2024.3394738.
The motivation behind the use of Uncertainty Estimation with Frozen Gradients is not clear.
The methods mentioned in Table 2 are from before 2022. It is necessary to compare with some recent SOTA methods","1. How the proposed method is different from SOTA?
2. What is the novel contribution that makes this paper unique?
3. What is the computational complexity?
4. How equation 10 become optimized?"
B5VEi5d3p2,B5VEi5d3p2,SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination,Accept (Poster),JZg0VvcbDA,ICLR.cc/2025/Conference/Submission626/Reviewer_HHyg,"To address the issue that only one modality is available in ubiquitous scenarios, this paper introduce multimodal collaboration in sleep staging, leveraging multiple auxiliary modalities to improve the performance of primary modality-based sleep staging in an end-to-end manner. The paper utilize supervised modality-level instance contrastive coordination and uncertainty estimates to learn coordinated features. The experiment results show that the proposed method achieves SOTA performance in multimodal scenarios and unimodal scenarios.","The paper aims to address the issue that only one modality is available in ubiquitous scenarios, which has certain practical significance.

The paper utilizes uncertainty estimates to adaptively weight auxiliary modality features during
training, which ensures that more reliable auxiliary modality features contribute more significantly to the contrastive learning process.

The presentation of the paper is generally quite clear.","(1) This paper does not introduce a new concept; rather, it applies existing multimodal coordination and uncertainty estimates to the ubiquitous scenarios of sleep staging. Moreover, the proposed method for improving sleep staging does not seem to differ significantly from the multimodal coordination methods already used in areas such as vision. Some design details in the method, such as certain aspects related to uncertainty estimates, do not demonstrate a specific focus on the sleep staging task. 

(2) The authors seem to use a single-epoch sleep staging paradigm instead of a sequence-to-sequence sleep staging approach. Why is this the case? In fact, there is a strong correlation between adjacent epochs in sleep staging, and the single-epoch paradigm has already fallen behind in performance and been largely abandoned. As a paper aimed at improving sleep staging performance, it is difficult to understand the choice of using a single-epoch paradigm. Could the authors explain why this paradigm was chosen and whether the proposed method can be adapted to the sequence-to-sequence paradigm?

(3) The reviewer has some concerns about the experimental design and results. The baselines chosen by the authors appear to be relatively weak and outdated, such as the selection of SimCLR, which is a very old self-supervised learning method. The baselines in the paper perform very weakly in multimodal scenarios (shown in Table 1), making it difficult to demonstrate the advantages of the proposed method. Some very strong baselines, such as SalientSleepNet [1], BSTT [2] and XSleepNet [3] were not compared. Meanwhile, the reviewer cannot understand why the authors chose ISRUC-S3 instead of ISRUC-S1 as the primary evaluation dataset. ISRUC-S1 has more subjects and a larger data volume, making it more convincing compared to ISRUC-S3.

(4) Although the presentation of the paper is relatively clear, the content in ""DETAILED ANALYSIS AND PROOFS IN SECTION 4"" is difficult for readers to understand. As a supplement and proof of certain concepts in the main text, this section does not sufficiently clarify the formal definitions of each concept and the complete proof process. For example, what is the complete proposition that needs to be proven in this section? What are the assumptions underlying the proof process? What is the formal definition of ""margin""? What is the formal definition of ""information transfer""? The authors should provide clear explanations and complete definitions of these concepts in the appendix to ensure the readability of the paper.

[1] Jia Z, Lin Y, Wang J, et al. SalientSleepNet: Multimodal salient wave detection network for sleep staging[J]. arXiv preprint arXiv:2105.13864, 2021.

[2] Liu Y, Jia Z. Bstt: A bayesian spatial-temporal transformer for sleep staging[C]//The Eleventh International Conference on Learning Representations. 2023.

[3] Phan H, Chén O Y, Tran M C, et al. XSleepNet: Multi-view sequential model for automatic sleep staging[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021, 44(9): 5903-5915.",Please See Weakness
B5VEi5d3p2,B5VEi5d3p2,SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination,Accept (Poster),4uM3e05J4x,ICLR.cc/2025/Conference/Submission626/Reviewer_A2VX,"The manuscript proposes SleepSMC, a method for detecting sleep stages leveraging contrastive learning and feature weighting based on uncertainty. The approach is specifically designed to handle scenarios where multiple data modalities are available during training, but only a single modality is accessible during testing (multimodal scenario has also been evaluated). Using three publicly available datasets, the authors demonstrate that SleepSMC achieves performance improvements over existing baselines.","- The paper is well-structured and clear, and easy to follow.
- Includes extensive experiments comparing the proposed method with various baselines.","- Limited technical novelty.
- More recent & missing-modality-specific baselines could be considered as stronger baselines.

[Details]
1. Could the authors explain why ""Supervised Contrastive Learning"" by Khosla et al. (2020) was not considered as one of the baselines? What specific motivations underlie the use of contrastive learning here, and in what ways does the proposed technique differ (in terms of technical novelty) from the original approach in Khosla et al.? 
        - Khosla, Prannay, et al. ""Supervised contrastive learning."" Advances in Neural Information Processing Systems 33 (2020): 18661-18673.

2. Is there any method or technique in this work that is specifically tailored for sleep stage detection? The proposed approach appears applicable to most multimodal data; If that's the case, other recent works targeting learning/inference with missing modalities could be considered as baselines as well:
    - Wang, Hu, et al. ""Multi-modal learning with missing modality via shared-specific feature modelling."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
   - Yao, Wenfang, et al. ""DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.","[Problem setting & method] 
1. Could the authors clarify the rationale for assuming that only one modality is available during inference? While it is understandable that not all modalities used in training may be accessible at test time, it seems uncommon to assume the presence of only a single modality. A more practical scenario might involve each user having a unique set of available modalities at test time, rather than relying on one fixed modality. Could the authors elaborate on the motivation behind this specific setting?

2. Would it be expected that the auxiliary modality classifiers exhibit higher uncertainty at the beginning of training? If so, could this uncertainty indicate an opportunity for the model to learn more effectively? Did the authors experiment with dynamic tuning of weights over the course of training? Additionally, numerous metrics (such as entropy, model confidence, loss) could potentially assess sample importance. What was the rationale for selecting model confidence? Were other metrics considered, and if so, what were the comparative results?

[Related work & Baselines]  
1. In Section 2, Liu et al. (2023b) and Liu et al. (2024) are referenced as leveraging multimodal data to enhance performance, with the former employing contrastive learning to align modalities. Could the authors clarify why these methods were not considered as baselines? Additionally, it is noted that these works focus on multimodal consistency without capturing class-specific information. However, the design choices of related works should not necessarily be considered limitations. Could the authors discuss the impact of excluding class-specific information? How can we assess the relative performance without a comparison to the proposed method?

2.  In Section 2, several works related to uncertainty estimation are discussed, but the conclusion remains somewhat ambiguous. Why are these methods not applicable to the target scenario? What challenges prevent their direct application to the proposed task, especially given that multimodal data are still used during training?

[Results]
1. In most scenarios, including unimodal cases, the primary improvements appear to stem from supervised contrastive learning. However, in Section 5.5, it is stated that “uncertainty-based feature weighting has a greater impact in the unimodal testing scenario,” which may be an overstatement. Could the authors provide comparative improvement results (e.g., average ± standard deviation) for both components? If the enhancement from uncertainty-based feature weighting is minor, what justifies its inclusion?

2. Could the authors explain the observed fluctuations in EEG and EOG uncertainty weights as training progresses (as seen in Figure 3c)?

[Minor comments]:
- Font size in Figure 3 appears small.
- Class labels in Figure 4 are difficult to read; increasing the legend font size could improve readability.
- Typo in Section 4.2, first sentence: “adaptively weighted” should be “adaptively weights.”"
w5h443GIGo,w5h443GIGo,On the Convergence of Symbolic Pattern Forests and Silhouette Coefficients for Robust Time Series Clustering,Reject,sTIbskqzki,ICLR.cc/2025/Conference/Submission602/Reviewer_S2jm,"The paper proposes SPF, a methodology that identifies the number of clusters for time-series data, often a critical parameter for subsequent routines and clustering methods. The idea combines concepts such as SAX, TF-IDF vectors over SAX representations and relies on the Silhouette coefficients to calibrate the number of clusters. Experimental results on several UCR datasets demonstrate the potential of this solution.","S1. Timely and important problem especially due to the rise of IoT applications and the need for unsupervised data exploration
S2. Simply and intuitive ideas
S3. Results support the overall claims in the paper","W1. Lack of technical depth
W2. Unclear how different methods/distances can be compared
W3. Missing potential baselines
W4. Duplicate references or wrong references","W1. Lack of technical depth

The paper combines existing ideas for solving this problem. Therefore, the technical depth is low, even though the combination of these ideas might be novel.

W2. Unclear how different methods/distances can be compared

It's unclear how this comparison is meaningful when we need to compare methods relying on different distances. The paper does not clearly articulate how such distances affect the results and it mainly shows results for SAX variants (so inherently for euclidean distance)

W3. Missing potential baselines

Simple baselines, like assign the objective functions of k-means like algorithms are missing. Also there are tons of variants for internal clusteirng validation. Why Silhouette ?

W4. Duplicate references or wrong references

Many references are duplicates. Other references does not exist

duplicates
Xiaosheng Li, Jessica Lin, and Liang Zhao. Linear time complexity time series clustering with
symbolic pattern forest. In IJCAI, 2019a.
Xiaosheng Li, Jessica Lin, and Liang Zhao. Linear time complexity time series clustering with
symbolic pattern forest. IJCAI, 2019b.

duplicates
Jaewon Yang and Jure Leskovec. Patterns of temporal variation in online media. In Proceedings of
the Fourth ACM International Conference on Web Search and Data Mining, 2011a.
Jaewon Yang and Jure Leskovec. Patterns of temporal variation in online media. In Proceedings of
the fourth ACM international conference on Web search and data mining, pp. 177–186, 2011b.

it's wrong
John Paparrizos, Paul Boniol, Themis Palpanas, Ruey S Tsay, Aaron Elmore, and Michael J
Franklin. Fast and exact time series motif and discord discovery in trillions of data points. The
VLDB Journal, 31:1079–1101, 2022."
w5h443GIGo,w5h443GIGo,On the Convergence of Symbolic Pattern Forests and Silhouette Coefficients for Robust Time Series Clustering,Reject,evzyLFUwSy,ICLR.cc/2025/Conference/Submission602/Reviewer_gn2N,"The manuscript presents an extension of the symbolic pattern forest (SPF) algorithm for clustering of time series data. Using bag-of-words on the symbolic representation, TF-IDF vectors are constructed. The best clustering is selected as the one that maximises the silhouette coefficient (SC).","S1. The paper addresses the relevant problem of automatically determining the number of clusters.
S2. The empirical evaluation makes use of a large number of benchmarking datasets.","W1. The method assumes that silhouette coefficient is a suitable metric for finding the best number of clusters, without justifying this choice. This is a major concern as the silhouette coefficient considers (Euclidean) distance to cluster centres, which is not aligned with the clustering objective of the SPF method. The paper should provide justification for using the silhouette coefficient, or discuss potential limitations of this choice given the SPF method's clustering approach. Moreover, the silhouette coefficient is a well-known metric, so it is unclear what the novelty should be.
W2. The empirical evaluation does not consider the SPF method, but only weak baselines constructed from the proposed method, meaning that the empirical evaluation does not allow assessment of the performance of the proposed method with respect to state of the art. It is important to compare directly to SPF in the experiments, in order to demonstrate improvement over state of the art.
W3. The empirical evaluation only considers performance metrics accuracy and near-miss-rate, different from other work in the field, and in the SPF paper (e.g. NMI), making it impossible to compare with those works directly.
W4. The discussion of related work is overly brief, and fails to present clear assessment of the suitability of existing methods and metrics. E.g. Davies-Bouldin Index and its perceived suitability for the task. Also, there is a large body of work on similarity assessment of time series or clustering of time series, e.g. Keogh et al 2005, Rakthanmanon  et al 2012, Paparrizos et al 2015. The paper should discuss these, and explain differences and similarities with the proposed method.
W5. On the other hand, references UTSAD and STGAT seem out of context, as they do not address clustering of time series. The paper should clarify the relevance of UTSAD and STGAT to the proposed work, or remove these references if they are indeed not directly related.
W6. The paper contains several redundant sections, such as the description of SAX.
W7. There are some minor issues, such that Li et al 2019 appears twice in the references, there is a typesetting error in the definition of pi_i(T_i).",
w5h443GIGo,w5h443GIGo,On the Convergence of Symbolic Pattern Forests and Silhouette Coefficients for Robust Time Series Clustering,Reject,TyIvB8cZM2,ICLR.cc/2025/Conference/Submission602/Reviewer_yRSo,"The submission proposes an extension to the SPF algorithm, a clustering approach for clustering time series with linear complexity. The extension allows for the automatic determination of the number of clusters. It is done by performing optimization on the silhouette score using either Bag of Words or TF-IDF.","(S1) Incorporating BoW and TF-IDF with the concepts of the SPF algorithm sounds like a very sensible approach. Both are a good choice for term-based similarity evaluation and are still commonly used in other settings.

(S2) Aside from minor issues, the submission is well-written and easily understandable while providing an extensive overview of the formulas related to the problem.

(S3) The problem setting is significant as k-estimation is a significant part of clustering in general, which also applies to the setting of time series clustering. The usage of SPF is well-founded due to its low complexity. Introducing k-estimation to the approach helps mitigate one of its weaknesses.","(W1) Novelty: The abstract of the submission makes the claim that there are no time series clustering methods capable of working without the specification of cluster number k. However, such methods exist already:

a) “Spectral Clustering for Time Series” by Fei Wang and Changshui Zhang (2005) is able to discover the optimal number of clusters based on the eigenstructure using a threshold on the value of the eigenvalues.
b) “Clustering Time Series with Hidden Markov Models and Dynamic Time Warping” by Tim Oates et. al. (1999) also provides a way to estimate the number of clusters based on Dynamic Time Warping. However, even if the submission is not the only method that does k-estimation on time series, it is still a valid and useful direction. It also appears to be the only method that does so for the Symbolic Pattern Forest algorithm.
c) The paper “Trendlets: A novel probabilistic representational structures for clustering the time series data” by Johnpaul C I et al. (2020) uses the Silhouette Score for cluster number analysis for time series as well, though it does so based on hierarchical clustering methods. This paper should be explicitly covered in related work or even a competitor.

(W2) Despite TF-IDF being considered the better of the two proposed strategies, there is no actual description of the performance metrics outside of the graph and the overall relative performance value. Similarly, near misses should be added to the text for BoW. The results of both BoW and TF-IDF are the same in the Tables in the supplementary files, though Figure 1 claims that TF-IDF performed slightly better.

(W3) As the method works by optimizing the silhouette score, both the values for the score and the actual clustering performance with the given parameters should be indicated. While the cluster numbers match, the detected clusters may not necessarily correspond to the actual ground truth clusters, which could further mean that different cluster numbers may lead to a better performance. Furthermore, an analysis of the stability of the parameters should have been performed, especially as the method has multiple parameters, which themselves include an upper and lower bound. Additionally, an intuition behind choosing the parameters should be given if they strongly affect the performance.

(W4) Regarding the actual experiment, a better analysis of the behavior should be done, considering under what conditions the k-estimation of each of the three approaches failed and whether or not a reason behind it could be established. The section on Relative Improvement is redundant as it only recontextualizes prior results, and the space could be used to do a more in-depth result analysis instead. Similarly, the remaining 2 pages could have been used for this.

(W6) Neither the parameter w nor the alpha ranges seem to be specified anywhere. The code is unavailable, though it should be possible to reimplement given the information provided. Still, this hampers the reproducibility of the results.

(W7) There should be citations for TF-IDF and BoW. Other papers also do not consistently do it, so it is not a major issue. Nonetheless, it would have been better if it had been done. Furthermore, UCI should be cited upon first mention outside of the abstract, not just at a later point.

Minor Issues:
* Linear time complexity time series clustering with symbolic pattern forest by Li et. al., is cited twice as 2019a and 2019b despite referring to the same paper 
* The formatting appears to be broken for lists, as they are just written in a line without comma separation (see line 291 and lines 314-315) 
* A similar issue happened with the variables for the optimization problem, as they are also not properly separated in line 305
* The subscript on several equations appears to be broken (see (22)/319 and (23)/321)
* The near miss metric should probably be more dynamic based on the ground truth cluster number, as claiming 2 clusters for a 3-cluster setting seems more problematic than claiming 70 for 71 true clusters. The chosen datasets generally only have a few clusters, so the current definition isn’t problematic for the submission. It may be relevant for the extension to the full UCI database, however. 
* The formulation for Near Misses, as currently given, would also include all correctly determined cluster counts but does not do so in the evaluation.","(Q1) How would you modify the paper to address the issues regarding related work? How does the proposed method compare to other k-estimation approaches on time series data?

(Q2) What effect do the properties of the chosen UCI datasets have on the performance of the k-estimation using the proposed technique, and how does the clustering performance change on them depending on the chosen k? Is the performance of the SPF algorithm with the ground truth k always the best one, or could other parametrizations outperform it?

(Q3) How impactful are the parameters of the proposed method?

(Q4) Is there any advantage to using BoW over TF-IDF (or the other way around)?"
7NtAIghBsE,7NtAIghBsE,Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-trained Models,Reject,5ZSh899J9u,ICLR.cc/2025/Conference/Submission439/Reviewer_jBiQ,"The authors introduce FedCOF, a training-free federated learning approach that utilizes a pretrained model's feature extractor while updating only the classifier. Unlike previous methods that only aggregate global means, FedCOF improves performance by also deriving and leveraging unbiased global covariances from these means. Local clients send first-order statistics (class-wise feature means) to the server, which then uses these to estimate global covariances. This innovation allows for efficient communication while significantly enhancing the effectiveness of the global classifier updates.","**Strengths of  FedCOF:**

FedCOF presents a timely contribution that leverages pretrained models in federated learning (FL), addressing the deep learning (DL) field's growing emphasis on foundation models. FedNCM is communication-efficient but suffers from limited performance, while Fed3R improves performance using both first- and second-order statistics but at a high communication cost. In contrast, FedCOF achieves strong performance with minimal communication overhead by deriving unbiased global covariances using only first-order statistics.","***Concerns on Presentation***

**Related Works**
I recommend enhancing the Related Works section to include a broader range of studies that cover both the use of fixed classifiers and the potential limitations of pretrained models in federated learning settings.

In **Line L101**, the section discussing the application of **fixed classifiers** in federated learning could be expanded by incorporating recent and relevant studies. Specifically, it would be valuable to reference works such as FedBABU [1], SphereFed [2], and Neural Collapse-inspired approaches [3,4], which explore the impact of classifier freezing in federated scenarios.

Furthermore, the discussion in **Line L102** and beyond about **federated learning with pretrained models** should present a more balanced view. The current description highlights only the positive outcomes of using pretrained models. However, it is important to acknowledge that pretrained models are not always advantageous in federated settings. For example, findings from FedFN [5], particularly in Section 5.2, demonstrate situations where pretrained models can adversely affect the performance of the global model, especially under heterogeneous data conditions. Including this perspective would provide a more comprehensive understanding of the complexities involved in using pretrained models within federated learning frameworks.

**Preliminaries**

L117: D_k seems to refer to the local dataset rather than local data.

L127: There is no clarification on the type of loss function or how the loss is calculated (whether as a batch mean or batch sum).

L129-130: ""After initializing \\theta with pretrained weights, the models can be optimized in a federated manner"" — In this paper, local clients do not perform local updates based on the pretrained model, and this information seems to hinder the understanding of the paper.

**Concerns on Privacy Discussion in Section 4:**

The algorithm sends the class-wise frequency of the data held by clients to the central server. I believe this information could also raise privacy concerns, yet there is no mention of this issue. In fact, there are many previous FL papers that have communicated class frequency information and provided justifications. Citing these studies would strengthen the discussion, but this type of content is entirely missing.

***Concern on Incremental Contribution***

The problem may seem incremental, as it combines existing methods' strengths, but it addresses the practical challenge of balancing communication cost and performance in FL.


[1]FedBABU: Toward Enhanced Representation for Federated Image Classification, ICLR 2022.

[2]SphereFed: Hyperspherical Federated Learning, ECCV 2022

[3]No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier, ICCV 2023

[4] FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning. FedKDD 2024

[5]FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem in Federated Learning. NeurIPS Workshop 2023, Federated Learning in the Age of Foundation Models.","While I currently have several concerns that have led to a lower score, I am open to increasing the score if these issues are adequately addressed during the rebuttal period.

**Questions and Suggestions:**

The use of pretrained models in federated learning (FL) is a promising and timely research direction, especially given the current trend in the deep learning community toward leveraging foundation models effectively. However, as mentioned earlier in the Related Work section, using a pretrained model is not always superior to using a randomly initialized model. Specifically, findings from FedFN [1], Section 5.2, highlight scenarios where pretrained models can negatively impact global model performance in high heterogeneous settings.

Given this, it would be beneficial for the authors to include experimental comparisons between using a pretrained model and a randomly initialized model. These comparisons should cover various baselines and the proposed algorithm to provide a clearer understanding of whether the pretrained model genuinely improves performance.

Additionally, if the characteristics of the training data used for the pretrained model(e.g. ImageNet) are significantly different from the test data(e.g. SVHN) targeted by the global model, using a pretrained model could potentially be detrimental. It is important to clarify what specific test dataset the final foundation model and redefined classifier are targeting, as this information does not seem to be explicitly stated in the Preliminaries section.


Furthermore, a fundamental challenge in FL is the heterogeneity of client data, which often leads to **class imbalance** issues within each client’s local dataset. However, what differentiates FL from traditional class imbalance problems is the presence of **missing classes**, where certain classes are entirely absent from a client's dataset. This problem is especially pronounced as data heterogeneity increases, causing missing classes to occur more frequently across clients.

The proposed algorithm sends class frequency information from each client, but in the case of missing classes, this would simply convey a value of 0. I am concerned that the algorithm might be particularly vulnerable to the impact of these missing classes. Could the authors explain how their proposed algorithm is designed to mitigate this vulnerability in the context of FL, and why it might still perform well despite the challenges posed by missing classes?


[1]FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem in Federated Learning. NeurIPS Workshop 2023, Federated Learning in the Age of Foundation Models."
7NtAIghBsE,7NtAIghBsE,Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-trained Models,Reject,NcBpn5ddrg,ICLR.cc/2025/Conference/Submission439/Reviewer_Wumb,"The paper proposes to use pre-trained models to perform federated classification. More specifically, Fed-COF uses pre-trained models to extract features of each example on the client, then averages the features within each class. The class-averaged features from clients are aggregated in the server to estimate the first-order and second-order statistics, which are then used in ridge regression to fit a classifier. The communication cost of Fed-COF scales only linearly with the size of the embedding.",The paper is well written. The derivations are clear and easy to understand. The proposed Fed-COF achieves decent empirical performance. Fed-COF also seems to work well with fine-tuning.,"- The steps of Fed-COF + Fine-tuning can be made clearer. The description around line 357 is difficult to follow.

- The choice of ridge regularization parameter $\\lambda$ seems important for the classification performance. Can authors give more empirical suggestions on how $\\lambda$ should change with different numbers of clients/means per client?","- What is the difference between Fed-COF oracle and Fed3R? In Table 2, it is surprising to see Fed3R sometimes achieves lower accuracy with more communication. The authors should provide more explanations for the phenomenon.

- In line 472, atleast should be at least."
7NtAIghBsE,7NtAIghBsE,Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-trained Models,Reject,vWoj55YR9j,ICLR.cc/2025/Conference/Submission439/Reviewer_z1Dd,"The paper tackles training-free federated learning. It proposes to estimate the per-class covariances in a federated learning setting using the variance of local per-class means. This covariance can be used to obtain a ridge regression classifier, which outperforms a nearest-neighbor classifier based on class means. The proposed approach thereby avoids sending local covariance matrices which reduces communication and potential privacy risks. 

The derivation of the estimator for the per-class covariance is sound. The empirical evaluation is comprehensive.","- Training-free federated learning using feature extractors is a relevant and interesting problem.
- The proposed estimator of covariances is sound and novel.
- Experiments show substantial improvement over existing training-free methods and potential for its combination with federated fine-tuning or linear probing.","- The impact of the iid assumption on realistic scenarios is evaluated empirically. It would be great to quantify how heterogeneous distributions impact the estimator theoretically, e.g., under the assumptions that local distributions are Gaussians with different mean or covariance.
- The dimension of the feature space could impact the accuracy of the estimator. This impact should be evaluated, e.g., using a synthetic dataset with varying number of feature dimensions.
- The paper focusses on label shifts, i.e., a heterogeneous distribution of classes. It is unclear how the method performs in case of feature shift, i.e., a heterogeneous distribution of features, e.g., via locally different covariance structures [1].","- Why is this approach better in terms of accuracy that Fed3R? Shouldn't it perform slightly worse or en-par, since it only approximates the covariance matrix? Here it would be good to investigate the approximation of the covariance matrix and compare it to the one produced by Fed3R.
- It would be great to compare the results using a strong pre-trained feature extractor with a classical end-to-end federated learning baseline, e.g., training a ResNet-50 on CIFAR100. 
- For consistency I suggest to use $\\widehat{\\Sigma}$ instead of $\\widehat{S}$ in Eq. 10.
- Please compare your approach also to distributed training of linear models (using standard FedAvg), since ideally the training-free approach should perform at least en-par in terms of model performance and should outperform them in terms of communication. Here, it would be particularly interesting to compare to communication-efficient approaches [2]. 
- Since ridge regression might not always be ideal approach given a fixed feature extractor, I wonder whether a kernel ridge regression could be applicable. This would require sending the kernel matrix, but also has a closed form solution. The communication cost in that case would be quadratic in the number of data points, rather then linear in the features, so for many scenarios communication might be higher. Once could employ compression techniques here, though, like the Nyström method.


[1] Li, Xiaoxiao, et al. ""FedBN: Federated Learning on Non-IID Features via Local Batch Normalization."" International Conference on Learning Representations, 2021.
[2] Kamp, Michael, et al. ""Communication-efficient distributed online prediction by dynamic model synchronization."" Machine Learning and Knowledge Discovery in Databases, 2014."
7NtAIghBsE,7NtAIghBsE,Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-trained Models,Reject,qTJF4h1WaX,ICLR.cc/2025/Conference/Submission439/Reviewer_6242,"This paper proposes a novel training-free FL method called FedCOF, which approximates the covariance on the server side to eliminate the enormous communication overhead. Numerical results demonstrate that FedCOF achieves comparable performance to Fed3R by merely transmitting class means to the server.","1. Building on theoretical guarantees, this paper introduces a novel algorithm that eliminates the need for transmitting covariances between the server and clients while maintaining performance levels. This represents a valuable step for training-free federated learning.

2. The authors have carried out extensive experiments to validate the effectiveness of the proposed method, demonstrating considerable effort in their research.","While the motivation of this paper is clear, I have the following questions/discussions.

1. The algorithm necessitates the transmission of $n_{k,c}$ to the server, which introduces certain privacy concerns. Although other methods also require this information, it would be beneficial if the authors could discuss potential techniques to address or mitigate this issue.

2. As modern pre-trained models tend to be generative models (e.g., GPT), it would be interesting to explore the possibility of extending the proposed methods to handle generative models by initializing the decoding heads accordingly.",Please see weakness.
2Oh2EOcFSO,2Oh2EOcFSO,Can a Bayesian oracle prevent harm from an agent?,Reject,JvzstXgFIj,ICLR.cc/2025/Conference/Submission424/Reviewer_n2RR,"This paper explores the problem of designing AI systems that satisfy probabilistic safety guarantees. Within a Bayesian framework and given the safety specifications (as a probability), the authors provide risk bounds for potentially harmful decisions, showing that the probability of harm can be upper-bounded by a probability that can be estimated by approximating Bayseian posterior over theories given the observed data. They study two settings: i.i.d case and non i.i.d case and provide a simple experiment to evaluate the performance of safety guardrails.","This is a very well written paper, and it is easy to follow. 

The proposed approach represents a promising initial step toward designing AI systems that ensure safety through built-in probabilistic guarantees, rather than relying solely on external safety mechanisms.

The authors also outline several open problems for future work.","The authors present an upper bound on the harm probability, though it appears to be highly conservative. It would be valuable if they could offer a convergence rate or practical guarantees to make the framework more usable. Additionally, it is unclear how this approach compares to other conservative methods for preventing harm. 

Since the theoretical results lack practical assurances, I would have appreciated more experimental validation, especially in complex and realistic settings. 

Obtaining a Bayesian oracle could be very challenging (posterior distribution). 

Overall, while the paper introduces a promising method for designing safer AI systems, it would greatly benefit from additional components (both theoretical and experimental) before publication.",None.
2Oh2EOcFSO,2Oh2EOcFSO,Can a Bayesian oracle prevent harm from an agent?,Reject,DgWvP835G1,ICLR.cc/2025/Conference/Submission424/Reviewer_pbrg,"This paper studies the problem of bounding the probability of some event in the context of an unknown but consistent distribution and a Bayesian setting. 

The paper is motivated by the prevention of harm by AI agents. In short, harm is inherently unavoidable since in real applications we have no direct access to the distribution governing the environment. However, if we assume a fixed distribution, and a prior assumption of that distribution, we can get better and better approximations when data is presented to us, by using the data to update our prior knowledge of the distribution. With this, we can theoretically bound the probability of doing harm. In deployment, actions whose probability of harm is larger than some threshold can be blocked.

The paper explores two cases: incoming data as iid and non iid, and obtains bounds on the probability of harm in both cases.

The paper presents an experimental evaluation on a multi-armed bandits example, blocking actions that are considered unsafe according to the different bounds obtained as well as a baseline (with an unrealistic assumption of the underlying model). The paper ends with a discussion of the open problems still to be solved to be able to use this method as a reliable guardrails for AI agents.","S1. The topic of AI safety is timely and relevant for ICLR.

S2. The theoretical results (as far as I could check) are sound.

S3. The experimental evaluation serves to showcase how these bounds could be used in a realistic scenario.","W1. I understand the appeal to frame this work in the context of harm by an AI agent, and I think it is an interesting point. However, there is nothing inherent to ""harm"" in the concept presented. The concept of ""harm"" could be substituted by ""reward at a state"" and we could be discussing the same results in a different light. I think the paper may benefit from a more general motivation.

W2. While the experimental evaluation is welcome, it is a very simple example, and one wonders if these theoretical bounds would find applicability in problems that are more complex and close to the real applications of guardrails.

W3. The concept of guardrails presented here, as an algorithm that blocks an action if it shows an expected harm larger than some threshold, is very similar to the concept of probabilistic shielding in MDPs [1] (which is essentially the ""cheating guardrail in Sec. 5), and this can be extended to partially observable MDPs to eliminate the (unrealistic) assumption of having full knowledge of the ground truth [2]. The paper would benefit from comparing to these methods, especially with [2].

W4. The paper does not engage in some recent work on defining harm in similar scenarios, see for example [3] or [4]. It could be useful to understand, in light of different definitions of harm, whether the results are specific to harm prevention, or can be framed in a more general understanding of bounds over rewards.



OTHER (MINOR) REMARKS

R1. The paper is mathematically dense and difficult to follow in parts. I'm not sure whether this is a weakness on its own, but I have the feeling that the ideas conveyed are simpler than the dense mathematical presentation seems to suggest. 


REFERENCES

[1] N. Jansen et al. Safe Reinforcement Learning Using Probabilistic Shields. CONCUR 2020.

[2] S. Carr et al. Safe Reinforcement Learning via Shielding under Partial Observability. AAAI 2024.

[3] S. Beckers et al. Quantifying Harm. IJCAI 2023.

[4] J. G. Richens. Counterfactual harm. NeurIPS 2022.","Q1. How do you envision these guardrails to be applied in realistic scenarios? For example, consider the situation of a language model trying to obtain your passwords, or an autonomous car trying to crash with another vehicle. Could this notion of harm be applied efficiently to these realistic scenarios?

Q2. How sensitive are the results to the choice of priors in the Bayesian framework? Can the authors discuss the robustness of the proposed approach under different prior choices?"
2Oh2EOcFSO,2Oh2EOcFSO,Can a Bayesian oracle prevent harm from an agent?,Reject,gnvSTQYFLn,ICLR.cc/2025/Conference/Submission424/Reviewer_ADYh,"This paper studies the problem of evaluating an unknown world model from observed data to determine whether it satisfies a certain safety metric. The safety metric, or guardrail, is a binary variable $H$, taking other variables in the world model as input. The authors utilize a Bayesian approach. It assumes access to the actual prior distribution over the groundtruth world model. The authors first prove that under certain parametric assumptions, the posterior distribution over candidate models will uniquely converge to the ground-truth model at the limit of large samples. Building on this concentration results, the authors derive an upper bound over the posterior probability of the harmful event $H = 1$ conditioning on the observed data. This concentration bound is then extended to non-i.i.d settings where observed samples are correlated. Finally, simulations were performed, and results supported the proposed theory.","- The paper is well-organized and clearly written. All the theoretical assumptions have been stated.
- The proposed concentration results seem reasonable. The derivations seem technically sound.
- Training large AI systems to satisfy certain safety criteria (i.e., with guardrails) is an exciting problem. This paper formulates this problem as a hypothesis-testing problem and presents non-trivial algorithms to perform the test. This problem formulation could be inspiring for other AI researchers across domains.","- The concentration result in Prop. 3.1 assumes ""all theories in $M$ are distinct as probability measures."" This assumption does not seem to hold many common probabilistic models. For instance, in the linear component analysis, the number of independent components is generally not uniquely discernible (i.e., not identifiable) with non-linear mixing functions. Also, the number of latent components in Gaussian mixtures is generally not identifiable from the observed data. This seems to suggest that the application of the proposed concentration results might be limited.
- The proposed concentration results also assume access to the actual prior distribution generating the ground-truth world model. It is unclear whether the upper bound could still hold when the prior distribution is unknown and misspecified.
- Other concentration bounds exist over the target estimates using Baysian methods. Generally, one should be able to translate empirical concentration bounds to the Bayesian settings. For instance, (Osband & Van Roy, ICML'17) translates the concentration bounds for online reinforcement learning to Bayesian regret. How does the proposed method compare to other related work? This paper should include a section discussing related work in large deviation theory and how this paper is situated in the existing literature.

- Reference: _""Osband, Ian, and Benjamin Van Roy. ""Why is posterior sampling better than optimism for reinforcement learning?."" International conference on machine learning. PMLR, 2017.""_","1. How does the upper bound in Prop. 3.4 apply if the prior distribution $P$ is misspecified?
2. How does this work compare to the existing literature on concentration bounds in the Bayesian setting? For instance, these methods could include analysis of Bayesian regret in RL, and PAC Bayes."
2Oh2EOcFSO,2Oh2EOcFSO,Can a Bayesian oracle prevent harm from an agent?,Reject,ivJJB5uIzs,ICLR.cc/2025/Conference/Submission424/Reviewer_bi5T,"The paper is tackling the problem of safety in AI. The authors take the view of defining safety as avoiding certain undesirable states in specific contexts.
They introduce a framework based on Bayesian inference from which an agent can derive safe policies that come with (probabilistic) guarantees of preventing harm.
The approach is safe-by-design, i.e. able to prevent undesired outcomes even if no concrete example of harmful states was ever observed in the system.","The main strength of the paper is the introduction of a (as far as I am aware) novel view at safe-by-design in AI at runtime and opening the possibilities for future Bayesian methods to utilize the safety guarantees shown in the paper. Allowing for safety guarantees and steering future work in a direction that empashizes those is a significant problem in AI.
I especially appreciate the discussion on open problems of the approach in the conclusion.

The theory being developed is also quite general and spans over a wide range of possible systems/problems.

The paper is well motivated and generally well structured, introducing formal concepts as needed in the respective sections. A small experimental evaluation is performed and well discussed. Proofs are provided in the appendix and I could not find any mistakes.","My main critique points of the paper are the lack of technical novelty (or at least it is not clarified enough if there are new results) and questions on applicability.

For the former, essentially all Propositions and Lemmata are either adaptations of well known results (Prop 3.1), taken from previous literature (Lemma 4.1, Prop 4.2), or rather simple Corollary derived from them (Lemma 3.3, Props. 3.4, 4.4, 4.5, 4.6). For Prop. 4.2 is is shown that the result is tight (Remark 4.3). To me it did not became clear whether this is a known result or a new contribution. It is also not clear whether the derived results (Props. 4.4, 4.5, 4.6) are also tight as a consequence or whether there is room for improvement.
For Prop. 4.5 and 4.6 in particular, restricting the possible world models indeces to $\\mathcal{I}^{\\alpha}_{1\\colon t}$ is essential, however, the choice of definition of $\\mathcal{I}^{\\alpha}$ is not really motivated. At the same time, Fig. 2(a) shows a substantial gap between applying Prop 4.6 in practice, and the theoretical optimum. This begs the question whether a different definition of $\\mathcal{I}^{\\alpha}$ (e.g. a simple cutoff, or requiring $\\mathcal{I}^{\\alpha}$ to have a certain probability mass) has potential to yield tighter bounds. However, as the definition of $\\mathcal{I}^{\\alpha}$ is not motivated, these questions remain unadressed.

For the applicability, my core concern is that the main problem in AI is not providing safety guarantees under certain assumptions, but rather designing a Bayesian agent that actually works well for a given problem while satisfying these assumptions. To go into detail, section 3 only provides ""law of large numbers""-style guarantees which are not useful in practice. A small paragraph on the rate of convergence (which would be very helpful to know) is included but essentially is very problem-dependent and thus not discussed in detail in this more general framework. In the experimental evaluation, where Prop. 3.4 is utilized, it is not even clear whether $t$ is large enough for the guarantee statement of Prop. 3.4 to hold (on top of Prop 3.4 not being applicable due to non-i.i.d. as the authors mention themselves). Section 4 then relaxes to probabilistic guarantees, which is a more practical approach. However, to apply the results of section 4 in practice it ultimately relies on defining a hyperparameter alpha. On the theoretical side, the guarantees in section 4 only hold if alpha is chosen small enough (which is impossible to know without knowing the system in the first place) and on the practical side, the evaluation in section 5 shows that choosing alpha too large can have catastrophic consequences, even for the simple bandit system considered in section 5. In summary, I do not see any immediate way to take advantage of the theoretical results the paper provides. This is also amplified by the fact that the main body essentially does not discuss related work, and how existing approaches can be embedded into the framework.

*These weaknesses make the paper feel like more of a statement paper with some additional mathematical background, rather than a fully fletched research paper.*

As a minor comment, from a reader's POV, the paper can be hard to follow at times, especially in the formal sections. Many paragraphs are written in a very technical way, assuming a deep mathematical background. While this surely can be expected from an audience like ICLR, I feel like many sections disrupt the flow of the paper, e.g. the two paragraphs ""Setting"" (l.155ff and l. 268ff). While these are defnitely important to make the paper rigorous, they are not strictly required to convey the main ideas of the paper. In the interest of readability, it might be advantageous to instead outsource the technical definitions to a separate section.","1. Can you detail how you can utilize the CLT to obtain convergence rates (line 238ff)? If you applied this to the example in seciton 5, would it yield practical bounds?

2. Are the results in Propositions 4.4 to 4.6 tight (in a similar vein as Remark 4.3 shows for Proposition 4.2)?

3. How do you motivate the definition of $\\mathcal{I}^{\\alpha}_{1\\colon t}$ and have you considered different approaches?

4. Can you provide some heuristics on choosing a safe, yet effective $\\alpha$ a priori? Which information might be helpful for this from e.g. which model paramteres have the biggest impact on $\\alpha$ and what information from a domain expert could be incorporated?

5. Can you make any predictions on how you proposed guardrails perform on larger, more complex models? In particular, how do you expect the overestimation of harm (see Fig. 2) to be affected?

6. Are there any existing works in which your framework fits, i.e. for which you can give (probabilistic) guarantees where they were previously unavailable?
If not, are there certain settings in which you can make reasonable a priori assumptions such that your framework is applicable and concrete guarantees can be derived for a given data set?

minor comments:
- line 96: explain what $q$ is
- line 193: introduce delta as dirac notation beforehand
- the axis and legends in the figures in section 5 are barely readable"
UvMSKonce8,UvMSKonce8,Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers,Accept (Poster),KZtiuB4nLB,ICLR.cc/2025/Conference/Submission357/Reviewer_5F4V,"This paper aims to address the tradeoff between prediction accuracy and interpretability of machine learning models. That is develop an inherently explainable model without compromising its prediction performance. To achieve this, the authors propose to integrate small side networks into black-box models and use them to generate Shapley value explanations. The paper provides comparisons of computational costs and memory usage, and empirical results on both prediction accuracy and explanation quality.","1. The idea is intuitive and novel.
2. The paper is well-written and easy to follow.
3. The proposed method significantly cuts down the trainable parameters, memory usage, and FLOPs without compromising prediction accuracy and explanation quality.","1. Since the proposed method relies on the frozen pretrained black-box model’s parameters, will the biases or spurious correlations in the pretrained model also affect the generated explanations from the side network? Can the side network mitigate these issues?
2. Following Weakness 1, robustness evaluations are missing from the current experiments. How the side network will behave under the out-of-distribution (OoD) or even adversarial data w.r.t. the pretrained model?
3. The side network is designed to be parameter-efficient, but is there an optimal balance between its complexity and explanation fidelity? It would be better to see ablation studies on different side network sizes or architectures.",See Weaknesses for details.
UvMSKonce8,UvMSKonce8,Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers,Accept (Poster),HNdaKGkK9X,ICLR.cc/2025/Conference/Submission357/Reviewer_Qio4,"The paper proposes a method called AutoGnothi for generating Shapley-based explanations in black-box models, such as Vision Transformers (ViTs) and BERT, using side-tuning. Instead of modifying the main model, AutoGnothi trains a lightweight, parallel ""surrogate"" network that learns to approximate the importance of different input features without requiring iterative masking during inference, as is typically needed for Shapley values.
The main advantage of AutoGnothi lies in its efficiency. By using side-tuning, AutoGnothi is more memory- and time-efficient compared to previous approaches like ViT-Shapley, which require a fully fine-tuned surrogate model.

The authors measure the quality of explanations using insertion and deletion metrics, which test whether removing or adding key features impacts the model’s prediction as expected. In these tests, AutoGnothi’s explanations are shown to be comparable to ViT-Shapley (I read through the Appendix as well to make this conclusion). Besides, although qualitative examples suggest that AutoGnothi may better highlight main objects in images, the paper doesn’t fully convince me that it strictly produces better explanations than ViT-Shapley. Putting the explanations into a human-in-the-loop evaluation would help answer my concern.

The general idea is not new (e.g. compared vs. ViT-Shapley) but I appreciate the efficient solution, clear/detailed writing style, and thorough evaluation. I especially enjoyed reading Section 4.2.

In summary, I like the paper and recommend accepting it. My ratings can be adjusted during the rebuttal.","Originality: While AutoGnothi's goal of providing Shapley-based explanations overlaps with previous work like ViT-Shapley, it introduces a new side-tuning approach to reduce memory and computational demands.

Quality: The paper provides a thorough evaluation across multiple datasets (both vision and language), using various metrics to prove the efficiency and explanation faithfulness.

Clarity: The writing is clear and  well-organized.","The evaluation of explanation: 
Although qualitative examples (Fig.7) suggest that AutoGnothi may better highlight main objects in images, the paper doesn’t fully convince me that it strictly produces better explanations than ViT-Shapley. Putting the explanations into a human-in-the-loop evaluation [1,2] would strengthen claims about its explanation quality.

[1] The Effectiveness of Feature Attribution Methods and Its Correlation with Automatic Evaluation Scores

[2] What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods

Minor: I think the authors could also be upfront about the limitations of the works (e.g. approximation, explanation evaluation). The current writing does not discuss any of its **possible** limitations.","Q1. The general idea in AutoGnothi is that the main model makes predictions, and the surrogate model explains those predictions. However, how can the authors ensure that the surrogate model **truly** reflects how the main model makes its decisions? Although the paper shows efforts to align the two main and surrogate models, how well would AutoGnothi work if we scale it to more complex models (where aligning networks structure is prohibited) with complicated output distributions, where using KL divergence might not be enough to keep them aligned? Are there better alternatives than KL divergence?

Q2. I’m curious to see how AutoGnothi performs on more complex tasks. Currently, the tasks feel easy (especially the text classification). I’m concerned that side-tuning may not scale well with more complex tasks, like question answering or fine-grained image classification (e.g., CUB-200). How would the authors think the current side-tuning can satisfy interpretability and accuracy for complex tasks?

To clarify more, Q1 is about alignment and Q2 is about the feasibility of side-tuning."
UvMSKonce8,UvMSKonce8,Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers,Accept (Poster),V9ScKmpH6n,ICLR.cc/2025/Conference/Submission357/Reviewer_2vJv,"This paper is concerned with the setting of training an explainer for an explainee. 
Improvements to the previous work, which trains a surrogate model that can handle masked inputs and another explainer, that can predict the saliency, are primarily in terms of required ressources, both for training and inference with respect to time and memory. 
This is achieved by utilizing Ladder-Side-Tuning to only train a reduced number of parameters on top of the explainee.
When saliency and prediction are both desired, only the explainee and the few additional parameters are needed, incurring significantly reduced costs, while still matching the previous SOTA performance.","The paper is presented very clearly, well written and easy to understand.

The proposed method combines ideas of PETL and Covert et al. in an original way. 

The method at least matches the performance of the significantly more resource intensive method of Covert et al., clearly improving upon it overall.

The evaluation covers two modalities and multiple datasets with consistent results.

A convincing ablation demonstrates the superiority of the proposed approach to two naive baselines, called Froyo and Duo, showing that explaining and predicting cause orthogonal gradients.","The paper discusses a  simple, resource efficient alternative to a niche paradigm of explaining a model, that is only applicable to Transformers.

Patch Dropout has recently emerged as training augmentation for Vision Transformers, that enables transformers to handle masked inputs. (https://openaccess.thecvf.com/content/WACV2023/papers/Liu_PatchDropout_Economizing_Vision_Transformers_Using_Patch_Dropout_WACV_2023_paper.pdf)
Since the surrogate models are more accurate on multiple datasets, it is not clear why one would want to stick to the explainee.
The paper misses a discussion of that.

Combining the two points above, I am unsure how significant this work is, as the field might move towards Transformers trained with Patch Dropout. 

While the results in Table 3 suggest a gap, they are cherry-picked and no ""clear margin"" is visible to Covert et al. across the different architectures, typcially more evenly matched (As in Tab.  4 -8).

The evaluation using the surrogate model is frickle, as it is not actually the model to be explained. 

It is unclear why ImageNette is used as main dataset, as it seems too easy, with accuracies above 99%.

Fig. 2 is too full. The table is repeated in the paper anyway. (Table 1 and 2)

typos in lines : 264, 535","How do the saliency maps transfer across surrogate models?
As the saliency maps explain the explainee, they should work across a range of surrogate models, especially those the explainers were not trained on,  but which still try to explain the same explainee.

Which surrogate model is used to evaluate the baselines?"
LuGHbK8qTa,LuGHbK8qTa,Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes,Accept (Poster),71kzY3zRiB,ICLR.cc/2025/Conference/Submission298/Reviewer_44mR,"The proposed method uses two representations jointly to model a 3D object over time from forward-facing monocular video to produce state of the art results on publicly available datasets. The method produces a volumetric representation anchored to a mesh and can be used to track the deformation of the object over time. The tracked mesh can then be manipulated such as editing the texture. To achieve this, the proposed method uses Gaussians anchored to a canonical mesh using a forward / backward deformation field and Gaussian merging / creation rules based on mesh vertices. The method uses DPSR, DiffMC and laplacian regularization to generate intermediate meshes.",The paper is well written and follows a fairly intuitive and novel approach to generating both a deformable mesh and gaussian representation in parallel leveraging the benefits of both representations in the loss functions. The method performs at interactive speeds during inference while producing high quality results both qualitatively and quantitatively for both the mesh and Gaussian representations. The paper is evaluated on a variety of both real-world and synthetic datasets and produces tracked results using the forward / backward deformation of a canonical mesh. This is likely a significant improvement in learning deformable 3D scenes.,"- The paper demonstrates that using both Gaussians and meshes improves the overall quality of novel view synthesis. There are several qualitative examples of higher quality meshes but it is unclear how / if appearance improves between the baselines and the proposed method when the underlying geometry is of similar quality.
- There is a lack of rigorous quantitative evaluation for several key decisions such as choosing the initialization, DMTet/DiffMC/FlexiCubes, mesh anchoring frequency and the training methodology and relies on qualitative observations. While these qualitative observations likely hold up well, having quantitative evaluations of various modules is desirable.","- What's the intuition behind the improvement when using both Gaussian and mesh rendering losses?
- How would the method handle a synthetic large textured but planar surface such as a checkerboard where there might not be enough vertices to anchor gaussians to?
- In the attached supplementary materials, the anchored gaussians are represented as blue dots. What do these blue dots represent - is it just the mean positions?
- How is the ""canonical"" frame chosen given that the input video sequence is monocular and the scene is not static? Can you share more details about about the (frozen) initialization of the forward / backward deformation networks when training the canonical Gaussians and the SfM initialization?
- Are there any qualitative observations relating to decomposition of geometry / appearance information between the mesh representation and the gaussians?
- How do you arrive at running mesh anchoring every 100 iteration? Is it possible to perform differentiable mesh anchoring?"
LuGHbK8qTa,LuGHbK8qTa,Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes,Accept (Poster),wiv3WCa8Ar,ICLR.cc/2025/Conference/Submission298/Reviewer_wAGq,"The paper introduces Dynamic Gaussians Mesh (DG-Mesh), a novel framework designed to reconstruct time-consistent, high-fidelity meshes from monocular video footage. Utilizing 3D Gaussian Splatting (3DGS), the framework ensures temporal consistency and superior mesh reconstruction quality. Innovations such as Gaussian-Mesh Anchoring and cycle-consistent deformation significantly enhance the distribution and stability of 3D Gaussians across frames.","1. The manuscript is well-structured, with fluent writing and a clear explanation of methods, making it easy for readers to understand.

2. This method combines the advantages of Mesh and 3D GS and achieves remarkable improvements in surface detail and rendering quality in dynamic object reconstruction.

3. The experiment is thorough, with comprehensive benchmarks against established methods, demonstrating significant enhancements in both mesh quality and image rendering.","1. The integration of 3D Gaussians with mesh surfaces is good, yet the paper does not explore the potential benefits of using 2D Gaussian Splatting for surface and image reconstruction accuracy. Could a 2D representation be more effective or efficient when Gaussians lie on the mesh surface?

2. The method involves separate rasterization processes for Mesh and 3D Gaussians. However, the paper lacks a detailed explanation of how color information is harmonized between these components. How is color from the 3DGS sh presentation passed to the mesh during rasterization?

3. Binding a single Gaussian to each mesh face raises questions about the distribution logic. Why was this strategy chosen? Did the authors consider alternatives, such as placing multiple Gaussians on a single mesh face or varying the number of Gaussians based on mesh face properties?

4. Table 1 only presents PSNRm metrics; however, standard PSNR metrics for direct 3DGS rendering results are absent. Could the authors clarify the reason for this omission?

5. The paper does not address how varying the number of mesh surfaces extracted from the same scene affects rendering accuracy and computational efficiency.",Please see the weaknesses part.
LuGHbK8qTa,LuGHbK8qTa,Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes,Accept (Poster),xuUFIDUyQA,ICLR.cc/2025/Conference/Submission298/Reviewer_mKQh,"The paper introduces a method for using deformable Gaussian Splatting to generate dynamic meshes for each frame of a monocular video. It employs a set of canonical Gaussians, deforming them at each timestep with an MLP. These deformed Gaussians are then used to construct a mesh for each frame through Poisson Reconstruction and Marching Cubes. The mesh serves as an anchor for the Gaussians, ensuring a one-to-one correspondence between mesh faces and Gaussians. Instead of textures, vertex colors are applied to the mesh, which is rendered as an image for supervision.

The paper is clear and straightforward, though it does have some limitations. The resulting mesh is a per-frame set rather than a truly deformable mesh. Additionally, the mesh is colored using vertex colors instead of a high-resolution UV map. These constraints raise concerns about the method’s applicability.","The paper sets an ambitious goal: reconstructing dynamic meshes from monocular videos, a critical challenge in graphics and vision. It begins with a baseline approach using Deformable GS to model the dynamic scene. Mesh reconstruction is then applied to each frame, incorporating an anchoring constraint on the Gaussians through the mesh. This approach helps achieve a smooth surface but can adversely affect the rendering results.","Here are the weaknesses of the paper:

1. **Mesh Representation:** The method produces a set of per-frame meshes rather than a deformable mesh. Although mesh representations are known for their rich operations and memory efficiency, the paper does not demonstrate how to register each frame to a unified mesh. This per-frame approach is not memory-efficient and makes editing geometry and textures cumbersome, as changes cannot be easily propagated across frames without manual effort.

2. **Detail Preservation:** The mesh optimization and anchoring mechanism do not preserve details effectively, as demonstrated in Table 1. While meshes efficiently represent low-frequency geometry, high-frequency textures should ideally be stored in a UV map. This method colors the mesh at vertices, limiting its ability to capture high-frequency details. The one-to-one mapping between mesh faces and Gaussians necessitates a high-resolution mesh to accommodate the large number of Gaussians needed for rich texture and geometry representation. However, the mesh number is limited by the resolution of Marching Cubes and is hard to increase as much as needed Gaussians. This results in inefficient memory use, as mesh faces are not effectively utilized for storing detailed textures.

3. **Missing Related Works:** The paper omits several closely related works:
   - ""SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes"" by Huang et al.
   - ""Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis"" by Li et al.
   - ""Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction"" by Das et al.
   
   It is recommended to compare with **SC-GS** in Table 1, as it is the current state-of-the-art method in this area. It's not necessary to overwhelm its result since the focus of the two papers differs.","1. Is there a way to unify the per-frame meshes into a deformable mesh? Once unified, this mesh could be optimized with a UV map to capture rich texture details. Neural temporal textures might also be useful, as they can address inaccuracies in deformation correspondence across frames by providing time-dependent texture adjustments.

2. Can DG-Mesh be used to obtain a skeleton and skinned mesh? Extracting animatable assets from dynamic videos is crucial. This could be suggested as a potential enhancement in the future work section. The paper can articulate how it advances the field toward this goal to some extent."
LuGHbK8qTa,LuGHbK8qTa,Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes,Accept (Poster),PHUxk3527M,ICLR.cc/2025/Conference/Submission298/Reviewer_9ERR,This paper proposes a method to extract high-quality mesh from image inputs in monocular/ efficient multiview setups. The authors propose a Gaussian-Mesh anchoring method and cycle-consistent deformation loss to enhance the upper bounds of DG-Mesh representations. The authors propose many downstream tasks to demonstrate the widely applications of DG-Mesh.,"1. This study presents a novel framework that enables the reconstruction of high-fidelity, time-consistent meshes from monocular videos. Generating meshes for dynamic scenes is a challenging yet critical task, and the author makes a significant contribution in addressing it.

2. The proposed approach includes a Gaussian-Mesh anchoring technique that produces uniformly distributed Gaussians in each frame, enhancing the effectiveness of mesh reconstruction. Cycle-consistent deformation loss adjusts the deformed Gaussians, offering a new method for densification in deformation-based 3D Gaussians.","1. **Baseline Selection:** I suggest that the authors consider incorporating additional baselines such as Spacetime-GS (Li et al., CVPR 2024), 4DGS (Yang et al., ICLR 2024), and 4D-GS (Wu et al., CVPR 2024), given the recent advancements in Dynamic Gaussian representations. It would also be helpful to include some NeRF baselines in the supplementary materials.

2. **Color Prediction:** As mentioned in Section 4.2, the mesh is extracted using Nvdiffrast. How do the authors handle the color extraction for the mesh? Is the DG-Mesh representation used for this purpose? If so, why? I find it challenging to apply this approach in real-world scenarios.

3. **Real-World Monocular Scenes:** In my opinion, DG-Mesh is still difficult to apply in real-world settings, as the original 4D representations struggle to maintain multi-view space-time consistency. While the results appear satisfactory in training views, they might deteriorate in novel views or the final extracted mesh. I recommend that the authors emphasize the robustness of multi-view videos rather than focusing solely on monocular videos. In terms of monocular dynamic scene reconstruction, there are several relevant works (e.g., MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds, Dynamic Gaussian Marbles, Shape of Motion, Dreamscene4D, MoDGS) that have yet to deliver satisfactory results. Claiming effectiveness in monocular videos might be an overstatement.","Please refer to weakness. 
Overall, I think this is a pioneer work and My main consideration is weakness2,3."
ji6MYm4Htg,ji6MYm4Htg,Pruning Aggregation Parameters for Large Language Models,Reject,BVQp1uUWyj,ICLR.cc/2025/Conference/Submission260/Reviewer_54uK,"This paper introduces AggregationPruner, a pruning algorithm designed to improve the memory efficiency of Large Language Models (LLMs) by focusing on ""aggregation parameters"" (queries and keys) in the higher layers, specifically targeting parameters in the attention mechanism without additional training. The authors argue that aggregation parameters contribute less unique information in higher layers, enabling their selective pruning to reduce memory demands, particularly in the key-value (KV) cache. The proposed method is experimentally tested on various LLMs, including LLaMA and Qwen models, across several tasks, reportedly outperforming other pruning strategies.","- The aim of reducing memory usage without retraining is practical and relevant for real-world LLM deployments.
- By exclusively targeting aggregation parameters, the authors contribute to a niche aspect of pruning in LLMs.
- Broad Experimentation: The experiments cover a wide range of tasks and models, suggesting the authors’ commitment to evaluating their approach comprehensively","- The central idea of selectively pruning only the aggregation parameters lacks a theoretical or empirical foundation that justifies this choice over simpler alternatives. The GNN analogy is interesting but ultimately weakly connected to the experimental findings.
- The paper would be stronger with a clearer examination of how AggregationPruner performs relative to simpler or alternative pruning baselines (e.g., random initialization or whole-layer pruning). This would clarify if the exclusive focus on aggregation parameters provides a true advantage.
- There is limited discussion of when and why AggregationPruner might fail or struggle compared to other methods. This omission leaves the impression that the results are selectively presented to favor AggregationPruner without sufficient critical analysis.
- Without a clearer framework or theoretical underpinning, this method appears to offer only incremental improvements in memory efficiency rather than advancing pruning methods as a whole.","1. Could the authors explain why simpler alternatives were not included as baselines? Given the minor variation, this comparison might help illustrate the impact of targeting only aggregation parameters.
2. On what basis do the authors claim that aggregation parameters contribute less unique information in higher layers? Was this hypothesis tested directly, or is it purely derived from the analogy to GNNs?
3. How would AggregationPruner perform if applied to smaller LLMs or other architectures with different attention mechanisms? Could this method generalize across architectures?
4. Why was a grid search over the α parameter chosen rather than a more optimized approach? Could this choice be a source of inefficiency?
5. Can the authors clarify whether the effectiveness of AggregationPruner would differ between shorter and longer inference tasks?"
ji6MYm4Htg,ji6MYm4Htg,Pruning Aggregation Parameters for Large Language Models,Reject,A0QrcPZtya,ICLR.cc/2025/Conference/Submission260/Reviewer_3hHL,"The work proposes a pruning algorithm aimed at compressing the KV cache in large language models (LLMs) without additional training. Referring to GNN, the authors classify LLM parameters into three categories: aggregation, transformation and normalization, focusing on the pruning of high-level aggregation parameters (KV cache). The authors evaluate their approach on various LLMs and benchmarks, reporting improvements over recent pruning techniques.","1. Pruning the KV cache is important for compressing LLMs.
2. There is something interesting about analogizing the parameters of LLM to GNN.","1. Classifying LLM parameters based on the nature of GNNs is of some interest, but lacks some theoretical and experimental support. The different layers of the LLM have their different effects, and the higher layers have their special role in some cases of work. The authors need to have more experiments to support this claim.
2. The authors lack comparisons with SOTA methods e.g. FLAP, Wanda. It would be unfair to simply compare with selfattn/ffn/layer pruner, as no control model parameter is identical.
3. Alg. 1 is redundant and can be moved to the appendix. But Alg.2 lacks a detailed description and the authors need to describe the method in more detail.",See the weakness part.
ji6MYm4Htg,ji6MYm4Htg,Pruning Aggregation Parameters for Large Language Models,Reject,lUbGP7xLrI,ICLR.cc/2025/Conference/Submission260/Reviewer_Kwoc,"This paper proposes a methodology for pruning LLMs without needing to do any fine-tuning of the model, by targeting ""aggregation"" parameters, as defined by the authors, in the later layers of the mode. By doing so, the KV-cache can be reduced in size, which should result in cheaper inference. The authors demonstrate that accuracy losses for several popular LLMs can be manageable with this methodology.","Pruning techniques that don't require fine-tuning the model are always welcome, and this technique does seem to be able to change many of the layers in popular LLMs without hurting the accuracy too much. I also thought the reasoning as to why the authors wanted to prune these particular parameters was interesting.","The biggest problem with this paper is that it presents no quantitative data as to whether inference using these pruned models is actually cheaper or faster than traditional inference, and it doesn't contextualize the results against other ways of making a model cheaper to run. The strongest claim in the paper is that the KV-cache size can be reduced, which is useful, but that is not the same as a speedup. The authors did not compare against pruning methodologies that require fine-tuning, or against other forms of quantization or sparsity. Techniques like these have to be understood in context, and the paper doesn't provide it.","1. What is the measured speedup of this approach, especially when changing the batch size to take advantage of the KV-cache compression.
2. Did you evaluate using BF16 number formats? Could you compare against PTQ with FP8?
3. Could you compare against other pruning approaches?"
ji6MYm4Htg,ji6MYm4Htg,Pruning Aggregation Parameters for Large Language Models,Reject,9WnIiUDLl4,ICLR.cc/2025/Conference/Submission260/Reviewer_UEim,The paper introduces a pruning algorithm (AggregationPruner) which prunes the Key and Value weights (aggregation parameters) of higher layers of LLMs to reduce the GPU memory consumption during LLM inference tasks (primarily during generative tasks). The method (being post training-free) outperforms the baselines in retaining the performance on various benchmark datasets and models.,"1. The paper is clearly motivated, very well structured and has a great value for LLM inference community.
2. The connection between GNNs and LLMs and various design choices (why they picked only higher layers, picking aggregation matrices) are well established.
3. The analysis that pruned models might be good for discriminative tasks while have effect on generative tasks (Fig 8) is pretty interesting and sort of intuitive. (Infact, I believe this statement holds true for any compression method given the nature/complexity of these tasks but might need validation). That being said, I've a question wrto AggregationPruner, refer 1st point in weakness","These are not exactly weaknesses per se, but some thoughts I've to improve the paper. 

1. [**Experiment**] Can the authors have an experiment to truly validate the claim that picking lower layers will degrade the performance due to the importance of the layers (line 3 in Algorithm 4)? While the math is quite intuitive from GNNs, an experiment/ablation on all layers (0-32) for model on various tasks can be a good addition? 
2. [**Comment**] Continuing from previous point, can the authors comment on design choice on choosing the number of layers for real time usage of the method (line 3 in Algorithm 4)? From Fig 2, it seems to be having a lot of variation for different models/datasets.  
3. [**Comment**] As mentioned in L419-420, KV cache doesn't get created for discriminative tasks. So, can the authors explain how effective AggregationPruner is for discriminative tasks? I believe authors might have to consider explaining this part more clearly in the paper!
4. [**Experiment**] The paper has been motivated to be effective on GPU memory consumption on generative tasks. Can the authors provide an actual experiment to demonstrate this? 
    - *For eg*: Pick a model, a generative task, measure the perplexity/wall-clock time/speed/memory for the baseline model vs AggregratedPruner method. If an experiment of this sort can't be perform, please comment on why that might be the case.
5. [**Experiment/Comment**] L144-146: The authors have mentioned that ""given the black-box nature, we choose the KV matrices"" and demonstrated the results in the paper. And I also believe Self-AttentionPruner, FFNPruner and LayerPruner are the terms introduced in this paper (please correct me if I am wrong and cite these methods accordingly). 
   - Now, given these key design choices, I would like to know if it's possible to report the sparsity ratio for these various methods? For eg: AggregationPruner might be better compared to FFNPruner because the number of parameters being pruned is very-very less in the former (similar to [1]). While I am not expecting an apples-apples comparison in terms of pruned parameters, these details will be informative. 
    - I am also aware that the total number of parameters to be pruned in AggregationPruner might be different depending on the size of KV cache which will vary; so the authors can make assumptions while reporting results in the tables. If it's not possible for some reason, please comment.
6. [**Experiment**] Can the authors perform an experiment on the choice of dataset on Greedy Search of Alpha? i.e how dependent the alpha values are with respect to dataset. Suppose it turns out to be dataset dependent, then is it safe to say that the method is calibration-dataset dependent? If so, I believe addition of these details (either in Appendix or main section) will be beneficial.","**Possible Relevant citation**
- L464 - L468: Following the point 5 from weakness section, while not the exact relation is established, I believe this study [1] has some connection with respect to compressing the FFN blocks vs Attention blocks and the number of parameters involved while pruning different blocks. 

[1] The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models - https://arxiv.org/abs/2312.00960

**Format**
1. [**Typo**] - L366: It is FFNPruner
2. [**Presentation Suggestion**] The authors might consider a small block diagram explaining the difference between FFNPruner, LayerPruner, AttentionPruner and AggregationPruner. Or maybe a diagram for the algorithm
3. [**Presentation Suggestion**] It seems like the assumptions/claims on GPU memory consumption has been mentioned at different places without an experiment to validate. So, the authors might reconsider text formatting.

**Note:** Suggestions are not mandatory improvements and the authors can wish to ignore it totally."
ji6MYm4Htg,ji6MYm4Htg,Pruning Aggregation Parameters for Large Language Models,Reject,wjliSY4VWh,ICLR.cc/2025/Conference/Submission260/Reviewer_o9F7,"The study introduces a novel pruning algorithm that specifically targets aggregation parameters within large language models to reduce the model size and lower GPU memory usage during inference. By incorporating a rescaling parameter,  the method enhances the performance of pruned models without additional training.","1. This work proposes a novel pruning algorithm that requires no additional training and targets specific parameters within LLMs. 
2. This work introduces a rescaling parameter that adjusts the output of the pruned block to further improve the performance of the pruned LLM.
3. Extensive experiments demonstrate that the proposed method outperforms the recent block pruning algorithms.","1. While the experimental results support the effectiveness of the proposed method, the paper lacks theoretical analysis on why pruning aggregation parameters has minimal impact on model performance. The authors should provide more theoretical support or in-depth analysis.
2. The paper bases the selection of the rescaling parameter α on grid search but does not discuss its impact on model performance in detail. The authors should further explore how the choice of α affects model performance.
3. The paper focuses on model compression and acceleration but few discuss the generalization of pruned models across different domain tasks. 
4. The paper lessly discusses the model's performance and efficiency on actual hardware.
5. To promote the study's reproducibility, authors are recommended to provide the code used in the experiment and the preprocessed data so that other researchers can reproduce the results.","1. Existing methods bring additional training and huge computational overhead. What is the extra cost? What is the cost comparison between the proposed method and the existing method?
2. Why do existing methods maintain performance in domains not well covered by additional training data? What was the experiment? Analyze the specific reasons.
3. What is the key problem to be solved in this paper?"
xXTkbTBmqq,xXTkbTBmqq,OLMoE: Open Mixture-of-Experts Language Models,Accept (Oral),wsaqtgZmFV,ICLR.cc/2025/Conference/Submission211/Reviewer_7Ufw,"This paper introduces OLMoE, a fully open, state-of-the-art language model built on a sparse Mixture-of-Experts (MoE) architecture. The authors conducted extensive experiments to validate the effectiveness of the proposed method, including evaluations after pre-training and adaptation phases. Additionally, they explored key design choices within the MoE framework, examining factors like expert granularity, routing strategies. Their analyses provided valuable insights into MoE, including router saturation, expert co-activation, and domain/vocabulary specialization.","- The writing in this paper is clear and easy to follow.
- The paper advances MoE research by providing a fully open-sourced, state-of-the-art MoE architecture, which is beneficial for the research community.
- The paper presents a thorough analysis of key design choices in MoE, offering valuable guidance on building high-performance MoE models.
- The analysis is insightful, with discussions on phenomena such as router saturation and expert co-activation providing fresh perspectives and meaningful implications for the field.","I have a question regarding the experimental results: were the model parameters quoted directly from the original paper for the results shown in Table 2? For instance, in the original paper, OpenMOE’s activation parameter count is reported as 2.1B, whereas Table 2 shows an activation parameter count of 2.9B for OpenMOE. I recommend that the authors carefully verify the accuracy of these values.",See Above.
xXTkbTBmqq,xXTkbTBmqq,OLMoE: Open Mixture-of-Experts Language Models,Accept (Oral),QOI4jeOedV,ICLR.cc/2025/Conference/Submission211/Reviewer_PJU8,This paper presents a mixture-of-experts (MoE) LLM model called OLMoE that has 1B active parameters and 7B total parameters. The OLMoE model Pareto dominates many state-of-the-art models in the performance vs. active parameters space. The paper explores and presents insights on what is optimal in the design-space of MoE parameters and present analysis of routing behavior in MoEs.,"1) Strong empirical results with state-of-the-art performance for 1B active parameters.
2) Good exploration of the MoE design space which forms a good guide for MoE model design.
3) Novel analysis of routing behavior in MoE models during training and inference.
4) This is the only MoE model where the model weights, code, data and checkpoints are openly available and thus the work is entirely reproducible.","1) Other state-of-the art MoE models in related works are not exactly in the same parameter count configuration (1B/7B) so an exact comparison cannot be made to this model's performance.
2) Most of the design choices and training choices are based on prior work and the novelty is more in the design space exploration and analysis of routing behavior.",The work is well presented and possible suggestions for improvements are addressed in the future work section.
xXTkbTBmqq,xXTkbTBmqq,OLMoE: Open Mixture-of-Experts Language Models,Accept (Oral),uRHBIfCcem,ICLR.cc/2025/Conference/Submission211/Reviewer_vUvg,"This work is devoted to sharing the insights, data, and checkpoints of a series of MoE LLMs. The model achieved promising results on various benchmarks as a fully open model family.","1) There is no doubt that training MoE LLMs is challenging. This work offers a couple of important takeaways about how to train a good MoE LLMs, which is very helpful to the community.
2) The presentation is very clear. For instance, the Table 1 delivers many key designs clearly at the early section of the paper.
3) The model performance is good as well. As shown in Table 2 and 3, the model performs competitive with dense open models and partially open models (e.g. Qwen, Deepseek).
4) The Analysis in Section 5 is informative, which greatly help readers and authors to understand how is the model working. This can also greatly speedup the growth of the community.","1) Although the model has been relatively large, it is still much smaller than the SoTA MoE LLMs. I understand it is hard to get enough training resource for a fully open projects.","1) What do you think about the necessity of expert parallelism? This model used dropless MoE, so it anyway will be unbalanced when using expert parallelism during training and inference. Without expert parallelism, it is still okay when the model is small. However, if we are aiming at a very large model, which has very large experts even if we are using the ""fine-grained MoE"", the expert parallelism would still be required? So how can we handle the token drop problem in this case?"
IZjBfdVRB0,IZjBfdVRB0,Parameter-Efficient Fine-Tuning   via Circular Convolution,Reject,hCgkdBat29,ICLR.cc/2025/Conference/Submission178/Reviewer_F24A,"This work introduces Circular Convolution Adaptation (C3A) as an advanced parameter-efficient fine-tuning (PEFT) technique, addressing limitations of existing methods like Low-Rank Adaptation. C3A leverages the circular convolution operator, enabling high-rank adaptation without a proportional increase in trainable parameters. Utilizing Fast Fourier Transform (FFT) operations, C3A achieves efficient memory and computational performance while retaining expressive power. The method effectively reduces memory requirements and computational load compared to high-parameter alternatives like VeRA. A notable strength of C3A is its flexibility in adjusting the number of trainable parameters through block-circular convolution, which adapts to diverse downstream tasks. Experimental results validate C3A's superior accuracy and memory efficiency across various tasks.","1. High-Rank Adaptation with Low Parameters: C3A achieves high-rank adaptation without increasing the number of trainable parameters. By leveraging circular convolution, it avoids the limitations of low-rank adaptation in methods like LoRA, maintaining expressive power with fewer resources.
﻿
2. Efficient Use of FFT for Computational Gains. The use of Fast Fourier Transform (FFT) in both forward and backward propagation enhances computational speed and reduces complexity, achieving a time complexity comparable to LoRA but with better efficiency in memory use.
﻿
3. Memory Savings with Block-Circular Convolution. By implementing block-circular convolution, C3A allows for flexible control of trainable parameters. This adaptability to block size enables parameter tuning based on task requirements, optimizing both memory and computational resources.
﻿
4. Comprehensive Benchmark Performance. C3A demonstrates robustness performance across multiple tasks, including GLUE benchmarks for NLP and Vision Transformer (ViT) for image classification, showing significant improvements in accuracy with fewer parameters.
﻿
5. Adaptable Parameter Count with Minimal Performance Trade-Offs. C3A allows for adjustable parameter configurations through its block-circular design, offering a range of parameter counts without significant sacrifices in performance, providing versatility across tasks.","1. Although C3A claims robustness to initialization methods, the paper does not thoroughly analyze how different initializations may impact convergence speed or model performance across varying tasks. Additional experiments on initialization sensitivity could further validate this robustness claim and clarify any subtle effects on fine-tuning stability.
﻿
2. While the method demonstrates efficiency on moderate NLP and CV benchmarks, it lacks evaluation on tasks with substantially higher dimensions or complexity, where circular convolution’s rank flexibility may not be as effective. Testing C3A on such high-dimensional data would strengthen claims about its scalability.
﻿
3. Inadequate Justification for Block Size Selection: The paper introduces block-circular convolution with adaptable block sizes but provides minimal guidance on how to select optimal block sizes for specific tasks. This omission makes it challenging for readers to understand the trade-offs and practical considerations in choosing block sizes to balance accuracy and parameter efficiency.
﻿
6. The proposed method relies on block-circular convolution for flexibility, but the paper does not clearly address its limitations, such as potential rank deficiencies or performance degradation in non-square matrices. Expanding on these limitations and detailing their implications would provide a more balanced view of C3A’s adaptability across different architectures.","1. I will appreciate additional analysis on how initialization affects model convergence speed and accuracy across varied tasks. Specifically, experiments with detailed metric comparisons for convergence time under different initializations would strengthen this claim.

2. The method lacks experimental validation on tasks involving significantly higher dimensions or complexity, where circular convolution may face generalization challenges. We encourage the authors to include experiments on larger-scale datasets or more complex tasks to confirm C3A's adaptability and performance.

3. How different block sizes may impact both parameter efficiency and performance across tasks? We request the authors to provide guidelines for block size selection.

4. Aside from the existing comparison with baseline methods LoRA and VeRA, we suggest authors to Include more recent PEFT methods as baselines, as that will provide a more comprehensive understanding of C3A's standing relative to broader industry benchmarks.

5. Please elaborates the limitations of the proposed Block-Circular Convolution."
IZjBfdVRB0,IZjBfdVRB0,Parameter-Efficient Fine-Tuning   via Circular Convolution,Reject,7klyt3QjCx,ICLR.cc/2025/Conference/Submission178/Reviewer_VhRA,"The paper proposes a new parameter-efficient fine-tuning (PEFT) method (denoted C³A) that leverages circulant matrices and Fast Fourier Transform (FFT) to achieve high-rank adaptations while maintaining computational efficiency. The proposed method allows independent control of the rank and number of trainable parameters through block-circular convolution. The authors evaluate their method across various NLP and vision tasks, including NLP, comparing the performance of C³A to existing PEFT.","1. Providing an interesting approach for low-rank adaptation that relies on FFT instead of the typical residual matrix to try to address existing limitations.
2. The extension to block-circular convolution allows for better control over parameter numbers (and makes the method applicable to non-square matrix weights) without compromising rank.
3. Providing detailed practical implementation and foundation for the different components (e.g., circular convolution)","1. While using circular convolution and FFT in PEFT is interesting, the improvement over LoRA and similar methods may be seen as incremental, with modest/marginal gains rather than a significant improvement over existing PEFT methodologies.
2. The method's reliance on FFT may introduce implementation complexity, particularly in environments with limited support for these operations, which could restrict the practical applicability of the method.
3. Many of the presented results don’t show clear gains or dominance, which questions the reliability of the presented method, which is the main weakness of the paper.","1. Given that the results don’t dominate existing methods in many scenarios, what would be the ideal scenario/environment/conditions in which the proposed method is most effective?
2. Is there an ablation study for the block size (b)? I understand that it is constrained because of the divisibility, but I believe it is important to see some insights about its impact on the result.
3. Can you present more insights/analysis about the decision and the impact of using FFT (e.g., some ablation for the FFT part on its own)?"
IZjBfdVRB0,IZjBfdVRB0,Parameter-Efficient Fine-Tuning   via Circular Convolution,Reject,CDNPwE80pY,ICLR.cc/2025/Conference/Submission178/Reviewer_tV7B,"This paper borrows the block circulant method from existing work and applies to LLMs. The proposed method is about using block circulant matrix for training which has been already published in the referenced work (Ding et al., 2017). However, the entire method writing (section 3) does not acknowledge/cite the work at all and instead present as if authors created block circulant based training. As a result, the major contribution narrows down to the empirical verification of block circulant training method on different large models.","1. Experimental results are very promising, and evaluated models cover both the base and large version of RoBERTa and ViT, also including  LLaMA-2 and LLaMA-3. 
2. There is also synthetic data experiment which is simple and easy to demonstrate the effectiveness of proposed method against LoRA and full fine-tuning.","1. No technical novelty, because the proposed method is borrowed from the referenced work (Ding et al., 2017)
2. Lack of analysis regarding the impact of block size setting, e.g., smaller block size gets more parameters but smaller GPU cost (Table 2)? Why more parameters result in smaller GPU cost? According to theoretical space analysis (section 3.5.2), with d1=d2=768, the formulation cannot explain the observation. 
3. FFT based solution is naturally sensitive to intialization, as a small change in frequence domain can be easily spreaded to all time domain. The experimental design in section 4.5 should include results on LLaMA models, as the FFT size is large in LLaMA models. The hypothesis is inclined to be that intialization for FFT based method is sensitive in the case of large models or large FFT size settings.","1. Can you compare with the state-of-the-art FourierFT method (https://icml.cc/virtual/2024/poster/33821), which is also a Fourier domain based method? 
2. Can you provide memory cost details in Table 3 and Table 4, as you did for Table 2?
3. Need a breakdown of the memory cost of proposed method, since in Table 2, more parameters somehow results in less GPU memory for both base and large models. 
4. Initialization test for large FFT size setting like LLaMA for section 4.5
5. FFT operations can easily result in nan/inf values in low precision computation. Does the proposed method actually run in half precision (as LLMs are stored) or full precision?"
fyl82vAale,fyl82vAale,Sequential Order-Robust Mamba for Time Series Forecasting,Reject,zOscO3IUW3,ICLR.cc/2025/Conference/Submission69/Reviewer_PJuw,This paper proposes SOR-Mamba to solve the sequential order bias introduced by mamba capturing channel dependencies. Model SOR-Mamba incorporates a regularization strategy to minimize the distance between embeddings from data and reversed data and eliminates the 1D-convolution in the original Mamba block. And a pretraining task channel correlation modeling is introduced to preserve the channel correlation from the data space to the latent space. The effectiveness of the proposed method is demonstrated by the extensive experiment results.,"* This paper uses a single unidirectional Mamba with regularization and non-1D convolutions to capture channel correlations. By modifying the traditional Mamba module, it can better address the issue of sequential order bias. The research problem is clearly defined, and the solution is reasonable. And the pretraining task channel correlation modeling also enhances the model's generalization and performance.

* The details of the model are mostly clear, with each module explained and supported by equations. The experiments  include time series forecasting, transfer learning and many ablation studies to validate the effectiveness of each module.","* The proposed method is mostly constructed on existing models -- reverse modeling has been widely used in literature and removing the 1d-conv is trival.
* The experimental results show that the model's performance improvement across various datasets is not significant, mostly in the thousandth.
* The removal of 1D-conv negatively impacts PEMS dataset in tale 7 and figure 5. The necessity to remove 1D-conv is uncertain.
* The model uses the Mamba backbone, but the baseline only includes a single Mamba model: S-Mamba. Providing more models from the Mamba framework for comparison would be better.","* When randomly shuffling the channel order, how is it done? Is a fixed shuffling method used for all sequences in a dataset, or is it shuffled randomly each time?
* In the last row of Table 1, is the calculation formula written incorrectly?"
fyl82vAale,fyl82vAale,Sequential Order-Robust Mamba for Time Series Forecasting,Reject,h4noOkviVj,ICLR.cc/2025/Conference/Submission69/Reviewer_WEwK,"This paper introduces SOR-Mamba, a time series forecasting method that addresses the sequential order bias  while capture channel dependencies in Mamba, through a regularization strategy to minimize the discrepancy between two embedding vectors generated from data with reversed channel orders, and 1D-convolution removal, The authors also propose Channel Correlation Modeling (CCM) as a pretraining task.","1. Clear problem identification regarding Mamba's limitations in handling unordered channels
2. Comprehensive experiments
3. Improved efficiency compared to other approaches","**Limited Technical Novelty**:
- The regularization strategy is overly simplistic that minimizing the distance between embeddings from different channel orders
- The removal of 1D-conv lacks theoretical justification and appears to be an ad-hoc solution, As shown in Figure 5 of the paper, its removal may negatively impact datasets with ordered channels such as PEMS datasets","1. In the Manba architecture, are there other advances in order-invariant architectures that can be compared?
2. The definition of sequential order bias in Figure 4 and how it is calculated?"
fyl82vAale,fyl82vAale,Sequential Order-Robust Mamba for Time Series Forecasting,Reject,JdVLFOLE1c,ICLR.cc/2025/Conference/Submission69/Reviewer_eFUR,"This paper addresses a limitation in applying Mamba to time series forecasting: the sequential order bias when modelling channel dependencies, as channels typically lack inherent order. The authors propose SOR-Mamba, which introduces two main innovations: a regularization strategy that minimizes discrepancy between embedding vectors from reversed channel orders, and the removal of the unnecessary 1D-convolution layer. They also introduce Channel Correlation Modelling (CCM), a pretraining task that preserves channel correlations from data space to latent space. The method reported promising performance across different scenarios, including cases with missing data and varying channel orders, while maintaining computational efficiency in terms of memory and runtime.","The proposed method addresses the sequential order bias in channel dependencies (CD). This bias can degrade performance when channels lack inherent order in multi-channel time series data. The SOR-Mamba method mitigates this bias by introducing a regularization technique that minimizes the discrepancy between embeddings generated from reversed channel orders, thus improving robustness. 

Additionally, the paper proposes Channel Correlation Modeling (CCM) as a pretraining task, which preserves channel correlations from data to latent space, enhancing CD capture. The architecture modifications, including removing the 1D-convolution and focusing on unidirectional Mamba, can reduce computational overhead, achieving efficiency gains in memory and parameter usage compared to other models like S-Mamba. This should be of interests to the group of readers working on multi channel time series data forecasting.","The paper’s approach is relatively complex, involving several architectural changes (e.g., removing 1D-convolutions, applying specific regularization, and adding CCM pretraining) that may complicate replication and limit accessibility. 

While it emphasizes efficiency improvements, the paper could provide more detailed explanations regarding the trade-offs involved in removing the 1D-convolution, especially on datasets where channel order may have some inherent structure (e.g., traffic data). The study also lacks a comprehensive discussion of potential limitations in real-world deployment, such as sensitivity to hyperparameters (e.g., λ in regularization) and its impact across different datasets.","1. How does the choice of the distance metric d in the regularization term impact the model's performance across datasets with varying levels of channel correlation? - is the regularization term’s effectiveness dependent on the number of channels or dataset-specific factors etc.?

2. The paper uses CCM to preserve channel correlations in the latent space during pretraining. How does CCM compare to masked modelling and reconstruction pretraining across different dataset sizes and channel configurations? - and is there any limitation of CCM if there's only exist weak channel correlations?"
fyl82vAale,fyl82vAale,Sequential Order-Robust Mamba for Time Series Forecasting,Reject,Jc94nxQCsE,ICLR.cc/2025/Conference/Submission69/Reviewer_9Yaz,"This manuscript proposes an algorithm to regularize bidirectional Mamba channel vectors in order to solve the problem that there is no intrinsic order relationship between channels in time series data. The authors also design a new pre-training method (CCM) to help the model better establish the association between channels of multi-dimensional time series. Extensive experiments and ablation experiments are conducted, and the experimental results show the effectiveness of the relevant contributions.","1. This study achieves a good improvement in results with only a few changes to the method. The proposed method is simple, novel and effective.
2. The experiments carried out by the authors are very detailed and the relevant parameters are well discussed. This is an experimentally rigorous and solid study.
3. The manuscript is well-written and the figures are clear and intuitive.","1. The authors lack a complete description of the motivation for regularization. Why regularizing the bidirectional mamba channel vectors and minimizing the distance between them can help the model better capture the correlation between channels? This requires the author to give a more intuitive and reasonable motivation.
2. Although the authors have done a lot of experiments, they lack experiments on longer input lengths. Some widely used baselines in this field, such as PatchTST (arXiv preprint arXiv:2211.14730, 2022.), all use longer input sequences.
3. The authors use too many abbreviations in their manuscripts and experimental result tables, and lack clear and complete explanations of the abbreviations. This makes the relevant content very difficult to read and easily confusing.","1. Table I.1 seems to show that the hyperparameter lambda does not affect the results. Is this a reasonable phenomenon for the key contribution?
2. What are the principles and motivations for using parameter-sharing CD-Mamba block and removing 1D-conv? Can the authors provide additional details?
3. The description of the pre-training method used in this study is somewhat vague. Can the authors supplement the algorithm flow or provide detailed explanation?"
fyl82vAale,fyl82vAale,Sequential Order-Robust Mamba for Time Series Forecasting,Reject,sZWs0V7cvG,ICLR.cc/2025/Conference/Submission69/Reviewer_vPYA,"This paper introduce a new Mamba-based model for long term time series forecasting named SOR-Mamba which utilizes modified Mamba structure to capture channel dependencies. Furthermore, the authors also introduces channel correlation modeling as a pretraining task aimed at enhancing the ability to capture channel dependencies.","1. The proposed SOR-Mamba uses a modified Mamba structure (also with a regularization strategy) to eliminate the sequential order bias of Mamba which is more useful in capture channel dependencies.
2. The authors also introduce a pretraining task named channel correlation modeling (CCM) to preserve correlations between channels from the data space to the latent space in order to enhance the ability to capture CD.","1. Since Mamba is a sequential-based model, and the channels lake sequential order, why we need Mamba to extract channel dependencies? It is important to clarify the importance of capturing channel dependencies with Mamba.
2. The experimental comparisons appear unfair, as models like PatchTST exhibit better performance with longer lookback window. I strongly suggest the authors conduct additional experiments, such as setting $L=336$.","See Weaknesses above.
3. In line 369-370, I’m curious about how to calculate the global correlation."
fyl82vAale,fyl82vAale,Sequential Order-Robust Mamba for Time Series Forecasting,Reject,f3hYBM6BuW,ICLR.cc/2025/Conference/Submission69/Reviewer_Cgt6,"In this manuscript, the authors claim that channels in time series (TS) data have no specific order in general, recent studies have adopted
Mamba to capture channel dependencies (CD) in TS, introducing a sequential order bias. To address this issue, the authors propose SOR-Mamba, a TS forecasting method that 1) incorporates a regularization strategy to minimize the discrepancy between
two embedding vectors generated from data with reversed channel orders, thereby enhancing robustness to channel order.","1. The authors propose SOR-Mamba, a TS forecasting method that handles the sequential order bias by
1) regularizing the unidirectional Mamba to minimize the distance between two embedding vectors
generated from data with reversed channel orders for robustness to channel order.
2. The authors introduce CCM, a novel pretraining task that preserves the correlation between channels from
the data space to the latent space, thereby enhancing the model’s ability to capture CD.
3. With the exception of the power-related datasets (ETTm1 & m2, etc.), the experimental performance exceeds the current SOTA.","1. It is puzzling why the authors mention 'robust' but does not give experiments to resist noise or PGD attacks. Or it may be necessary to modify the terminology in the manuscript to be more precise, after all, 13 different domains of time series prediction tasks have been tried.
2. With the exception of the power-related datasets (ETTm1 & m2, etc.), the experimental performance exceeds the current SOTA. Why is the performance of the author's scheme inferior to SOTA (i.e., PatchTST) on power-related datasets (ETTm1 & m2, etc.) with more significant global regularity?
3. Despite the computational complexity comparison, the reviewer is concerned about the actual training and inference time.
4. The description in Figure 1 seems to be the difference between different exogenous time series due to sampling frequency and periodic change rule, and the description of sequence order is prone to misunderstanding (i.e., covariables have orders).",Please see weaknesses.
