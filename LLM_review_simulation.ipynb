{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff98653",
   "metadata": {},
   "source": [
    "# ICLR LLM Review Simulation\n",
    "\n",
    "This notebook simulates an ICLR-style paper review pipeline using Microsoft AutoGen and OpenAI `gpt-4o-mini`. Provide a PDF manuscript, configure reviewer personas and editor behavior, then run multi-round simulations to observe decision distributions (oral, spotlight, poster, reject).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1fb19",
   "metadata": {},
   "source": [
    "> **Setup**\n",
    "> 1. Install dependencies with the next cell.\n",
    "> 2. Set the `OPENAI_API_KEY` environment variable (or populate a local `.env`).  The notebook will refuse to run simulations without it.\n",
    "> 3. Provide an absolute path to the target PDF when prompted later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83466b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "# %pip install --quiet \"autogen-agentchat>=0.2\" pymupdf pandas matplotlib python-dotenv requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdeb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import statistics as stats\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import zlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError as exc:  # pragma: no cover\n",
    "    raise ImportError(\"PyMuPDF (fitz) is required. Run the pip install cell first.\") from exc\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    from autogen import AssistantAgent, UserProxyAgent\n",
    "except ImportError as exc:  # pragma: no cover\n",
    "    raise ImportError(\"Microsoft AutoGen is required. Run the pip install cell first.\") from exc\n",
    "\n",
    "load_dotenv()\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "DEFAULT_LLM_CONFIG = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "if \"gpt-5\" in DEFAULT_LLM_CONFIG[\"model\"]:\n",
    "    DEFAULT_LLM_CONFIG[\"max_completion_tokens\"] = 10_000\n",
    "else:\n",
    "    DEFAULT_LLM_CONFIG[\"max_tokens\"] = 10_000\n",
    "\n",
    "DECISION_LABELS = [\"oral\", \"spotlight\", \"poster\", \"reject\"]\n",
    "REMOTE_SCHEMES = {\"http\", \"https\"}\n",
    "\n",
    "\n",
    "def require_api_key() -> str:\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set. Export it or use a .env file.\")\n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0477473",
   "metadata": {},
   "source": [
    "# Exercise 1: Design a LLM-based Editor (i.e., the Area Chair)\n",
    "\n",
    "You can think of the editor as a function mapping from a set of reviews (from reviewers) to a final decision regarding the paper. There are four possible outcomes:\n",
    "\n",
    "- Accept (Oral)\n",
    "- Accept (Spotlight)\n",
    "- Accept (Poster)\n",
    "- Reject\n",
    "\n",
    "We have gathered 100 paper submtted to the ICLR 2025 conference together with their human reviews and decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1526fc",
   "metadata": {},
   "source": [
    "## Load Reviews and Decisions for 100 ICLR 2025 Paper Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5037084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ICLR_2025 = pd.read_csv(\"./data/ICLR2025/classroom_ICLR2025_human_reviews_with_decision_selected_100.csv\")\n",
    "df_ICLR_2025[\"review\"] = \"Summary:\\n\" + df_ICLR_2025[\"summary\"].fillna(\"None\") + \"\\n\\nStrengths:\\n\" + df_ICLR_2025[\"strengths\"].fillna(\"None\") + \"\\n\\nWeaknesses:\\n\" + df_ICLR_2025[\"weaknesses\"].fillna(\"None\") + \"\\n\\nQuestions:\\n\" + df_ICLR_2025[\"questions\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879236d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ICLR_2025.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ICLR_2025[\"paper_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffaeb0",
   "metadata": {},
   "source": [
    "## Distribution of Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_counts = df_ICLR_2025[\"decision\"].value_counts()\n",
    "colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']  # green, blue, orange, red\n",
    "labels_with_counts = [f'{label}\\n(n={count})' for label, count in zip(decision_counts.index, decision_counts.values)]\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=300)\n",
    "plt.pie(decision_counts, labels=labels_with_counts, autopct='%1.1f%%', startangle=90,\n",
    "        colors=colors, explode=[0.02]*len(decision_counts), shadow=False,\n",
    "        textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "plt.title(\"Distribution of Paper Decisions\", fontsize=16, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7b02b",
   "metadata": {},
   "source": [
    "## A typcial review looks like the following: it includes a summary, lists of strengths, weaknesses, and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e860a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ICLR_2025[\"review\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee028632",
   "metadata": {},
   "source": [
    "## Distribution of number of reviews: each paper receives around 4 reviews, in our simplified simulation, the editor will make a decision based on these four reviews.\n",
    "\n",
    "### Note: in the real ICLR review process, each paper receives around 4 reviews, and there is a \"rebuttal\" round after the first round of reviews (i.e., the authors write a response to the reviews, and the reviewers write a response to the rebuttal). In our simulation, we simplify this process by directly using the initial reviews to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of reviews per paper\n",
    "reviews_per_paper = df_ICLR_2025.groupby(\"paper_id\").size()\n",
    "\n",
    "# Calculate statistics\n",
    "mean_reviews = reviews_per_paper.mean()\n",
    "median_reviews = reviews_per_paper.median()\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.hist(reviews_per_paper, bins=range(1, reviews_per_paper.max() + 2),\n",
    "         edgecolor='black', alpha=0.7, color='#3498db', align='left')\n",
    "plt.axvline(mean_reviews, color='#e74c3c', linestyle='--', linewidth=2, label=f'Mean: {mean_reviews:.2f}')\n",
    "plt.axvline(median_reviews, color='#2ecc71', linestyle='--', linewidth=2, label=f'Median: {median_reviews:.2f}')\n",
    "plt.xlabel(\"Number of Reviews per Paper\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Number of Papers\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"Distribution of Number of Reviews per Paper\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(1, reviews_per_paper.max() + 1))\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean number of reviews per paper: {mean_reviews:.2f}\")\n",
    "print(f\"Median number of reviews per paper: {median_reviews:.2f}\")\n",
    "print(f\"Total papers: {len(reviews_per_paper)}\")\n",
    "print(f\"Total reviews: {len(df_ICLR_2025)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c3c17",
   "metadata": {},
   "source": [
    "### Note: we do not know the identity of the reviewers (but we do for ICLR 2026... unethical to use that data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECISION_LABELS = pd.unique(df_ICLR_2025[\"decision\"]).tolist()\n",
    "print(f\"Decision labels: {DECISION_LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9868cb",
   "metadata": {},
   "source": [
    "# Design the AI Editor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b669fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDITOR_PROMPT = \"\"\"You are serving as the Area Chair for an ICLR submission.\n",
    "You will receive the manuscript text plus a list of reviews (some human-written, some LLM-generated).\n",
    "\n",
    "Your responsibilities:\n",
    "\n",
    "1. Analyze all reviews:\n",
    "   - Identify clear points of consensus across reviewers.\n",
    "   - Identify and explain disagreements, including which reviewers disagree and why.\n",
    "   - Flag any reviewer misunderstandings, unsupported claims, or inconsistencies.\n",
    "\n",
    "2. Conduct an independent Area Chair assessment:\n",
    "   - Evaluate novelty, technical correctness, significance, empirical rigor, and clarity.\n",
    "   - Weigh reviewer feedback appropriately, but do not rely on it blindly.\n",
    "   - Distinguish between major and minor concerns.\n",
    "   - Note any methodological flaws, missing experiments, ethical issues, or correctness problems that materially affect acceptance.\n",
    "\n",
    "3. Make and justify the final decision (one of {decisions}):\n",
    "   - Base the decision on reviewer consensus, resolved disagreements, and your own AC-level judgment.\n",
    "   - Ensure the justification reflects ICLR standards: correctness > novelty > significance > clarity.\n",
    "   - The justification must be factual, grounded in the manuscript + reviews, and internally consistent.\n",
    "\n",
    "4. Return a strict JSON object with no code fences:\n",
    "{{\n",
    "  \"decision\": one of {decisions},\n",
    "  \"confidence\": float between 0 and 1,\n",
    "  \"rationale\": \"A concise paragraph explaining the decision, referencing consensus, disagreements, and your independent assessment.\",\n",
    "  \"highlights\": [\n",
    "    \"Key factor influencing the decision\",\n",
    "    \"Another key factor\",\n",
    "    \"Major consensus or disagreement point\",\n",
    "    \"Any important caveat or uncertainty\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\".format(decisions=DECISION_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LLM_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llm_config(overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    config: Dict[str, Any] = {**DEFAULT_LLM_CONFIG}\n",
    "    if overrides:\n",
    "        config.update(overrides)\n",
    "    config_list = [\n",
    "        {\n",
    "            \"model\": config[\"model\"],\n",
    "            \"api_key\": require_api_key(),\n",
    "        }\n",
    "    ]\n",
    "    config[\"config_list\"] = config_list\n",
    "    return config\n",
    "\n",
    "def make_editor_agent(overrides: Optional[Dict[str, Any]] = None) -> AssistantAgent:\n",
    "    llm_config = build_llm_config(overrides)\n",
    "    return AssistantAgent(\n",
    "        name=\"area_chair\",\n",
    "        system_message=EDITOR_PROMPT,\n",
    "        llm_config=llm_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b2434",
   "metadata": {},
   "source": [
    "## Try this editor agent on a few papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PAPERS = 5\n",
    "simulated_decisions = []\n",
    "selected_list = [\"d5HUnyByAI\", \"Q2bJ2qgcP1\", \"X9OfMNNepI\", \"hjROBHstZ3\", \"xXTkbTBmqq\", \"BpfsxFqhGa\", \"VsxbWTDHjh\", \"d8hYXbxX71\", \"dTPz4rEDok\", \"awvJBtB2op\"]\n",
    "other_paper_ids = list(set(df_ICLR_2025[\"paper_id\"].tolist()) - set(selected_list))\n",
    "\n",
    "sampled_paper_ids = list(np.random.choice(selected_list, size=NUM_PAPERS // 2, replace=False)) + list(np.random.choice(other_paper_ids, size=NUM_PAPERS - NUM_PAPERS // 2, replace=False))\n",
    "\n",
    "for paper_id in tqdm(sampled_paper_ids, desc=\"Simulating decisions\"):\n",
    "    editor_agent = make_editor_agent()\n",
    "    paper_data = df_ICLR_2025[df_ICLR_2025['paper_id'] == paper_id]\n",
    "    actual_decision = paper_data['decision'].iloc[0]\n",
    "    reviews = paper_data['review'].tolist()\n",
    "\n",
    "    # Build the input message for the editor agent\n",
    "    reviews_text = \"\\n\\n---\\n\\n\".join([f\"Review {i+1}:\\n{r}\" for i, r in enumerate(reviews)])\n",
    "    input_message = f\"Please make a decision for this paper based on the following reviews:\\n\\n{reviews_text}\"\n",
    "\n",
    "    # Get simulated decision from the editor agent\n",
    "    response = editor_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": input_message}])\n",
    "\n",
    "    # Record the result\n",
    "    result = {\n",
    "        \"paper_id\": paper_id,\n",
    "        \"actual_decision\": actual_decision,\n",
    "        \"number_of_reviews\": len(reviews),\n",
    "        \"simulated_response\": response\n",
    "    }\n",
    "    simulated_decisions.append(result)\n",
    "\n",
    "df_simulated = pd.DataFrame(simulated_decisions)\n",
    "df_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a few reasoning examples from the simulated responses\n",
    "for i, row in df_simulated.head(5).iterrows():\n",
    "    print(f\"[{i}] Paper ID: {row['paper_id']}\")\n",
    "    print(f\"Actual Decision: {row['actual_decision']}\")\n",
    "\n",
    "    # Parse the decision and confidence from the simulated_response\n",
    "    try:\n",
    "        response_json = json.loads(row['simulated_response'])\n",
    "        simulated_decision = response_json.get('decision', 'N/A')\n",
    "        simulated_confidence = response_json.get('confidence', 'N/A')\n",
    "        print(f\"Simulated Decision: {simulated_decision}\")\n",
    "        print(f\"Confidence: {simulated_confidence}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Display the reasoning if available\n",
    "        if 'rationale' in response_json:\n",
    "            print(f\"Rationale:\\n{response_json['rationale']}\")\n",
    "        if 'highlights' in response_json:\n",
    "            print(f\"Highlights:\")\n",
    "            for highlight in response_json['highlights']:\n",
    "                print(f\"  - {highlight}\")\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print(f\"Simulated Decision: N/A (parse error)\")\n",
    "        print(f\"Confidence: N/A (parse error)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Raw Response:\\n{row['simulated_response']}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c664b",
   "metadata": {},
   "source": [
    "### Limitation: the editor agent here is only making decision based on the first round reviews, and it does not have access to the paper text yet (due to computational cost to parse these PDFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875310e0",
   "metadata": {},
   "source": [
    "### We will now load one paper with the full text, which was parsed from the PDF file. We will be focusing on how comments from different reviews will shape the (LLM) editor's decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7d833",
   "metadata": {},
   "source": [
    "# Load Pre-parsed Paper Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6027db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "from IPython.display import display\n",
    "import ipywidgets as W\n",
    "\n",
    "LOAD_FROM_GITHUB = True  # if you are running on colab, set this to True so that the notebook fetches the paper from Github directly.\n",
    "GITHUB_BASE_URL = \"https://raw.githubusercontent.com/TianyuDu/OB639-LLM-ABM-Simulation/refs/heads/main/pdfs_parsed/\"\n",
    "LOCAL_PATH = \"./pdf_parsed\"\n",
    "\n",
    "# GitHub index of available papers (paper_id, file_name)\n",
    "papers_parsed = [\n",
    "    # (\"d5HUnyByAI\", \"BIO-5177_CLIBD_Bridging_Vision_and\"),\n",
    "    (\"Q2bJ2qgcP1\", \"CAUSALITY-11132_Do_Contemporary_Causal_I\"),\n",
    "    # (\"X9OfMNNepI\", \"CHEM-4321_MOOSE_Chem_Large_Language\"),\n",
    "    # (\"hjROBHstZ3\", \"HEALTH-11612_Causal_Representation_Le\"),\n",
    "    # (\"xXTkbTBmqq\", \"LLMs-211_OLMoE_Open_Mixture_of_Expe\"),\n",
    "    # (\"BpfsxFqhGa\", \"NEURO-3520_Animate_Your_Thoughts_Rec\"),\n",
    "    # (\"VsxbWTDHjh\", \"PHYSICS-3880_Fengbo_a_Clifford_Neural_\"),\n",
    "    # (\"d8hYXbxX71\", \"POLICY-11925_Policy_Design_in_Long_ru\"),\n",
    "    # (\"dTPz4rEDok\", \"RL-10175_Offline_Hierarchical_Rei\"),\n",
    "    # (\"awvJBtB2op\", \"ROBOTICS-11292_Generating_Freeform_Endo\"),\n",
    "]\n",
    "\n",
    "# choose one paper to load, you can change the paper_parsed to load different papers.\n",
    "PAPER_ID, PAPER_FILE_NAME = random.choice(papers_parsed)\n",
    "PAPER_PATH = f\"{PAPER_FILE_NAME}/{PAPER_FILE_NAME}.tei.xml\"\n",
    "\n",
    "if LOAD_FROM_GITHUB:\n",
    "    PAPER_PATH = f\"{GITHUB_BASE_URL}{PAPER_PATH}\"\n",
    "else:\n",
    "    PAPER_PATH = f\"{LOCAL_PATH}/{PAPER_PATH}\"\n",
    "\n",
    "print(f\"Loading paper from: {PAPER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "if LOAD_FROM_GITHUB:\n",
    "    response = requests.get(PAPER_PATH)\n",
    "    response.raise_for_status()\n",
    "    TEI_XML_CONTENT = response.text\n",
    "else:\n",
    "    with open(PAPER_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        TEI_XML_CONTENT = f.read()\n",
    "\n",
    "print(f\"Loaded paper content: {len(TEI_XML_CONTENT)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e34a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def extract_text_from_tei(tei_content: str) -> str:\n",
    "    \"\"\"Extract paper text from TEI XML content.\"\"\"\n",
    "    if not tei_content:\n",
    "        return None\n",
    "\n",
    "    root = ET.fromstring(tei_content)\n",
    "\n",
    "    # Define namespaces\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    text_parts = []\n",
    "\n",
    "    # Extract title\n",
    "    title = root.find('.//tei:titleStmt/tei:title[@level=\"a\"][@type=\"main\"]', ns)\n",
    "    if title is not None and title.text:\n",
    "        text_parts.append(f\"Title: {title.text}\\n\")\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract = root.find('.//tei:abstract', ns)\n",
    "    if abstract is not None:\n",
    "        abstract_text = ' '.join(abstract.itertext())\n",
    "        text_parts.append(f\"Abstract: {abstract_text}\\n\\n\")\n",
    "\n",
    "    # Extract body text\n",
    "    body = root.find('.//tei:body', ns)\n",
    "    if body is not None:\n",
    "        for div in body.findall('.//tei:div', ns):\n",
    "            # Get section heading\n",
    "            head = div.find('tei:head', ns)\n",
    "            if head is not None and head.text:\n",
    "                text_parts.append(f\"\\n{head.text}\\n\")\n",
    "\n",
    "            # Get all paragraphs\n",
    "            for p in div.findall('.//tei:p', ns):\n",
    "                p_text = ' '.join(p.itertext())\n",
    "                # Clean up extra whitespace\n",
    "                p_text = ' '.join(p_text.split())\n",
    "                if p_text:\n",
    "                    text_parts.append(f\"{p_text}\\n\\n\")\n",
    "\n",
    "    return ''.join(text_parts)\n",
    "\n",
    "PAPER_TEXT = extract_text_from_tei(TEI_XML_CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_summary(text: str) -> str:\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentence_count = len([s for s in sentences if s.strip()])\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    estimated_pages = word_count / 500\n",
    "    return f\"{sentence_count:,} sentences, {word_count:,} words, ~{estimated_pages:.1f} pages (double-spaced)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179585ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first 1000 characters of the paper text\n",
    "print(f\"{PAPER_ID=:}\\n{PAPER_TEXT[:1000]=:}\")\n",
    "\n",
    "# summarize sentence and word counts\n",
    "print(f\"\\nPaper text summary: {text_summary(PAPER_TEXT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17bf7b",
   "metadata": {},
   "source": [
    "## Load human reviews and decisions for this paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_REVIEWS = df_ICLR_2025[df_ICLR_2025[\"paper_id\"] == PAPER_ID][\"review\"].tolist()\n",
    "ACTUAL_DECISION = df_ICLR_2025[df_ICLR_2025[\"paper_id\"] == PAPER_ID][\"decision\"].iloc[0]\n",
    "print(f\"Loaded {len(HUMAN_REVIEWS)} human reviews for paper {PAPER_ID}\")\n",
    "print(\"\\nHuman review summaries:\")\n",
    "for i, review in enumerate(HUMAN_REVIEWS, 1):\n",
    "    print(f\"  Review {i}: {text_summary(review)}\")\n",
    "print(f\"\\nActual decision (human editor on human reviewers): {ACTUAL_DECISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfa34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PAPER_DOMAIN = \"Machine Learning\"\n",
    "\n",
    "PAPER_FILE_NAME_TO_DOMAIN = {\n",
    "    \"BIO\": \"Biology\",\n",
    "    \"CAUSALITY\": \"Causality\",\n",
    "    \"CHEM\": \"Chemistry\",\n",
    "    \"CV\": \"Computer Vision\",\n",
    "    \"DL\": \"Deep Learning\",\n",
    "    \"GNN\": \"Graph Neural Networks\",\n",
    "    \"HEALTH\": \"Health\",\n",
    "    \"LLMs\": \"Large Language Models\",\n",
    "    \"ML\": \"Machine Learning\",\n",
    "    \"NEURO\": \"Neuroscience\",\n",
    "    \"NLP\": \"Natural Language Processing\",\n",
    "    \"OPT\": \"Optimization\",\n",
    "    \"PHYSICS\": \"Physics\",\n",
    "    \"POLICY\": \"Policy\",\n",
    "    \"ACCEL\": \"Accelerated Computing\",\n",
    "    \"RL\": \"Reinforcement Learning\",\n",
    "    \"ROBOTICS\": \"Robotics\",\n",
    "    \"SOFT\": \"Software Engineering\",\n",
    "    \"SP\": \"Signal Processing\",\n",
    "    \"SYST\": \"Systems\",\n",
    "    \"ACOU\": \"Acoustics\",\n",
    "    \"AGI\": \"Artificial General Intelligence\",\n",
    "    \"ALIGN\": \"AI Alignment\",\n",
    "    \"AUDIO\": \"Audio Processing\",\n",
    "    \"AUTO\": \"Autonomous Systems\",\n",
    "    \"BAYES\": \"Bayesian Methods\",\n",
    "    \"BENCH\": \"Benchmarking\",\n",
    "    \"CLIP\": \"Vision-Language Models\",\n",
    "    \"CONT\": \"Continual Learning\",\n",
    "    \"DATA\": \"Data Science\",\n",
    "    \"DIFF\": \"Diffusion Models\",\n",
    "    \"DIST\": \"Distributed Systems\",\n",
    "    \"ECON\": \"Economics\",\n",
    "    \"ENERGY\": \"Energy\",\n",
    "    \"FAIR\": \"Fairness and Bias\",\n",
    "    \"FED\": \"Federated Learning\",\n",
    "    \"FM\": \"Foundation Models\",\n",
    "    \"GEN\": \"Generative Models\",\n",
    "    \"GEOM\": \"Geometric Deep Learning\",\n",
    "    \"INTERP\": \"Interpretability\",\n",
    "    \"KG\": \"Knowledge Graphs\",\n",
    "    \"META\": \"Meta-Learning\",\n",
    "    \"MULTI\": \"Multimodal Learning\",\n",
    "    \"NAS\": \"Neural Architecture Search\",\n",
    "    \"PINN\": \"Physics-Informed Neural Networks\",\n",
    "    \"PRIV\": \"Privacy\",\n",
    "    \"PROB\": \"Probabilistic Methods\",\n",
    "    \"QUANT\": \"Quantization\",\n",
    "    \"REC\": \"Recommender Systems\",\n",
    "    \"REP\": \"Representation Learning\",\n",
    "    \"SAFE\": \"AI Safety\",\n",
    "    \"SCALE\": \"Scalability\",\n",
    "    \"SEC\": \"Security\",\n",
    "    \"SELF\": \"Self-Supervised Learning\",\n",
    "    \"SIM\": \"Simulation\",\n",
    "    \"SPEECH\": \"Speech Processing\",\n",
    "    \"STAT\": \"Statistics\",\n",
    "    \"THEO\": \"Theory\",\n",
    "    \"TIME\": \"Time Series\",\n",
    "    \"TRANS\": \"Transformers\",\n",
    "    \"TRUST\": \"Trustworthy AI\",\n",
    "    \"VID\": \"Video Understanding\",\n",
    "}\n",
    "\n",
    "PAPER_DOMAIN = PAPER_FILE_NAME_TO_DOMAIN.get(PAPER_FILE_NAME.split(\"-\")[0], DEFAULT_PAPER_DOMAIN)\n",
    "print(\"The domain of this paper is:\", PAPER_DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2748bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_PAPER_DOMAIN = \"Machine Learning\"\n",
    "\n",
    "\n",
    "# def parse_paper_domain(\n",
    "#     paper_id: Optional[str] = None,\n",
    "#     selection: Optional[Union[str, Tuple[str, str]]] = None,\n",
    "# ) -> str:\n",
    "#     \"\"\"Infer a coarse domain label from the selected paper slug.\"\"\"\n",
    "\n",
    "#     selection = selection if selection is not None else SELECTED_PAPER_DIR\n",
    "#     slug = None\n",
    "#     if isinstance(selection, (tuple, list)) and selection:\n",
    "#         slug = selection[-1]\n",
    "#     elif selection:\n",
    "#         slug = str(selection)\n",
    "\n",
    "#     token = None\n",
    "#     if slug:\n",
    "#         slug_name = Path(slug).name\n",
    "#         slug_no_ext = slug_name.split(\".\")[0]\n",
    "#         token = slug_no_ext.split(\"_\")[0]\n",
    "#         if token:\n",
    "#             token = token.split(\"-\")[0]\n",
    "\n",
    "#     candidate = token or (paper_id or \"\").strip()\n",
    "#     if not candidate or candidate.isnumeric():\n",
    "#         return DEFAULT_PAPER_DOMAIN\n",
    "\n",
    "#     cleaned = candidate.replace(\"-\", \" \").replace(\"_\", \" \").strip()\n",
    "#     return cleaned.title() if cleaned else DEFAULT_PAPER_DOMAIN\n",
    "\n",
    "\n",
    "# def human_reviews_to_editor_json(\n",
    "#     reviews: Sequence[str],\n",
    "#     domain: str,\n",
    "#     seniority_label: str = \"human\",\n",
    "# ) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"Wrap raw human review strings into the schema expected by the editor agent.\"\"\"\n",
    "\n",
    "#     payload: List[Dict[str, Any]] = []\n",
    "#     for idx, text in enumerate(reviews, start=1):\n",
    "#         payload.append(\n",
    "#             {\n",
    "#                 \"persona\": {\n",
    "#                     \"name\": f\"human_{idx}\",\n",
    "#                     \"expertise\": domain,\n",
    "#                     \"seniority\": seniority_label,\n",
    "#                 },\n",
    "#                 \"review\": {\n",
    "#                     \"summary\": [text],\n",
    "#                     \"strengths\": [],\n",
    "#                     \"weaknesses\": [],\n",
    "#                     \"questions\": [],\n",
    "#                     \"rating\": 0,\n",
    "#                     \"confidence\": 0,\n",
    "#                 },\n",
    "#             }\n",
    "#         )\n",
    "#     return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106821dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAPER_DOMAIN = parse_paper_domain(PAPER_ID, SELECTED_PAPER_DIR)\n",
    "# HUMAN_REVIEWS_FOR_EDITOR = human_reviews_to_editor_json(HUMAN_REVIEWS, PAPER_DOMAIN)\n",
    "\n",
    "# print(f\"Paper domain guess: {PAPER_DOMAIN}\")\n",
    "# print(f\"Prepared {len(HUMAN_REVIEWS_FOR_EDITOR)} human reviews for the editor agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_REVIEWS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ICLR_REVIEWS_DF is not None:\n",
    "#     ICLR_REVIEWS_DF[\"decision\"].unique()\n",
    "# else:\n",
    "#     sorted({entry[\"decision\"] for entry in EMBEDDED_HUMAN_DATA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICLR_REVIEWS_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733e2b3",
   "metadata": {},
   "source": [
    "# Review Summarization Helper Agent\n",
    "Here we define a LLM-bsaed helper function `aspects_of_comments` that takes a review comment and has the LLM score how much the comment talks about six aspects (accuracy, clarity, consistency, novelty, replicability, thoroughness). Optionally, you can call it for multiple rounds on the same comment and average the results, so the scores are “smoothed out” instead of depending on a single model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client_summarizer = OpenAI()\n",
    "\n",
    "def _aspects_of_comments(comment: str, model: str = DEFAULT_LLM_CONFIG[\"model\"]) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze a review comment and score how much it invokes each epistemic\n",
    "    aspect/value of the paper (following Smith et al., 2025):\n",
    "\n",
    "    - accuracy   : truth / correctness / soundness of claims & methods\n",
    "    - clarity    : writing, structure, and intelligibility (a proxy for simplicity)\n",
    "    - consistency: internal coherence and fair comparison to prior work\n",
    "    - novelty    : originality + impact/scope of contribution\n",
    "    - replicability: reproducibility, data/code availability, and procedural detail\n",
    "    - thoroughness: completeness of evidence, analyses, controls, and literature\n",
    "\n",
    "    Returns integer scores from 1 (not mentioned) to 5 (major focus).\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert at analyzing academic paper reviews and identifying the\n",
    "epistemic values that reviewers invoke in their comments.\n",
    "\n",
    "Your task is to evaluate how much a given review comment addresses each of the\n",
    "following six epistemic aspects of a paper. You are coding the *content of the\n",
    "comment*, not judging the paper itself.\n",
    "\n",
    "Treat praise and criticism symmetrically: a strong critique of clarity and a\n",
    "strong compliment on clarity are both \"about clarity.\" You are measuring how\n",
    "central each aspect is in the comment, not whether it is positive or negative.\n",
    "\n",
    "Use these definitions (adapted from work on epistemic values in scientific\n",
    "theory choice and peer review):\n",
    "\n",
    "1. Accuracy\n",
    "   - What it is: Truth, correctness, and soundness of the paper’s claims,\n",
    "     methods, analyses, and conclusions.\n",
    "   - Look for: Comments about validity of methods or identification, bias,\n",
    "     confounding, missing controls, flawed experimental design, incorrect\n",
    "     statistics, overclaiming, weak or strong linkage between evidence and\n",
    "     conclusions, robustness of inferences.\n",
    "\n",
    "2. Clarity\n",
    "   - What it is: How clearly the paper communicates its ideas in writing and\n",
    "     presentation (a practical proxy for simplicity).\n",
    "   - Look for: Readability, organization, structure of sections, figure and\n",
    "     table clarity, confusing notation, heavy or undefined jargon, ambiguous\n",
    "     wording, whether the main contribution is easy or hard to understand.\n",
    "   - Includes: Remarks like \"well written,\" \"hard to follow,\" \"unclear,\"\n",
    "     \"the exposition needs work,\" or detailed suggestions on organization.\n",
    "\n",
    "3. Consistency\n",
    "   - What it is: Internal coherence and alignment within the paper, and fairness\n",
    "     and adequacy of comparison to prior work.\n",
    "   - Look for: Logical flow from theory to methods to results to conclusions;\n",
    "     contradictions between sections; inconsistent definitions or measures;\n",
    "     mismatch between stated goals and what is actually done; missing or unfair\n",
    "     comparisons to baselines or previous studies; improper use of “standard”\n",
    "     paradigms or benchmarks.\n",
    "\n",
    "4. Novelty\n",
    "   - What it is: Originality of the contribution plus its impact/scope relative\n",
    "     to existing work (combining “fruitfulness” and “scope”).\n",
    "   - Look for: Comments on whether the work is new, incremental, derivative,\n",
    "     or groundbreaking; whether it opens new questions, methods, or datasets;\n",
    "     significance/importance of the problem; fit with the venue; likelihood\n",
    "     that others will build on the ideas.\n",
    "   - Includes: Phrases like \"incremental,\" \"original,\" \"innovative,\" \"adds\n",
    "     little beyond X,\" \"important contribution,\" \"major advance.\"\n",
    "\n",
    "5. Replicability\n",
    "   - What it is: How easy it would be for others to reproduce the study or\n",
    "     re-run the analysis given the information and resources provided.\n",
    "   - Look for: Data and code availability; sufficiency of methodological detail\n",
    "     (preprocessing, model specification, parameter settings, instruments,\n",
    "     protocols); randomization, sample size justification; whether procedures\n",
    "     are described clearly enough to reproduce results.\n",
    "   - Includes: Critiques like \"not enough implementation detail,\" \"no code/data\n",
    "     provided,\" \"methods section too thin,\" as well as explicit praise for\n",
    "     detailed, reproducible setups.\n",
    "\n",
    "6. Thoroughness\n",
    "   - What it is: Completeness and sufficiency of the empirical and conceptual\n",
    "     work supporting the paper’s claims.\n",
    "   - Look for: Scope and coverage of experiments or analyses; missing robustness\n",
    "     checks or ablations; lack of relevant baselines or controls; narrow data\n",
    "     that don’t match the ambition of the claims; superficial or incomplete\n",
    "     engagement with related work; calls for “more experiments,” “more\n",
    "     analysis,” or “stronger evidence.”\n",
    "   - Includes: Comments that additional tests, datasets, conditions, or\n",
    "     literature are needed to fully support the claims.\n",
    "\n",
    "SCORING RULES (APPLY TO EACH ASPECT INDEPENDENTLY)\n",
    "- 1: Not mentioned at all (no clear signal for this aspect).\n",
    "- 2: Briefly touched upon (a passing remark or short phrase).\n",
    "- 3: Moderately discussed (one or two sentences with some detail).\n",
    "- 4: Substantially addressed (a major part of the comment, with explanation).\n",
    "- 5: Major focus of the comment (dominant theme; most of the text concerns it).\n",
    "\n",
    "Important:\n",
    "- Multiple aspects can have high scores if the comment weaves together several\n",
    "  values (e.g., novelty and accuracy).\n",
    "- Do NOT let your own view of the paper influence scores; rely only on the\n",
    "  reviewer's text.\n",
    "- Treat positive and negative statements equally as evidence that the aspect\n",
    "  is being addressed.\n",
    "\n",
    "You MUST respond with ONLY a valid JSON object in this exact format:\n",
    "{\"accuracy\": <score>, \"clarity\": <score>, \"consistency\": <score>, \"novelty\": <score>, \"replicability\": <score>, \"thoroughness\": <score>}\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Please analyze the following review comment and score each aspect from 1-5,\n",
    "based solely on how much the *comment* addresses that aspect:\n",
    "\n",
    "=== REVIEW COMMENT ===\n",
    "{comment}\n",
    "\n",
    "Respond with only the JSON object containing integer scores for each aspect.\n",
    "\"\"\"\n",
    "\n",
    "    response = client_summarizer.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    return result\n",
    "\n",
    "def aspects_of_comments(comment: str, model: str = DEFAULT_LLM_CONFIG[\"model\"], num_trials: int = 1, normalize_scores: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Call _aspects_of_comments multiple times and return average scores.\n",
    "\n",
    "    Args:\n",
    "        comment: The review comment to analyze\n",
    "        model: The model to use for analysis\n",
    "        num_trials: Number of times to run the analysis\n",
    "\n",
    "    Returns:\n",
    "        dict with average scores for each aspect\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for _ in range(num_trials):\n",
    "        result = _aspects_of_comments(comment, model)\n",
    "        all_results.append(result)\n",
    "\n",
    "    # Calculate average scores for each aspect\n",
    "    avg_scores = {}\n",
    "    aspects = [\"accuracy\", \"clarity\", \"consistency\", \"novelty\", \"replicability\", \"thoroughness\"]\n",
    "    for aspect in aspects:\n",
    "        avg_scores[aspect] = sum(r[aspect] for r in all_results) / num_trials\n",
    "\n",
    "    if normalize_scores:\n",
    "        total = sum(avg_scores.values())\n",
    "        if total > 0:\n",
    "            avg_scores = {k: v / total for k, v in avg_scores.items()}\n",
    "    return avg_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "ASPECT_COLUMNS = [\"accuracy\", \"clarity\", \"consistency\", \"novelty\", \"replicability\", \"thoroughness\"]\n",
    "\n",
    "\n",
    "def flatten_review_text(record: Union[Dict[str, Any], str]) -> str:\n",
    "    if isinstance(record, str):\n",
    "        return record\n",
    "    block = (record or {}).get(\"review\", {})\n",
    "    parts: List[str] = []\n",
    "    for key in (\"summary\", \"strengths\", \"weaknesses\", \"questions\"):\n",
    "        value = block.get(key, [])\n",
    "        if isinstance(value, str):\n",
    "            parts.append(value)\n",
    "        elif isinstance(value, list):\n",
    "            parts.extend([str(item) for item in value if item])\n",
    "    text = \"\\n\".join(part for part in parts if part).strip()\n",
    "    return text or \"\"\n",
    "\n",
    "\n",
    "def score_review_records(\n",
    "    records: List[Union[Dict[str, Any], str]],\n",
    "    source_label: str,\n",
    "    num_trials: int = 1,\n",
    "    model: str = DEFAULT_LLM_CONFIG[\"model\"],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    scored: List[Dict[str, Any]] = []\n",
    "    if not records:\n",
    "        return scored\n",
    "    for idx, record in enumerate(records, start=1):\n",
    "        if isinstance(record, str):\n",
    "            persona = {}\n",
    "        else:\n",
    "            persona = record.get(\"persona\", {})\n",
    "        review_text = flatten_review_text(record)\n",
    "        if not review_text:\n",
    "            continue\n",
    "        scores = aspects_of_comments(review_text, model=model, num_trials=num_trials)\n",
    "        scored.append(\n",
    "            {\n",
    "                \"source\": source_label,\n",
    "                \"name\": persona.get(\"name\", f\"{source_label}_{idx}\") if persona else f\"{source_label}_{idx}\",\n",
    "                \"domain\": persona.get(\"expertise\", PAPER_DOMAIN) if persona else PAPER_DOMAIN,\n",
    "                \"seniority\": persona.get(\"seniority\", \"unknown\") if persona else \"unknown\",\n",
    "                **scores,\n",
    "            }\n",
    "        )\n",
    "    return scored\n",
    "\n",
    "\n",
    "def compute_aspect_scores(num_trials: int = 1) -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    rows.extend(score_review_records(HUMAN_REVIEWS, \"human\", num_trials=num_trials))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "print(\"Scoring aspects for human reviews …\")\n",
    "ASPECT_SCORES_DF = compute_aspect_scores(num_trials=1)\n",
    "print(ASPECT_SCORES_DF)\n",
    "\n",
    "# Draw histogram of aspect scores\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'figure.dpi': 150,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# Color palette for consistent styling\n",
    "COLORS = ['#4C72B0', '#55A868', '#C44E52', '#8172B3', '#CCB974', '#64B5CD']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, aspect in enumerate(ASPECT_COLUMNS):\n",
    "    ax = axes[idx]\n",
    "    scores = ASPECT_SCORES_DF[aspect].values\n",
    "\n",
    "    # Create histogram with improved styling\n",
    "    n, bins, patches = ax.hist(\n",
    "        scores,\n",
    "        bins=np.arange(0.5, 6.5, 1),\n",
    "        edgecolor='white',\n",
    "        linewidth=1.2,\n",
    "        color=COLORS[idx],\n",
    "        alpha=0.85\n",
    "    )\n",
    "\n",
    "    # Add mean line\n",
    "    mean_val = np.mean(scores)\n",
    "    ax.axvline(mean_val, color='#333333', linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "    ax.text(mean_val + 0.1, ax.get_ylim()[1] * 0.9, f'μ={mean_val:.2f}',\n",
    "            fontsize=9, color='#333333', fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('Score', fontweight='medium')\n",
    "    ax.set_ylabel('Count', fontweight='medium')\n",
    "    ax.set_title(f'{aspect.title()}', fontweight='bold', pad=10)\n",
    "    # ax.set_xticks([1, 2, 3, 4, 5])\n",
    "    # ax.set_xlim(0.5, 5.5)\n",
    "\n",
    "    # Add subtle grid\n",
    "    ax.yaxis.grid(True, linestyle='-', alpha=0.3)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "plt.suptitle('Distribution of Aspect Scores for Human Reviews', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fe227",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_SCORES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c650652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 150,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "if ASPECT_SCORES_DF.empty:\n",
    "    raise RuntimeError(\"Run the aspect scoring cell first to populate ASPECT_SCORES_DF.\")\n",
    "\n",
    "source_avg = ASPECT_SCORES_DF.groupby(\"source\")[ASPECT_COLUMNS].mean()\n",
    "print(\"Average aspect focus by reviewer source:\")\n",
    "display(source_avg)\n",
    "\n",
    "# Comparison plot: human vs AI\n",
    "x = np.arange(len(ASPECT_COLUMNS))\n",
    "bar_width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for idx, (source, row) in enumerate(source_avg.iterrows()):\n",
    "    offset = (idx - (len(source_avg) - 1) / 2) * bar_width\n",
    "    ax.bar(x + offset, row.values, bar_width, label=source.title())\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([label.title() for label in ASPECT_COLUMNS], rotation=30, ha='right')\n",
    "ax.set_ylabel('Average score (1-5)')\n",
    "ax.set_title('Aspect focus: human vs AI reviewers')\n",
    "# ax.set_ylim(0, 5)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ai_only = ASPECT_SCORES_DF[ASPECT_SCORES_DF[\"source\"] == \"ai\"]\n",
    "if not ai_only.empty:\n",
    "    domain_avg = ai_only.groupby(\"domain\")[ASPECT_COLUMNS].mean()\n",
    "    seniority_avg = ai_only.groupby(\"seniority\")[ASPECT_COLUMNS].mean()\n",
    "    print(\"AI reviewer aspect averages by domain:\")\n",
    "    display(domain_avg)\n",
    "    print(\"AI reviewer aspect averages by seniority:\")\n",
    "    display(seniority_avg)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=False)\n",
    "    heatmap_data = [\n",
    "        (axes[0], domain_avg, \"AI reviewers by domain\"),\n",
    "        (axes[1], seniority_avg, \"AI reviewers by seniority\"),\n",
    "    ]\n",
    "    im = None\n",
    "    for ax, data, title in heatmap_data:\n",
    "        if data.empty:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        im = ax.imshow(data[ASPECT_COLUMNS].values, aspect='auto', cmap='YlGnBu', vmin=1, vmax=5)\n",
    "        ax.set_xticks(np.arange(len(ASPECT_COLUMNS)))\n",
    "        ax.set_xticklabels([label.title() for label in ASPECT_COLUMNS], rotation=45, ha='right')\n",
    "        ax.set_yticks(np.arange(len(data.index)))\n",
    "        ax.set_yticklabels(data.index)\n",
    "        ax.set_title(title)\n",
    "    if im is not None:\n",
    "        fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.8, label='Avg score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Generate AI reviews and rerun the scoring cell to compare domains/seniority.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b468fd43",
   "metadata": {},
   "source": [
    "# Create AI reviewers\n",
    "\n",
    "Configure synthetic reviewers by selecting their domain expertise and seniority. The selections here determine which AI reviewer personas will be generated in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1917416",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_DOMAINS = list(PAPER_FILE_NAME_TO_DOMAIN.values())\n",
    "AVAILABLE_DOMAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_PANEL_ROWS: List[Dict[str, Any]] = []\n",
    "AI_PANEL_STATE: List[Dict[str, str]] = []\n",
    "\n",
    "# Level of seniority for AI reviewers.\n",
    "SENIORITY_OPTIONS_ALLOWED = [\n",
    "    \"Graduate student\",\n",
    "    \"Junior faculty\",\n",
    "    \"Senior faculty\",\n",
    "    \"random\",  # let the system randomly pick one from the list.\n",
    "]\n",
    "\n",
    "# Domain of expertise for AI reviewers.\n",
    "DOMAIN_OPTIONS = [\n",
    "    \"same\",  # use the same domain as the paper.\n",
    "    \"random_different\",  # let the system randomly pick one from the list of domains AVAILABLE_DOMAINS that is different from the paper's domain.\n",
    "    # You can customize the AVAILABLE_DOMAINS variable above to include more domains or remove some domains.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51641bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your AI reviewer panel by editing this list.\n",
    "# Each entry should have:\n",
    "#   - \"domain\": expertise area (use PAPER_DOMAIN for same as paper, or pick from AVAILABLE_DOMAINS)\n",
    "#   - \"seniority\": one of \"Graduate student\", \"Junior faculty\", \"Senior faculty\", or \"random\"\n",
    "\n",
    "AI_PANEL_STATE: List[Dict[str, str]] = [\n",
    "    {\"domain\": \"same\", \"seniority\": \"Graduate student\"},\n",
    "    {\"domain\": \"same\", \"seniority\": \"Junior faculty\"},\n",
    "    {\"domain\": \"same\", \"seniority\": \"Senior faculty\"},\n",
    "    {\"domain\": \"same\", \"seniority\": \"Senior faculty\"},\n",
    "    {\"domain\": \"random_different\", \"seniority\": \"Senior faculty\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ReviewerPersona objects from the AI panel configuration.\n",
    "# This function resolves \"same\"/\"random_different\" domain settings and \"random\" seniority\n",
    "# to create concrete reviewer personas with specific expertise and seniority levels.\n",
    "\n",
    "@dataclass\n",
    "class ReviewerPersona:\n",
    "    name: str\n",
    "    expertise: str\n",
    "    seniority: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.seniority not in SENIORITY_OPTIONS_ALLOWED[:-1]:  # Exclude \"random\"\n",
    "            raise ValueError(\n",
    "                f\"Invalid seniority '{self.seniority}'. \"\n",
    "                f\"Must be one of: {SENIORITY_OPTIONS_ALLOWED[:-1]}\"\n",
    "            )\n",
    "        if self.expertise not in AVAILABLE_DOMAINS:\n",
    "            raise ValueError(\n",
    "                f\"Invalid expertise '{self.expertise}'. \"\n",
    "                f\"Must be one of: {AVAILABLE_DOMAINS}\"\n",
    "            )\n",
    "\n",
    "def create_reviewer_personas_from_state(\n",
    "    panel_state: List[Dict[str, str]],\n",
    "    paper_domain: str,\n",
    "    available_domains: List[str] = AVAILABLE_DOMAINS,\n",
    "    seniority_options: List[str] = SENIORITY_OPTIONS_ALLOWED[:-1],  # Exclude \"random\"\n",
    ") -> List[ReviewerPersona]:\n",
    "    \"\"\"\n",
    "    Create a list of ReviewerPersona objects from the AI_PANEL_STATE configuration.\n",
    "\n",
    "    Args:\n",
    "        panel_state: List of dicts with \"domain\" and \"seniority\" keys.\n",
    "        paper_domain: The domain of the paper being reviewed.\n",
    "        available_domains: List of available domains to choose from.\n",
    "        seniority_options: List of valid seniority levels (excluding \"random\").\n",
    "\n",
    "    Returns:\n",
    "        List of ReviewerPersona objects.\n",
    "    \"\"\"\n",
    "    personas = []\n",
    "\n",
    "    for i, config in enumerate(panel_state):\n",
    "        # Resolve domain\n",
    "        domain = config.get(\"domain\", \"same\")\n",
    "        if domain == \"same\":\n",
    "            expertise = paper_domain\n",
    "        elif domain == \"random_different\":\n",
    "            different_domains = [d for d in available_domains if d != paper_domain]\n",
    "            expertise = random.choice(different_domains) if different_domains else paper_domain\n",
    "        else:\n",
    "            expertise = domain  # Use the specified domain directly\n",
    "\n",
    "        # Resolve seniority\n",
    "        seniority = config.get(\"seniority\", \"random\")\n",
    "        if seniority == \"random\":\n",
    "            seniority = random.choice(seniority_options)\n",
    "\n",
    "        persona = ReviewerPersona(\n",
    "            name=f\"AI Reviewer_{i + 1}: {seniority} in {expertise}\",\n",
    "            expertise=expertise,\n",
    "            seniority=seniority,\n",
    "        )\n",
    "        personas.append(persona)\n",
    "\n",
    "    return personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66484bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = create_reviewer_personas_from_state(AI_PANEL_STATE, PAPER_DOMAIN)\n",
    "personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035dae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWER_PROMPT_TEMPLATE = \"\"\"Your task is to compose a high-quality review for the top-tier Machine Learning (ML) conference International Conference on Learning Representations (ICLR).\n",
    "\n",
    "You are a {seniority} and an expert in {expertise}.\n",
    "\n",
    "A review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\n",
    "\n",
    "* Read the paper: It's important to carefully read through the entire paper, and to consider any related work and citations that will help you comprehensively evaluate it. While reading, consider the following:\n",
    "  * Objective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\n",
    "  * Is the approach well motivated, including being well-placed in the literature?\n",
    "  * Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n",
    "  * What is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\n",
    "\n",
    "* Write and submit your initial review, organizing it as follows:\n",
    "  * summary: Summarize what the paper claims to contribute. What is the specific question and/or problem tackled by the paper? Be positive and constructive.\n",
    "  * strengths: List strong points of the paper: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)? Be as comprehensive, specific, and detailed as possible.\n",
    "  * weaknesses: List weak points of the paper: is it weak in any of the aspects listed in the previous point? Be as comprehensive, specific, and detailed as possible.\n",
    "  * questions: Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\n",
    "  * rating: A numeric evaluation of the paper, chosen from the following:\n",
    "    * 10: strong accept, should be highlighted at the conference\n",
    "    * 8: accept, good paper\n",
    "    * 6: marginally above the acceptance threshold\n",
    "    * 5: marginally below the acceptance threshold\n",
    "    * 3: reject, not good enough\n",
    "  * confidence: how confident you are in your assessment. It should be an integer chosen from: 2, 3, 4, or 5, based on the following descriptions:\n",
    "    * 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n",
    "    * 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n",
    "    * 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n",
    "    * 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n",
    "\n",
    "To reiterate, you are a {seniority} and an expert in {expertise}.\n",
    "\n",
    "Final guidelines:\n",
    "* Provide a fair, evidence-based review using first person plural.\n",
    "* Produce exactly one JSON object as specified below; do not add commentary or prose outside JSON.\n",
    "\n",
    "JSON schema (keys are required):\n",
    "{{\n",
    "  \"persona\": {{\"expertise\": string, \"seniority\": string}},\n",
    "  \"review\": {{\n",
    "    \"summary\": [string],\n",
    "    \"strengths\": [string],\n",
    "    \"weaknesses\": [string],\n",
    "    \"questions\": [string],\n",
    "    \"rating\": integer 3-10,\n",
    "    \"confidence\": integer 2-5\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "print(REVIEWER_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reviewer_agent(persona: ReviewerPersona) -> AssistantAgent:\n",
    "    llm_config = build_llm_config()\n",
    "    system_message = REVIEWER_PROMPT_TEMPLATE.format(\n",
    "        expertise=persona.expertise,\n",
    "        seniority=persona.seniority,\n",
    "    )\n",
    "    return AssistantAgent(\n",
    "        name=persona.name,\n",
    "        system_message=system_message,\n",
    "        llm_config=llm_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e315355",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_REVIEWERS = [make_reviewer_agent(persona) for persona in personas]\n",
    "AI_REVIEWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e44c0",
   "metadata": {},
   "source": [
    "## Generate AI reviewer feedback\n",
    "\n",
    "Click the button below to synthesize reviews for each configured AI persona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_REVIEWS: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def generate_ai_reviews():\n",
    "    \"\"\"Generate AI reviews for the paper using configured reviewer personas.\"\"\"\n",
    "    global AI_REVIEWS\n",
    "\n",
    "    if not AI_REVIEWERS:\n",
    "        print(\"No AI reviewers configured. Please set up reviewer personas first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating {len(AI_REVIEWERS)} AI reviews …\")\n",
    "    start = time.perf_counter()\n",
    "    generated: List[Dict[str, Any]] = []\n",
    "\n",
    "    for reviewer in AI_REVIEWERS:\n",
    "        try:\n",
    "            response = reviewer.generate_reply(\n",
    "                messages=[{\"role\": \"user\", \"content\": PAPER_TEXT}]\n",
    "            )\n",
    "            generated.append({\n",
    "                \"reviewer\": reviewer.name,\n",
    "                \"response\": response,\n",
    "            })\n",
    "            print(f\"✓ {reviewer.name}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"✗ {reviewer.name}: {exc}\")\n",
    "\n",
    "    AI_REVIEWS = generated\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Completed in {elapsed:.1f}s. Generated {len(AI_REVIEWS)} reviews.\")\n",
    "\n",
    "    for review in AI_REVIEWS:\n",
    "        print(f\"\\n--- {review['reviewer']} ---\")\n",
    "        print(review['response'])\n",
    "\n",
    "\n",
    "# Run the generation\n",
    "generate_ai_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in AI_REVIEWS:\n",
    "    print(f\"\\n--- {review['reviewer']} ---\")\n",
    "    summary = text_summary(review['response'])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_REVIEWS[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee142d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build AI review records from AI_REVIEWS for scoring\n",
    "ai_review_records = []\n",
    "for review in AI_REVIEWS:\n",
    "    # Parse the reviewer name to extract persona info\n",
    "    reviewer_name = review.get(\"reviewer\", \"\")\n",
    "    response = review.get(\"response\", \"\")\n",
    "\n",
    "    # Extract seniority and expertise from reviewer name if possible\n",
    "    seniority = \"unknown\"\n",
    "    expertise = PAPER_DOMAIN\n",
    "    if \"Graduate student\" in reviewer_name:\n",
    "        seniority = \"Graduate student\"\n",
    "    elif \"Junior faculty\" in reviewer_name:\n",
    "        seniority = \"Junior faculty\"\n",
    "    elif \"Senior faculty\" in reviewer_name:\n",
    "        seniority = \"Senior faculty\"\n",
    "\n",
    "    # Extract expertise from reviewer name (after \"in \")\n",
    "    if \" in \" in reviewer_name:\n",
    "        expertise = reviewer_name.split(\" in \")[-1]\n",
    "\n",
    "    ai_review_records.append({\n",
    "        \"persona\": {\n",
    "            \"name\": reviewer_name,\n",
    "            \"expertise\": expertise,\n",
    "            \"seniority\": seniority,\n",
    "        },\n",
    "        \"response\": response,\n",
    "    })\n",
    "\n",
    "def flatten_review_text_ai(record: Union[Dict[str, Any], str]) -> str:\n",
    "    \"\"\"Flatten AI review record to text.\"\"\"\n",
    "    if isinstance(record, str):\n",
    "        return record\n",
    "    response = record.get(\"response\", \"\")\n",
    "    if isinstance(response, str):\n",
    "        return response\n",
    "    # If response is a dict, try to extract text parts\n",
    "    if isinstance(response, dict):\n",
    "        parts = []\n",
    "        for key in (\"summary\", \"strengths\", \"weaknesses\", \"questions\"):\n",
    "            value = response.get(key, \"\")\n",
    "            if isinstance(value, str):\n",
    "                parts.append(value)\n",
    "            elif isinstance(value, list):\n",
    "                parts.extend([str(v) for v in value])\n",
    "        return \"\\n\".join(parts)\n",
    "    return str(response)\n",
    "\n",
    "def score_ai_review_records(\n",
    "    records: List[Dict[str, Any]],\n",
    "    source_label: str = \"AI\",\n",
    "    num_trials: int = 1,\n",
    "    model: str = DEFAULT_LLM_CONFIG[\"model\"],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Score AI review records using aspects_of_comments.\"\"\"\n",
    "    scored: List[Dict[str, Any]] = []\n",
    "    if not records:\n",
    "        return scored\n",
    "    for idx, record in enumerate(records, start=1):\n",
    "        persona = record.get(\"persona\", {})\n",
    "        review_text = flatten_review_text_ai(record)\n",
    "        if not review_text:\n",
    "            continue\n",
    "        scores = aspects_of_comments(review_text, model=model, num_trials=num_trials)\n",
    "        scored.append(\n",
    "            {\n",
    "                \"source\": source_label,\n",
    "                \"name\": persona.get(\"name\", f\"{source_label}_{idx}\"),\n",
    "                \"domain\": persona.get(\"expertise\", PAPER_DOMAIN),\n",
    "                \"seniority\": persona.get(\"seniority\", \"unknown\"),\n",
    "                \"reviewer\": persona.get(\"name\", f\"{source_label}_{idx}\"),\n",
    "                **scores,\n",
    "            }\n",
    "        )\n",
    "    return scored\n",
    "\n",
    "# Score the AI reviews\n",
    "ai_scored = score_ai_review_records(ai_review_records, source_label=\"AI\")\n",
    "\n",
    "# Combine with human scores if available\n",
    "if 'ASPECT_SCORES_DF' in dir() and ASPECT_SCORES_DF is not None:\n",
    "    ai_scores_df = pd.DataFrame(ai_scored)\n",
    "    ASPECT_SCORES_DF = pd.concat([ASPECT_SCORES_DF, ai_scores_df], ignore_index=True)\n",
    "else:\n",
    "    ASPECT_SCORES_DF = pd.DataFrame(ai_scored)\n",
    "\n",
    "print(ASPECT_SCORES_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4abbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_SCORES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20644289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram comparing distribution of topics that AI and human reviewers care about\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the aspect columns\n",
    "aspect_cols = ['accuracy', 'clarity', 'consistency', 'novelty', 'replicability', 'thoroughness']\n",
    "\n",
    "# Separate human and AI scores\n",
    "human_scores = ASPECT_SCORES_DF[ASPECT_SCORES_DF['source'] == 'human'][aspect_cols]\n",
    "ai_scores = ASPECT_SCORES_DF[ASPECT_SCORES_DF['source'] == 'AI'][aspect_cols]\n",
    "\n",
    "# Calculate mean scores for each aspect\n",
    "human_means = human_scores.mean()\n",
    "ai_means = ai_scores.mean()\n",
    "\n",
    "# Calculate standard errors for error bars\n",
    "human_stds = human_scores.std()\n",
    "ai_stds = ai_scores.std()\n",
    "human_sems = human_stds / np.sqrt(len(human_scores))\n",
    "ai_sems = ai_stds / np.sqrt(len(ai_scores))\n",
    "\n",
    "# Create the histogram comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(aspect_cols))\n",
    "width = 0.35\n",
    "\n",
    "bars_human = ax.bar(x - width/2, human_means.values, width, label='Human Reviewers',\n",
    "                    color='steelblue', alpha=0.8, yerr=human_sems.values, capsize=4)\n",
    "bars_ai = ax.bar(x + width/2, ai_means.values, width, label='AI Reviewers',\n",
    "                 color='coral', alpha=0.8, yerr=ai_sems.values, capsize=4)\n",
    "\n",
    "ax.set_xlabel('Review Aspects', fontsize=12)\n",
    "ax.set_ylabel('Mean Score', fontsize=12)\n",
    "ax.set_title('Distribution of Topics/Aspects: Human vs AI Reviewers', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([col.capitalize() for col in aspect_cols], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars_human:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars_ai:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nHuman Reviewers - Mean Aspect Scores:\")\n",
    "print(human_means.to_frame('Mean').T)\n",
    "print(\"\\nAI Reviewers - Mean Aspect Scores:\")\n",
    "print(ai_means.to_frame('Mean').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9f35f",
   "metadata": {},
   "source": [
    "# Mix Human Reviews with AI Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for reviewer mixture experiment\n",
    "# We will randomly replace X human reviews with AI reviews (1 <= X <= TOTAL_REVIEWS)\n",
    "# and measure how this affects the editor's decision\n",
    "\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "TOTAL_REVIEWS = len(HUMAN_REVIEWS)\n",
    "\n",
    "# Number of AI reviews to substitute (from 1 to all reviews)\n",
    "NUM_AI_REVIEWS_TO_TEST = list(range(1, TOTAL_REVIEWS + 1))\n",
    "\n",
    "# Number of random shuffles/runs per configuration to account for variance\n",
    "# For each X, we randomly select which X human reviews to replace with AI reviews\n",
    "NUM_SHUFFLE_RUNS = 3\n",
    "\n",
    "# Store results for analysis\n",
    "mixture_results = []\n",
    "\n",
    "def create_mixed_review_set(human_reviews, ai_reviews, num_ai_to_include, seed=None):\n",
    "    \"\"\"\n",
    "    Create a mixed set of reviews by randomly replacing some human reviews with AI reviews.\n",
    "\n",
    "    Args:\n",
    "        human_reviews: List of human review texts\n",
    "        ai_reviews: List of AI review texts\n",
    "        num_ai_to_include: Number of human reviews to replace with AI reviews (1 to len(human_reviews))\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        mixed_reviews: List of reviews with some human reviews replaced by AI reviews\n",
    "        replacement_indices: Indices of human reviews that were replaced\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    num_human = len(human_reviews)\n",
    "    num_ai = len(ai_reviews)\n",
    "\n",
    "    # Ensure we don't try to replace more reviews than we have\n",
    "    num_ai_to_include = min(num_ai_to_include, num_human, num_ai)\n",
    "\n",
    "    # Randomly select which human review positions to replace\n",
    "    replacement_indices = sorted(random.sample(range(num_human), num_ai_to_include))\n",
    "\n",
    "    # Randomly select which AI reviews to use\n",
    "    ai_indices = random.sample(range(num_ai), num_ai_to_include)\n",
    "\n",
    "    # Create the mixed review set\n",
    "    mixed_reviews = human_reviews.copy()\n",
    "    review_sources = ['human'] * num_human  # Track source of each review\n",
    "\n",
    "    for i, (human_idx, ai_idx) in enumerate(zip(replacement_indices, ai_indices)):\n",
    "        mixed_reviews[human_idx] = ai_reviews[ai_idx]\n",
    "        review_sources[human_idx] = 'AI'\n",
    "\n",
    "    return mixed_reviews, replacement_indices, review_sources\n",
    "\n",
    "print(f\"Total human reviews available: {TOTAL_REVIEWS}\")\n",
    "print(f\"AI review counts to test: {NUM_AI_REVIEWS_TO_TEST}\")\n",
    "print(f\"Shuffle runs per configuration: {NUM_SHUFFLE_RUNS}\")\n",
    "print(f\"Total simulations to run: {len(NUM_AI_REVIEWS_TO_TEST) * NUM_SHUFFLE_RUNS}\")\n",
    "print(f\"\\nExperiment design:\")\n",
    "print(f\"  - For each X in {NUM_AI_REVIEWS_TO_TEST}:\")\n",
    "print(f\"    - Randomly select X human reviews to replace with AI reviews\")\n",
    "print(f\"    - Repeat {NUM_SHUFFLE_RUNS} times with different random selections\")\n",
    "print(f\"    - Measure editor decision for each configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d28623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in a structured format\n",
    "AI_EDITOR_AI_REVIEWERS_SIMULATION_RESULTS = []\n",
    "\n",
    "# Calculate total iterations for progress bar\n",
    "total_iterations = len([1, 2, 3]) * NUM_SHUFFLE_RUNS\n",
    "\n",
    "with tqdm(total=total_iterations, desc=\"Running simulations\") as pbar:\n",
    "    for num_ai_reviews in [1, 2, 3]:\n",
    "        for run_idx in range(NUM_SHUFFLE_RUNS):\n",
    "            reviews, replacement_indices, review_sources = create_mixed_review_set(HUMAN_REVIEWS, AI_REVIEWS, num_ai_reviews)\n",
    "            editor = make_editor_agent()\n",
    "            # Build the input message for the editor agent\n",
    "\n",
    "            reviews_text = \"\\n\\n---\\n\\n\".join([f\"Review {i+1}:\\n{r}\" for i, r in enumerate(reviews)])\n",
    "            input_message = f\"Paper:\\n{PAPER_TEXT}\\n\\nPlease make a decision for this paper based on the following reviews:\\n\\n{reviews_text}\"\n",
    "\n",
    "            # Get simulated decision from the editor agent\n",
    "            response = editor.generate_reply(messages=[{\"role\": \"user\", \"content\": input_message}])\n",
    "\n",
    "            # Store the result\n",
    "            AI_EDITOR_AI_REVIEWERS_SIMULATION_RESULTS.append({\n",
    "                \"num_ai_reviews\": num_ai_reviews,\n",
    "                \"run_idx\": run_idx,\n",
    "                \"replacement_indices\": replacement_indices,\n",
    "                \"review_sources\": review_sources,\n",
    "                \"editor_response\": response\n",
    "            })\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76274178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse decisions from editor responses\n",
    "import json\n",
    "import re\n",
    "\n",
    "def parse_editor_decision(response):\n",
    "    \"\"\"\n",
    "    Parse the decision from the editor's response.\n",
    "    Handles both JSON format and plain text responses.\n",
    "    \"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Try to parse as JSON first\n",
    "        if isinstance(response, str):\n",
    "            # Look for JSON object in the response\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                parsed = json.loads(json_match.group())\n",
    "                return parsed.get('decision')\n",
    "        elif isinstance(response, dict):\n",
    "            return response.get('decision')\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        pass\n",
    "\n",
    "    # Fallback: look for common decision patterns in text\n",
    "    response_str = str(response).lower()\n",
    "    if 'accept (poster)' in response_str or 'accept(poster)' in response_str:\n",
    "        return 'Accept (Poster)'\n",
    "    elif 'accept (oral)' in response_str or 'accept(oral)' in response_str:\n",
    "        return 'Accept (Oral)'\n",
    "    elif 'accept' in response_str:\n",
    "        return 'Accept'\n",
    "    elif 'reject' in response_str:\n",
    "        return 'Reject'\n",
    "\n",
    "    return None\n",
    "\n",
    "# Add parsed decisions to results\n",
    "for result in AI_EDITOR_AI_REVIEWERS_SIMULATION_RESULTS:\n",
    "    result['parsed_decision'] = parse_editor_decision(result['editor_response'])\n",
    "\n",
    "# Display summary\n",
    "print(\"Parsed decisions summary:\")\n",
    "for result in AI_EDITOR_AI_REVIEWERS_SIMULATION_RESULTS:\n",
    "    print(f\"Run {result['run_idx']} with {result['num_ai_reviews']} AI reviews: {result['parsed_decision']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_DECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73862cd2",
   "metadata": {},
   "source": [
    "# Mix (AI) Reviewers from Different Domains\n",
    "### Note: because we don't know the identity of real human reviewers, we cannot easily control for the domain and seniority of human reviewers. We will generate AI reviewers from different domains and seniorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fbf13eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI reviewer panel: 5 same-domain + 5 random-different (all junior faculty)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>expertise</th>\n",
       "      <th>seniority</th>\n",
       "      <th>domain_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI Reviewer_1: Junior faculty in Causality</td>\n",
       "      <td>Causality</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI Reviewer_2: Junior faculty in Causality</td>\n",
       "      <td>Causality</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Reviewer_3: Junior faculty in Causality</td>\n",
       "      <td>Causality</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI Reviewer_4: Junior faculty in Causality</td>\n",
       "      <td>Causality</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI Reviewer_5: Junior faculty in Causality</td>\n",
       "      <td>Causality</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AI Reviewer_6: Junior faculty in Time Series</td>\n",
       "      <td>Time Series</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>random_different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AI Reviewer_7: Junior faculty in Neuroscience</td>\n",
       "      <td>Neuroscience</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>random_different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AI Reviewer_8: Junior faculty in Health</td>\n",
       "      <td>Health</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>random_different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AI Reviewer_9: Junior faculty in Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>random_different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AI Reviewer_10: Junior faculty in Energy</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Junior faculty</td>\n",
       "      <td>random_different</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name     expertise  \\\n",
       "0     AI Reviewer_1: Junior faculty in Causality     Causality   \n",
       "1     AI Reviewer_2: Junior faculty in Causality     Causality   \n",
       "2     AI Reviewer_3: Junior faculty in Causality     Causality   \n",
       "3     AI Reviewer_4: Junior faculty in Causality     Causality   \n",
       "4     AI Reviewer_5: Junior faculty in Causality     Causality   \n",
       "5   AI Reviewer_6: Junior faculty in Time Series   Time Series   \n",
       "6  AI Reviewer_7: Junior faculty in Neuroscience  Neuroscience   \n",
       "7        AI Reviewer_8: Junior faculty in Health        Health   \n",
       "8      AI Reviewer_9: Junior faculty in Security      Security   \n",
       "9       AI Reviewer_10: Junior faculty in Energy        Energy   \n",
       "\n",
       "        seniority     domain_source  \n",
       "0  Junior faculty              same  \n",
       "1  Junior faculty              same  \n",
       "2  Junior faculty              same  \n",
       "3  Junior faculty              same  \n",
       "4  Junior faculty              same  \n",
       "5  Junior faculty  random_different  \n",
       "6  Junior faculty  random_different  \n",
       "7  Junior faculty  random_different  \n",
       "8  Junior faculty  random_different  \n",
       "9  Junior faculty  random_different  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_source\n",
      "same                5\n",
      "random_different    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Build a balanced panel of 10 AI reviewers (all junior faculty)\n",
    "NUM_SAME_DOMAIN_JUNIOR = 5\n",
    "NUM_RANDOM_DOMAIN_JUNIOR = 5\n",
    "\n",
    "MIXED_DOMAIN_AI_PANEL_STATE = (\n",
    "    [{\"domain\": \"same\", \"seniority\": \"Junior faculty\"} for _ in range(NUM_SAME_DOMAIN_JUNIOR)]\n",
    "    + [{\"domain\": \"random_different\", \"seniority\": \"Junior faculty\"} for _ in range(NUM_RANDOM_DOMAIN_JUNIOR)]\n",
    ")\n",
    "\n",
    "mixed_domain_personas = create_reviewer_personas_from_state(\n",
    "    MIXED_DOMAIN_AI_PANEL_STATE,\n",
    "    paper_domain=PAPER_DOMAIN,\n",
    ")\n",
    "\n",
    "MIXED_DOMAIN_AI_REVIEWERS = [make_reviewer_agent(persona) for persona in mixed_domain_personas]\n",
    "\n",
    "# Reuse global reviewer references so downstream cells operate on this configuration\n",
    "AI_PANEL_STATE = MIXED_DOMAIN_AI_PANEL_STATE\n",
    "personas = mixed_domain_personas\n",
    "AI_REVIEWERS = MIXED_DOMAIN_AI_REVIEWERS\n",
    "\n",
    "mixed_domain_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"name\": persona.name,\n",
    "            \"expertise\": persona.expertise,\n",
    "            \"seniority\": persona.seniority,\n",
    "            \"domain_source\": \"same\" if persona.expertise == PAPER_DOMAIN else \"random_different\",\n",
    "        }\n",
    "        for persona in mixed_domain_personas\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"AI reviewer panel: 5 same-domain + 5 random-different (all junior faculty)\")\n",
    "display(mixed_domain_summary)\n",
    "print(mixed_domain_summary[\"domain_source\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eedf37",
   "metadata": {},
   "source": [
    "### Now we experiment how the number of outsider reviewers (i.e., reviewers from different domains) affects the editor's decision.\n",
    "We fix the panel size to five AI reviews per editor call and vary how many of those five come from outside the paper's domain (0-5 outsiders). Each configuration is sampled multiple times to average out randomness in the reviewer pool and the editor's LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dbefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating AI reviews for the mixed-domain personas …\n",
      "Generating 10 AI reviews …\n",
      "✓ AI Reviewer_1: Junior faculty in Causality\n",
      "✓ AI Reviewer_2: Junior faculty in Causality\n",
      "✓ AI Reviewer_3: Junior faculty in Causality\n",
      "✓ AI Reviewer_4: Junior faculty in Causality\n",
      "✓ AI Reviewer_5: Junior faculty in Causality\n",
      "✓ AI Reviewer_6: Junior faculty in Time Series\n",
      "✓ AI Reviewer_7: Junior faculty in Neuroscience\n",
      "✓ AI Reviewer_8: Junior faculty in Health\n",
      "✓ AI Reviewer_9: Junior faculty in Security\n",
      "✓ AI Reviewer_10: Junior faculty in Energy\n",
      "Completed in 50.1s. Generated 10 reviews.\n",
      "\n",
      "--- AI Reviewer_1: Junior faculty in Causality ---\n",
      "```json\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Causality\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper investigates the performance of contemporary Causal Average Treatment Effect (CATE) models through an extensive benchmark study involving various datasets and sampling strategies. The authors reveal critical findings regarding the effectiveness of these models, showing that a significant portion of them perform poorly compared to trivial models, thus emphasizing the need for improved evaluation methods and model designs.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The paper presents a comprehensive large-scale benchmark study involving 16 CATE models across 12 datasets and 43,200 variants, which is a considerable contribution to the field.\",\n",
      "      \"The introduction of the statistical parameter Q, which allows for evaluation of CATE estimates without requiring counterfactuals, represents a significant methodological innovation.\",\n",
      "      \"The study uses real-world datasets, which enhances the practical relevance of the findings compared to previous works that relied on simulated data.\",\n",
      "      \"The paper thoroughly discusses the limitations of current CATE models and raises important questions regarding their ability to capture complexity in real-world data, spurring further research in this area.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the findings are impactful, the paper could benefit from a clearer presentation of its methodology, particularly in the details regarding how the observational sampling method is implemented.\",\n",
      "      \"The reliance on specific datasets may introduce biases; future work could involve a broader range of datasets to assess the generalizability of the findings.\",\n",
      "      \"The results suggest that many models performed poorly, but the paper does not sufficiently explore the reasons behind the low performance rates in certain cases, which would be useful for understanding the failures.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Can the authors provide more details on how the observational sampling method affects the benchmark results and its limitations?\",\n",
      "      \"How do you envisage extending your evaluation framework to incorporate more CATE estimation models or different classes of models?\",\n",
      "      \"Are there any specific strategies you suggest for practitioners who may rely on the findings of this paper to select appropriate CATE models for their work?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- AI Reviewer_2: Junior faculty in Causality ---\n",
      "```json\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Causality\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper investigates the performance of contemporary Causal Average Treatment Effect (CATE) models through an extensive benchmark study involving various datasets and sampling strategies. The authors reveal critical findings regarding the effectiveness of these models, showing that a significant portion of them perform poorly compared to trivial models, thus emphasizing the need for improved evaluation methods and model designs.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The paper presents a comprehensive large-scale benchmark study involving 16 CATE models across 12 datasets and 43,200 variants, which is a considerable contribution to the field.\",\n",
      "      \"The introduction of the statistical parameter Q, which allows for evaluation of CATE estimates without requiring counterfactuals, represents a significant methodological innovation.\",\n",
      "      \"The study uses real-world datasets, which enhances the practical relevance of the findings compared to previous works that relied on simulated data.\",\n",
      "      \"The paper thoroughly discusses the limitations of current CATE models and raises important questions regarding their ability to capture complexity in real-world data, spurring further research in this area.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the findings are impactful, the paper could benefit from a clearer presentation of its methodology, particularly in the details regarding how the observational sampling method is implemented.\",\n",
      "      \"The reliance on specific datasets may introduce biases; future work could involve a broader range of datasets to assess the generalizability of the findings.\",\n",
      "      \"The results suggest that many models performed poorly, but the paper does not sufficiently explore the reasons behind the low performance rates in certain cases, which would be useful for understanding the failures.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Can the authors provide more details on how the observational sampling method affects the benchmark results and its limitations?\",\n",
      "      \"How do you envisage extending your evaluation framework to incorporate more CATE estimation models or different classes of models?\",\n",
      "      \"Are there any specific strategies you suggest for practitioners who may rely on the findings of this paper to select appropriate CATE models for their work?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- AI Reviewer_3: Junior faculty in Causality ---\n",
      "```json\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Causality\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper investigates the performance of contemporary Causal Average Treatment Effect (CATE) models through an extensive benchmark study involving various datasets and sampling strategies. The authors reveal critical findings regarding the effectiveness of these models, showing that a significant portion of them perform poorly compared to trivial models, thus emphasizing the need for improved evaluation methods and model designs.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The paper presents a comprehensive large-scale benchmark study involving 16 CATE models across 12 datasets and 43,200 variants, which is a considerable contribution to the field.\",\n",
      "      \"The introduction of the statistical parameter Q, which allows for evaluation of CATE estimates without requiring counterfactuals, represents a significant methodological innovation.\",\n",
      "      \"The study uses real-world datasets, which enhances the practical relevance of the findings compared to previous works that relied on simulated data.\",\n",
      "      \"The paper thoroughly discusses the limitations of current CATE models and raises important questions regarding their ability to capture complexity in real-world data, spurring further research in this area.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the findings are impactful, the paper could benefit from a clearer presentation of its methodology, particularly in the details regarding how the observational sampling method is implemented.\",\n",
      "      \"The reliance on specific datasets may introduce biases; future work could involve a broader range of datasets to assess the generalizability of the findings.\",\n",
      "      \"The results suggest that many models performed poorly, but the paper does not sufficiently explore the reasons behind the low performance rates in certain cases, which would be useful for understanding the failures.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Can the authors provide more details on how the observational sampling method affects the benchmark results and its limitations?\",\n",
      "      \"How do you envisage extending your evaluation framework to incorporate more CATE estimation models or different classes of models?\",\n",
      "      \"Are there any specific strategies you suggest for practitioners who may rely on the findings of this paper to select appropriate CATE models for their work?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- AI Reviewer_4: Junior faculty in Causality ---\n",
      "```json\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Causality\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper investigates the performance of contemporary Causal Average Treatment Effect (CATE) models through an extensive benchmark study involving various datasets and sampling strategies. The authors reveal critical findings regarding the effectiveness of these models, showing that a significant portion of them perform poorly compared to trivial models, thus emphasizing the need for improved evaluation methods and model designs.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The paper presents a comprehensive large-scale benchmark study involving 16 CATE models across 12 datasets and 43,200 variants, which is a considerable contribution to the field.\",\n",
      "      \"The introduction of the statistical parameter Q, which allows for evaluation of CATE estimates without requiring counterfactuals, represents a significant methodological innovation.\",\n",
      "      \"The study uses real-world datasets, which enhances the practical relevance of the findings compared to previous works that relied on simulated data.\",\n",
      "      \"The paper thoroughly discusses the limitations of current CATE models and raises important questions regarding their ability to capture complexity in real-world data, spurring further research in this area.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the findings are impactful, the paper could benefit from a clearer presentation of its methodology, particularly in the details regarding how the observational sampling method is implemented.\",\n",
      "      \"The reliance on specific datasets may introduce biases; future work could involve a broader range of datasets to assess the generalizability of the findings.\",\n",
      "      \"The results suggest that many models performed poorly, but the paper does not sufficiently explore the reasons behind the low performance rates in certain cases, which would be useful for understanding the failures.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Can the authors provide more details on how the observational sampling method affects the benchmark results and its limitations?\",\n",
      "      \"How do you envisage extending your evaluation framework to incorporate more CATE estimation models or different classes of models?\",\n",
      "      \"Are there any specific strategies you suggest for practitioners who may rely on the findings of this paper to select appropriate CATE models for their work?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- AI Reviewer_5: Junior faculty in Causality ---\n",
      "```json\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Causality\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper investigates the performance of contemporary Causal Average Treatment Effect (CATE) models through an extensive benchmark study involving various datasets and sampling strategies. The authors reveal critical findings regarding the effectiveness of these models, showing that a significant portion of them perform poorly compared to trivial models, thus emphasizing the need for improved evaluation methods and model designs.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The paper presents a comprehensive large-scale benchmark study involving 16 CATE models across 12 datasets and 43,200 variants, which is a considerable contribution to the field.\",\n",
      "      \"The introduction of the statistical parameter Q, which allows for evaluation of CATE estimates without requiring counterfactuals, represents a significant methodological innovation.\",\n",
      "      \"The study uses real-world datasets, which enhances the practical relevance of the findings compared to previous works that relied on simulated data.\",\n",
      "      \"The paper thoroughly discusses the limitations of current CATE models and raises important questions regarding their ability to capture complexity in real-world data, spurring further research in this area.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the findings are impactful, the paper could benefit from a clearer presentation of its methodology, particularly in the details regarding how the observational sampling method is implemented.\",\n",
      "      \"The reliance on specific datasets may introduce biases; future work could involve a broader range of datasets to assess the generalizability of the findings.\",\n",
      "      \"The results suggest that many models performed poorly, but the paper does not sufficiently explore the reasons behind the low performance rates in certain cases, which would be useful for understanding the failures.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Can the authors provide more details on how the observational sampling method affects the benchmark results and its limitations?\",\n",
      "      \"How do you envisage extending your evaluation framework to incorporate more CATE estimation models or different classes of models?\",\n",
      "      \"Are there any specific strategies you suggest for practitioners who may rely on the findings of this paper to select appropriate CATE models for their work?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- AI Reviewer_6: Junior faculty in Time Series ---\n",
      "```json\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Time Series\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper presents a comprehensive evaluation of Conditional Average Treatment Effect (CATE) estimation algorithms and identifies significant shortcomings in their performance through a large-scale benchmark study. Notably, it challenges the effectiveness of modern CATE models in capturing real-world heterogeneity and proposes a novel evaluation metric.\",\n",
      "      \"The study employs 16 CATE models across 12 datasets and introduces a statistical parameter, Q, that circumvents the need for counterfactual outcomes, offering unbiased performance evaluations against the ground truth.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The empirical results provide a robust evaluation framework that exposes critical weaknesses in the current CATE models, highlighting that a considerable percentage of these models fail to outperform simple benchmarks.\",\n",
      "      \"The use of observational sampling to enhance model evaluation without relying on potentially misleading simulated data leads to significant insights into the ability of these models to generalize to real-world scenarios.\",\n",
      "      \"The proposed metric Q and its associated family of statistics demonstrate promising theoretical properties, allowing for meaningful rankings of CATE estimators, enhancing the reproducibility and reliability of results.\",\n",
      "      \"The investigation focuses on real-world datasets rather than simulated environments, which adds credibility and relevance to the findings, particularly for practitioners.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"Although the findings are significant, the authors could further expand on the practical implications of their results, particularly addressing how practitioners might mitigate the identified shortcomings in CATE models.\",\n",
      "      \"The paper might benefit from additional discussions on the limitations of the selected datasets and whether these datasets adequately represent the broad spectrum of applications for CATE models.\",\n",
      "      \"There could be a more detailed exploration of the modeling choices for CATE estimation, especially regarding the hybrid approaches employed, to better understand their implications on model performance.\",\n",
      "      \"While the statistical parameter Q is a novel contribution, providing more extensive empirical evidence comparing Q to MSE in numerous scenarios may help establish its utility across various contexts more convincingly.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Could you elaborate on the potential steps or recommendations you could provide for practitioners seeking to improve their CATE model performance based on your findings?\",\n",
      "      \"What future research directions do you foresee stemming from this study to further improve the evaluation of CATE models?\",\n",
      "      \"How do you think the proposed methodology of observational sampling can be extended to other types of causal inference models beyond CATE?\",\n",
      "      \"What specific challenges did you encounter while implementing the observational sampling method, and how did you overcome them?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- AI Reviewer_7: Junior faculty in Neuroscience ---\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Neuroscience\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper presents a significant contribution by evaluating contemporary Conditional Average Treatment Effect (CATE) models through a large-scale benchmarking study that incorporates observational sampling techniques. The study finds that a substantial proportion of the CATE estimates exhibit high Mean Squared Error (MSE) when compared to trivial baselines, raising important questions about the efficacy of these models in capturing real-world heterogeneity.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The methodology employed in applying observational sampling for CATE evaluation is innovative and provides meaningful insights by facilitating a rigorous assessment of existing models on real-world data.\",\n",
      "      \"The introduction of the statistical parameter Q is a novel approach that bypasses the traditional reliance on counterfactual ground truth for evaluating CATE estimators, showcasing a clear advancement in the field of causal inference.\",\n",
      "      \"The large-scale benchmark consisting of 43,200 sampled variants from 12 diverse datasets enhances the robustness and generalizability of the findings, making it a valuable resource for the community.\",\n",
      "      \"The clear and systematic presentation allows for reproducibility of results, as the authors use pre-existing models and a well-documented methodology.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"The paper could benefit from a deeper exploration of the implications of the findings, particularly in terms of practical applications and suggestions for improvement of existing CATE methods.\",\n",
      "      \"Although the discussion of alternative explanations for the results is helpful, it feels somewhat brief and could delve into how the findings inform the future direction of CATE model development.\",\n",
      "      \"A comprehensive comparison with other existing benchmarks or methods in causal inference could further contextulize their findings and enhance the insights provided.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Could the authors provide more concrete recommendations for practitioners on how to choose the most appropriate CATE estimators based on the findings?\",\n",
      "      \"How do you envision the integration of the new evaluation metric Q impacting future research and model development within the field?\",\n",
      "      \"What steps could be taken to address the high proportion of degenerate estimators found in the study?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "\n",
      "--- AI Reviewer_8: Junior faculty in Health ---\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Health\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"This paper presents a comprehensive evaluation of Conditional Average Treatment Effect (CATE) estimation algorithms through a large-scale benchmark study involving 43,200 sampled variants derived from 12 datasets. The primary contributions are the significant findings about the limitations of contemporary CATE models, new statistical parameters for evaluation, and a novel observational sampling procedure for CATE assessment, revealing critical insights into model performance.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The paper addresses a pertinent issue in causal inference by evaluating CATE models specifically in complex, real-world scenarios, thus providing practical relevance and applicability.\",\n",
      "      \"The use of a large-scale benchmark significantly enhances the robustness of the findings and contributes to the methodological framework in causal inference, making it a valuable resource for future research.\",\n",
      "      \"The introduction of the statistical parameter Q for CATE model evaluation is innovative and allows for the effective comparison of models without relying on simulated outcomes, thereby mitigating biases encountered in previous studies.\",\n",
      "      \"The study employs real-world datasets instead of semi-synthetic data, which enhances the credibility of the evaluation and offers realistic insights about model performance in capturing heterogeneity.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"The primary focus on existing models and their evaluation rather than proposing new modeling techniques may limit the paper's contributions to model advancement.\",\n",
      "      \"While the findings are robust, the paper could provide a more thorough discussion on the implications of the results for practitioners who may apply these models in real-world settings.\",\n",
      "      \"Further exploration of the circumstances under which certain models fail to capture real-world heterogeneity could strengthen the discussion and aid in model selection for practitioners.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"What specific steps can practitioners take to select the most effective CATE estimator based on the findings of the benchmark?\",\n",
      "      \"Can you elaborate on the potential implications of the results for fields heavily reliant on CATE estimates, such as health policy and economics?\",\n",
      "      \"How do the authors envision the future evolution of CATE estimation methodologies in light of these insights?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "\n",
      "--- AI Reviewer_9: Junior faculty in Security ---\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Security\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper presents a large-scale benchmark study evaluating Conditional Average Treatment Effect (CATE) estimation algorithms, revealing significant performance deficiencies among contemporary models in capturing real-world heterogeneity.\",\n",
      "      \"It introduces a novel statistical parameter Q for assessing the estimator's performance and provides a new evaluation procedure based on observational sampling without counterfactual ground truth.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The study's large-scale empirical evaluation across 12 datasets and 43,200 sampled variants demonstrates robustness and relevance, addressing the critical area of CATE model performance in real-world applications.\",\n",
      "      \"The introduction of the statistical parameter Q and its application to evaluate various CATE models offers a fresh perspective on benchmarking, potentially leading to better model selection for practitioners.\",\n",
      "      \"The paper is well-structured and provides a thorough review of existing literature, clearly illustrating the limitations of current CATE evaluation methods.”\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the benchmarking approach is robust, one could argue that the selection of CATE models might still miss out on promising newer architectures from deep learning or other approaches, limiting the generalizability of the findings.\",\n",
      "      \"The empirical analysis focuses primarily on the average performance of CATE models, and further investigation into specific model characteristics and failure modes could enrich findings and guide future research directions.\",\n",
      "      \"The reliance on the statistical parameter Q should also be validated against a broader range of benchmarks beyond the selected datasets to confirm its efficacy and robustness.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"Can you discuss how the method of observational sampling can be generalizable to different domains or data characteristics?\",\n",
      "      \"What specific modifications or recommendations do you propose for practitioners based on your findings, particularly in high-stakes decisions?\",\n",
      "      \"How do you foresee the integration of emerging CATE models, possibly from deep learning, fitting into your proposed evaluation framework?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n",
      "\n",
      "--- AI Reviewer_10: Junior faculty in Energy ---\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"expertise\": \"Energy\",\n",
      "    \"seniority\": \"Junior faculty\"\n",
      "  },\n",
      "  \"review\": {\n",
      "    \"summary\": [\n",
      "      \"The paper investigates the efficacy of contemporary Conditional Average Treatment Effect (CATE) estimation models through a large-scale benchmark study. It highlights that many existing models often fail to outperform relatively trivial benchmarks, asserting significant challenges in capturing real-world heterogeneity in causal inference.\"\n",
      "    ],\n",
      "    \"strengths\": [\n",
      "      \"The study employs a robust experimental design, evaluating 16 different CATE models across 43,200 variants derived from 12 real-world datasets, which enhances the reliability of the findings.\",\n",
      "      \"Innovative integration of observational sampling techniques to develop new evaluation metrics (notably the statistical parameter Q), providing significant advancements in the methodology for assessing CATE models.\",\n",
      "      \"The paper identifies substantial performance gaps in existing CATE models, underscoring a critical need for methodological improvements, which is valuable for researchers and practitioners alike.\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "      \"While the focus on real-world data is commendable, the selection of only 12 datasets may not capture the breadth of potential scenarios in which CATE models may be deployed, potentially limiting generalizability.\",\n",
      "      \"The reliance on statistical assumptions may introduce bias in evaluative performance, particularly in the context of very complex real-world applications where underlying assumptions regarding model behavior are violated.\",\n",
      "      \"The paper could benefit from additional clarity regarding the implementation details of the CATE models and how specific choices impacted the results, enhancing reproducibility.\"\n",
      "    ],\n",
      "    \"questions\": [\n",
      "      \"How do the authors envision addressing potential biases introduced by statistical assumptions during model evaluations in future studies?\",\n",
      "      \"Can the authors provide insight into the potential implications these findings may have on policy-making decisions that rely on CATE estimations?\",\n",
      "      \"What steps do the authors recommend for practitioners to improve their use of current CATE models based on these findings, particularly if they lack the domain expertise for fine-tuning?\"\n",
      "    ],\n",
      "    \"rating\": 8,\n",
      "    \"confidence\": 4\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outsider reviewer simulations: 100%|██████████| 18/18 [01:15<00:00,  4.21s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'panel_size': 5, 'runs_per_setting': 3, 'outsider_counts': [0, 1, 2, 3, 4, 5]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_outsiders</th>\n",
       "      <th>num_same</th>\n",
       "      <th>run_idx</th>\n",
       "      <th>selected_reviewers</th>\n",
       "      <th>editor_response</th>\n",
       "      <th>parsed_decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_4: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_1: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_outsiders  num_same  run_idx  \\\n",
       "0              0         5        0   \n",
       "1              0         5        1   \n",
       "2              0         5        2   \n",
       "3              1         4        0   \n",
       "4              1         4        1   \n",
       "\n",
       "                                  selected_reviewers  \\\n",
       "0  [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "1  [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "2  [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "3  [AI Reviewer_4: Junior faculty in Causality, A...   \n",
       "4  [AI Reviewer_1: Junior faculty in Causality, A...   \n",
       "\n",
       "                                     editor_response  parsed_decision  \n",
       "0  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "1  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "2  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "3  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "4  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Editor decision counts by number of outsider reviewers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>parsed_decision</th>\n",
       "      <th>Accept (Poster)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_outsiders</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "parsed_decision  Accept (Poster)\n",
       "num_outsiders                   \n",
       "0                              3\n",
       "1                              3\n",
       "2                              3\n",
       "3                              3\n",
       "4                              3\n",
       "5                              3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Regenerating AI reviews for the mixed-domain personas …\")\n",
    "generate_ai_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64f6e7",
   "metadata": {},
   "source": [
    "# Different aspects of reviews: are in-domain and outsider reviews focusing on the same aspects while writing their reviews? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a57a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38dae139",
   "metadata": {},
   "source": [
    "# How does the number of outsider reviews affect the editor's decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e664919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTSIDER_PANEL_SIZE = min(5, len(AI_REVIEWS))\n",
    "OUTSIDER_RUNS_PER_SETTING = 3\n",
    "\n",
    "if OUTSIDER_PANEL_SIZE == 0:\n",
    "    raise RuntimeError(\"Generate AI reviews for the mixed-domain panel before running this cell.\")\n",
    "\n",
    "persona_lookup = {persona.name: persona for persona in personas}\n",
    "\n",
    "def _infer_domain_source(persona_obj: Optional[ReviewerPersona]) -> str:\n",
    "    if persona_obj is None:\n",
    "        return \"unknown\"\n",
    "    return \"same\" if persona_obj.expertise == PAPER_DOMAIN else \"random_different\"\n",
    "\n",
    "review_records: List[Dict[str, Any]] = []\n",
    "for idx, review in enumerate(AI_REVIEWS, start=1):\n",
    "    reviewer_name = review.get(\"reviewer\", f\"ai_reviewer_{idx}\")\n",
    "    persona_obj = persona_lookup.get(reviewer_name)\n",
    "    domain_source = _infer_domain_source(persona_obj)\n",
    "    review_text = flatten_review_text_ai(review)\n",
    "    if not review_text:\n",
    "        continue\n",
    "    review_records.append(\n",
    "        {\n",
    "            \"reviewer\": reviewer_name,\n",
    "            \"domain_source\": domain_source,\n",
    "            \"text\": review_text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "if not review_records:\n",
    "    raise RuntimeError(\"AI_REVIEWS is empty after filtering; rerun generate_ai_reviews().\")\n",
    "\n",
    "same_domain_records = [r for r in review_records if r[\"domain_source\"] == \"same\"]\n",
    "outsider_records = [r for r in review_records if r[\"domain_source\"] != \"same\"]\n",
    "\n",
    "if not same_domain_records or not outsider_records:\n",
    "    raise RuntimeError(\"Need both same-domain and outsider reviews to run this experiment.\")\n",
    "\n",
    "if len(same_domain_records) < OUTSIDER_PANEL_SIZE:\n",
    "    raise RuntimeError(\n",
    "        f\"Not enough same-domain reviews ({len(same_domain_records)}) to fill a panel of {OUTSIDER_PANEL_SIZE} reviewers.\"\n",
    "    )\n",
    "\n",
    "max_outsiders_allowed = min(len(outsider_records), OUTSIDER_PANEL_SIZE)\n",
    "outsider_counts = list(range(0, max_outsiders_allowed + 1))\n",
    "\n",
    "OUTSIDER_EXPERIMENT_CONFIG = {\n",
    "    \"panel_size\": OUTSIDER_PANEL_SIZE,\n",
    "    \"runs_per_setting\": OUTSIDER_RUNS_PER_SETTING,\n",
    "    \"outsider_counts\": outsider_counts,\n",
    "}\n",
    "\n",
    "OUTSIDER_EDITOR_EXPERIMENT_RESULTS: List[Dict[str, Any]] = []\n",
    "total_runs = len(outsider_counts) * OUTSIDER_RUNS_PER_SETTING\n",
    "\n",
    "with tqdm(total=total_runs, desc=\"Outsider reviewer simulations\") as pbar:\n",
    "    for num_outsiders in outsider_counts:\n",
    "        num_same_needed = OUTSIDER_PANEL_SIZE - num_outsiders\n",
    "        if num_same_needed > len(same_domain_records):\n",
    "            continue\n",
    "        for run_idx in range(OUTSIDER_RUNS_PER_SETTING):\n",
    "            selected_same = random.sample(same_domain_records, num_same_needed) if num_same_needed else []\n",
    "            selected_outsiders = random.sample(outsider_records, num_outsiders) if num_outsiders else []\n",
    "            selected_records = selected_same + selected_outsiders\n",
    "            random.shuffle(selected_records)\n",
    "\n",
    "            reviews_text = \"\\n\\n---\\n\\n\".join(\n",
    "                [\n",
    "                    f\"{rec['reviewer']} ({rec['domain_source']}):\\n{rec['text']}\"\n",
    "                    for rec in selected_records\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            input_message = (\n",
    "                f\"Paper:\\n{PAPER_TEXT}\\n\\n\"\n",
    "                f\"Please make a decision for this paper based on the following reviews:\\n\\n{reviews_text}\"\n",
    "            )\n",
    "\n",
    "            editor = make_editor_agent()\n",
    "            response = editor.generate_reply(messages=[{\"role\": \"user\", \"content\": input_message}])\n",
    "\n",
    "            OUTSIDER_EDITOR_EXPERIMENT_RESULTS.append(\n",
    "                {\n",
    "                    \"num_outsiders\": num_outsiders,\n",
    "                    \"num_same\": num_same_needed,\n",
    "                    \"run_idx\": run_idx,\n",
    "                    \"selected_reviewers\": [rec[\"reviewer\"] for rec in selected_records],\n",
    "                    \"editor_response\": response,\n",
    "                    \"parsed_decision\": parse_editor_decision(response),\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "outsider_results_df = pd.DataFrame(OUTSIDER_EDITOR_EXPERIMENT_RESULTS)\n",
    "display(OUTSIDER_EXPERIMENT_CONFIG)\n",
    "display(outsider_results_df.head())\n",
    "\n",
    "if not outsider_results_df.empty:\n",
    "    decision_summary = (\n",
    "        outsider_results_df.groupby([\"num_outsiders\", \"parsed_decision\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=outsider_counts, fill_value=0)\n",
    "    )\n",
    "    print(\"Editor decision counts by number of outsider reviewers:\")\n",
    "    display(decision_summary)\n",
    "else:\n",
    "    print(\"No simulations were recorded; verify AI reviews exist and rerun this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b21e0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_outsiders</th>\n",
       "      <th>num_same</th>\n",
       "      <th>run_idx</th>\n",
       "      <th>selected_reviewers</th>\n",
       "      <th>editor_response</th>\n",
       "      <th>parsed_decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_4: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_1: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_9: Junior faculty in Security, AI...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_2: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_5: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_4: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_9: Junior faculty in Security, AI...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_8: Junior faculty in Health, AI R...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_9: Junior faculty in Security, AI...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_3: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_5: Junior faculty in Causality, A...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_7: Junior faculty in Neuroscience...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[AI Reviewer_9: Junior faculty in Security, AI...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[AI Reviewer_6: Junior faculty in Time Series,...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[AI Reviewer_10: Junior faculty in Energy, AI ...</td>\n",
       "      <td>{\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_outsiders  num_same  run_idx  \\\n",
       "0               0         5        0   \n",
       "1               0         5        1   \n",
       "2               0         5        2   \n",
       "3               1         4        0   \n",
       "4               1         4        1   \n",
       "5               1         4        2   \n",
       "6               2         3        0   \n",
       "7               2         3        1   \n",
       "8               2         3        2   \n",
       "9               3         2        0   \n",
       "10              3         2        1   \n",
       "11              3         2        2   \n",
       "12              4         1        0   \n",
       "13              4         1        1   \n",
       "14              4         1        2   \n",
       "15              5         0        0   \n",
       "16              5         0        1   \n",
       "17              5         0        2   \n",
       "\n",
       "                                   selected_reviewers  \\\n",
       "0   [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "1   [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "2   [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "3   [AI Reviewer_4: Junior faculty in Causality, A...   \n",
       "4   [AI Reviewer_1: Junior faculty in Causality, A...   \n",
       "5   [AI Reviewer_9: Junior faculty in Security, AI...   \n",
       "6   [AI Reviewer_2: Junior faculty in Causality, A...   \n",
       "7   [AI Reviewer_5: Junior faculty in Causality, A...   \n",
       "8   [AI Reviewer_4: Junior faculty in Causality, A...   \n",
       "9   [AI Reviewer_9: Junior faculty in Security, AI...   \n",
       "10  [AI Reviewer_8: Junior faculty in Health, AI R...   \n",
       "11  [AI Reviewer_9: Junior faculty in Security, AI...   \n",
       "12  [AI Reviewer_3: Junior faculty in Causality, A...   \n",
       "13  [AI Reviewer_5: Junior faculty in Causality, A...   \n",
       "14  [AI Reviewer_7: Junior faculty in Neuroscience...   \n",
       "15  [AI Reviewer_9: Junior faculty in Security, AI...   \n",
       "16  [AI Reviewer_6: Junior faculty in Time Series,...   \n",
       "17  [AI Reviewer_10: Junior faculty in Energy, AI ...   \n",
       "\n",
       "                                      editor_response  parsed_decision  \n",
       "0   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "1   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "2   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "3   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "4   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "5   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "6   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "7   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "8   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "9   {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "10  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "11  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "12  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "13  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "14  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "15  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "16  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  \n",
       "17  {\\n  \"decision\": \"Accept (Poster)\",\\n  \"confid...  Accept (Poster)  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsider_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56d79b",
   "metadata": {},
   "source": [
    "# Summarization data pack\n",
    "Please share the following data pack with us so that we can aggregate the simulation results from the entire class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
